[{"title": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:   Human Language Technologies (Volume 1: Long Papers), pages 6279\u20136296   June 16-21, 2024 \u00a92024 Association for Computational Linguistics   Grounding Gaps in Language Model Generations   Omar Shaikh\u22c6   Kristina Gligori\u00b4c\u22c6   Ashna Khetan   Matthias Gerstgrasser   Diyi Yang   Dan Jurafsky   Stanford University   {oshaikh, gligoric, ashnak, mgerst, diyiy, jurafsky}@stanford.edu", "abstract": "Effective   conversation   requires   common   ground:   a shared understanding between   the participants.   Common ground, however,   does not emerge spontaneously in   conversation.   Speakers and listeners work   together to both identify and construct a   shared basis while avoiding misunderstanding.   To accomplish grounding, humans rely on   a range of dialogue acts, like clarification   (What do you mean?) and acknowledgment (I   understand.). However, it is unclear whether   large language models (LLMs) generate   text that reflects human grounding. To this   end, we curate a set of grounding acts and   propose corresponding metrics that quantify   attempted grounding. We study whether LLM   generations contain grounding acts, simulating   turn-taking from several dialogue datasets   and comparing results to humans. We find   that\u2014compared to humans\u2014LLMs generate   language with less conversational grounding,   instead generating text that appears to simply   presume common ground.   To understand   the roots of the identified grounding gap,   we examine the role of instruction tuning   and preference optimization, finding that   training on contemporary preference data   leads to a reduction in generated grounding   acts. Altogether, we highlight the need for   more research investigating conversational   grounding in human-AI interaction.   1", "content": "In dialogue, common ground refers to the mutual knowledge, beliefs, and assumptions shared by   participants in a conversation. This shared understanding is essential for effective communication,   as it underpins the ability of individuals to interpret, predict, and respond to each other\u2019s statements   and actions accurately (Clark, 1996). Through each   \u22c6Equal contribution.   Figure 1: Mental health supporters carefully employ   clarification questions (one of the three grounding acts)   with a seeker, taking multiple turns to ground. In contrast, simulated supporters\u2014with the same conversational context\u2014generate presumptive answers.   conversational turn, individuals collaborate to build   common ground, preventing potential misunderstandings (Clark and Schaefer, 1989). Humans   therefore rely on dialogue acts like clarifying meaning or posing information-seeking followup questions. When individuals fail to ground, they often   proactively repair misunderstandings.   Failing to construct common ground in humanhuman conversation can be at best misleading and   at worst harmful. Consider mental-health support:   if there\u2019s any indication of risk to a client (e.g., suicidal ideation or intentions of self-harm), a healthcare professional will ask clarifying questions to   assess risk. Failure to do so has harmful consequences (unnecessary hospitalization) which can   traumatize a client and place an unjustified financial burden (Strumwasser et al., 1991). In domains   like education, ineffective grounding might result   in misunderstanding a student, resulting in irrelevant feedback (Graesser et al., 1995).   From an NLP perspective, establishing and leveraging common ground is a complex challenge: dialogue agents must recognize and utilize implicit aspects of human communication. Recent chat-based   6279   large language models (LLMs), however, are designed explicitly for following instructions. While   humans carefully construct common ground, LLMs   are trained to directly act on commands specified   by end-users (Ouyang et al., 2022). We hypothesize that the instruction-following paradigm\u2014a   combination of supervised fine-tuning (SFT) and   preference optimization (PO through RLHF, for   example)\u2014might result in discrepancies between   how humans actually ground in dialogue, and how   LLMs generate text similar to human grounding.   We call this discrepancy the grounding gap.   Despite the grounding gap, LLMs interact regularly with humans across various applications. For   a subset of interactions, however, LLMs should   generate grounding language before completing a   user\u2019s task, instead of executing literal instructions   or disregarding a user\u2019s underlying goals. This   is particularly crucial in LLM-powered training   systems, where LLMs simulate practice scenarios and allow individuals to rehearse and refine   domain-specific skills (Shaikh et al., 2023a). LLMbased training already facilitates interaction in domains like education (Kasneci et al., 2023; Demszky et al., 2021; Wang and Demszky, 2023), conflict resolution (Shaikh et al., 2023a; Argyle et al.,   2023), and emotional support (Carlbring et al.,   2023; Hsu et al., 2023). In these settings, effective   dialog agents must coordinate to build common   ground when interacting with people.   Given the importance of generating language   for conversational grounding, we ask: Do current   LLMs generate dialogue acts that reflect grounding   patterns between humans? If not, what aspect of   LLM training exacerbates the grounding gap?   We address these questions by measuring   LLM generations through linguistically validated   grounding acts. For example, acts that clarify or   acknowledge a prior utterance offer a strong signal for measuring shared understanding (Clark and   Schaefer, 1989). Building on prior work in dialogue and conversational analysis, we curate a collection of dialogue acts used to construct common   ground (\u00a72). Then, we select datasets & domains   to study human-LM grounding. We focus on settings where human-human grounding is critical,   and where LLMs have been applied: namely, emotional support, persuasion, and teaching (\u00a73).   After curating a set of grounding acts, we   build prompted few-shot classifiers to detect them   (\u00a74). We then use LLMs to simulate turn-taking   in our human-human dialogue datasets and compare agreement between human and GPT-generated   grounding strategies (\u00a75).   Because we use the exact same conversational   context as in human conversations, we can quantify the grounding gap: off-the-shelf LLM generations are, on average, 77.5% less likely to contain   grounding acts than humans (\u00a76). Even in situations where LLM generations do contain a grounding act, they differ from human generations\u2014we   observe poor human-LM agreement across a range   of models.   To isolate potential causes of the grounding gap,   we explore a range of possible interventions, from   ablating training iterations on instruction following   data (SFT and PO) to designing a simple prompting   mitigation (\u00a77). We find that SFT does not improve   conversational grounding, and PO erodes it. Across   our experiments, we generally observe significant   disagreement between grounding acts in human   utterances and LLM generations.   In summary, we contribute (1) a set of linguistically informed grounding acts, tailored towards   understanding grounding in contemporary LLMs.   Using grounding acts, we (2) conduct a controlled   LLM simulation of conversational grounding and   a characterization of grounding agreement: a comparison between grounding acts in human utterances and LLM generations. Finally, we (3) investigate reasons for the grounding gap, exploring   different models, ablating SFT and PO, and testing   prompting interventions.1   2   Grounding Acts   Grounding acts are a set of dialogue acts used   to characterize conversational grounding. While   grounding failures are qualitatively well-observed   in current pretrained models (Benotti and Blackburn, 2021b; Fried et al., 2022; Chandu et al.,   2021), measuring where or how frequently these   failures occur in dialogue remains unexplored.   Here, we outline concrete acts to measure these   failures, discuss our curation process for selecting   grounding acts, and describe each act in detail.   Grounding as an Expert   To benchmark conversational grounding in LLMs, we focus on dialogue acts from the perspective of an expert listener.   Users frequently come to LLMs with a direct but   potentially underspecified task (Tamkin et al., 2022;   1Code and prompts for evaluation can be found at https:   //github.com/oshaikh13/grounding-gaps   6280   Grounding Act   Example   Clarification   student: So I am the leader of my class. It is like the president or mayor.   TSCC   teacher: Oh, like a head girl?   ,\u2192GPT: That\u2019s great to hear! Being the leader of your class is a big responsibility [...]   Follow-up   seeker: My friends, who I also work with, are upset with me because I am leaving for a   ESConv   new position. I am sad that they are upset.   support: Ah, that can be awkward. Are you happy about the new position?   ,\u2192GPT: I can understand why that would make you feel down... Remember, it\u2019s okay to [...].   Acknowledgment   persuadee: I [donate] as much as I can, which is admittedly not as much as I would like to.   Persuasion   persuader: I know what you mean! Sometimes it is hard to find the extra time or money to help   those that need it.   ,\u2192GPT: That\u2019s wonderful to hear! [...] Would you be interested in supporting Save the [...]?   Table 1: Examples of grounding acts from our datasets, where the act is employed by the expert from each   dataset (teacher, supporter, or persuader). Additionally, we include Chat GPT-3.5, where the model does not   employ the same grounding act.   Zheng et al., 2023). In these settings, the user has   privileged knowledge of their task; therefore, the   onus of building common ground before providing   a response lies initially with a model. More concretely, the perceived expert LLM should generate   text that verifies grounding before completing a   task. We therefore curate grounding acts from the   perspective of an expert listener.   Conversational Grounding Ontologies   In curating grounding acts, we draw on prior dialogue   research. Several ontologies propose discrete dialogue acts for measuring conversational grounding. For example, Clark and Schaefer (1989) propose a hierarchy of methods to achieve common   ground, including the use of explicit discourse acts.   They discuss relationships between acts like acknowledgment (e.g. I understand), and evidence   of grounding in conversation. Traum and Hinkelman (1992) outline a range of concrete grounding   acts, from simply continuing a conversation to repairing a misunderstanding or requesting repair.   Purver (2004) further contributes a theory of repair   through clarification requests\u2014dialogue acts used   to verify if contributions should be added to the   common ground.   We curate a small but general subset from the   large pool of proposed acts, selecting acts that   are relevant to interaction with LLMs, or their   current applications (e.g. teaching or emotional   support). For example, Motivational Interviewing   for emotional support emphasizes asking followup   questions and signaling acknowledgment for empathy (Miller and Rollnick, 2012). Similarly, a range   of pedagogical theories incentivize asking careful   clarification and followup questions (Wiske, 1998).   And effective conflict resolution requires a careful   construction of common ground (Deutsch, 1973).   We therefore select the following three grounding   acts that are especially relevant to these current   applications of LLMs but also generalize across a   range of domains where grounding is critical.   Clarification   requests occur when a speaker   seeks clarification on an utterance instead of initiating repair. Clarification is used primarily to avoid   a misunderstanding, and concerns information presented in prior utterances u1..i. In other words, clarifications serve to \u201cclear up\u201d a potential future misunderstanding (e.g. did you mean...? or are you referring to?), avoiding repair from a listener (Purver,   2004; Ginzburg and Cooper, 2001; Purver et al.,   2003b,a; Healey et al., 2011, 2003; Madureira and   Schlangen, 2023a; Kuhn et al., 2022; Stoyanchev   et al., 2013; Rahmani et al., 2023).   Acknowledgement   explicitly signals understanding (e.g. \"ok,\" \"I understand,\" \"I see,\" \"Yes, and\",   etc.). Unlike clarification/repair, acknowledgment   indicates that a speaker is ready for the next relevant turn in a conversation. We only consider utterances whose sole purpose is acknowledgment (i.e.   they exclusively contain ack. markers.) (Schegloff,   1982; Sacks et al., 1978; Schiffrin, 1987; Clark and   Schaefer, 1989; Cho and May, 2020)   Followup   questions ask for elaboration on a   prior utterance u. Followups implicitly signal understanding of u. Unlike clarifications\u2014which   are concerned wholly with misunderstandings\u2014   followups signal understanding by seeking addi6281   tional information on a prior utterance u. Concretely, follow-ups indicate understanding by attempting to ground on related information. While   clarification is about understanding the existing   interaction more thoroughly, a follow-up is concerned with continuing the current interaction2   (Davis, 1982; Graesser et al., 1995; Traum and   Hinkelman, 1992; Bunt et al., 2017).   3   Data   We choose to analyze conversations in three domains where grounding is critical and where   LLMs are already used for social skill training\u2014   education, emotional support, and persuasion. Our   datasets are task-oriented in nature, have multiple   turns, and consist of two participants. Since our   grounding acts are curated around an expert listener, each dataset also has one expert participant.   To identify grounding gaps, we can simulate utterances from the expert using an LLM. We briefly   outline our datasets (in English).   For emotional support, we use Emotional Support Conversations (ESConv), a corpus of one-toone online conversations between a help-seeker and   a help-provider, collected via crowdsourcing (Liu   et al., 2021). To analyze grounding acts in education, we use the Teacher Student Chatroom Corpora (TSCC), a collection of written conversations   captured during one-to-one lessons between teachers and learners of English (Caines et al., 2020,   2022). Finally, for persuasion, we use Persuasion for Good, a dataset consisting of one-to-one   online conversations between a persuader and a persuadee, where the persuader solicits donations from   payouts on a crowd working website (Wang et al.,   2019). Due to resource constraints, we sample 100   conversations from each dataset and truncate them   to the median length (TSCC = 92, ESConv = 22   Persuasion = 20). Details on our selected datasets,   sampling, and truncation are in Appendix C.   4   Classifying Grounding Acts   To analyze disparities in grounding acts between   human and LLM generations, we must first classify   grounding acts in our datasets. In this section, we   outline the construction of test/validation sets, and   our prompting setup to classify grounding acts.   2One nifty way to distinguish follow-ups and clarification   questions is the \"O.K.\" test, introduced by Benotti and Blackburn (2021a). In short, if prefixing a potential clarification (e.g.   \"Do you mean X?\") with an acknowledgment (e.g. \"O.K.\")   sounds awkward, then it\u2019s likely a clarification.   4.1   Dataset Splits and Prompting   Method   We turn to classifying the grounding   acts in our datasets. First, we withheld and annotated a validation (10%) and test (10%) set of   conversations from each dataset in \u00a73. Messages   were annotated for a single primary act\u2014if an   utterance contained multiple grounding acts, we   selected the most relevant one. Two authors participated in the annotation process. After annotating,   the authors highlighted disagreements, discussed,   and broke ties, yielding a final Cohen Kappa agreement of \u03ba = 0.72. Following annotation, the first   author prompt-engineered a multi-class classification prompt on the validation set, using both zeroshot and few-shot prompting. We used the latest   GPT model during the time of writing (GPT-4) as   our classification model, with temperature = 0 but   default parameters otherwise.3 Queries were run   between July-November 2023.   Results   GPT-4 can identify grounding acts in   conversations with reasonably high accuracy (avg.   Macro F-1 across datasets = 0.89, Appdx. Table   2). In the 0-shot setting, however, we find that   GPT-4 frequently misclassifies follow-ups and clarification questions (avg. F-1 clarification = 0.40;   follow-up = 0.70). Few-shot prompting substantially increases model performance (clarification   F-1 = 0.85; follow-up F-1 = 0.91).   5   Analyzing Grounding Acts in LLMs   Given our three grounding acts, a reasonably performing GPT-4 classifier for labeling them, and a   set of target datasets & metrics, we can now measure disparities between humans and LM simulations. In this section, we outline our controlled   simulation process (\u00a75.1) and metrics to measure   conversational grounding (\u00a75.2).   5.1   Simulation Method   We start with a conversation C1..N consisting of N   ordered role utterance pairs (rt, ut). Each of our   selected datasets has two unique roles, expert and   listener. Since LLMs are generally leveraged for   the role of an expert listener, we focus on simulating the expert. Given this setup, our simulation   process generates controlled counterfactual messages gt for each ut.   Concretely, to simulate an utterance at timestep   t, we extract all messages until t:   C1...t\u22121.   3We use standard parameters provided in OpenAI\u2019s API   (max_tokens = 256).   6282   Then, we input this context into a selected LM   (LM(C1...t\u22121)), along with a high-level instruction   (e.g. Roleplay a therapist). The LM then generates the next message, offering a counterfactual to   the human ground truth. Using this process, each   ground truth utterance ut from our selected datasets   has a controlled, LM-generated counterpart gt, conditioned on the same conversational history. Figure   1 summarizes this process.   After generating LM counterfactuals gt, we can   similarly compute the rate of grounding acts across   our generated messages. While we use GPT-4 to   classify grounding acts, simulating conversation   for all our datasets is prohibitively expensive. In   this section, we use GPT-3.5 with default parameters for conversation simulation (later, in \u00a77, we   experiment with a wider range of models).   5.2   Metrics to Measure Grounding Acts   How should we measure the use of our selected   grounding acts? We propose two metrics: base rate   and Cohen\u2019s \u03ba. The base rate provides a measure   of grounding frequency; specifically, how often   a human utterance or LM generation contains a   specific grounding act. In contrast, Cohen\u2019s \u03ba measures agreement in grounding acts between humans   and LMs. Just because an LM generation contains   an act does not mean it occurs in the same place   as in human dialogue. Our metrics apply to any   dialog dataset D consisting of conversations with   utterances-role pairs {(r1, u1), ..., (rn, un)}.   Base Rate   We first compute the overall frequency   of grounding acts subset G as P(ui \u2208G). The   base rate provides a reference point for the overall   presence of specific conversational grounding acts.   Cohen Kappa \u03ba   While the base rate provides   a measure of frequency, it does not capture the   discrepancy between grounding acts used between   a candidate and reference conversation. For instance, an LM-generated conversation M may use   more grounding strategies, but only in locations   that rarely match human H use. Consider a simulated dialogue agent that always generates a grounding act. While the language this agent generates   doesn\u2019t have the problem of presuming common   ground, an agent that always generates grounding acts can be understandably irritating (Horvitz,   1999).   To this end, we use Cohen \u03ba (Cohen, 1960), a   measure of inter-rater agreement bounded between   -1 and 1. Cohen \u03ba has several useful properties: it   Act   ChatGPT 3.5   Human   Cohen \u03ba   Emotional Support Conv   Follow   10.78 \u00b1 2.1   27.87 \u00b1 4.4   12.47 \u00b1 6.4   Ack.   1.05 \u00b1 0.8   12.9 \u00b1 3.7   3.14 \u00b1 4.9   Clar.   0.0 \u00b1 0.0   3.05 \u00b1 1.2   0.0 \u00b1 0.0   Teacher Student Chatroom   Follow   11.56 \u00b1 1.9   12.04 \u00b1 2.1   16.75 \u00b1 4.6   Ack.   5.68 \u00b1 1.4   16.59 \u00b1 2.4   18.25 \u00b1 5.4   Clar.   0.57 \u00b1 0.3   3.77 \u00b1 0.9   0.36 \u00b1 2.5   Persuasion for Good   Follow   1.66 \u00b1 0.9   8.18 \u00b1 2.4   2.94 \u00b1 7.6   Ack.   1.8 \u00b1 1.0   6.11 \u00b1 1.9   25.73 \u00b1 16.7   Clar.   0.0 \u00b1 0.0   0.28 \u00b1 0.4   0.0 \u00b1 0.0   Table 2: Grounding acts and associated metrics   across our datasets. \u00b1 represents 95% confidence   intervals (bootstrapped). Humans use grounding acts   more than LLMs; furthermore, LLMs show poor agreement (Cohen \u03ba) with humans.   is well-validated across social scientific studies\u2014   agreement can be compared to pre-existing work   to determine strength. Furthermore, \u03ba adjusts for   random chance: values of \u03ba < 0 indicate that agreement is worse than chance. In measuring grounding   acts, we treat M and H as individual raters.   6   Gaps in Generating Grounding Acts   Having introduced the controlled simulation process,   we now report metrics on grounding   acts (\u00a76.1) and qualitatively analyze errors (\u00a76.2).   6.1   Simulation Results   First, we find significant discrepancies between   humans and LLMs when using GPT-3.5 for conversation simulation (Table 2). Across all datasets,   LLMs generations contain fewer grounding acts,   like followups (avg.   64.3% decrease) and ack.   (83.4%) acts. Clarifications never occur when   using ChatGPT-3.5 for ESConv and Persuasion.   While ChatGPT-3.5 does initiate some clarification on TSCC, we observe a 84.8% decrease.   Beyond rate discrepancies, we observe low   agreement between ChatGPT-3.5 and humans\u2014   LLM generations rarely contain grounding acts   in same position as human utterances. Of the 3   grounding acts \u00d7 3 dataset pairs, only 3 / 9 have a   Cohen \u03ba agreement significantly greater than zero,   with \u03ba averaging 10.73 for followup, 11.13 for   acknowledgment, 0.23 for clarification. To con6283   Generator   ACT   + Followup   + Acknowledgement   + Clarification   Base Rate   Cohen \u03ba   Base Rate   Cohen \u03ba   Base Rate   Cohen \u03ba   Human   27.87 \u00b1 4.4   36.41   12.89 \u00b1 3.7   30.12   3.05 \u00b1 1.2   48.456   3.5-instruct-turbo   29.35 \u00b1 3.3   22.16 \u00b1 7.7   1.49 \u00b1 1.1   5.5 \u00b1 6.0   0.12 \u00b1 0.2   -0.22 \u00b1 0.4   3.5-turbo (ChatGPT 3.5)   10.76 \u00b1 2.1   12.47 \u00b1 6.4   1.05 \u00b1 0.8   3.14 \u00b1 4.9   0.0 \u00b1 0.0   0.0 \u00b1 0.0   4   12.26 \u00b1 2.6   11.04 \u00b1 7.4   1.17 \u00b1 0.8   11.18 \u00b1 7.7   0.35 \u00b1 0.4   -0.61 \u00b1 0.6   mistral-sft   20.03 \u00b1 3.1   18.48 \u00b1 7.7   12.08 \u00b1 3.4   26.92 \u00b1 9.6   0.47 \u00b1 0.5   12.39 \u00b1 16.1   mistral-dpo   15.99 \u00b1 2.7   15.25 \u00b1 8.0   19.11 \u00b1 4.1   19.33 \u00b1 7.7   0.93 \u00b1 0.8   9.79 \u00b1 12.7   3.5-turbo + mitigation   42.32 \u00b1 4.7   26.41 \u00b1 6.6   2.54 \u00b1 1.3   5.02 \u00b1 5.9   1.16 \u00b1 0.8   -1.65 \u00b1 0.9   Table 3: ESConv grounding acts and metrics across model variants. We find that more recent OpenAI   (3.5-turbo, 4) models use significantly fewer grounding acts than humans; and that agreement (Cohen \u03ba) with   humans is poor to fair across all evaluated models.   firm that human-LM agreement is low, we run a   human-human study with ESConv (Appendix B),   and observe significantly higher \u03ba across grounding cts (avg. \u03ba \u224837.5).   6.2   Error Analysis   We performed qualitative analyses to develop an   in-depth understanding of how LM generations fail   to incorporate grounding acts. First, we performed   inductive coding to produce a set of frequent errors.   An annotator (one of the authors) read a random   sample of 160 instances where a human uses a   grounding act, while the simulated supporter does   not. Relevant turns were examined together with   the previous turn for context (details in Appdx. D).   Simulated supporters fail to use (1) acknowledgment to show empathy (43.94%) or ask (2) followup questions for more information (26.52%) /   to continue a conversation (12.88%). Furthermore,   simulated supporters do not ask (3) clarification   questions to verify understanding (9.09%), or resolve a specific ambiguity (7.58%). Table 4 in   the Appendix contains more detail on each error   type and qualitative examples. Next, we investigate potential causes of the grounding gap and   use discovered error types to design an informed   prompting mitigation.   7   Why do Grounding Gaps Emerge?   Here, we explore potential mechanisms for how   grounding gaps emerge. We focus on analyzing   ESConv, a target domain where disagreement in   grounding acts is especially consequential.   First, we examine grounding acts across several   OpenAI GPT variants and observe a larger grounding gap in newer models (\u00a77.1). To understand the   roots of this trend, we evaluate open-source models.   We hypothesize that current supervised-finetuning   (SFT) and preference optimization (PO) datasets   drive human-LM disagreement. To test this, we rigorously isolate the effects of SFT and PO training   on grounding agreement (\u00a77.2).   7.1   LLM Variants   Method   To further investigate the grounding   gap,   we test a wider range of GPT models, rerunning our simulation process for ESConv on gpt-3.5-instruct (a replacement for   legacy OpenAI models, trained similarly to the   text-davinci-00X series) and gpt-4. We examine grounding acts across these models.   Results   Generations from the OpenAI model   variants have lower grounding acts base rates and   poor agreement with humans (\u03ba < 0.2). A surprising exception to this is 3.5-instruct-turbo for   followups, where instruct shows fair agreement   with humans and uses grounding acts at a similar rate (29.5 instruct vs. 27.9 Human). While   instruct is trained using an \u201colder\u201d procedure, it   produces a better \u03ba / base-rate tradeoff compared   to most models.   7.2   SFT & Preference Optimization   OpenAI models, however, are closed source: to   isolate training procedures that impact grounding   agreement, we independently evaluate the role of   current SFT and PO datasets.   Method   We investigate if the standard SFT + PO   training setup improves use of grounding acts; and   if the amount of SFT + PO matters. At a high level,   we replicate the training procedure for Zephyr, an   open-source instruction following LM (Tunstall   6284   200   400   600   800   1000   SFT Train Steps   5   10   15   20   25   30   35   40   Cohen Kappa   (a)   0   500   1000   1500   2000   2500   DPO Steps   5   10   15   20   25   30   35   40   Cohen Kappa   Acknowledgement   Followup   Clarification   (b)   Figure 2: The role of SFT and preference optimization in grounding agreement. (a) We observe no correlation between SFT training steps and Cohen \u03ba agreement on grounding acts, with a Pearson R correlation   test yielding insignificant results: p > 0.1 (b) We observe negative correlation between DPO train steps   and Cohen \u03ba agreement on grounding acts, with Pearson   R averaging R = \u22120.79, and p < 0.05 for all acts.   et al., 2023). First, we SFT Mistral 7B (Jiang et al.,   2023) for three epochs on a filtered version of the   UltraChat dataset (Ding et al., 2023; Tunstall et al.,   2023).4 During the SFT process, we save a total of   10 evenly-spaced checkpoints across a training run.   Each checkpoint is used to re-simulate conversations and measure the use of grounding acts. Next,   we use p", "conf": "NAACL", "year": "2024", "index": 198}, {"title": "LLaVA-Grounding: Grounded Visual Chat with   Large Multimodal Models   Hao Zhang1\u2217, Hongyang Li2\u2217, Feng Li1, Tianhe Ren4, Xueyan Zou5   , Shilong Liu6, Shijia Huang7, Jianfeng Gao3\u2021, Leizhang4\u2021   , Chunyuan Li3\u2020, and Jainwei Yang3\u2020   *Equal Contribution, \u2020 Directional Lead, \u2021 Equal Advisory   1 The Hong Kong University of Science and Technology   2 South China University of Technology   3 Microsoft Research, Redmond   4 International Digital Economy Academy (IDEA)   5 University of Wisconsin\u2013Madison   6 Tsinghua University   7 The Chinese University of Hong Kong", "abstract": ". With the recent significant advancements in large multimodal   models (LMMs), the importance of their grounding capability in visual   chat is increasingly recognized. Despite recent efforts to enable LMMs to   support grounding, their capabilities for grounding and chat are usually   separate, and their chat performance drops dramatically when asked   to ground. The problem is the lack of a dataset for grounded visual   chat (GVC). Existing grounding datasets only contain short captions.   To address this issue, we have created GVC data that allows for the   combination of grounding and chat capabilities. To better evaluate the   GVC capabilities, we have introduced a benchmark called GroundingBench. Additionally, we have proposed a model design that can support   GVC and various types of visual prompts by connecting segmentation   models with language models. Experimental results demonstrate that our   model outperforms other LMMs on Grounding-Bench. Furthermore, our   model achieves competitive performance on classic grounding benchmarks   like RefCOCO/+/g and Flickr30K Entities.   1", "content": "With the success of large language models (LLMs) like GPT-4 [19] and the opensourced substitutes LLaMA [24], researchers are eager to leverage their strong   language capabilities in the field of vision. This enthusiasm has led to a surge   in the development of large multimodal models (LLMs). Previous LMMs, such   as LLaVA [14] and miniGPT-4 [36], have demonstrated exceptional visual chat   abilities by generating plausible responses based on images and user instructions.   However, they often encounter challenges in providing responses that exhibit a   fine-grained understanding of images, including specific regions and alignment   with related image regions\u2014this is often referred to as visual grounding.   2   H. Zhang, H .Li, et al.   Recognizing the significance of visual grounding for LMMs, recent research   efforts have focused on developing grounding and referring capabilities for   LMMs [2, 3, 7, 25, 30]. While these models have achieved performance comparable to specialized models [15, 17] on classic grounding benchmarks such as   RefCOCO [5] and Flickr30K [23], they often treat grounding as a distinct task   that requires customized prompts to initiate. Consequently, their text responses   undergo significant changes when tasked with grounding. Most models, such as   MiniGPT-v2 [2] and CogVLM-Grounding [25], can only generate short captions   when performing grounding, as they are primarily trained on grounding caption   data like Flickr30K. As illustrated in Fig.1(a), these earlier models struggle to   excel simultaneously in both chat and grounding tasks. BuboGPT [35] maintains   chat capability by leveraging an external grounding model for grounding, but   this approach can be constrained by the performance of the language encoder   in the grounding model. Shikra [3] engages in referential dialog, which includes   grounded chat, but its performance is limited due to the scarcity of available data.   All existing LMMs [2,3,25,30] only support outputting coordinates as text, which   restricts localization performance, and they do not support pixel-wise grounding   and referring. In summary, previous LMMs struggle to perform grounded visual   chat effectively due to the scarcity of grounded visual chat data and suboptimal   model designs. Furthermore, they lack the capability for pixel-wise grounding   and referring.   To address these challenges, we contribute to grounded visual chat in three key   areas: data creation, network architecture, and benchmarking. When annotating   grounding data, previous methods such as Kosmos-2 [22] and GPT4ROI [34] rely   on pretrained grounding models or detection models to predict bounding boxes   based on existing captions. In contrast, we label grounded visual chat data using   human-labeled object detection data [11].   Our data creation process begins by leveraging GPT-4 [19], following the   data creation method used in LLaVA [14]. We provide GPT-4 with chat data   and ground-truth instances, instructing it to match instances with noun phrases   in the chat data. This approach benefits from the high quality of human-labeled   instances and chat data generated by GPT-4, ensuring minimal noise in the data   annotation pipeline. In total, we annotated 150K grounded visual chat data.   In terms of network architecture, we propose connecting the output features   of the Language Model (LLM) with a grounding model to handle grounding tasks,   relieving the language model from the burden of vision localization tasks. For this   purpose, we use the open-set segmentation and detection model OpenSeeD [32] as   the grounding model, enabling both box and pixel-level grounding simultaneously.   To evaluate the capability of grounded visual chat, we introduce the Grounding   Bench, a benchmark that assesses grounding and chat performances concurrently.   Built upon the foundation of LLaVA bench, our benchmark evaluates chat   and phrase grounding in three contexts: conversation, detailed description, and   complex reasoning. Additionally, recognizing that grounded detailed description   is the most challenging aspect of grounded visual chat, we propose grounded   recall and precision metrics. Grounded recall measures the proportion of groundLLaVA-Grounding   3   truth instances correctly mentioned and grounded, while grounded precision   measures the accuracy of groundings or predicted boxes. We also calculate the   F1 score, a combination of precision and recall. To evaluate the correctness of   semantic matching since the models generate free-form phrases, we rely on GPT-4.   In summary, our contributions are as follows:   1. We introduce a data annotation pipeline to label high-quality Grounded Visual   Chat (GVC) data. Leveraging human-labeled object detection data [11] and   harnessing the robust matching capability of GPT-4 [20], we have successfully   labeled 150K GVC instances using the LLaVA instruction tuning dataset.   2. We present an end-to-end model, named LLaVA-Grounding (LLaVA-G for   brevity), which connects a Large Multimodal Model (LMM) with a grounding   model to facilitate grounded visual chat. Our model supports both object   and pixel-level grounding, accommodating various visual prompts such as   mark, click, box, and scribble. Table 1 demonstrates that our model offers a   broader range of input and output prompt types compared to other LMMs.   3. We establish the Grounding-Benchbenchmark for evaluating grounded visual   chat and propose an auto-evaluation pipeline aided by GPT-4. This benchmark assesses grounded visual chat capabilities and provides performance   metrics for other state-of-the-art methods.   4. Through extensive experiments, we demonstrate that our model surpasses   other grounding LMMs in terms of performance on Grounding-Bench, while   also achieving competitive results on classic grounding benchmarks like   RefCOCO/+/g and Flickr30K.   2   Method   2.1   Overview   To advance the development of grounded visual chat for Large Multimodal Models   (LMMs), we introduce a comprehensive pipeline for labeling grounded visual chat   data, a tailored modeling approach designed for the grounded visual chat task,   and a benchmark for evaluating grounded visual chat performance, as illustrated   in Figure 1(b). We will provide further details on these three components in the   following subsections.   2.2   Grounded Visual Chat Data Creation   To perform grounded visual chat (GVC) effectively, it is crucial to have highquality data that encompasses both meaningful conversations and accurate   grounding. We have constructed our dataset based on LLaVA instruction tuning   data for two primary reasons. Firstly, the conversations within this dataset are   generated by GPT-4, known for its high linguistic quality. Secondly, the images   used are sourced from COCO, which contains human-annotated grounding box   instances.   4   H. Zhang, H .Li, et al.   Data    Creation    Pipeline   Training    Data   Metrics &   Benchmark   E2E-LMM   Modeling   Test    Data   Grounding   Visual Chat   22   24   26   28   30   32   34   36   38   30   40   50   60   70   LLaVA-G   (Ours)   CogVLM-Grounding   miniGPT v2   Shikra   BuboGPT   CogVLM-Chat   80   0   LLaVA   0   (a)   (b)   Fig. 1: (a) A comparison on the integrated ability of visual grounding and visual chat   of open-source LMMs on Grounding-Bench. LLaVA-G achieves a good trade-off on   both abilities simultaneously. For CogVLM [25], two different model checkpoints are   released: CogVLM-Grounding is the grounding model and CogVLM-Chat is the chat   model. Grounding and Visual Chat scores represent the F1 score and Chat scores of   detailed descriptions in Table 3, respectively. Circle size indicates the model size. (b)   An overview of our main contributions. We use the data creation pipeline to create   training and test data. The training data is used to train our LLaVA-G. The test data   is used to build our Grounding-Bench.   Our data annotation process aims to associate phrases from conversations   with specific instances. To achieve this, we leverage the capabilities of GPT-4.   As illustrated in Table 2, we provide GPT-4 with ground-truth (GT) boxes   containing class labels and a sentence from the conversation. We task GPT-4   with matching noun phrases from the sentence to the GT instances. Once noun   phrases are successfully grounded by GPT-4, we mark them with special start   tokens, \u27e8gs\u27e9and \u27e8ge\u27e9, followed by a token, \u27e8seg\u27e9, which corresponds to the output   feature used by the grounding model to segment the grounded region. An example   of a question and its answer in the dataset is as follows:   Q: What is the man doing? A: \u27e8gs\u27e9The man \u27e8ge\u27e9\u27e8seg\u27e9is using \u27e8gs\u27e9a clothing   iron \u27e8ge\u27e9\u27e8seg\u27e9on the back of \u27e8gs\u27e9a yellow taxi \u27e8ge\u27e9\u27e8seg\u27e9.   For each \u27e8seg\u27e9, we have a corresponding segmentation mask. This annotated   data forms the basis of our Grounded Visual Chat (GVC) dataset. To support   visual prompts in user instructions, we apply a similar annotation process to   instances in the question itself. The resulting data appears as follows:   Q: What is the object \u27e8obj\u27e9doing? A: \u27e8gs\u27e9The man \u27e8ge\u27e9\u27e8seg\u27e9is using \u27e8gs\u27e9   a clothing iron \u27e8ge\u27e9\u27e8seg\u27e9on the back of \u27e8gs\u27e9a yellow taxi \u27e8ge\u27e9\u27e8seg\u27e9.   It\u2019s important to note that we modify \"the man\" to \"the object\" in cases   where the model might disregard the visual prompts. For each \u27e8obj\u27e9in the   question, we provide a corresponding segmentation mask. This dataset is referred   to as GVC-R (Grounded Visual Chat with Referring). To examine the quality,   we randomly sampled 100 images to do a human study. It turned out the overall   grounding precision of GPT-4 is up to 95.4%, with some occasional failures   when multiple people are grounded separately.   LLaVA-Grounding   5   Context type 1: Boxes (for data annotation)   1.person: [0.681, 0.242, 0.774, 0.694], 2.person: [0.63, 0.222, 0.686, 0.516],   3.person: [0.444, 0.233, 0.487, 0.34], 4.backpack: [0.384, 0.696, 0.485, 0.914],   5.backpack: [0.755, 0.413, 0.846, 0.692], 6.suitcase: [0.758, 0.413, 0.845, 0.69],   7.suitcase: [0.1, 0.497, 0.173, 0.579], 8.bicycle: [0.282, 0.363, 0.327, 0.442],   9.car: [0.786, 0.25, 0.848, 0.322], 10.car: [0.783, 0.27, 0.827, 0.335],   11.car: [0.86, 0.254, 0.891, 0.3], 12.car: [0.261, 0.101, 0.787, 0.626]   Context type 2: user responses (for data annotation)   The image is an underground parking area with a black sport utility vehicle (SUV)   parked. There are three people in the scene, with one person standing closer to the left   side of the vehicle, another person in the middle, and the third person on the right side.   They are all working together to pack their luggage into the SUV for a trip.   Response: grounded responses (for data annotation)   The image is an underground parking area with a (black sport utility vehicle) [10.car] (SUV) parked. There are (three people) [1.person,   2.person, 3.person] in the scene, with (one person) [3.person] standing closer to the left side of the vehicle, (another person) [2.person] in the   middle, and (the third person) [1.person] on the right side. They are all working together to pack (their luggage) [4.backpack, 5.backpack,   6.suitcase, 7.suitcase] into the SUV for a trip.   Context type 3: predicted grounded responses (for evaluation)   The depiction is of a below-ground parking facility, where a sleek, black vehicle [9.car] is situated. In the vicinity of this SUV, a trio of   individuals [1.person, 2.person, 3.person] is engaged in an activity: the first person [1.person] is adjacent to the left side of the vehicle, the   second [2.person] is situated centrally, and the third [3.person] is near the right side. They are collaboratively arranging their travel bags in   the SUV, signaling the onset of an impending journey.   Response: TPpred and TPgt (for evaluation)   \"a sleek, black vehicle\" [9.car] - Incorrectly referred.   \"a trio of individuals\" [1.person, 2.person, 3.person] - 3 Correctly referred.   \"the first person\" [1.person] - Incorrectly referred.   \"the second\" [2.person] - Correctly referred.   \"the third\" [3.person] - Incorrectly referred.   There are 4 correct", "conf": "ECCV", "year": "2024", "index": 211}, {"title": "Comprehensive Visual Grounding for Video Description   Wenhui Jiang1, Yibo Cheng1, Linxin Liu1, Yuming Fang1*, Yuxin Peng2, Yang Liu3   1Jiangxi University of Finance and Economics, Nanchang, China   2Peking University, Beijing, China   3Sany Heavy Industry CO., LTD,China   jiang1st@bupt.cn, cyb592891032@gmail.com, linxinliu2010@gmail.com, fa0001ng@e.ntu.edu.sg, pengyuxin@pku.edu.cn,   luy2655@sany.com.cn", "abstract": "The grounding accuracy of existing video captioners is still   behind the expectation. The majority of existing methods   perform grounded video captioning on sparse entity annotations, whereas the captioning accuracy often suffers from   degenerated object appearances on the annotated area such   as motion blur and video defocus. Moreover, these methods seldom consider the complex interactions among entities.   In this paper, we propose a comprehensive visual grounding   network to improve video captioning, by explicitly linking   the entities and actions to the visual clues across the video   frames. Specifically, the network consists of spatial-temporal   entity grounding and action grounding. The proposed entity   grounding encourages the attention mechanism to focus on   informative spatial areas across video frames, even if the entity is annotated in only one frame of a video. The action   grounding dynamically associates the verbs to related subjects and the corresponding context, which keeps fine-grained   spatial and temporal details for action prediction. Both entity   grounding and action grounding are formulated as a unified   task guided by a soft grounding supervision, which brings architecture simplification and improves training efficiency as   well. We conduct extensive experiments on two challenging   datasets, and demonstrate significant performance improvements of +2.3 CIDEr on ActivityNet-Entities and +2.2 CIDEr   on MSR-VTT compared to state-of-the-arts.", "content": "Video captioning aims to describe the visual content in the   video using natural language sentences. It remains a challenging task as it requires a deep understanding of the objects and their interactions.   Existing methods for video captioning usually employ attention mechanisms, which are expected to ground correct   visual regions for proper word generation. Although these   models have achieved remarkable performance, previous researches (Zhou et al. 2019, 2020; Fei 2022) have shown that   attention mechanisms are incapable of correctly associating generated words with meaningful visual regions, which   makes the model less interpretable.   To address this problem, recent works (Liu et al. 2017;   Jiang et al. 2022) have exploited region-phrase annotations   *Corresponding author: fa0001ng@e.ntu.edu.sg   Copyright \u00a9 2024, Association for the Advancement of Artificial   Intelligence (www.aaai.org). All rights reserved.   Figure 1: Examples of ActivityNet-Entities dataset. The   frames with grounding annotations are marked in red. Only   one bounding box per entity is labeled for each video.   in the training stage and designed diverse objective functions   to guide the attention module to ground on appropriate visual areas. These methods achieve desirable improvements   in still images. However, directly applying these grounding   modules for video captioning is highly challenging due to   the following reasons:   1. Relevant visual regions corresponding to the object entities can span several frames. However, carefully labeling   the bounding box of each entity frame by frame is laborconsuming. As is exemplified in Fig. 1, existing datasets   provide only sparse annotations (Zhou et al. 2019), i.e.,   annotating each entity with a bounding box in one frame   of the video. Recent grounded video captioning models then perform spatial-level grounding within the annotated frame (Zhou et al. 2019; Wan, Jiang, and Fang   2022), ignoring the temporal dynamics of the entities   across video frames.   2. Unlike image captioning that emphasizes the prediction   of nouns, video descriptions are featured for the complex actions and interactions of objects. However, due   to the lack of explicit visual annotations of verbs, action grounding remains challenging. Several methods (Ye   et al. 2022; Zheng, Wang, and Tao 2020) associate verbs   with global motion features, which may cause considerable spatial details missing.   To fully explore the spatial and temporal correlations   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2552   among the video to achieve accurate grounded video captions, we propose a comprehensive visual grounding network. It performs spatial-temporal grounding on both entities and actions, aiming to predict accurate nouns and verbs.   For entity grounding (EG), our observation is that the annotated entities may suffer from deteriorated appearances in   videos, such as motion blur, inappropriate viewpoint, etc.   As a result, the labeled visual clues may not be informative   enough to generate the target word. In contrast, recognizing   the entities in adjacent frames can be easier (see Fig. 1 for   illustrations). Therefore, we propose dynamic label propagation from the labeled frame to adjacent frames using \u201cdetection by tracking\u201d strategy. The generated entity tracklet   enables spatial-temporal entity grounding.   For action grounding, we are motivated by the syntax   triplet \u27e8subject, predicate, object\u27e9, where actions are always   associated with subjects and objects. We therefore automatically generate grounding annotations of actions by referring   to the union of the areas related to the subject, objects and   corresponding context, and encourage the attention mechanism to ground on these areas.   To achieve video grounding, we further propose a soft   grounding supervision (SGS) which encourages the attention mechanism grounding on informative spatial-temporal   areas softly. The attention mechanism supervised by SGS   enables the generated caption to have a reasonable meaning.   More importantly, SGS unifies entity grounding and action   grounding. The task unification not only simplifies the captioning architecture, but also improves training efficiency.   We evaluate our method on ActivityNet-Entities and   MSR-VTT (Xu et al. 2016). Both quantitative and qualitative comparisons verify that our method significantly improves video captioning. Notably, our method achieves the   CIDEr scores of 51.8 and 60.2 on ActivityNet-Entities and   MSR-VTT, respectively, which are +2.3 and +2.2 higher   than the best competitors.   In sum, the contributions of this work are threefold:   \u2022 Propose spatial-temporal entity grounding (EG) which   dynamically focuses on informative spatial areas across   video frames, albeit the entity is annotated in only one   frame. EG strengthens the temporal context and improves visual nouns prediction.   \u2022 Propose action grounding that associates the actions to   object-related spatial-temporal areas. To the best of our   knowledge, there has not been any deep exploration of   action grounding for video captioning.   \u2022 Propose a soft grounding supervision (SGS) that encourages the captioner grounding on informative spatialtemporal areas softly. SGS simplifies the grounding architecture and makes the captioning model interpretable.   Related Work   Video Captioning.   Most recent video captioning methods employ an encoder-decoder framework. The encoder extracts video representations from a set of video frames, and   the decoder generates the sentence word-by-word according   to the video representation.   To improve vision-word alignment for precise word generation, Yao et al. (Yao et al. 2015) introduce a temporal attention mechanism into video captioning, which allows the   decoder to automatically focus on the most relevant frames   conditioned by the LSTM hidden state. Other representative methods (Yan et al. 2019; Chen and Jiang 2021; Tang   et al. 2022) combine spatial and temporal attention. The spatial attention emphasizes important regions in each video   frame for word generation. Meanwhile, the temporal attention is used to derive a subset of frames that is correlated to   the video caption. More recently, researchers have exploited   spatial and temporal attention simultaneously to build crossmodal associations. For example, LSRT (Li et al. 2022)   builds spatial-temporal relationships between adjacent objects and proposes a coarse-to-fine decoder that attends to   relevant objects spatially and temporally. SwinBERT (Lin   et al. 2022) exploits Video Swin Transformer to encode   spatial-temporal visual tokens and adopt a multimodal transformer that simultaneously attends to sparse visual tokens   within the video to perform precise decoding. Although   these works have significantly promoted video captioning, it   is widely acknowledged that existing attention-based models are not correctly grounded.   Video Grounding.   Video grounding aims to localize the   starting and ending time of the target video segment that corresponds to the given text. Conventional methods (Liu et al.   2022a) firstly generate candidate proposals, then semantically match a given query text with each candidate through   video-text matching. Yang et al. (Yang et al. 2022) further   study spatio-temporal video grounding, which aims at localizing a spatio-temporal tube corresponding to the given   text. Different from video grounding that leverages provided   video captions for spatial-temporal localization, our work   exploits video grounding as an intermediate step to improve   captioning.   Recent efforts have been put into improving visual   grounding for captioning. As a representative work,   GLIPv2 (Zhang et al. 2022) takes visual grounding as model   pre-training and takes image captioning as the downstream   task. Different from GLIPv2, our work directly builds the   connection between video grounding and video captioning.   Other works focus on grounded captioning. Conventional   methods (Jiang et al. 2022; Liu et al. 2017; Zhou et al. 2019)   introduce an auxiliary task that builds correct correlations   between object words and the corresponding image regions   during the caption generation. For example, GVD (Zhou   et al. 2019) explicitly links each noun phrase with the corresponding bounding box in one of the frames of a video.   However, it only emphasizes the prediction of objects in the   video, while ignoring the rich actions and events implied in   the video. In addition, GVD only focuses on grounding on a   single sampled frame, ignoring the temporal dynamics of the   objects across frames. In contrast, SAAT (Zheng, Wang, and   Tao 2020) and HMN (Ye et al. 2022) improve action prediction by associating actions with motion features. However,   they only focus on temporal action correspondences, which   disregard spatial details.   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2553   \u201cA man is flipping   a violin in his hands\u201d   Embbeding   Motion   Attention   \u03b2   Visual   Attention   \u03b1    Language LSTM   MLP   Soft Grounding Mask   \u03b1 :   \u03b2 :   Attention Weight   \u03b3 :   Caption   Loss    Grounding    Loss   Video Grounding   Caption Generation   Video Encoding   add   Add Operation   Motion vector   Visual vector   Dynamic Label Propagation   \u03b4 :   Soft Grounding Supervision   action label generation    entity label propagation   Figure 2: Overview of our comprehensive visual grounding for video captioning. Dynamic label propagation generates entity   and action annotations for all video frames. Soft grounding supervision guides the attention mechanism dynamically attending   to the relevant spatial-temporal regions of the input video for both entities and actions. Dynamic label propagation and soft   grounding supervision are only employed in the training phase, and thus would not impact inference efficiency.   Proposed Method   Overview   The grounded video captioning model takes a sequence of   raw video frames as inputs and outputs a caption Y . We   denote the ground truth sentence with T words as Y   =   (y1, y2, ..., yT ). Each noun yt is associated with one bounding box annotation Bt that indicates its appearance in one   of the video frames. A captioning model is learned to maximize the conditional probability p(Y |I; \u03b8) for each video,   where \u03b8 denotes the model parameters.   The overall framework is shown in Fig. 2. We follow   the conventional encoder-decoder pipeline, which consists   of three modules, feature encoding, video grounding, and   caption generation. The feature encoder extracts grid-level   features to retain spatial-wise information of the video. The   visual grounding module performs spatial-temporal entity   grounding and action grounding, leading to an improved attention mechanism. We introduce dynamic label propagation to estimate target entities and action areas across the   video frames. Both entity grounding and action grounding   are formulated as a unified task guided by a soft grounding   supervision and are learned to dynamically focus on the relevant spatial-temporal regions. The grounding module finally   forms a high-level impression of the video content and feeds   it for caption generation. We describe each module in detail   as follows.   Video Encoding   The video encoder detects informative visual clues from the   video. We employ a vision transformer as the video encoder to extract the visual feature V and motion feature M.   Specifically, we uniformly sample F frames from each video   segment. These frames are then processed by the Video   Swin Transformer to compute temporal-aware visual features. Each frame f produces N grid feature vectors Vf =   [v1   f, v2   f, ...vN   f ]. To obtain the visual feature of the video, we   concatenate the feature vectors from all frames, where the   visual feature vector represented as V = [v1, v2, ..., vG],   where G is the total number of visual grid vectors extracted   from the entire video and G = F \u00d7 N. Meanwhile, we utilize Text4Vis model (Wu, Sun, and Ouyang 2023) to extract   grid-level motion features Mf = [m1   f, m2   f, ..., mN   f ] from   the sampled frames. Similar to the visual features, we obtain   the motion features as M = [m1, m2, ..., mH], where H   denotes the total number of motion vectors extracted from   the whole video.   Video Grounding   Attention Mechanism.   The video attention learns to selectively attend to relevant visual areas for sentence generation. Following GVD (Zhou et al. 2019), we employ the   widely used additive attention on visual features and motion   features, respectively. Formally, at decoding time step t, an   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2554   Caption: A man is padding a canoe along the water    man   canoe    t   t-1   t+2   t+3   t+1   Action Label Generation   Entity Label Propagate   padding   man   canoe    (a)   (b)   Figure 3: Illustration of dynamic label propagation. (a)Entity label propagation. (b)Action label generation.   attention LSTM first takes the visual feature, motion feature   and word embedding of input word yt\u22121 as inputs, and outputs a hidden state ht   1:   ht   1 = LSTM1([ \u00afV + \u00af   M; Weyt\u22121], ht\u22121   1   )   (1)   where \u00afV =   1   G   PG   i=1 vi; \u00af   M =   1   H   PH   i=1 mi, We denotes   the word embedding matrix. Then, conditioned on the hidden state ht   1, we can calculate the attention distribution \u03b1t   and \u03b2t for V and M as follows:   \u03b1t = softmax(W\u03b1   a [tanh(W\u03b1   k V + W\u03b1   q ht   1)])   \u03b2t = softmax(W\u03b2   a[tanh(W\u03b2   kM + W\u03b2   q ht   1)])   (2)   where Wa, Wk and Wq are the embedding matrices, \u03b1t =   [\u03b1t   1, \u03b1t   2, ..., \u03b1t   G] and \u03b2t = [\u03b2t   1, \u03b2t   2, ..., \u03b2t   H] denote the attention weights for V and M, respectively. For simplicity, we   drop the superscript t in the rest of the paper unless explicitly   mentioned.   Dynamic Label Propagation.   As \u03b1 and \u03b2 are learned as   latent variables without explicit supervision, the attention   models are criticized for the \u201cdeviated focus\u201d problem (Fei   2022; Jiang et al. 2022). The most straightforward way to   overcome this issue is to introduce grounding supervision.   However, existing video captioning datasets annotate only   one bounding box for each entity throughout the video sequence, which provides insufficient visual clues in case of   small object scale and inappropriate viewpoint. Moreover,   no action annotation is provided. To fully leverage the spatial and temporal correlations among the video, we propose   dynamic label propagation (DLP).   For entities, the DLP generates pseudo box annotations   in adjacent video frames using \u201cdetection by tracking\u201d strategy. Specifically, as shown in Fig. 3(a), for each entity with   a labeled bounding box B, we leverage ToMP (Mayer et al.   2022), a high-performing object tracker, to generate a tracklet over the entire video. ToMP eventually outputs a pseudo   annotation Bf for each unlabeled frame. Each Bf is also   associated with a score sf ranging from 0 to 1, indicating   the confidence of the identified box. As ToMP may generate boxes with wrong locations and false positives, we apply   confidence-based thresholding to further reduce potentially   wrong pseudo boxes, i.e., only pseudo annotations with confidence scores higher than sth are maintained.   A critical issue for action grounding is the lack of visual annotations for action words. We observe that action   words are always associated with entities (i.e., subjects and   objects). We therefore automatically generate grounding annotations for actions by referring to the union of the areas   related to the entities of the action. The procedure is exemplified in Fig.3(b). Formally, for video frame f, given K associated entities and the corresponding boxes {Bi   f}K   i=1, we   generate the tightest bounding rectangle that covers these   boxes as the annotation for the action, denoted by B\u2032   f. We   also generate a confidence score for B\u2032   f by aggregating the   scores from action-related entities:   s\u2032   f = min   i {si   f}   (3)   The generated action annotations allow us to build entity   grounding and action grounding as a unified model easily.   Soft Grounding Supervision.   Subsequently, we propose   a soft grounding supervision (SGS) that encourages the   attention mechanism grounding on informative spatialtemporal areas for both entities and actions. We take an   example of grounding an entity word for illustration. The   motivation behind the supervision is that \u03b1 and \u03b2 should   be more concentrated on the annotated spatial-temporal areas with higher sf. To that end, we construct a sequence   of heatmaps \u0393 = [\u03931, \u03932, ..., \u0393F ] on visual grid features,   where \u0393f = [\u03b31   f, \u03b32   f, ..., \u03b3N   f ] has the same spatial resolution   as Vf. We encourage the attention model to focus on annotated areas by setting \u0393f with soft scores1:   \u03b3i   f =   \u001asf   i \u2208Bf   0   otherwise   (4)   In order to facilitate calculation, we flatten heatmaps \u0393   into a vector \u03b3 = [\u03b31, \u03b32, ..., \u03b3G]. Similarly, we construct   heatmaps on motion features and flatten them into a vector   \u03b4. The per-word grounding loss function is defined as follows:   Lvg =\u2212log(   G   X   j=1   \u03b3j\u03b1j)\u2212log(   H   X   j=1   \u03b4j\u03b2j)   (5)   1We set the confidence score of the labeled box B to 1.   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2555   Eq.(5) encourages most grid features located inside annotated boxes to output high attention scores. Here \u03b3 and \u03b4   serve as soft voters. Bf with higher sf is more likely to be   concentrated since a larger \u03b3j enforces a larger \u03b1j.   The same loss is applied to action words. The only difference is to use B\u2032   f instead of Bf for heatmaps generation.   The final loss on Lvg is the average of losses on all visually groundable words. In contrast to previous methods (Ye   et al. 2022; Zheng, Wang, and Tao 2020) that refine entity   prediction and action prediction with different designs, our   work unifies entity and action grounding, which simplifies   the captioning architecture and improves training efficiency.   Caption Generation   For caption generation, we apply the widely used attentionenhanced LSTM decoder. The video grounding module finally forms a high-level impression of the visual content by   accumulating V and M with the attention weights:   qt =   G   X   j=1   \u03b1jvj +   H   X   j=1   \u03b2jmj   (6)   where qt corresponds specifically to individual words being   generated, and is fed into a language LSTM to predict the   next word y\u2217   t :   ht   2 = LSTM2([qt; ht   A], ht\u22121   L   )   (7)   y\u2217   t \u223cpt = softmax(Wsht   2)   (8)   where Ws is the embedding matrix, pt denotes the output   probability distribution of the decoder, and the generated   word y\u2217   t is sampled from pt.   Training Objectives   We formulate the grounded video captioning as a joint optimization over the language and grounding tasks. The overall   objective function is defined as follows:   L = Lcap + \u03bbLvg   (9)   where Lcap denotes the caption generation loss, which compares the output sentence with the ground truth. Specifically,   we employ the cross-entropy loss as follows:   Lcap = \u2212   T   X   t=1   log(pt(yt|y1:t\u22121))   (10)   Lvg corresponds to the soft grounding supervision. \u03bb is used   to balance the two types of losses. Lvg serves as a wordregion alignment regularization, which assists the captioning   model in attending to informative regions.   Experiments   Experimental Setups   Datasets.   We conduct our experiments on ActivityNetEntities and MSR-VTT. The ActivityNet-Entities not only   contains the video caption annotation of the video but also   EG DLP AG SGS B@1 B@4   S   M   C   24.6   3.1   16.0   11.7   51.9   \u2713   25.0   3.2   16.1   11.8   52.6   \u2713   \u2713   24.8   3.1   16.0   11.7   53.0   \u2713   25.0   3.2   16.0   11.8   52.8   \u2713   \u2713   \u2713   25.1   3.3   16.3   11.8   53.1   \u2713   \u2713   \u2713   25.0   3.2   16.2   11.8   53.5   \u2713   \u2713   \u2713   \u2713   25.1   3.3   16.6   11.9   54.5   Table 1: Ablation studies on ActivityNet-Entities val set.   B@N, M, S, and C stand for BLEU@N, METEOR, SPICE,   and CIDEr, respectively. The symbol \u201c\u2713\u201d indicates the inclusion of the following component. Bold for the best.   provides the box annotation of the noun phrase in the caption. The dataset contains 15,000 videos, including 52,000   video segments, and 1 caption annotation for each video   segment. The dataset provides a total of 158,000 valid box   annotations of 432 classes. The MSR-VTT contains 10,000   video clips from YouTube. There are 20 human descriptions   for each video clip. The dataset contains 6,573 samples for   training, 497 samples for validation, and 2,990 for testing.   Evaluation Metrics.   Following the standard video captioning evaluation protocol, we use 5 common captioning metrics to evaluate the captioning quality, i.e.,   CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015),   BLEU (Papineni et al. 2002), METEOR (Denkowski and   Lavie 2014), ROUGE-L (Lin 2004) and SPICE (Anderson   et al. 2016).   Implementation Details.   For ActivityNet-Entities, we   uniformly sample 10 frames for each video segment. For   MSR-VTT, 32 video frames are sampled from the video   clip. We employ Video Swin Transformer (Liu et al. 2022c)   pre-trained on ImageNet (Deng et al. 2009) to extract visual   features. Besides, we use Text4Vis (Wu, Sun, and Ouyang   2023) model pre-trained on Kinetics-400 (Carreira and Zisserman 2017) to extract motion features.   The same model hyperparameters and data preprocessing   step as GVD are adopted. The word embedding size is set   to 512. Empirically, \u03bb is set to 0.1. During training, we optimize the model with Adam for 25 epochs. The learning rate   is initialized to be 5e-4 and decayed by a factor of 0.8 every   three epochs.   Ablation Studies   To quantify the impact of different components for video   captioning, we conduct ablation studies on the ActivityNetEntities val set.   Effectiveness of Grounding Modules.   To show the effectiveness of the grounding modules, we compare different variants of the proposed model. For clarity, we disable the soft grounding supervision. We start from the baseline model without any grounding modules, then we gradually incorporate entity grounding (EG) and action grounding   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2556   Figure 4: The impact of sth on grounded captioning performance. A large sth indicates high precision but low recall   of the generated annotations. Experiments are conducted on   ActivityNet-Entities val set.   (AG) to examine the effectiveness. As shown in Table 1, performing entity grounding on the labeled boxes solidly promotes the baseline. With dynamic label propagation (DLP),   the entity grounding further improves the captioning performance overall metrics substantially, which suggests that the   spatial-temporal entity grounding can exploit more temporal contexts to better solve video captioning. We also notice   that action grounding alone improves the video captioning   baseline significantly, which verifies that action grounding is   critical for predicting verbs. Finally, when we integrate both   grounding modules together, we obtain a CIDEr of 53.5,   which outperforms the baseline by 1.6, thus demonstrating   the superiority of the proposed grounding model.   The Impact of Label Propagation.   We further investigate the importance of the proposed dynamic label propagation (DLP). Towards this goal, we adjust sth to see how label propagation impacts video grounding. We notice that entity label propagation with varying sth achieves consistently   better captioning performance compared with the baseline.   Specifically, when sth varies from 0 to 0.5, the performance   monotonically increases. The reason is that a high sth ensures the accuracy of generated pseudo annotations, while   the defective annotations may mislead the attention module conversely. Then the captioning performance reaches the   peak when sth reaches 0.5. Finally, the performance drops   as sth continues increasing. This is probably due to informative entities in adjacent frames being filtered as sth goes   higher. In an extreme case, setting sth to 1.0 is equivalent to   disabling entity label propagation. In the following experiments, we set sth to 0.5 if not otherwise specified.   Effectiveness of Soft Grounding Supervision.   To explore the effect of soft grounding supervision (SGS), we   compare it with baseline supervision which treats all generated annotations as equally important. As reported in Table 1, soft grounding further promotes the performance of   Method   B@1   B@4   S   M   C   Mask-TF   22.9   2.41   13.7   10.6   46.1   BiLSTM+TA   22.8   2.17   11.8   10.2   42.2   Cyclical   23.4   2.43   14.3   10.8   46.6   GVD   23.6   2.35   14.7   11.0   45.5   KNN-HAST   2.61   15.1   11.3   48.5   IAS   24.2   2.76   11.3   49.5   SwinBERT*   21.4   1.97   16.2   10.5   39.3   VIOLETv2*   21.4   1.83   15.2   10.7   38.5   Ours   24.8   3.00   16.3   11.8   51.8   Table 2: Comparisons of the state-of-the-art methods   on   ActivityNet-Entities   test   set.   *   denotes   our   reimplementation.   video captioning substantially. Specifically, we achieved a   performance of 54.5 for CIDEr score, which is +2.6 higher   than the baseline. Consistent performance boosts are observed on all other metrics. The reason is that soft grounding   reduces the noise brought by generated pseudo annotations.   Comparison with State-of-the-art   Results on ActivityNet-Entities.   We compare our model   with several recent methods, including Mask-TF (Zhou et al.   2018), BiLSTM+TA (Zhou et al. 2018), Cyclical (Ma et al.   2020), GVD (Zhou et al. 2019), KNN-HAST (Shen et al.   2020), IAS (Wan, Jiang, and Fang 2022), SwinBERT (Lin   et al. 2022) and VIOLETv2 (Fu et al. 2023).   Table 2 shows the detailed comparisons. It is clear that   our model consistently exhibits better performance than the   other competitors in terms of all metrics by a large margin. To be specific, our method achieves the performance   of 51.8 for CIDEr score, which is +2.3 higher than that of   IAS, the best-performing grounded video captioning model.   We observe similar improvements in other evaluation metrics. It is worth noticing that our method achieves more significant improvements over recent video captioning models,   e.g., SwinBERT (+9.9 on CIDEr) and VIOLETv2 (+13.3   on CIDEr). The reason is that SwinBERT and VIOLETv2   only employ Video Swin Transformer as the video encoder,   which may lack explicit motion information that is valuable   for predicting the complex actions in ActivityNet-Entities.   Results on MSR-VTT.   We further conduct analysis   on the challenging MSR-VTT dataset. As MSR-VTT   does not include any grounding annotations, we employ   GLIPv2 (Zhang et al. 2022), a prevailing open-vocabulary   object detector, to generate the most confidence bounding   box as the seed label for each entity.   Table 3 summarizes the performance of our method and   existing state-of-the-art methods, including MARN (Pei   et   al.   2019),   OA-BTG   (Zhang   and   Peng   2019),   SAAT (Zheng, Wang, and Tao 2020), ORG-TRL (Zhang   et al. 2020), UniVL (Luo et al. 2020), LSRT (Li et al.   2022), SwinBERT (Lin et al. 2022), HMN (Ye et al.   2022), BME-WCO (Liu et al. 2022b), MELTR (Ko et al.   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2557   Method   B@1   B@4   M   R   C   MARN   40.4   28.1   60.7   47.1   OA-BTG   41.4   28.2   46.9   SAAT   79.6   39.9   27.7   61.2   51.0   ORG-TRL   43.6   28.8   62.1   50.9   UniVL   41.8   28.9   60.8   50.0   LSRT   42.6   28.3   61.0   49.5   SwinBERT   83.1   41.9   29.9   62.1   53.8   HMN   81.3   43.5   29.0   62.7   51.5   BME-WCO   40.6   28.1   61.2   53.4   MELTR   44.2   29.3   62.4   52.8   MAN   41.3   28.0   61.4   49.8   RSFD   43.4   29.3   62.3   53.1   VL-Prompt   43.2   30.1   62.7   55.3   VIOLETv2   58.0   Ours   84.8   46.5   31.2   64.6   60.2   Table 3: Comparisons of the state-of-the-art methods on   MSR-VTT test set.   2023), MAN (Jing et al. 2023), RSFD (Zhong et al.   2023), VL-Prompt (Yan et al. 2023) and VIOLETv2 (Fu   et al. 2023). As it can be observed, our method reaches   60.1 in terms of CIDEr and 46.5 in terms of BLEU@4,   surpassing all other approaches significantly. Notably, our   method significantly outperforms HMN (+8.7 on CIDEr   score), which is designed to enhance entity and predicate   generation. Our model also advances UniVL, MELTR and   VIOLETv2 across all metrics, which leverage large-scale   vision and language pretraining. These results solidly verify   the superiority of the proposed method.   Qualitative Results   Fig. 5 showcases qualitative examples of the captions generated by SwinBERT, IAS and our method. The advantages   of our method can be divided into two primary categories.   Firstly, our method can recognize the entities more accurately. For example, in the first example, the \u201cexercise equipment\u201d is wrongly recognized as \u201ca couch\u201d in SwinBERT and   \u201ca bed\u201d in IAS, while our method depicts the entity more   precisely. In addition, our method provides fine-grained descriptions of the entities, benefiting from the rich temporal information of the entities across frames. For example,   in the second example, our method enriches \u201ca baby\u201d with   the attribute \u201csmiling to the camera\u201d. Secondly, our method   provides richer and more accurate content about the actions. As illustrated in the third example, our method predicts \u201cput makeup\u201d, which is more informative than \u201chold   up a brush\u201d,\u201clooking off into the distance\u201d provided by other   methods. The reason is that our method is learned to focus   on informative regions related to the subjects to predict actions. In the last example, our method provides comprehensive description of the actions as \u201cjumps off the beam\u201d and   \u201clands on the mat\u201d. More examples are presented in the supplementary.   GT:           Ours:        Base:      IAS:         SwinB:    A woman jumps off the balance beam onto a blue mat.   The woman jumps off the beam and lands on the mat.    The girl flips on the beam.    The woman then take the stick and walk away.   The girl flips and lands on the mat.   A young girl is seen looking at the camera and leads into her putting    eyeliner on as well as mascara.   A young girl is seen speaking to the camera and leads into her putting    makeup on her face.   A young girl is seen sitting on a chair and looking off into the distance.   A woman be see speak to the camera and lead into she hold up a brush    and rub it down.   She then puts mascara on her eye and then puts it on her eye.   GT:   Ours:   Base:      IAS:         SwinB:    A small boy is rocking back and forth on a piece of exercise equipment.   A young child is seen sitting on a piece of exercise equipment and looking    to the camera.   A little boy is sitting on a couch.   A little boy be sit on a bed.   A young boy is seen sitting on a couch with a vacuum.   A baby is laughing as he swings in a swing set.   A baby is seen sitting on a swing set and smiling to the camera.   A baby is sitting on a swing set.   A baby be sit on a swing.   A baby is seen sitting in a swing.   GT:    Ours:           Base:      IAS:         SwinB:    GT:    Ours:     Base:      IAS:         SwinB:    Figure 5: Examples of captions generated by our method and   several state-of-the-art methods, as well as the corresponding ground-truths.   Conclusion   In this work, we aim to enhance the accuracy of grounded   video captioning by introducing a comprehensive visual grounding network. This network comprises spatialtemporal entity grounding and action grounding. The entity grounding is responsible for directing attention to relevant spatial areas over the entire video, leveraging entity   labels in only one frame of the video. In this meanwhile, the   action grounding dynamically associates actions with relevant subjects and their respective contexts. This association   allows the model to capture fine-grained spatial and temporal details necessary for accurate action prediction. Both   entity grounding and action grounding are guided by a soft   grounding supervision (SGS). SGS encourages the attention   mechanism grounding on informative spatial-temporal areas   softly. More importantly, SGS unifies entity grounding and   action grounding, which simplifies the captioning architecture and improves training efficiency. Extensive experiments   have demonstrated the superiority of our method compared   with the state-of-the-arts.   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   2558   Acknowledgements   This work was supported by the National Key Research and Development Program of China under Grant   2020AAA0109301.", "conf": "AAAI", "year": "2024", "index": 694}, {"title": "Joint Visual Grounding and Tracking with Natural Language Specification   Li Zhou1, Zikun Zhou2,1*, Kaige Mao1, and Zhenyu He1,*   1Harbin Institute of Technology, Shenzhen   2Peng Cheng Laboratory   lizhou.hit@gmail.com   zhouzikunhit@gmail.com   maokaige.hit@gmail.com   zhenyuhe@hit.edu.cn", "abstract": "Tracking by natural language specification aims to locate the referred target in a sequence based on the natural language description. Existing algorithms solve this   issue in two steps, visual grounding and tracking, and accordingly deploy the separated grounding model and tracking model to implement these two steps, respectively. Such   a separated framework overlooks the link between visual   grounding and tracking, which is that the natural language   descriptions provide global semantic cues for localizing the   target for both two steps. Besides, the separated framework can hardly be trained end-to-end. To handle these   issues, we propose a joint visual grounding and tracking   framework, which reformulates grounding and tracking as   a unified task: localizing the referred target based on the   given visual-language references. Specifically, we propose   a multi-source relation modeling module to effectively build   the relation between the visual-language references and the   test image. In addition, we design a temporal modeling   module to provide a temporal clue with the guidance of   the global semantic information for our model, which effectively improves the adaptability to the appearance variations of the target.   Extensive experimental results on   TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that   our method performs favorably against state-of-the-art algorithms for both tracking and grounding. Code is available at https://github.com/lizhou-cs/JointNLT.   1.", "content": "Tracking by natural language specification [18] is a task   aiming to locate the target in every frame of a sequence according to the state specified by the natural language. Compared with the classical tracking task [25, 33, 34, 41] using   a bounding box to specify the target of interest, tracking   by natural language specification provides a novel humanmachine interaction manner for visual tracking. In addition,   the natural language specification also has two advantages   \u2217Corresponding authors: Zikun Zhou and Zhenyu He.   Grounding    Model   Tracking    Model   Crop   Visual Grounding   Object Tracking   (a) Separated visual grounding and tracking framework   Visual Grounding   Object Tracking   (b) Joint visual grounding and tracking framework   Grounding Frame   Template   Search Frame   \u201czebra running on the    grass with a black horse\u201d   Grounding Frame   Template   Search  Frame   \u201czebra running on the    grass with a black horse\u201d   \u201czebra running on the    grass with a black horse\u201d   Joint Grounding    &   Tracking Model   Joint Grounding    &   Tracking Model   Figure 1. Illustration of two different frameworks for tracking by   natural language specification. (a) The separated visual grounding and tracking framework, which consists of two independent   models for visual grounding and tracking, respectively. (b) The   proposed joint visual grounding and tracking framework, which   employs a single model for both visual grounding and tracking.   for the tracking task compared to the bounding box specification. First, the bounding box only provides a static representation of the target state, while the natural language can   describe the variation of the target for the long term. Second, the bounding box contains no direct semantics about   the target and even results in ambiguity [32], but the natural   language can provide clear semantics of the target used for   assisting the tracker to recognize the target. In spite of the   above merits, tracking by natural language specification has   not been fully explored.   Most existing solutions [17,18,32,39] for this task could   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   23151   be generally divided into two steps: (1) localizing the target   of interest according to the natural language description in   the first frame, i.e., visual grounding; (2) tracking the localized target in the subsequent frames based on the target state   predicted in the first frame, i.e., visual tracking. Accordingly, many algorithms [17,32,39] are designed to incorporate a grounding model and a tracking model, as shown in   Figure 1(a). Herein the grounding model performs relation   modeling between the language and vision signal to localize   the target, while the tracking model performs relation modeling between the template and search region to localize the   target. The drawback of this framework is that the grounding model and the tracking model are two separate parts   and work independently, ignoring the connections between   the two steps. Besides, many of them [17, 32, 39] choose   to adopt the off-the-shelf grounding model [38] or tracking   model [16] to construct their framework, which means that   the overall framework cannot be trained end-to-end.   The tracking model in most existing algorithms [17, 32,   39] predicts the target state only based on the template,   overlooking the natural language description. By contrast,   the tracking mechanism that considers both the target template and the natural language for predicting the target state   has proven to have great potential [12, 13, 18, 31]. Such   a tracking mechanism requires the tracking model to own   the ability to simultaneously model the vision-language relation and the template-search region relation. Inspired by   this tracking mechanism, we come up with the idea to build   a joint relation modeling model to accomplish the abovementioned two-step pipeline. Herein a joint relation modeling model can naturally connect visual grounding and tracking together and also can be trained end-to-end.   To this end, we propose a joint visual grounding and   tracking framework for tracking by natural language specification, as shown in Figure 1(b). Specifically, we look   at these two tasks from a unified perspective and reformulate them as a unified one: localizing the referred target   according to the given visual-language", "conf": "CVPR", "year": "2023", "index": 347}, {"title": "The Thirty-Fourth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-20)   An Annotated Corpus of Reference   Resolution for Interpreting Common Grounding   Takuma Udagawa   The University of Tokyo, Tokyo, Japan   takuma udagawa@nii.ac.jp   Akiko Aizawa   National Institute of Informatics, Tokyo, Japan   aizawa@nii.ac.jp", "abstract": "Common grounding is the process of creating, repairing and   updating mutual understandings, which is a fundamental aspect of natural language conversation. However, interpreting   the process of common grounding is a challenging task, especially under continuous and partially-observable context   where complex ambiguity, uncertainty, partial understandings and misunderstandings are introduced. Interpretation becomes even more challenging when we deal with dialogue   systems which still have limited capability of natural language understanding and generation. To address this problem, we consider reference resolution as the central subtask   of common grounding and propose a new resource to study   its intermediate process. Based on a simple and general annotation schema, we collected a total of 40,172 referring expressions in 5,191 dialogues curated from an existing corpus,   along with multiple judgements of referent interpretations.   We show that our annotation is highly reliable, captures the   complexity of common grounding through a natural degree   of reasonable disagreements, and allows for more detailed   and quantitative analyses of common grounding strategies.   Finally, we demonstrate the advantages of our annotation for   interpreting, analyzing and improving common grounding in   baseline dialogue systems.   1", "content": "Common grounding is the process of creating, repairing and   updating mutual understandings, which is a critical aspect   of sophisticated human communication (Clark 1996) as well   as a longstanding goal in dialogue modeling (Traum 1994).   Recently, there have been several new proposals of dialogue   tasks which require advanced skills of common grounding   under continuous and partially-observable context (Udagawa and Aizawa 2019; Haber et al. 2019). Their main contributions include proposal of clear evaluation metrics based   on task success rate, collection of large-scale datasets (thousands of dialogues) and introduction of complex ambiguity, uncertainty, partial understandings and misunderstandings which are minimally observed under traditional settings   based on either categorical or fully-observable context.   However, interpretation of the process of common   grounding remains largely an open problem. Although a formal theory such as Poesio and Rieser (2010) can account   Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   for some of the important details in common grounding,   constructing such precise semantic representation is a dif\ufb01cult and costly process, especially under continuous and   partially-observable context with high ambiguity and uncertainty. Interpretation becomes even more challenging when   we deal with dialogue systems represented by end-to-end   neural models (Vinyals and Le 2015; Bordes and Weston   2016), which can converse \ufb02uently but still lack true competency of natural language understanding and generation.   In this work, we approach this problem by decomposing   the common grounding task based on its intermediate subtasks. Speci\ufb01cally, we consider reference resolution as the   central subtask of common grounding (in the sense that mutual understanding can only be created through successful", "conf": "AAAI", "year": "2020", "index": 436}, {"title": "Learning to Generate Grounded Visual   Captions without Localization Supervision   Chih-Yao Ma1, 3, Yannis Kalantidis\u22c62, Ghassan AlRegib1,   Peter Vajda3, Marcus Rohrbach3, Zsolt Kira1   1Georgia Tech, 2NAVER LABS Europe, 3Facebook   {cyma,alregib,zkira}@gatech.edu, yannis.kalantidis@naverlabs.com, {cyma,vajdap,mrf}@fb.com", "abstract": ". When automatically generating a sentence description for an   image or video, it often remains unclear how well the generated caption   is grounded, that is whether the model uses the correct image regions to   output particular words, or if the model is hallucinating based on priors in the dataset and/or the language model. The most common way   of relating image regions with words in caption models is through an   attention mechanism over the regions that are used as input to predict   the next word. The model must therefore learn to predict the attentional weights without knowing the word it should localize. This is dif\ufb01cult to train without grounding supervision since recurrent models can   propagate past information and there is no explicit signal to force the   captioning model to properly ground the individual decoded words. In   this work, we help the model to achieve this via a novel cyclical training   regimen that forces the model to localize each word in the image after the sentence decoder generates it, and then reconstruct the sentence   from the localized image region(s) to match the ground-truth. Our proposed framework only requires learning one extra fully-connected layer   (the localizer), a layer that can be removed at test time. We show that   our model signi\ufb01cantly improves grounding accuracy without relying on   grounding supervision or introducing extra computation during inference, for both image and video captioning tasks. Code is available at   https://github.com/chihyaoma/cyclical-visual-captioning.   Keywords: image captioning, video captioning, self-supervised learning, visual grounding   1", "content": "Vision and language tasks such as visual captioning or question answering, combine linguistic descriptions with data from real-world scenes. Deep learning models have achieved great success for such tasks, driven in part by the development   of attention mechanisms that focus on various objects in the scene while generating captions. The resulting models, however, are known to have poor grounding   performance [19], leading to undesirable behaviors (such as object hallucinations [26]), despite having high captioning accuracy. That is, they often do not   \u22c6Work done while at Facebook.   2   C. Ma et al.   correctly associate generated words with the appropriate image regions (e.g.,   objects) in the scene, resulting in models that lack interpretability.   Several existing approaches have tried to improve the grounding of captioning models. One class of methods generate sentence templates with slot locations   explicitly tied to speci\ufb01c image regions. These slots are then \ufb01lled in by visual   concepts identi\ufb01ed by o\ufb00-the-shelf object detectors [20]. Other methods have developed speci\ufb01c grounding or attention modules that aim to attend to the correct   region(s) for generating visually groundable word. Such methods, however, rely   on explicit supervision for optimizing the grounding or attention modules [19,46]   and require bounding box annotations for each visually groundable word.   In this work, we propose a novel cyclical training regimen that is able to signi\ufb01cantly improve grounding performance without any grounding annotations.   An important insight of our work is that current models use attention mechanisms conditioned on the hidden features of recurrent modules such as LSTMs,   which leads to e\ufb00ective models with high accuracy but entangle grounding and   decoding. Since LSTMs are e\ufb00ective at propagating information across the decoding process, the network does not necessarily need to associate particular   decoded words with their corresponding image region(s). However, for a captioning model to be visually grounded, the model has to predict attentional   weights without knowing the word to localize.   Based on this insight, we develop a cyclical training regimen to force the   network to ground individual decoded words: decoding \u2212\u2192localization \u2212\u2192reconstruction. Speci\ufb01cally, the model of the decoding stage can be any state-of-theart captioning model; in this work, we follow GVD [46] to extend the widely   used Up-Down model [2]. At the localization stage, each word generated by the   \ufb01rst decoding stage is localized through a localizer, and the resulting grounded   image region(s) are then used to reconstruct the ground-truth caption in the   \ufb01nal stage. Both decoding and reconstruction stages are trained using a standard cross-entropy loss. Important to our method, both stages share the same   decoder, thereby causing the localization stage to guide the decoder to improve   its attention mechanism. Our method is simple and only adds a fully-connected   layer to perform localization. During inference, we only use the (shared) decoder,   thus we do not add any computational cost.   To compare with the state-of-the-art [46], we evaluate our proposed method   on the challenging Flickr30k Entities image captioning dataset [24] and the   ActivityNet-Entities video captioning dataset [46] on both captioning and grounding. In addition to the existing grounding metrics that calculate the grounding accuracy for each object class [46], we further include a grounding metric that compute grounding accuracy for each generated sentence. We achieve   around 18% relative improvements averaged over grounding metrics in terms of   bridging the gap between the unsupervised baseline and supervised methods on   Flickr30k Entites and around 34% on ActivityNet-Entities. We further \ufb01nd that   our method can even outperform the supervised method on infrequent words,   owing to its self-supervised nature. In addition, we also conduct human evalGenerate Grounded Visual Captions without Localization Supervision   3   cat   woman   laptop   cat   woman   cat   laptop   cup   Final Result   Model update!   Matched   Is          a woman?    Is         a cat?   Is             a laptop?   Localizing \u2026   Captioning \u2026   Localization   A person with a cat is    using a laptop.   woman   laptop   A woman with a dog is    using a laptop.   woman   laptop   dog   Model update!   Fig. 1: Visual captioning models are often not visually-grounded. As humans,   after generating a caption, we perform localization to check whether the generated caption is visually-grounded. If the localized image region is incorrect,   we will correct it. Same goes for training a model. We would like to update the   model accordingly. However, without the ground-truth grounding annotation,   how does the model know the localized region is incorrect? How can the model   then be updated? To overcome this issue, we propose to perform localization   and reconstruction to regularize the captioning model to be visually-grounded   without relying on the grounding annotations.   uation on visual grounding to further verify the improvement of the proposed   method.   Contributions summary. We propose object re-localization as a form of selfsupervision for grounded visual captioning and present a cyclical training regimen that re-generates sentences after re-localizing the objects conditioned on   each word, implicitly imposing grounding consistency. We evaluate our proposed   approach on both image and video captioning tasks. We show that the proposed   training regime can boost grounding accuracy over a state-of-the-art baseline, enabling grounded models to be trained without bounding box annotations, while   retaining high captioning quality across two datasets and various experimental   settings.   2   Related work   Visual captioning. Neural models for visual captioning have received signi\ufb01cant attention recently [2,21,20,8,37,29,36,28,32,23]. Most current state-of-theart models contain attention mechanisms, allowing the process to focus on subsets of the image when generating the next word. These attention mechanisms   can be de\ufb01ned over spatial locations [38], semantic metadata [18,42,43,48] or a   prede\ufb01ned set of regions extracted via a region proposal network [21,44,2,20,6,17].   In the latter case, o\ufb00-the-shelf object detectors are \ufb01rst used to extract object   proposals [25,11] and the captioning model then learns to dynamically attend   over them when generating the caption.   Visual grounding. Although attention mechanisms are generally shown to improve captioning quality and metrics, it has also been shown that they don\u2019t   4   C. Ma et al.   really focus on the same regions as a human would [5]. This make models less   trustworthy and interpretable, and therefore creating grounded image captioning   models, i.e., models that accurately link generated words or phrases to speci\ufb01c   regions of the image, has recently been an active research area. A number of   approaches have been proposed, e.g., for grounding phrases or objects from image descriptions [27,14,41,7,46,45], grounding visual explanations [12], visual coreference resolution for actors in video [28], or improving grounding via human   supervision [30]. Recently, Zhou et al. [46] presented a model with self-attention   based context encoding and direct grounding supervision that achieves stateof-the-art results in both the image and video tasks. They exploit ground-truth   bounding box annotations to signi\ufb01cantly improve the visual grounding accuracy.   In contrast, we focus on reinforcing the visual grounding capability of the existing captioning model via a cyclical training regimen without using bounding box   annotations and present a method that can increase grounding accuracy while   maintaining comparable captioning performance with state-of-the-arts. Another   closely related work is [27] where the authors focus on grounding or localizing   a given textual phrase in an image. This creates a critical di\ufb00erence, as the   ground-truth caption is provided during both training and test time and renders   the training regimen proposed in [27] not applicable to visual captioning tasks.   Cyclical training. Cycle consistency [40,49,10,4] has been used recently in a   wide range of domains, including machine translation [10], unpaired image-toimage translation [49], visual question answering [31], question answering [33],   image captioning [4], video captioning [39,9], captioning and drawing [15] as well   as domain adaptation [13]. While the cyclical training regime has been explored   vastly in both vision and language domains, it has not yet been used for enforcing   the visual grounding capability of a captioning model.   3   Method   Notation. For the visual captioning task we have pairs of target sentences and   images (or videos). Each image (or video) is represented by spatial feature map(s)   extracted by a ResNet-101 model and a bag of regions obtained from FasterRCNN [25] as R = [r1, r2, ..., rN] \u2208Rd\u00d7N. The target sentence is represented   as a sequence of one-hot vectors y\u2217   t \u2208Rs, where T is the sentence length, t \u2208   1, 2, ..., T, and s is the dictionary size.   3.1   Baseline   We reimplemented the model used in GVD [46] without self-attention for region   feature encoding [21,34] as our baseline1. It is an extension of the state-of-the-art   Up-Down [2] model with the grounding-aware region encoding (see Appendix).   1 We removed self-attention because we found that removing it slightly improved both   captioning and grounding accuracy in our implementation.   Generate Grounded Visual Captions without Localization Supervision   5   A   person   .   A   woman   with   .   Decoding   Localization   Reconstruction   Decoding loss   Recon. loss   Decoding   Localization   Recon.   +   \u0302\ud835\udc5f! Attended ROIs   \u0302\ud835\udc5f!\" Localized ROIs   \u0302\ud835\udc5f! Attended ROIs   Language    LSTM   \u0302\ud835\udc5f!   \" Localized ROIs   Decoding   Reconstruction   A   person   person   woman   woman   Ground-truth word (target)   \u2207gradient   Training Regimen   Regularizing Attention   A   equal inputs   Attention    LSTM   Attention LSTM   Localization   shared   shared   \u0302\ud835\udc5f! \u21a6\u0302\ud835\udc5f!\"   (no gradient)   Language    LSTM   a   dog   with   a   cat   is   using   a   laptop   is   using   a   laptop   Fig. 2: (Left) Proposed cyclical training regimen: decoding \u2212\u2192localization \u2212\u2192   reconstruction. The decoder attends to the image regions and sequentially generates each of the output words. The localizer then uses the generated words   as input to locate the image regions. Finally, the shared decoder during the   reconstruction stage uses the localized image regions to regenerate a sentence   that matches with the ground-truth sentence. (Right) Because of the shared   Attention LSTM, Language LSTM, and equal word inputs, this training regimen regularizes the attention mechanism inside the Attention LSTM so that the   attended ROIs get closer to the less biased and better localized ROIs \u02c6rt 7\u2192\u02c6rl   t.   Speci\ufb01cally, our baseline model uses two LSTM modules: Attention LSTM   and Language LSTM. The Attention LSTM identi\ufb01es which visual representation in the image is needed for the Language LSTM to generate the next word.   It encodes the global image feature vg, previous hidden state output of the Language LSTM hL   t\u22121, and the previous word embedding et\u22121 into the hidden state   hA   t .   hA   t = LSTMAttn([vg; hL   t\u22121; et\u22121]),   et\u22121 = Weyt\u22121,   (1)   where [; ] denotes concatenation, and We are learned parameters. We omit the   Attention LSTM input hidden and cell states to avoid notational clutter in the   exposition.   The Language LSTM uses the hidden state hA   t from the Attention LSTM to   dynamically attend on the bag of regions R for obtaining visual representations   of the image \u02c6rt to generate a word yt.   zt,n = Waatanh(WahA   t + rn),   \u03b1t = softmax(zt),   \u02c6rt = R\u03b1,   (2)   where Waa and Wa are learned parameters. The conditional probability distribution over possible output words yt is computed as:   hL   t = LSTMLang([\u02c6rt, hA   t ]),   p(yt|y1:t\u22121) = softmax(WohL   t ),   (3)   where y1:t\u22121 is a sequence of outputs (y1, ..., yt\u22121). We refer the Language LSTM   and the output logit layer as the complete language decoder.   6   C. Ma et al.   3.2   Overview   Our goal is to enforce the generated caption to be visually grounded, i.e., attended image regions correspond speci\ufb01cally to individual words being generated,   without ground-truth grounding supervision. Towards this end, we propose a   novel cyclical training regimen that is comprised of decoding, localization, and   reconstruction stages, as illustrated in Figure 2.   The intuition behind our method is that the baseline network is not forced   to generate a correct correspondence between the attended objects and generated words, since the LSTMs can learn priors in the data instead of looking at   the image or propagate information forward which can subsequently be used to   generate corresponding words in future time steps. The proposed cyclical training regimen, in contrast, aims at enforcing visual grounding to the model by   requiring the language decoder (Eq. 3) to rely on the localized image regions \u02c6rl   t   to reconstruct the ground-truth sentence, where the localization is conditioned   only on the generated word from the decoding stage. Our cyclical method can   therefore be done without using any annotations of the grounding itself.   Speci\ufb01cally, let yd   t = Dd(\u02c6rt; \u03b8d) be the initial language decoder with parameters \u03b8d (Eq. 3), trained to sequentially generate words yd   t . Let \u02c6rl   t = G(yd   t , R; \u03b8g)   de\ufb01ne a localizer unit with parameters \u03b8g, that learns to map (ground) each   generated word yd   t to region(s) in the image R. Finally, let yl   t = Dl(\u02c6rl   t; \u03b8l) be a   second decoder, that is required to reconstruct the ground-truth caption using   the localized region(s), instead of the attention computed by the decoder itself.   We de\ufb01ne the cycle:   yl   t = Dl(G(Dd(\u02c6rt; \u03b8d), R; \u03b8g); \u03b8l),   \u03b8d = \u03b8l,   (4)   where Dd and Dl share parameters. Although parameters are shared, the inputs   for the two language decoders di\ufb00er, leading to unique LSTM hidden state values   during a run. Note that the Attention LSTMs and logit layers in the two stages   also share parameters, though they are omitted for clarity.   Through cyclical joint training, both Dd and Dl are required to generate the   same ground-truth sentence. They are both optimized to maximize the likelihood   of the correct caption:   \u03b8\u2217= arg max   \u03b8d   X   log p(yd   t ; \u03b8d) + arg max   \u03b8l   X   log p(yl   t; \u03b8l),   (5)   During training, the localizer regularizes the region attention of the reconstructor and the e\ufb00ect is further propagated to the baseline network in the decoding   stage, since the parameters of Attention LSTM and Language LSTM are shared   for both decoding and reconstruction stages. Note that the gradient from reconstruction loss will not backprop to the decoder Dd in the \ufb01rst decoding stage   since the generated words used as input to the localizer are leafs in the computational graph. The network is implicitly regularized to update its attention   mechanism to match with the localized image regions \u02c6rt 7\u2192\u02c6rl   t. In Sec. 4.4, we   demonstrate that the localized image regions \u02c6rl   t indeed have higher attention accuracy than \u02c6rt when using ground-truth words as inputs for the localizer, which   Generate Grounded Visual Captions without Localization Supervision   7   drives the attention mechanism and helps the attended region \u02c6rt to be more   visually grounded.   3.3   Cyclical Training   We now describe each stage of our cyclical model, also illustrated in Figure 3.   Decoding. We \ufb01rst use the baseline model presented in Sec. 3.1 to generate   a sequence of words y = [yd   1, yd   2, ..., yd   T ], where T is the ground-truth sentence   length.   Localization. Following the decoding process, a localizer G is then learned to   localize the image regions from each generated word yt.   et = Weyd   t ,   zl   t,n = (Wlet)\u22a4rn   and   \u03b2t = softmax(zl   t),   (6)   where et is the embedding for the word generated during decoding stage at   step t, rn is the image representation of a region proposal, and We and Wl are   the learned parameters. Based on the localized weights \u03b2t, the localized region   representation can be obtained by \u02c6rl   t = R\u03b2. Our localizer essentially is a linear   layer, and we have experimented with non-linear layers but found it performed   worse (see Appendix for comparison).   Reconstruction. Finally, the shared language decoder Dl relies on the localized   region representation \u02c6rl   t to generate the next word. The probability over possible   output words is:   hL   t = LSTMLang([\u02c6rl   t; hA   t ]),   p(yl   t|yl   1:t\u22121) = softmax(WohL   t ),   (7)   Given the target ground truth caption y\u2217   1:T and our proposed captioning model   parameterized with \u03b8, we minimize the following cross-entropy losses:   LCE(\u03b8) = \u2212\u03bb1   T   X   t=1   log(p\u03b8(y\u2217   t |y\u2217   1:t\u22121))1(y\u2217   t =yd   t )   |   {z   }   decoding loss   \u2212\u03bb2   T   X   t=1   log(p\u03b8(y\u2217   t |y\u2217   1:t\u22121))1(y\u2217   t =yl   t)   |   {z   }   reconstruction loss   (8)   where \u03bb1 and \u03bb2 are weighting coe\ufb03cient selected on the validation split.   Note that before entering the proposed cyclical training regimen, the decoder   was \ufb01rst pre-trained until convergence to make sure the generated caption, which   used as inputs to the localizer, are reasonable. The cyclical training regimen   mainly serves as a regularization method.   4   Experiments   Datasets. We use the Flickr30k Entities image dataset [24] and the ActivityNetEntities video dataset [46] to provide a comparison with the state-of-the-art [46].   Flickr30k Entities contains 275k annotated bounding boxes from 31k images associated with natural language phrases. Each image is annotated with 5 crowdsourced captions. ActivityNet-Entities contains 15k videos with 158k spatially   annotated bounding boxes from 52k video segments.   8   C. Ma et al.   \ud835\udc63\"   \ud835\udc66$   \u210e$   &   \u210e$'(   &   \u210e$'(   )   \u210e$   )   \u210e$'(   &   \ud835\udc4a+\ud835\udc66$'(   \ud835\udc5f(\ud835\udc5f\ud835\udc5f.   \ud835\udefc$   \u2026   Soft-attention   Switch   (decoding or recon.)   \ud835\udc4a+\ud835\udc66$   \u210e$   )   Localizer   Attention    LSTM   Language    LSTM   Logit   Localization & Recon. stage   Decoding stage   Localizer   Localizer   \ud835\udc4a+\ud835\udc66$   Fig. 3: Proposed model architecture (left) and how the model operates during   decoding, localization, and reconstruction stages (right). During the decoding   stage, the soft-attention module uses the hidden state of the Attention LSTM   to compute attention weights on image regions. During the localization and   reconstruction stage, the soft-attention module instead uses the generated word   from decoding stage to compute attention weights on image regions.   4.1   Evaluation Metrics   Captioning evaluation metrics. We measure captioning performance using   common language metrics, i.e., BLEU [22], METEOR [3], CIDEr [35], and   SPICE [1].   Grounding evaluation metrics. Following the grounding evaluation from   GVD [46], we measure the attention accuracy on generated sentences, denoted   by F1all and F1loc. In F1all, a region prediction is considered correct if the object   word2 is correctly predicted and also correctly localized. We also compute F1loc,   which only considers correctly-predicted object words. Please see illustration of   the grounding metrics in Appendix.   In the original formulation, the precision and recall for the two F1 metrics are   computed for each object class, and it is set to zero if an object class has never   been predicted. The scores are computed for each object class and averaged over   the total number of classes. Such metrics are extremely stringent as captioning   models are generally biased toward certain words in the vocabulary, given the   long-tailed distribution of words. In fact, both the baseline and proposed method   only generate about 45% of the annotated object words within the val set in   Flickr30k Entities. The grounding accuracy of the other 55% of the classes are   therefore zero, making the averaged grounding accuracy seemingly low.   Measuring grounding per generated sentence. Instead of evaluating grounding on each object class (which might be less intuitive), we include a new grounding evaluation metric per sentence to directly re\ufb02ect the grounding measurement   of each generated sentence. The metrics are computed against a pool of object   words and their ground-truth bounding boxes (GT bbox) collected across \ufb01ve   2 The object words are words in the sentences that are annotated with corresponding   image regions.   Generate Grounded Visual Captions without Localization Supervision   9   Grounding   Captioning Evaluation   Grounding Evaluation   Method   supervision B@1 B@4   M   C   S   F1all   F1loc   F1all per sent   F1loc per sent   ATT-FCN [43]   64.7 19.9 18.5   NBT [20]   69.0 27.1 21.7 57.5 15.6   Up-Down [2]   69.4 27.3 21.7 56.6 16.0   4.14   12.3   GVD (w/o SelfAttn) [46]   69.2 26.9 22.1 60.1 16.1   3.97   11.6   GVD [46]   \u2713   69.9 27.3 22.5 62.3 16.5   7.77   22.2   Baseline*   \u2713   69.0 26.8 22.4 61.1 16.8 8.44 (+100%) 22.78 (+100%) 27.37 (+100%) 63.19 (+100%)   Baseline*   69.1 26.0 22.1 59.6 16.3   4.08 (+0%)   11.83 (+0%)   13.20 (+0%)   31.83 (+0%)   Cyclical*   69.9 27.4 22.3 61.4 16.6 4.98 (+21%) 13.53 (+16%) 15.03 (+13%) 35.54 (+12%)   Table 1: Performance comparison on the Flickr30k Entities test set. *: our   results are averaged across \ufb01ve runs. Only numbers reported by multiple runs   are considered to be bolded3.   GT captions on Flickr30k Entities (and one GT caption on ActivityNet-Entities).   We use the same Precall, Recall, Precloc, and Recloc as de\ufb01ned previously, but   their scores are averaged on each of the generated sentence. As a result, the   F1loc per sent measures the F1 score only on the generated words. The model will   not be punished if some object words are not generated, but it also needs to   maintain diversity to achieve high captioning performance.   4.2   Implementation and Training Details   Region proposal and spatial features. Following GVD [46], we extracted   100 region proposals from each image (video frame) and encode them via the   grounding-aware region encoding.   Training. We train the model with ADAM optimizer [16]. The initial learning rate is set to 1e \u22124. Learning rates automatically drop by 10x when the   CIDEr score is saturated. The batch size is 32 for Flickr30k Entities and 96   for ActivityNet-Entities. We learn the word embedding layer from scratch for   fair comparisons with existing work [46]. Please see the Appendix for additional   training and implementation details.   4.3   Captioning and Grounding Performance Comparison   Flickr30k Entities. We \ufb01rst compare the proposed method with our baseline   with or without grounding supervision on the Flickr30k Entities test set (see   Table 1). To train the supervised baseline, we train the attention mechanism   as well as add the region classi\ufb01cation task using the ground-truth grounding   annotation, similar to GVD [46]. We train the proposed baselines and our method   on the training set and choose the best performing checkpoints based on their   CIDEr score on the val set. Unlike previous work, our experimental results are   reported by averaging across \ufb01ve runs on the test set. We report only the mean   of the \ufb01ve runs to keep the table uncluttered.   3 Note that the since supervised methods are used as upper bound, their numbers are   not bolded.   10   C. Ma et al.   Grounding   Captioning Evaluation   Grounding Evaluation   Method   supervision B@1 B@4   M   C   S   F1all   F1loc   F1all per sent   F1loc per sent   GVD [46]   23.0 2.27 10.7 44.6 13.8   0.28   1.13   GVD (w/o SelfAttn) [46]   23.2 2.28 10.9 45.6 15.0   3.70   12.7   GVD [46]   \u2713   23.9 2.59 11.2 47.5 15.1   7.11   24.1   Baseline*   \u2713   23.1 2.13 10.7 45.0 14.6 7.30 (+100%) 25.02 (+100%) 17.88 (+100%) 60.23 (+100%)   Baseline*   23.2 2.22 10.8 45.9 15.1   3.75 (+0%)   12.00 (+0%)   9.41 (+0%)   31.68 (+0%)   Cyclical*   23.7 2.45 11.1 46.4 14.8 4.71 (+26%) 15.84 (+29%) 11.73 (+38%) 41.56 (+43%)   Table 2: Performance comparison on the ActivityNet-Entities val set. *: our   results are averaged across \ufb01ve runs. Only numbers reported by multiple runs   are considered to be bolded.   When compared to the existing state of the arts, our proposed baselines   achieve comparable captioning evaluation performances and better grounding   accuracy. Using the resulting supervised baseline as the upper bound, our proposed method with cyclical training statistically achieves around 15 to 20% relative grounding accuracy improvements for both F1all and F1loc and 10 to 15%   for F1all per sent and F1loc per sent without utilizing any grounding annotations   or additional computation during inference.   ActivityNet-Entities. We adapt our proposed baselines and method to the   ActivityNet-Entities video dataset (see Table 2 for results on the validation set   and Appendix for results on the test set). We can see that our baseline again   achieved comparable performance to the state of the arts. The proposed method   then signi\ufb01cantly improved the grounding accuracy around 25% to 30% relative   grounding accuracy improvements for both F1all and F1loc and around 40%   for F1all per sent and F1loc per sent. Interestingly, we observed that baseline with   grounding supervision does not improve the captioning accuracy, di\ufb00erent from   the observation in previous work [46].   4.4   Quantitative Analysis   Are localized image regions better than attended image regions during   training? Given our intuition described in Sec. 3, we expect the decoder to   be regularized to update its attention mechanism to match with the localized   image regions \u02c6rt 7\u2192\u02c6rl   t during training. This indicates that the localized image   regions should be more accurate than the attended image regions by the decoder   and drives the update on attention mechanism. To verify this, we compute the   attention accuracy for both decoder and localizer over ground-truth sentences   following [27,47]. The attention accuracy for localizer is 20.4% and is higher than   the 19.3% from the decoder at the end of training, which con\ufb01rms our hypothesis   on how cyclical training helps the captioning model to be more grounded.   Grounding performance when using a better object detector. In Table 1   and 2 we showed that our proposed method signi\ufb01cantly improved the grounding accuracy for both image and video captioning. These experimental settings   follow the widely used procedure for visual captioning systems: extract regional   proposal features and generate visual captions by attending to those extracted   Generate Grounded Visual Captions without Localization Supervision   11   Grounding Captioning Eval. Grounding Eval.   #   supervision   M   C   S   F1all   F1loc   F1loc per sent   Unrealistically perfect object detector   Baseline   \u2713   25.3 76.5   22.3   23.19   52.83   90.76   Baseline   25.2 76.3   22.0   20.82   48.74   77.81   Cyclical   25.8 80.2 22.7 25.27   54.54   81.56   Grounding-biased object detector   Baseline   \u2713   21.3 53.3   15.5   8.23   23.95   66.96   Baseline   21.2 52.4 15.4   5.95   17.51   42.84   Cyclical   21.2 52.0   15.4   6.87   19.65   50.25   Table 3: Comparison when using better   object detector on Flickr30k Entities test   set (see Appendix for complete version).   Fig. 4: Average F1all-score per class   as a function of class frequency.   visual features. One might ask, what if we have a better object detector that   can extract robust visual representation that are better aligned with the word   embeddings? Will visual grounding still an issue for captioning?   To answer this, we ran two sets of experiments (Table 3): (1) Perfect object detector: we replace the ROIs by ground-truth bbox and represent the new   ROIs by learning embedding features directly from ground-truth object words   associated with each ground-truth bbox. This experiment gives an estimate of   the captioning and grounding performance if we have (almost) perfect ROI representations (though unrealistic). We can see that the fully-supervised method   achieves an F1all of only 23%, which further con\ufb01rms the di\ufb03culty of the metric and the necessity of our grounding metric on a per sentence level (note that   F1loc per sent shows 90%). We can also see that baseline (unsup.) still leaves room   for improvement on grounding performance. Surprisingly, our method improved   both captioning and grounding accuracy and surpasses the fully-supervised baseline except on the F1loc per sent. We \ufb01nd that it is because the baseline (sup.)   over\ufb01ts to the training set, while ours is regularized from the cyclical training.   Also, our generated object words are more diverse, which is important for F1all   and F1loc. (2) Grounding-biased object detector: we extract ROI features   from an object detector pre-trained on Flickr30k. Thus, the ROI features and   their associated object predictions are biased toward the annotated object words   but do not generalize to predict diverse captions compared to the original object   detector trained from Visual Genome, resulting in lower captioning performance.   We can see that our proposed method still successfully improves grounding and   maintains captioning performance in this experiment setting as well.   How does the number of annotations a\ufb00ect grounding performance? In   Figure 4, we present the average F1-score on the Flickr30k Entities val set when   grouping classes according to their frequency of appearance in the training set4.   We see that, unsurprisingly, the largest di\ufb00erence in grounding accuracy between   the supervised and our proposed cyclical training is for the 50 most frequently   appearing object classes, where enough training data exists. As the number of   4 We group the 460 object classes in 10 groups, sorted by the number of annotated   bounding boxes.   12   C. Ma et al.   Captioning Eval. Grounding Eval.   #   M   C   S   F1all   F1loc   Baseline (Unsup.)   22.3 62.1   16.0   4.18   11.9   Cyclical   22.2 62.2 16.2 5.63   14.6   - Attention consistency 22.3 61.8 16.2 4.19   11.3   - Localizer using hA   22.2 61.8   16.1   4.58   11.3   Table 4: Model ablation study on the   Flickr30k Entities val set.   Human Grounding Eval.   Method   %   About equal   47.1   Cyclical is better   28.1   Baseline is better   24.8   Table 5: Human evaluation on grounding on the Flickr30k Entities val set.   annotated boxes decreases, however, the di\ufb00erence in performance diminishes,   and cyclical training appears to be more robust. Overall, we see that the supervised method is biased towards frequently appearing objects, while grounding   performance for the proposed approach is more balanced among classes.   Should we explicitly make attended image regions to be similar to   localized image regions? One possible way to regularize the attention mechanism of the decoder is to explicitly optimize \u02c6rt 7\u2192\u02c6rl   t via KL divergence over   two soft-attention weights \u03b1t and \u03b2t. The experimental results are shown in   Table 4 (Attention consistency). We use a single run unsupervised baseline with   a \ufb01x random seed as baseline model for ablation study. We can see that when   explicitly forcing the attended regions to be similar to the localized regions, both   the captioning performance and the grounding accuracy remain similar to the   baseline (unsup.). We conjecture that this is due to the noisy localized regions   at the initial training stage. When forcing the attended regions to be similar   to noisy localized regions, the Language LSTM will eventually learn to not rely   on the attended region at each step for generating sequence of words. To verify,   we increase the weight for attention consistency loss and observed that it has   lower grounding accuracy (F1all = 3.2), but the captioning will reach similar   performance while taking 1.5x longer to reach convergence.   Is using only the generated word for localization necessary? Our proposed localizer (Eq. 6 and Figure 3) relies on purely the word embedding representation to locate the image regions. This forces the localizer to rely only on the   word embedding without biasing it with the memorized information from the   Attention LSTM. As shown in the Table 4 (localizer using hA), although this   achieves comparable captioning performance, it has lower grounding accuracy   improvement compared to our proposed method.   4.5   Human Evaluation on Grounding   We conduct a human evaluation on the perceptual quality of the grounding.   We asked 10 human subjects (not familiar with the proposed method) to pick   the best among two grounded regions (by baseline and Cyclical) for each word.   The subjects have three options to choose from: 1) grounded region A is better,   2) grounded region B is better, and 3) they are about the same (see Appendix   for illustration). Each of the human subjects were given 25 images, each with a   varying number of groundable words. Each image was presented to two di\ufb00erent   human subjects in order to be able to measure inter-rater agreement. For this   Generate Grounded Visual Captions without Localization Supervision   13   Baseline: A young girl in a    blue coat is sitting in the    snow.   girl   coat   snow   Proposed: A young girl wearing a    winter hat and a purple coat is    smiling at the camera.   girl   hat   coat   camera   Proposed: A group of men in white    uniforms are standing in a field with    a crowd watching.    men   uniforms   field   men   Baseline: A group of people are    watching a game.   people   Baseline: Four skiers are skiing    down a snowy mountain.   skiers   mountain   Proposed: A skier is jumping over a    snowy hill while other skiers watch.   skiers   hill   skiers   Baseline: A white horse is    jumping over an obstacle.   horse   obstacle   Proposed: A white horse with a    rider in a blue helmet and white    shirt jumping over a hurtle.   horse   rider   helmet   shirt   hurtle   Fig. 5: Generated captions and corresponding visual grounding regions with comparison between baseline (left) and proposed approach (right). Our proposed   method is able to generate more descriptive sentences while selecting the correct   regions for generating the corresponding words.   study, we de\ufb01ne a word to be groundable if it is either a noun or verb. The image   regions are selected based on the region with the maximum attention weight in   \u03b1t for each word. The order of approaches was randomized for each sentence.   Our experiment on the Flickr30k Entities val set is shown in Table 5: 28.1%   of words are more grounded by Cyclical, 24.8% of words are more grounded by   baseline, and 47.1% of words are similarly grounded. We also measured interrater agreement between each pair of human subjects: 72.7% of ratings are the   same, 4.9% of ratings are the opposite, and 22.4% of ratings could be ambiguous   (e.g., one chose A is better, the other chose they are about the same).   We would also like to note that the grounded words judged to be similar   largely consisted of very easy or impossible cases. For example, words like mountain, water, street, etc, are typically rated to be \u201cabout the same\u201d since they   usually have many possible boxes and is very easy for both models to ground   the words correctly. On the other hand, for visually ungroundable cases, e.g.,   stand appears a lot and the subject would choose about the same since the image   does not cover the fact that the person\u2019s feet are on the ground. We see that   the human study results follow the grounding results presented in the paper and   show an improvement in grounding accuracy for the proposed method over a   strong baseline. The improvement is achieved without grounding annotations or   extra computation at test time.   4.6   Qualitative Analysis   We additionally present some qualitative results comparing the baseline (Unsup.)   and the proposed method in Figure 5. To avoid clutter, only the visually-relevant   object words in the generated sentences are highlighted with colors. Each highlighted word has a corresponding image region annotated on the original image.   14   C. Ma et al.   A man sits on a chair in front of    a lake.   A man in a red shirt is standing    on a wooden platform.   A girl in a purple sweater is    jumping on rocks.   A man in a yellow jacket and blue    helmet riding a bike.   An Asian woman is holding a red    umbrella and walking down the    sidewalk.   A man in a black shirt is holding    up a flag.   A young girl in a pink shirt and jeans   is walking down a brick wall.   A man in an orange shirt and a    hat is standing next to a blue wall.   man   chair   lake   man   platform   man   jacket   helmet   bike   man   shirt   hat   wall   girl   sweater   rocks   umbrella   sidewalk   man   shirt   girl   shirt   jeans   wall   shirt   woman   flag   Fig. 6: Examples with correct grounding (top) as well as failure cases (bottom).   The image regions are selected based on the region with the maximum attention   weight in \u03b1t. We can see that our proposed method signi\ufb01cantly outperforms   the baseline (Unsup.) in terms of both the quality of the generated sentence   and grounding accuracy. In Figure 6, we show a number of correct and incorrect examples of our proposed method. We observe that while the model is able   to generate grounded captions for the images, it may sometimes overlook the   semantic meaning of the generated sentences (e.g., \u201cA young girl [...] walking   down a brick wall\u201d), or the spatial relationship between the objects (\u201cA man   [...] is holding up a \ufb02ag\u201d).   5   Conclusion   Working from the intuition that typical attentional mechanisms in the visual   captioning task are not forced to ground generated words since recurrent models can propagate past information, we devise a novel cyclical training regime   to explicitly force the model to ground each word without grounding annotations. Our method only adds a fully-connected layer during training, which can   be removed during inference, and we show thorough quantitative and qualitative results demonstrating around 20% or 30% relative improvements in visual   grounding accuracy over existing methods for image and video captioning tasks.   Acknowledgments   Chih-Yao Ma and Zsolt Kira were partly supported by DARPA\u2019s Lifelong Learning Machines (L2M) program, under Cooperative Agreement HR0011-18-2-0019,   as part of their a\ufb03liation with Georgia Tech. We thank Chia-Jung Hsu for her   valuable and artistic help on the \ufb01gures.   Generate Grounded Visual Captions without Localization Supervision   15", "conf": "ECCV", "year": "2020", "index": 71}, {"title": "VideoGrounding-DINO: Towards Open-Vocabulary Spatio-Temporal Video   Grounding   Syed Talal Wasim1   Muzammal Naseer1   Salman Khan1,2   Ming-Hsuan Yang3,4   Fahad Shahbaz Khan1,5   1Mohamed bin Zayed University of AI   2Australian National University   3University of California, Merced   4Google Research   5Link\u00a8oping University", "abstract": "Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query.   This paper addresses a critical limitation in current   video grounding methodologies by introducing an OpenVocabulary Spatio-Temporal Video Grounding task. Unlike   prevalent closed-set approaches that struggle with openvocabulary scenarios due to limited training data and predefined vocabularies, our model leverages pre-trained representations from foundational spatial grounding models.   This empowers it to effectively bridge the semantic gap between natural language and diverse visual content, achieving strong performance in closed-set and open-vocabulary   settings. Our contributions include a novel spatio-temporal   video grounding model, surpassing state-of-the-art results   in closed-set evaluations on multiple datasets and demonstrating superior performance in open-vocabulary scenarios. Notably, the proposed model outperforms state-of-theart methods in closed-set settings on VidSTG (Declarative   and Interrogative) and HC-STVG (V1 and V2) datasets.   Furthermore, in open-vocabulary evaluations on HC-STVG   V1 and YouCook-Interactions, our model surpasses the recent best-performing models by 4.88 m vIoU and 1.83% accuracy, demonstrating its efficacy in handling diverse linguistic and visual concepts for improved video understanding. Our codes will be publicly released.   1.", "content": "Spatio-temporal video grounding is pivotal in linking visual   content with natural language descriptions, thus facilitating semantics interpretation within visual data. Prevailing   approaches in video grounding such as TubeDETR [28],   STCAT [9], and STVGFormer [11] focus mainly on supervised closed-set settings, where models are trained on   specific datasets [24, 32] with predefined vocabulary and   meticulously annotated data. While these current state-ofFigure 1. Performance comparison on conventional closed-set   and open-vocabulary settings for the video grounding task. We   compare our approach with TubeDETR [28] and STCAT [9] in   supervised setting for VidSTG [32] declarative/interrogative and   HC-STVG V1 [24], along with open-vocabulary evaluation on   HC-STVG V1 and YouCook-Interactions [23] datasets.   the-art methods excel in closed-set settings on datasets like   VidSTG [32] and HC-STVG [24], their limited generalization beyond training dataset distributions poses a significant   challenge. The relatively small scale and restricted sample   variety in existing video datasets hinder models from adapting to unseen scenarios effectively.   Motivated by the inherent limitation of supervised   closed-set approaches in terms of their restricted vocabulary, this paper investigates Open-Vocabulary SpatioTemporal Video Grounding. Unlike conventional methodologies, this paradigm addresses challenges posed by the   unrestricted diversity of language and visual concepts in   the wild. The goal is to train on a set of base categories   and generalize to unseen objects/actions based on the openvocabulary nature of backbone models.   This paper exThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   18909   plores the challenges and opportunities inherent in openvocabulary video grounding, laying the groundwork for   more robust and versatile video understanding.   However, training an effective open-vocabulary video   grounding model would require a large enough dataset with   a rich set of natural language expressions and corresponding   spatio-temporal localizations. Such an extensive dataset can   allow the model to learn generalized visual and textual representations and handle out-of-distribution samples. However, video grounding datasets [24, 32] are quite limited   in scale, e.g., VidSTG has only 5.4k training videos with   80.6k distinct sentences. In contrast, an image grounding   model GLIP [10] is trained with \u223c26.5M image-text pairs.   Therefore, our research explores the following fundamental question: How spatio-temporal video grounding models can achieve robust performance in both Closed-Set and   Open-Vocabulary scenarios without requiring large-scale   video annotations, ensuring effective generalization beyond   training datasets?   In addressing this question, we find inspiration in the accomplishments of foundational models specializing in spatial grounding [4, 8, 12, 14, 29]. These models undergo   training on an extensive corpus of image-text data, enabling   effective generalization to samples from a given target distribution. We aim to harness this pretrained representation   to enhance our video grounding model. Our proposed solution is a spatio-temporal video grounding model adopting a   DETR-like architecture enhanced by temporal aggregation   modules. The spatial modules are initialized using the pretrained representation of a foundational image model [12].   Meanwhile, the image and text feature extractors remain   frozen while the video-specific spatio-temporal adaptations   are modeled via learnable adapter blocks. This approach is   designed to preserve the nuanced representation of the foundational model, enhancing our model\u2019s ability to generalize   effectively to novel samples.   A summary of our closed-set and open-vocabulary results is shown in Fig. 1, where the proposed approach excels   in both settings by a clear margin. Our major contributions   are summarized as follows:   \u2022 For the first time, we evaluate spatio-temporal video   grounding models in an open-vocabulary setting on HCSTVG V1 [24] and YouCook-Interactions [23] benchmarks in a zero-shot manner. We outperform state-ofthe-art methods TubeDETR [28] and STCAT [9] by 4.26   m vIoU and 1.83% accuracy, respectively.   \u2022 By combining the strengths of spatial grounding models with complementary video-specific adapters, our approach consistently outperforms the previous state-ofthe-art in closed-set setting on four benchmarks, i.e.,   VidSTG (Declarative) [32], VidSTG (Interrogative) [32],   HC-STVG V1 [24] and HC-STVG V2 [24].   2. Related Work   Spatial Grounding Foundation Models: Recent literature   introduces notable spatial grounding models. GLIP [10]   unifies object detection and phrase grounding through a   language-image pre-training model, leveraging extensive   image-text pairs for semantic-rich representations. Grounding DINO [12] integrates language with a transformerbased detector to achieve an open-set detector, excelling   in benchmarks like COCO and ODinW. Kosmos-1 [8]   and Kosmos-2 [14] contribute Multimodal Large Language   Models (MLLMs) with capabilities such as zero-shot and   few-shot learning, language understanding, and multimodal   tasks. Kosmos-2 [14] specifically integrates grounding into   downstream applications, introducing GrIT, a large-scale   dataset of Grounded Image-Text pairs. Shikra [4] addresses   referential ability in MLLMs by handling spatial coordinates in inputs and outputs, showcasing promising performance in various vision-language tasks. Ferret [29] unifies   referring and grounding in the LLM paradigm, achieving   superior performance in classical referring and grounding   tasks, excelling in region-based multimodal chatting and   image description. Recently, GLaMM [16] allows pixellevel grounded conversations with an LLM, showcasing   generalizability to several captioning and referring segmentation tasks.   However, spatial methods cannot work for   grounding objects in videos, a gap addressed by this work.   Spatio-Temporal Video Grounding:   Several methods   tackle the challenge of localizing objects in untrimmed   videos based on query sentences. STVGBert [21] presents   a one-stage visual-linguistic transformer for simultaneous   spatial and temporal localization. TubeDETR [28] introduces a transformer-based architecture to model temporal, spatial, and multi-modal interactions efficiently. Augmented 2D-TAN [22] adopts a two-stage approach, enhancing the 2D-TAN with a temporal context-aware Bi-LSTM   Aggregation Module.   OMRN [33] addresses the challenge of unaligned data and multi-form sentences in spatiotemporal video grounding, proposing an object-aware   multi-branch relation network for effective relation discovery. MMN [26] introduces a Mutual Matching Network as a   metric-learning framework for temporal grounding, achieving competitive performance. STCAT [9] is an end-to-end   one-stage framework addressing feature alignment and prediction inconsistency. Finally, STVGFormer [11] proposes   an effective framework with static and dynamic branches   for cross-modal understanding. While the above methods   advance video grounding, their generalization to out-ofdistribution and open-vocabulary samples is limited due to   constrained video datasets [24, 32]. To address this issue,   we utilize the generalized representation of spatial grounding foundation models [4, 8, 10, 12, 14, 29] trained on a   large corpus of image-text data and can perform well on   both closed-set and open-vocabulary evaluations.   18910   3. Methodology   As discussed above, the current state-of-the-art spatiotemporal video grounding methods [9, 11, 21, 22, 24, 26,   28, 30, 32, 33] primarily evaluate in a supervised setting on   the VidSTG [32] and HC-STVG [24] datasets. However,   these methods lack the multimodal spatio-temporal understanding required to perform well on out-of-distribution   samples [2].   Therefore, this work aims to achieve improved open-vocabulary performance while maintaining   strong closed-set video-grounding performance.   We take inspiration from recent foundation models for   spatial grounding [4, 8, 10, 12, 14, 29]. These models are   trained on a large corpus of visual-textual data and hence,   generalize well to unseen samples.   We aim to leverage   the strong generalization capabilities of such foundation   models to achieve strong open-set spatio-temporal video   grounding performance.   Our proposed spatio-temporal   video grounding method uses DETR-like [1] design, with   temporal aggregation and adaptation modules for learning   video-specific representations.   Below, we explain our proposed methodology. We formally define the spatio-temporal video grounding problem   in Sec. 3.1.   We then explain our architecture details in   Sec. 3.2. We finally explain the loss formulation used to   train the model and model initialization in Sec. 3.3.   3.1. Problem Definition   The spatio-temporal video grounding task involves localizing and recognizing objects and actions in a video sequence   by integrating spatial and temporal information. In contrast   to spatial grounding, which focuses on recognizing and localizing objects or actions within individual frames, spatiotemporal grounding extends this concept to include the temporal dimension. This means understanding where objects   or actions are in each frame and how they evolve and move   over time.   Consider a video V   \u2208RT \u00d7H\u00d7W \u00d7C with T frames,   H \u00d7 W spatial resolution, and C channels, respectively,   along with a text prompt P. The spatial grounding problem   can be defined as the localization of one or more objects   associated with the prompt P in a frame Vt, t \u2208{1, ..., T}   using a bounding box Bt   i = (xt   i, yt   i, wt   i, ht   i), where xt   i and   yt   i are the coordinates of the top-left corner, wt   i and ht   i are   the width and height of the bounding box, i \u2208{1, ..., N} is   the object number for the frame t. The temporal grounding problem, on the other hand, involves understanding   how objects or actions evolve over time.   It aims at localizing the temporal interval (ts, te) where the specific   action/interaction happens in the entire temporal duration,   where interval (ts, te) indicates the start and end frame of   the object occurrence within the total frames T (1 \u2264ts <   te \u2264T). Hence, the spatio-temporal grounding problem for   object i associated with prompt P can be summarized as a   set of spatio-temporal coordinates associated with the subset of frames where the object exists: (xt   i, yt   i, wt   i, ht   i, t) and   t \u2208{ts, ..., te}. The interval (ts, te)|{1 \u2264ts < te \u2264T}   and is a subset of the total frames T.   3.2. Spatio-Temporal Video Grounding   Here, we explain our video grounding model in Fig. 2. As   discussed earlier, we aim to design a spatio-temporal video   grounding model that can perform well in closed-set and   open-vocabulary settings. Strong open-vocabulary performance requires learning a rich visual/textual representation,   which in turn requires a large amount of training data. Unfortunately, spatio-temporal video grounding datasets are   quite limited in scale [24, 32], resulting in current video   grounding methods failing in generalizing well to out-ofdistribution samples because they lack the requisite strong   visual/textual representation.   To solve this problem, our approach takes inspiration   from recent spatial grounding methods [4, 8, 12, 14, 29],   which have strong open-vocabulary performance thanks   to the large image-text corpus they are trained on.   We   can utilize the generalized representations of these models to enrich the weaker representation of video-grounding   approaches obtained from the limited number of training   samples. Our approach aims to leverage the strong pretrained representations of spatial grounding methods to   achieve strong closed-set supervised and open-vocabulary   video grounding performance. Our spatio-temporal video   grounding approach is based on the state-of-the-art DETRbased [1] object detection framework DINO [31] and also   borrows concepts of image-text alignment and grounding   from Grounded Language-Image Pre-training (GLIP) [10]   and Grounding DINO [12].   We extract initial features   from backbone vision and text encoders \u03b8v and \u03b8p. Following that, we model inter-frame and intra-frame features   and learn cross-modal visual/textual relations in the CrossModality Spatio-Temporal Encoder (Sec. 3.2.1). The result   enriched cross-modal features are used to initialize queries   for each frame (Sec. 3.2.2).   These queries are then decoded to predict the bounding boxes per frame and the temporal grounding start/end frame by aggregating information across spatial/temporal dimensions and injecting information through cross-attention from enriched visual/textual   context (Sec. 3.2.3).   Given the video V and text prompt P as described above,   we obtain per-frame features F 0   v and text features F 0   p from   vision and text encoders, \u03b8v and \u03b8p, respectively. The vision encoder is based on a Swin Transformer [13], and   the text-encoder is defined as a BERT [6] model.   Like   other DETR-based detectors [31, 35], image features are   extracted from different vision-encoder blocks at multiple   scales. The features are then passed to the Cross-Modality   Spatio-Temporal Encoder.   18911   Figure 2. Overall architecture: We present our video grounding architecture. It consists of vision and text encoders that produce   visual and textual features. A cross-modality spatio-temporal encoder which fuses information across spatial/temporal dimensions and   visual/textual modalities. A language guided query selction module to initialize cross-modal queries. A cross-modality spatio-temporal   decoder to decoder queries while fusing information from visual/textual features. And finally two prediction heads to predict the bounding   boxes per frame and the temporal tube. Modules with ( ) are trainable and those with (   ) are frozen.   3.2.1   Cross-Modality Spatio-Temporal Encoder   The initial vision and text features F 0   v and F 0   p neither   contain any cross-modal information nor model the temporal relationship across frames.   Therefore, we further   encode the initial features through the Cross-Modality   Spatio-Temporal Encoder to model the temporal information across frames and learn cross-modal features.   Each layer of the M layer encoder first applies a MultiHead Self-Attention (MHSA) [25] to the visual features Fv   along the temporal dimension, followed by a Deformable   Attention (DA) [35] along the spatial dimension.   This   is done to model relations within frames and temporally   across frames. Similarly, we apply a MHSA on the text   features Fp. This is illustrated in Eq. 1.   F m\u2032   v   = DAm   spatial(MHSAm   temporal(F m\u22121   v   )),   F m\u2032   p   = MHSAm   p (F m\u22121   p   ),   (1)   where F m\u22121   v   and F m\u22121   p   are visual and textual input features to layer m, F m\u2032   v   and F m\u2032   p   are the intermediate visual and textual feature representation, and DAm   spatial,   MHSAm   temporal and MHSAm   p are the spatial-deformable,   temporal and textual attentions at layer m \u2208{1, ..., M},   respectively. Following initial spatial, temporal, and textual attentions, we fuse features across the visual and textual   modalities, as done in GLIP [10].   More specifically, we calculate the joint visual-textual   attention, Attnm   joint, using projected intermediate features   F m\u2032   v   and F m\u2032   p . This attention is then used alongside the   intermediate features F m\u2032   v   and F m\u2032   p , to calculate the imageto-text and text-to-image cross-attentions as shown in Eq. 2   and Eq. 3.   Attnm   joint =       projm   q,v(F m\u2032   v )projm   q,p(F m\u2032   p )T   \u221a   dk   !   ,   (2)   where projm   q,v and projm   q,p are the query projections for the   visual and textual features, respectively, at layer m.   F m   v = FFNm   v (softmax(Attnm   joint)projm   p (F m\u2032   p ))),   F m   p = FFNm   p (softmax(Attnm   joint   T )projm   v (F m\u2032   v ))),   (3)   where F m   v and F m   p are the final output features at layer m   of the encoder, FFNm   v and FFNm   v are the visual and textual   Feed Forward Networks (FFN) and projm   v and projm   p are   the linear layers to project the visual and textual features.   The final encoded features at layer m = M are then utilized   to initialize cross-modal queries.   3.2.2   Language-Guided Query Selection   This module is designed to select features more relevant   to the input text as decoder queries for effective languagevision fusion. We combine a DETR/DINO [1, 31] style   queries with a sinusoidal temporal positional encoding to   the positional part of the queries. The sinusoidal positional   encoding added to the positional part of the queries adds   important contextual information regarding the sequence   of frames, allowing for improved temporal correlation and   18912   grounding [28]. The query selection module takes the encoder\u2019s visual and textual features as input and outputs   num query indices that correspond to the most relevant   features for object detection per frame, {Q0   t}T   t=1, where Q0   t   are the initial queries for the frame t. The module initializes the decoder queries using a combination of the selected   indices and dynamic anchor boxes. The content part of the   queries is set to be learnable during training, while the positional part is computed using the dynamic anchor boxes   initialized using the encoder outputs. We also add a sinusoidal temporal positional encoding to the positional part of   the queries.   3.2.3   Cross-Modality Spatio-Temporal Decoder   To decode the above queries into bounding box locations   and temporal start/end tubes, we need to transform them   into an output embedding, which can then be fed into prediction heads. The decoder allows for the queries to interact   globally with others within a frame and across frames while   utilizing the entire visual and textual features as context.   Formally, the queries produced earlier are fed into a N layer   decoder. Each layer starts with a temporal self-attention,   a spatial self-attention followed by a visual cross-attention   and textual cross-attention, and finally an FFN. This is represented in Eq. 4.   Qn\u2032   t = MHSAn   spatial(MHSAn   temporal(Qn\u22121   t   )),   Qn   t = FFNn(CAn   p(CAn   v(Qn\u2032   t , F M   v ), F M   p )),   (4)   where Qn\u22121   t   are the input queries at layer n \u2208{1, ..., N},   Qn\u2032   t   are the intermediate queries after spatial and temporal attention at layer n, Qn\u2032   t are the output queries at layer   n, and CAn   v and CAn   p are the visual and textual crossattentions at layer n. The cross-attentions are further elaborated in Eq. 5.   CAn   v (Qn\u2032   t , F M   v ) =       projn   q,v(Qn\u2032   t )projn   k,v(F M   v )T   \u221a   dk   projn   v (F M   v )T   !   ,   CAn   p(CAn   v , F M   p ) =       projn   q,p(CAn   v )projn   k,p(F M   p )T   \u221a   dk   projn   p (F M   p )T   !   ,   (5)   where projn   q,v, projn   k,v and projn   v are the visual query, key   and values projection for layer n and projn   q,p, projn   k,p and   projn   p are the textual query, key and value projections. The   final queries from the decoder at layer N, {QN   t }T   t=1, are   then used for prediction.   3.2.4   Prediction Heads   The decoder outputs refined queries per frame {QN   t }T   t=1.   We follow the standard DETR-like bounding box regression head implemented as a Multi-Layer Perceptron (MLP),   which predicts bounding boxes Bt   i = (xt   i, yt   i, wt   i, ht   i), per   frame. To predict the temporal interval (ts, te)|{1 \u2264ts <   te \u2264T}, we add a temporal grounding head, implemented   as an MLP, alongside the bounding box regression head,   similar to existing works like [9, 28]. The new head predicts the probabilities of the start \u03c4s \u2208[0, 1]T and ends   \u03c4e \u2208[0, 1]T of the interval. During inference, the start and   end interval (ts, te)|{1 \u2264ts < te \u2264T} is computed by taking the maximum of the joint distribution of (\u03c4s, \u03c4e). Any   invalid combinations with te \u2264ts are masked out.   3.3. Loss Function   To leverage the generalized pre-trained representation from   spatial-grounding foundation models, we initialize all spatial modules and cross attentions from the Grounding   DINO [12] spatial grounding model. To preserve this generalized representation while ensuring effective modeling   of the downstream task, we freeze the Vision and Text Encoders \u03b8v and \u03b8p and fine-tune the remaining components.   During training, the model receives a batch of videos V   with text prompt P. The ground-truth annotation contains   the bounding box sequence {Bt   i}te   t=ts, and the corresponding start and end timestamps (ts, te). For spatial grounding, we follow the standard loss formulation used in DETRlike [1, 31, 35], namely the L1 loss, LL1, and the Generalized Intersection over Union (GIoU) [17] loss, LGIoU.   Formally, the spatial grounding loss, Lspatial is defined in   Eq. 6.   Lspatial = \u03bbL1LL1( \u02c6B, B) + \u03bbGIoULGIoU( \u02c6B, B).   (6)   For temporal grounding, we follow [18, 21, 28] and generate two 1-dimensional gaussian heatmaps \u03c0s, \u03c0e \u2208RT ,   for the starting and ending positions. The temporal grounding loss is therefore defined in Eq. 7 as,   Ltemporal = Ls   KL( \u02c6\u03c0s, \u03c0s) + Le   KL( \u02c6\u03c0e, \u03c0e),   (7)   where Ls   KL and Le   KL are the KL divergence losses for the   start and end distributions, respectively.   Note that the model outputs the bounding boxes and   starting/ending distributions during inference.   We determine the temporal grounding segment, (ts, te), by taking   the segment with the maximal joint start and end probability. Then, we consider the bounding boxes only within that   tube for spatial grounding.   4. Results   4.1. Experimental Setup and Protocols   Below, we first briefly explain the implementation details   (Sec. 4.1.1), followed by evaluation settings (Sec. 4.1.2),   and datasets (Sec. 4.1.3) used in our work.   18913   Table 1. Performance comparisons of the state-of-the-art on HC-STVG V1 [24] and YouCook-Interactions [23] in open-vocabulary setting.   Method   Pre-training   HC-STVG V1   YouCook-Interactions   m vIoU   vIoU@0.3   vIoU@0.5   Accuracy   TubeDETR (CVPR\u201922) [28]   VidSTG   16.84   22.32   9.22   51.63   STCAT (NeurIPS\u201922) [9]   VidSTG   22.58   32.14   20.83   55.90   VideoGrounding-DINO   VidSTG   27.46   40.13   29.92   57.73   4.1.1   Implementation Details   As discussed in the methodology (Sec. 3), we initialize   the spatial modules in our model from the Grounding   DINO [12] spatial grounding model and keep the vision and   text encoders frozen. Our prediction heads for both spatial   and temporal predictions are set to be 3-layer Multi-Layer   Perceptrons (MLPs). We sample 128 frames during training   and inference, resized to a resolution of 448 on the shorter   side. We set both M and N to 6, and train the model with a   batch size of 8 and learning rate of 1e\u22124, and weight decay   if 10\u22124. The number of epochs for VidSTG is set to 10, and   for HC-STVG V1/V2 is set to 90.   4.1.2   Evaluation Settings   We evaluate our video grounding model in two settings,   Open-Vocabulary and Closed-Set Supervised.   Open-Vocabulary Evaluation:   In the open-vocabulary   setting we train our model on the VidSTG [32] dataset and   then evaluate on two different datasets, HC-STVG V1 [24]   and YouCook-Interactions [23] to understand how well the   model generalizes to new distributions.   The reason for   choosing these two datasets is that the former provides a   relatively minor distribution shift given the similar perspective/objects in the videos compared to the training dataset   VidSTG. In contrast, the latter provides a major distribution shift with changes in perspective and annotated objects/interactions.   Closed-Set Supervised Evaluation:   In the supervised   evaluation setting, we train on the training set and evaluate each dataset\u2019s respective validation/testing set. This   evaluation is conducted for three majorly used datasets   in spatio-temporal video grounding, namely VidSTG [32],   HC-STVG V1 [24] and HC-STVG V2 [24].   4.1.3   Datasets   We evaluate our approach and compare against the stateof-the-art in two settings: Open-Vocabulary and ClosedSet Supervised, across a total of four grounding datasets,   namely:   VidSTG [32], HCSTVG V1 [24], HCSTVG   V2 [24], and YouCook-Interactions [23].   VidSTG: The VidSTG [32] dataset is derived from the VidOR [20] dataset, incorporating object relation annotations.   It includes 99,943 video-text pairs, encompassing 44,808   declarative sentence queries and 55,135 interrogative sentence queries. The training, validation, and test sets consist   of 80,684, 8,956, and 10,303 sentences and 5,436, 602, and   732 videos, respectively. VidSTG\u2019s text queries are confined to describing pre-defined object/relation categories in   VidOR [20].   HC-STVG V1/V2: The HC-STVG datasets are sourced   from movie scenes, each video clip spanning approximately   20 seconds.   These datasets pose challenges in spatiotemporal grounding due to video clips featuring multiple   individuals engaged in similar actions. HC-STVG V1 comprises 4,500 training and 1,160 testing video-text pairs. HCSTVG V2 expands HC-STVG V1, enhancing annotation   quality with 10,131, 2,000, and 4,413 samples for training,   validation, and testing, respectively. As HC-STVG V2\u2019s   test set annotations are unavailable publicly, results are reported on the validation set.   YouCook-Interactions:   The YouCook-Interactions [23]   dataset serves as an expansion of the YouCook2 [34] dataset   focused on cooking instructions. This extension includes   bounding boxes for 6,000 carefully chosen frames, typically encompassing the hand and the tool specified in the   corresponding sentence-level annotations. Our assessment   revolves around examining models\u2019 spatial grounding capabilities using this dataset.   4.2. Experimental Results and Analysis   In this section, we present our results across the evaluation mentioned above settings (Sec. 4.1.2) and datasets   (Sec. 4.1.3).   We start with the closed-set evaluation in   Sec. 4.2.2, followed by the open-vocabulary evaluation in   Sec. 4.2.1.   4.2.1   Open-Vocabulary Evaluation   For open-vocabulary evaluation, we train on VidSTG [32]   and present results HC-STVG V1 [24] and YouCookInteractions [23]. The results are reported jointly in Tab. 1.   Results on HC-STVG V1: We report open-vocabulary   evaluation on m vIoU, vIoU@0.3, and vIoU@0.5.   We   achieve state-of-the-art performance over both TubeDETR [28] and STCAT [9]. We attribute this strong performance to our design, which leverages the strong pre18914   Table 2. Performance comparisons of the state-of-the-art on the VidSTG [32] test set in closed-set supervised setting.   Method   Declarative Sentences   Interrogative Sentences   m tIoU   m vIoU   vIoU@0.3   vIoU@0.5   m tIoU   m vIoU vIoU@0.3   vIoU@0.5   Factorized:   GroundeR (ECCV\u201916) [19]+TALL (ICCV\u201917) [7]   34.63   9.78   11.04   4.09   33.73   9.32   11.39   3.24   STPR (ICCV\u201917) [27]+TALL (ICCV\u201917) [7]   10.40   12.38   4.27   9.98   11.74   4.36   WSSTG (arXiv\u201919) [5]+TALL (ICCV\u201917) [7]   11.36   14.63   5.91   10.65   13.90   5.32   GroundeR (ECCV\u201916) [19]+L-Net (AAAI\u201919) [3]   40.86   11.89   15.32   5.45   39.79   11.05   14.28   5.11   STPR (ICCV\u201917) [27]+L-Net (AAAI\u201919) [3]   12.93   16.27   5.68   11.94   14.73   5.27   WSSTG (arXiv\u201919) [5]+L-Net (AAAI\u201919) [3]   14.45   18.00   7.89   13.36   17.39   7.06   Two-Stage:   STGRN (CVPR\u201920) [32]   48.47   19.75   25.77   14.60   46.98   18.32   21.10   12.83   STGVT (TCSVT\u201921) [24]   21.62   29.80   18.94   OMRN (IJCAI\u201921) [33]   50.73   23.11   32.61   16.42   49.19   20.63   28.35   14.11   One-Stage:   STVGBert (ICCV\u201921) [21]   23.97   30.91   18.39   22.51   25.97   15.95   TubeDETR (CVPR\u201922) [28]   48.10   30.40   42.50   28.20   46.90   25.70   35.70   23.20   STCAT (NeurIPS\u201922) [9]   50.82   33.14   46.20   32.58   49.67   28.22   39.24   26.63   STVGFormer (CVPR\u201923) [11]   33.70   47.20   32.80   28.50   39.90   26.20   VideoGrounding-DINO   51.97   34.67   48.11   33.96   50.83   29.89   41.03   27.58   Table 3. Performance comparisons of the state-of-the-art on the   HC-STVG V1 [24] test set in closed-set supervised setting.   Methods   m vIoU vIoU@0.3   vIoU@0.5   STGVT (TCSVT\u201921) [24]   18.15   26.81   9.48   STVGBert (ICCV\u201921) [21]   20.42   29.37   11.31   TubeDETR (CVPR\u201922) [28]   32.40   49.80   23.50   STCAT (NeurIPS\u201922) [9]   35.09   57.67   30.09   STVGFormer (CVPR\u201923) [11]   36.90   62.20   34.80   VideoGrounding-DINO   38.25   62.47   36.14   Table 4. Performance comparisons of the state-of-the-art on the   HC-STVG V2 [24] val set in closed-set supervised setting.   Methods   m vIoU vIoU@0.3   vIoU@0.5   Yu et al (arXiv\u201921) [30]   30.00   Aug. 2D-TAN (arXiv\u201921) [22]   30.40   50.40   18.80   TubeDETR (CVPR\u201922) [28]   36.40   58.80   30.60   STVGFormer (CVPR\u201923) [11]   38.70   65.50   33.80   VideoGrounding-DINO   39.88   67.13   34.49   trained generalized features of a spatial grounding foundation model.   Results on YouCook-Interactions:   We evaluate our   method further on the YouCook-Interactions [23] dataset,   reporting pointing game accuracy for spatial grounding.   Our approach gains nearly 2% in accuracy over STCAT [9]   and more than 6% compared to TubeDETR [28].   This   further shows our strong generalization capabilities in the   open-vocabulary setting.   4.2.2   Closed-Set Supervised Evaluation   We present closed-set evaluations across three datasets,   VidSTG [32], HC-STVG V1 [24] and HC-STVG V2 [24].   Results on VidSTG: We present results on the VidSTG   test set in closed-set setting in Tab. 2, reporting m tIoU,   m vIoU, vIoU@0.3 and vIoU@0.5. The results show that   our method achieves state-of-the-art performance in comparison to both Two-Stage and One-Stage methods.   In   particular, we achieve more than 1t IoU gain in temporal grounding over the previous best methods OMRN [33]   (One-Stage) and STVGFormer [11] (Two-Stage), both for   Declarative and Interrogative sentences.   Similarly, for   m vIoU, vIoU@0.3 and vIoU@0.5, we achieve a more than   1 unit gain the state-of-the-art methods STVGFormer [11]   and STCAT [9]. Note that our method uses a frozen visual   and textual encoder. In contrast, those mentioned above previous state-of-the-art methods all train the entire encoder.   Results on HC-STVG V1: We present results on the HCSTVG V1 dataset in Tab. 3, reporting m vIoU, vIoU@0.3   and vIoU@0.5. We achieve a nearly 1.5 unit gain in m vIoU   and vIoU@0.5 and a 1 unit gain in vIoU@0.3 over the previous best method STVGFormer [11]. This shows the consistent performance of our method on this dataset.   Results on HC-STVG V2: We present results on the HCSTVG V2 dataset in Tab. 4, reporting m vIoU, vIoU@0.3   and vIoU@0.5. Our performance gain on HC-STVG V1 is   reflected here as well, with a consistent performance gain   on HC-STVG V2, in comparison to the SoTA [11, 28].   18915   Table 5. Ablation on various design choices for our approach on the VidSTG [32] test set in closed-set supervised setting.   Method   Declarative Sentences   Interrogative Sentences   m tIoU   m vIoU   vIoU@0.3   vIoU@0.5   m tIoU   m vIoU   vIoU@0.3   vIoU@0.5   Naive Solution (Frozen Grounding DINO [12])   39.78   18.07   22.31   13.75   39.79   9.66   10.42   3.84   + Decoder Temporal Aggregation   42.81   20.74   26.53   15.41   43.81   12.38   16.71   8.62   + Encoder Temporal Aggregation   46.29   23.19   32.38   18.95   47.17   16.19   23.28   13.04   + Finetuned Spatial Modules in Decoder   48.06   28.97   41.60   26.06   49.58   24.27   32.85   20.11   + Finetuned Spatial Modules in Encoder   51.97   34.67   48.11   33.96   50.83   29.89   41.03   27.58   Figure 3. Sample visualization for video grounding result on HC-STVG V1 [24] for TubeDETR [28], STCAT [9] and ours with the prompt   The man behind the shirtless man turns and squats. We show bounding boxes for Ground-truth, Closed-Set Supervised, and OpenVocabulary results. Note how both TubeDETR and STCAT are close to the ground truth in the supervised setting (STCAT more so than   TubeDETR), they cannot correctly ground the text properly in the open-vocabulary setting.   4.3. Ablative Analysis   We perform an ablative analysis of the various design   choices for our model. In particular, we first evaluate a   naive baseline where no additional temporal aggregators are   added, and all pre-trained spatial modules are frozen in the   encoder and decoder. This baseline is relatively weak in   both temporal and spatial grounding. We next add temporal   modules in first the decoder, and subsequently in the encoder. We find that it gives significant improvements in the   temporal grounding, and additionally also improves the spatial grounding. Finally, we finetune the pre-trained spatial   modules in both the decoder and the encoder, which provides a strong improvement in spatial grounding, alongside   consistent improvement in temporal grounding.   4.4. Limitation   While our video grounding model excels in closed-set   and open-vocabulary scenarios, it leverages image-text pretrained models like Grounding DINO [12].   To enhance   understanding in open-vocabulary settings, an extension to   video-language pre-training on a larger and more diverse   dataset, akin to CLIP [15], can help further boost performance. Building a video-language pre-training dataset with   diverse natural language expressions and spatio-temporal   localization is imperative, given the constraints of datasets   like VidSTG [32] and HC-STVG [24].   5. Conclusion   This   paper   introduces   an   Open-Vocabulary   SpatioTemporal   Video   Grounding   task,   enhancing   current   closed-set methodologies by using pre-trained representations from spatial grounding models. The proposed model   performs well in closed-set and open-vocabulary scenarios,   surpassing state-of-the-art results in supervised setting   on VidSTG and HC-STVG datasets, and outperforming   recent models in open-vocabulary on HC-STVG V1 and   YouCook-Interactions. Its architecture includes learnable   adapter blocks for video-specific adaptation,   bridging   the semantic gap between natural language queries and   visual content.   This research addresses open-vocabulary   challenges and explores achieving robust performance   without extensive video annotations, paving the way for   open-vocabulary video grounding.   18916", "conf": "CVPR", "year": "2024", "index": 544}, {"title": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:   Human Language Technologies (Volume 1: Long Papers), pages 2437\u20132465   June 16-21, 2024 \u00a92024 Association for Computational Linguistics   How Well Do Large Language Models Truly Ground?   Hyunji Lee1\u2217   Se June Joo1\u2217   Chaeeun Kim1\u2020   Joel Jang2   Doyoung Kim1   Kyoung-Woon On3   Minjoon Seo1   1KAIST AI   2University of Washington   3Kakao Brain   {hyunji.amy.lee, sejune, minjoon}@kaist.ac.kr", "abstract": "To reduce issues like hallucinations and lack   of control in Large Language Models (LLMs),   a common method is to generate responses by   grounding on external contexts given as input,   known as knowledge-augmented models. However, previous research often narrowly defines   \u201cgrounding\u201d as just having the correct answer,   which does not ensure the reliability of the entire response. To overcome this, we propose   a stricter definition of grounding: a model is   truly grounded if it (1) fully utilizes the necessary knowledge from the provided context,   and (2) stays within the limits of that knowledge. We introduce a new dataset and a grounding metric to evaluate model capability under   the definition. We perform experiments across   25 LLMs of different sizes and training methods and provide insights into factors that influence grounding performance. Our findings   contribute to a better understanding of how to   improve grounding capabilities and suggest an   area of improvement toward more reliable and   controllable LLM applications1.   1", "content": "Large Language Models (LLMs) have shown superior performance on various tasks by leveraging the extensive world knowledge embedded in   their parameters. However, these models often produce hallucinations (Bender et al., 2021; Du et al.,   2023), lack controllability (Dathathri et al., 2019;   Zhang et al., 2022), and have trouble integrating   knowledge that changes over time (Lin et al., 2021;   Wang et al., 2021). Additionally, they may not   contain specialized knowledge unique to certain   entities, such as company-specific terminology, or   private information not contained in the training   data. Although it is technically possible to inject   *Denotes equal contribution   \u2020Work done during internship at KAIST AI   1Our   code   and   data   are   available   at   https://github.com/kaistAI/How-Well-Do-LLMs-TrulyGround   new knowledge by further training LLMs on a specific corpus, this approach is generally inefficient   and not practical in many scenarios (Mallen et al.,   2022; Panda et al., 2023; Tang et al., 2023). To address these issues, various systems 2 and work (Gao   et al., 2023; He et al., 2022; Xu et al., 2023; Yao   et al., 2022) have explored methods where such   dynamic, specialized, or private contexts provided   by users or general world knowledge contexts retrieved from a large corpus (retrieval-augmented   models) are provided to LLMs as additional inputs.   While previous work has shown enhanced performance by allowing LLMs to ground their outputs   on external contexts compared to solely relying   on the LLM\u2019s inherent knowledge (Andrew and   Gao, 2007; BehnamGhader et al., 2022; Mallen   et al., 2022), whether the model well-grounds to   the contexts is usually measured by simply checking whether the generated response contains the answer (Liu et al., 2023a; Mallen et al., 2022; Lewis   et al., 2020) or evaluating over NLI model to see   whether the knowledge from given context correlates with generated response (Gao et al., 2023;   Asai et al., 2023). However, in some cases, this   may not be sufficient and it may be more important   to ensure that the entire generated response is truly   grounded on the given external contexts.   For example, let\u2019s consider the scenario in Figure 1, where a company\u2019s HR team is utilizing an   LLM to question the qualifications of candidates   by providing their resumes as external contexts   and prompting the LLM to provide an answer to   questions about the candidates based on their resumes. Response 1 omits essential information   about the candidate and Response 2 contains misinformation about the candidate due to generating   knowledge contained in its parameters; both cases   do not truly represent the candidate\u2019s qualifications.   2https://www.bing.com/new,   https://www.   perplexity.ai/,   https://openai.com/blog/   chatgpt-plugins   2437   I'm trying to cast a student youtuber for an interview on    my channel. This is a application from a candidate.    What relevant experiences does the applicant have?   I\u2019m Gary, an earnest     in    Seoul, South Korea, living in Gwanak-gu. My engaging lifestyle    recently merited a    , a milestone that    reflects my passion and dedication to my pursuits.       I have also cultivated a vibrant online presence on    , where I share glimpses of my daily    life in Seoul.  As I navigate through my educational path,    student at Seoul National University (1)   feature in a magazine (2)   Instagram with    more than 100k followers (3)   Popular people on Instagram often have youtube channels (a)   LLM   Context   Parametric Knowledge   Question   Gary is a     who has    .    student at Seoul National    University (1)   100k    followers on instagram (3)   (2)   missing in    response   (a)    not from   context   Gary is a     with a    .    Gary has    student at Seoul National    Univeristy (1)   instagram    account of 100k followers (3)   been in a magazine (2).   Gary is a     and has    . Gary has a    and    .    student at Seoul National    University (1)   been in a    magazine (2)   youtube    channel  (a)    instagram account    of 100k followers (3)   Previous    Grounding   Response   Strict    Grounding   1   2   3   ISSUE   ISSUE   GOOD   Figure 1: An example scenario of a company\u2019s HR team using LLM to question upon candidate\u2019s resume which is given   as input context. The previous definition of grounding would consider responses 1 and 2 as well grounded due to their   high relevancy with the question and input context. However, as our definition considers all knowledge in a fine-grained   manner, we consider only response 3 as well-grounded. Response 1 misses key resume detail (2) which makes the candidate   underrated. Response 2 introduces knowledge (a) that is not from the given context but from the model\u2019s parametric knowledge,   inaccurately overrates the candidate, and unfairly influences comparison with others.   It either harms the applicant by missing important   information or makes the applicant overly qualified,   disadvantaging other applicants.   In this study, we introduce a strict definition of   grounding: a model is truly grounding on given   contexts when it (1) uses all essential knowledge   from the contexts and (2) strictly adheres to their   scope in response generation without hallucinated   information3. To quantify this definition, we introduce an automatic grounding metric that extends   upon Min et al. (2023) for fine-grained evaluation.   Furthermore, we curate a new dataset incorporating   crucial factors influencing LLMs\u2019 response (i.e.,   entity popularity, context length), to understand   their impact on LLM responses. Lastly, we present   a revised version of the dataset that modifies factual knowledge in external contexts to identify the   knowledge sources in responses.   We conduct experiments across 25 LLMs of different sizes and training methods to explore which   model attributes significantly contribute to grounding ability and identify some important factors.   \u2022 Training methods like Instruction Tuning or   RLHF have a more pronounced impact on   grounding performance than model size.   \u2022 High answer accuracy, commonly used to assess how well a model incorporates context in   previous works, does not ensure high grounding performance.   3In this paper, the term grounding refers to what is defined   here as truly grounding.   \u2022 Instruction-tuned models show high degradation when additional relevant contexts are   added as input.   \u2022 When given multiple contexts, performance   degradation is more influenced by how distracting these contexts are, rather than by their   length.   2   Related Works   Question Answering   Machine Reading Comprehension and Open Domain Question Answering   provide a question and context to a model, which   then answers the question using the given context.   The answers are usually short phrases or entities.   LongformQA shares similarities, as it also uses   contextual information to answer questions, but   its answers are longer and focus on how well the   model refers to the input context and generates   factual responses. Such datasets, while encompassing questions and contexts, are inadequate to measure the model\u2019s grounding ability under our definition; they lack annotation of which knowledge   from the external context is necessary (gold) to answer the query and are hard to verify the source   of knowledge in generated response (whether it is   from a given context or model parameter). Furthermore, since most datasets were created before the   emergence of modern LLMs, they\u2019re unsuitable for   understanding the diverse characteristics of these   models. Therefore, to evaluate a model\u2019s grounding ability under our defined criteria, we created a   2438   new dataset.   Generating Response with External Knowledge   Recent research efforts have focused on incorporating external knowledge during the generation   process to overcome issues such as hallucination,   increase controllability, and incorporate dynamic   knowledge.   It incorporates either by inputting   it directly (Lewis et al., 2020; Liu et al., 2023b;   Shi et al., 2023), using APIs in a multi-step manner (Yao et al., 2022; Xu et al., 2023), or by employing various tools (Schick et al., 2023; Yang   et al., 2023). Although the objective of adding external knowledge is for the model\u2019s response to   be intrinsically tied to the given knowledge, previous work naively evaluates and analyzes the ability.   With such a naive definition, users find it difficult   to ensure that the entire generated response is truly   grounded in the given context; the model may hallucinate or miss important knowledge even though   the overall response corresponds well to the external context. Thereby, in this work, we introduce   a strict definition of grounding and share the importance of checking the entire response in a finegrained manner.   Definition   of   Grounding   The   concept   of   \"grounding\" pervades several areas that interface   with natural language.   In robotics, grounding   bridges the chasm between abstract directives and   actionable robot commands, as highlighted by numerous studies (Ahn et al., 2022; Huang et al.,   2023; Kollar et al., 2010b,a; Tellex et al., 2011;   Mees et al., 2022; Faille et al.; Moon et al.; Brabant   et al., 2023; Clark and Brennan; Traum, 1991). In   the domain of vision and video, grounding predominantly involves associating image regions with their   pertinent linguistic descriptors (Zhu et al., 2022;   Deng et al., 2021; Li et al., 2022; Liu et al., 2022a).   In NLP, grounding frequently denotes finding the   relevant textual knowledge to a given input from   knowledge sources such as a set of documents,   knowledge graphs, or input context (Chandu et al.,   2021; Weller et al., 2023; Mallen et al., 2022); information retrieval task. In this work, we focus on   bridging the definition with when input context is   the knowledge source.   3   Grounding   In this paper, we define that the model grounds   well more strictly and share a dataset and metric   to measure performance under the definition. In   Section 3.1, we define the grounding ability and   share its importance with various use cases. In   Section 3.2, we share details of how we construct   the dataset, and in Section 3.3, we formulate an   automatic metric to measure the grounding ability.   3.1   Definition & Usage   Prior research (Liu et al., 2023a; He et al., 2022;   Mallen et al., 2022; Weller et al., 2023) defines   that a model is well-grounded when it generates   responses relevant to the query while utilizing   the given contexts. When given a set of external contexts C, a set of answers A, and generated response P, the previous definition often defines it well-grounded if \u2200a \u2208A, a \u2208P or   \u2203c \u2208C : NLI(P, c) = 1.   The former calculates whether the generated response contains all   answers and the latter measures whether any context entails the generated response. However, as   in Figure 1, we can see that such a definition of   grounding poses limitations in that it cannot capture whether the generated response misses relevant   knowledge from a given context or whether it hallucinates. In this work, to overcome the limitation,   we formally define a stricter definition of a model\u2019s   grounding performance, which evaluates the entire   generated response in a fine-grained manner.   We define that a model truly grounds on provided external context when (1) it utilizes all necessary knowledge in the context, and (2) it does not incorporate other knowledge apart from the contexts,   such as that stored in the model parameters. Here,   we see the \u201catomic facts\u201d (short sentences conveying one piece of information) as the knowledge unit.   As a sentence contains multiple knowledge, we disassemble4 a single sentence into multiple atomic   facts for a fine-grained evaluation (Min et al., 2023;   Liu et al., 2022b; Kamoi et al., 2023). For instance, \u201cNapoleon is a French general\u201d decomposes into two atomic facts (\u201cNapoleon is French.\u201d   and \u201cNapoleon is a general.\u201d).   In other words, when given a set of necessary   atomic facts (gold atomic facts) CG from the set of   external contexts C and a set of atomic facts PA   from the generated response P, we define that the   model is truly grounded when:   1. \u2200k \u2208CG, k \u2208P   4Following Min et al. (2023), we use InstructGPT (textdavinci-002) on decomposing context into atomic facts, where   it has shown a high correlation with humans. Examples of   atomic facts are in Appendix A.3.   2439   2. \u2200k \u2208PA, \u2203c \u2208C such that k \u2208c   Models that demonstrate strong grounding capabilities as per our definition are highly valued in   various use cases. It can be used in developing personalized chatbot services. By grounding contexts   with personal information, it adeptly uses it to generate responses. When new information is provided   by the user, it can be seamlessly integrated into the   input context for future interactions. Also, when   a company wants to add advertisement by promoting a certain product; by providing the model with   the necessary context, it can be guided to generate responses that favorably mention the product.   Moreover, models with a strong grounding ability   allow users to trust the responses generated without the need to verify for inaccuracies or omissions,   effectively addressing the issue of hallucinations.   3.2   Dataset Construction   We construct a new evaluation dataset specifically   designed to measure a model\u2019s grounding ability   due to limitations of existing datasets; they lack   annotation of which knowledge from the provided   context is necessary, hard to verify the source of   knowledge (whether the knowledge is from a given   context or its parameter), and most do not consider key variables known to influence LLM performance as they were constructed before the advent   of modern LLM.   As in Figure 2, our dataset comprises four versions: Original-Gold, Original-Dist, Conflict-Gold,   and Conflict-Dist. The differentiation lies in two   main aspects: (1) The nature of the input context, which is either an unaltered Wikipedia content (Original-*) or a modified, conflicting version   (Conflict-*) to determine whether the model\u2019s response is from its internal knowledge or by grounding on external knowledge.   (2) The inclusion   of distractor contexts: * -Gold versions contain   only \u201cgold contexts\u201d that directly answer the query,   whereas *-Dist versions also include distractor contexts, which are relevant but not gold.   Furthermore, we integrate three key factors (left   of Figure 2) known to bring qualitative differences in model responses for a more comprehensive analysis: [F1] Popularity of context topics (Mallen et al., 2022; Kandpal et al., 2022),   [F2] Number of required documents to answer the   query (BehnamGhader et al., 2022; Press et al.,   2022; C\u00edfka and Liutkus, 2022), and [F3] Required   response format (definite answer or free-form answer) (McCoy et al., 2021; Tuckute et al., 2022).   Our dataset construction is mainly divided into   five steps. Details of data construction including   human annotators, inter-labeler agreement, data   distribution of the factors, data examples, and more   are in Appendix A.   Step 1: Context Selection   In our first step, we   select sets of input contexts (C) considering F1   and F2.   Wikipedia documents were used for   context, considering their comprehensive metainformation pertinent to these aspects. For F1,   following Mallen et al. (2022), we utilize document pageviews, and for F2, we construct a document set sampled from the intersection between   the popularity list and the hyperlinked document.   Step 2: Instance Generation & Classification   Based on the document sets from Step 1, we use   GPT-3.55 to generate 10 candidate pairs of question   and answer. We classify the candidate pairs by F2   and F3, and select a single query with the highest   quality from each class. Note that the generated   answer was replaced by the annotators.   Step 3: Gold Atomic Fact Selection   To evaluate grounding performance, we decompose context sets C \u2208C into atomic facts {CA1, \u00b7 \u00b7 \u00b7 , CAk}.   From multiple atomic facts, we annotate gold   atomic facts, CGi. Gold atomic facts are the atomic   facts within the provided context that are essential   to answer the given question ({CG1, \u00b7 \u00b7 \u00b7 , CGm} \u2286   {CA1, \u00b7 \u00b7 \u00b7 , CAk}). We now get 480 complete instances that we call Original-Gold (Q, A, C, CG).   Step 4: Modify Context   Given an instance from   Original-Gold, annotators are instructed to revise   well-known and key knowledge to answer the question in the input context.   This step results in   Conflict-Gold (Q, A\u2032, C\u2032, C\u2032   G), a modified, conflicting version.   Step 5: Add Distractor Contexts   To analyze   the impact when additional knowledge apart from   the gold ones is added to the input context, we   sample distractor contexts, contexts with high similarity but not directly related to an answer, with   contriever (Izacard et al., 2022), a dense retriever   pretrained through contrastive learning, and include   them in the input context (Original-Dist when   added to original gold contexts and Revised-Dist   when added to revised gold contexts).   5gpt-3.5-turbo-0301   2440   Conflict-Gold   Conflict-Dist   Original-Gold   Original-Dist   RPG-7    ...The weapon has the GRAU    index (Russian armed forces    index) 6G3. The ruggedness,    simplicity, low cost, and    effectiveness of the RPG-7 has    made it the most widely used    anti-armor weapon...   Distractor Context   Gold Atomic Facts\u0090   w QJS-161 is for the     Armyf   w Users of CS/LR35 are People's Liberation Army Ground Force.   ==> YES   People's Liberation   Is the People's Liberation Army Ground Force    the user of both the QJS-161 and CS/LR35?   CS/LR35   ...== Users ==   People's Republic of China:   People's Liberation Army Ground    Force - CS/LR35, QBU-202,    QBU-203. ...   QJS-161   ... The QJS-161 (   ), also known as    the QJB-201, is a    light    machine gun designed and    manufactured by Norinco for the    Army. ...    Chinese: 161\u5f0f   \u4f1e\u5175\u673a\u67aa; pinyin: Y\u0101o li\u00f9 y\u0101o sh\u00ec    S\u01cenb\u012bng J\u012bqi\u0101ng   Chinese    People's Liberation    Gold Context   Conflict Atomic Facts\u0090   w QJS-161 is for the     Armyf   w Users of CS/LR35 are People's Liberation Army Ground Force.   ==>    French   NO   Is the People's Liberation Army Ground Force    the user of both the QJS-161 and CS/LR35?   CS/LR35   ...== Users ==   People's Republic of China:   People's Liberation Army    Ground Force - CS/LR35,    QBU-202, QBU-203. ...   QJS-161   ... The QJS-161 (   ), also known as the QJB-201,    is a     light machine gun    designed and manufactured by    Norinco for the     ...    French: Mitrailleuse    parachutiste de type 161 Mal\u00e9diction    : la rivi\u00e8re de la Montagne s'il faut la    laisser   French   French Army   Conflict Context   Popularity   Low   High   Single   Multi   Reference   Free form   Definite   Answer   Figure 2: Four versions of our dataset: Original-Gold, Original-Dist, Conflict-Gold, and Conflict-Dist. Conflict-* contains   modified gold contexts (conflict context) by human annotators. *-Dist differs from *-Gold in that it contains distractor contexts.   The left part of the figure shows three key factors we considered when constructing our dataset.   3.3   Metric   We evaluate model performance in two aspects:   grounding performance and answer accuracy.   Grounding Performance   We present an automatic metric to measure whether the model grounds   well under the definition in Section 3.1. We evaluate the presence of knowledge (whether an atomic   fact exists in context) by using an evaluation model   Meval, as the same facts can be conveyed in different ways. On selecting Meval we use the one   with the highest correlation with humans. We test   over five models: GPT-4 (OpenAI, 2023), Llama2-70b-chat (Touvron et al., 2023), TRUE (T5-11B   finetuned on various NLI datasets) (Honovich et al.,   2022), bi-encoder model (MiniLM finetuned on 1B   training pairs), and cross-encoder model (MiniLM   finetuned on MSMARCO) (Wang et al., 2020). Surprisingly, the cross-encoder model6 shows the highest correlation with human (84.1), outperforming   GPT-4 (78.7). It also closely matches the correlation between humans (88.6) Thereby, we utilize the   cross-encoder model as Meval.   We define grounding performance as the F1   score of precision and recall calculated as:   precision = Pk   i=1 Meval(PAi, C) and recall =   Pm   i=1 Meval(CGi, P) where Meval(a, B) returns   1 when knowledge of a exists in B and 0 elsewise.   Details of models, performance, and the process of   human evaluation are in Appendix B.   Answer Accuracy   This is a widely used metric   to naively measure the model\u2019s grounding ability   in previous works (Mallen et al., 2022; Borgeaud   et al., 2021); it measures if the answer is present   6cross-encoder/ms-marco-MiniLM-L-12-v2 from Sentence Transformers (Reimers and Gurevych, 2019)   within the generated response7.   4   Experiments   We experiment with 25 LLMs of various sizes and   training methods (Instruction-tuning, RLHF, DPO).   From the results, we share interesting findings of   how different factors of LLMs and different characteristics of input context lead to their grounding   ability. Section 4.1 shows brief details of the models we evaluate. Section 4.2 shows how different   factors of LLMs lead to their grounding ability   and interesting findings. Details of the input format, generation configurations, and others are in   Appendix C.   4.1   Models   We experiment with two proprietary LLMs: GPT3.5 (GPT) and GPT-3.5-instruct (GPT-I)8. The latter, GPT-instruct9, is a further finetuned version   of GPT, primarily for following instructions. Table 2 shows details of open-sourced LLMs we   experiment over: Llama2 (Touvron et al., 2023),   Llama2-chat (Llama2-C), Vicuna, T\u00dcLU1 (Wang   et al., 2023), T\u00dcLU2 (Ivison et al., 2023), T\u00dcLU2   with DPO (T\u00dcLU2-D), Mistral-Instruct (MistralI) (Jiang et al., 2023), Zephyr (Tunstall et al., 2023),   Falcon (Penedo et al., 2023), and Falcon-Instruct   (Falcon-I). All checkpoints are provided from huggingface (Wolf et al., 2019).   7We only measure the metric to queries with definite answers.   8Specific model names for each model were gpt-3.5-turbo0301 and gpt-3.5-turbo-instruct. Further detail can be found   at https://platform.openai.com/docs/models   9After this point, we shorten GPT-3.5 to \u201dGPT\u201d   2441   Size   7B   13B   40B   70B   UNK   Mpred   Llama2-C   Vicuna   T\u00dcLU2   Mistral-I   Zephyr   Llama2-C   Vicuna   T\u00dcLU2   Falcon-I   Llama2-C   T\u00dcLU2   GPT   GPT-I   Original-Gold   51.6   50.0   58.6   60.3   54.7   55.9   61.4   61.9   42.4   56.9   61.9   61.0   65.7   Original-Dist   45.1   45.0   54.9   54.9   53.7   35.8   56.5   55.3   36.3   55.8   56.7   56.8   56.9   Conflict-Gold   46.0   48.0   54.9   59.8   52.4   53.4   57.5   57.7   40.1   56.3   62.4   59.0   60.3   Conflict-Dist   40.4   39.8   47.9   54.3   52.4   46.5   55.0   50.4   32.6   54.4   54.9   56.1   54.5   Table 1: Grounding performance of twelve different models. For each setting, the best of all in bold and the best of open-sourced   models in underline.   Base   DPO   RLHF   Inst.   Size   Llama2   Llama2   x   x   x   [13]   Llama2-C   Llama2   x   o   o   [7, 13, 70]   Vicuna   Llama2   x   x   o   [7, 13, 33]   T\u00dcLU1   Llama1   x   x   o   [7, 13, 30, 65]   T\u00dcLU2   Llama2   x   x   o   [7, 13, 70]   T\u00dcLU2-D   Llama2   o   x   o   [7, 13, 70]   Falcon   Falcon   x   x   x   [40, 180]   Falcon-I   Falcon   x   x   o   [40, 180]   Mistral-I   Mistral   x   x   o   [7]   Zephyr   Mistral   o   x   o   [7]   Table 2: Abstract of open-sourced LLMs we experiment over.   The size column shows various sizes of the model we experimented over. The base column shows the pretrained model   each model is finetuned on. The rest of the columns show   different training methods; Inst. is instruction-tuned, DPO is   Direct Preference Optimization, and RLHF is Reinforcement   Learning from Human Feedback.   4.2   Results   Overall performance   Table 1 shows the overall   grounding performance of various models over four   different dataset versions10. Due to limited space,   the results of all models in four dataset versions   are in Appendix D.2. GPT-I shows the highest performance for original datasets (Original-Gold and   Original-Dist), and T\u00dcLU2-70B shows the highest   performance among open-sourced models, similar   performance with GPT. Performance of ConflictGold consistently shows lower performance than   Original-Gold (average of 4.7 drops), which we   hypothesize is due to conflict between parametric   space and external knowledge. The performance   also consistently degrades with distractor contexts   added: an average of 10.7 drops for Original-Dist   from Original-Gold and an average of 10.0 drops   for Conflict-Dist from Conflict-Gold. The drop   is higher than when given conflicting knowledge,   which highlights the LLM\u2019s tendency to deviate   from the primary context when presented with extraneous information and the importance of providing only the gold contexts for high grounding   performance. When comparing the different model   10Details of each dataset scenarios in Section 3.2   sizes of the same model (i.e., T\u00dcLU2 and LlamaC), the grounding performance of all four dataset   versions tends to steadily increase. The improvement rate by a larger model tends to be stronger as   the dataset is difficult; Conflict-Dist is considered   more difficult over Original-Gold as it contains   more knowledge in input context and contains conflict knowledge with its parametric space. When   comparing the performance of precision and recall,   a common trend across all models is a superior performance in precision over recall (Appendix D.3).   This suggests a challenge in utilizing all necessary   knowledge when generating a response and it tends   to utilize only a partial of them.   Training method shows stronger effect than   model size in grounding performance   Figure 3   (a) shows that model size tends to show a small   effect on the grounding performance of OriginalGold, but how the model was tuned tends to show   a stronger effect; for high grounding performance,   instruction tuning seems to be the most important   factor. To determine if grounding performance is   strongly dependent on instruction-following ability, we see the correlation between grounding performance with performance on RULES benchmark (Mu et al., 2023), a benchmark to determine   how well it follows the given rule. Figure 3 (b)   shows that there is weak correlation between the   two scores. This suggests that grounding performance does not appear to be strongly reliant on the   capacity to adhere to instructions. We could see a   similar trend with MMLU benchmark (Hendrycks   et al., 2020) in Appendix D.1.   Grounding performance by different query and   context characteristics   Figure 3 (c) displays the   detailed analysis of each model\u2019s grounding performance of Original-Gold, over the three factors described in Section 3.2. A consistent trend   emerges across all models. For F1, the model generally outperforms when provided with less common contexts (low), compared to when provided   2442   10   20   30   40   50   60   70   Model Size   20   30   40   50   60   Performance   Tulu2-I   Tulu2-D   Vicuna2-I   Mistral-I   Llama2-C   Mistral-D   Tulu1-I   Falcon-I   Llama2   Falcon   (a)   45   50   55   60   65   Grounding Performance   44   46   48   50   52   RULES   Llama2-C (70B)   Vicuna (33B)   Llama2-C (13B)   Mistral-I (7B)   Llama2-C (7B)   GPT   GPT-I   (b)   high   low   0   10   20   30   40   50   60   70   [F1] Popularity   multi   single   0   10   20   30   40   50   60   70   [F2] Number of Contexts   free-form   definite   0   10   20   30   40   50   60   70   [F3] Response Format   gpt-I   gpt   falcon-I   falcon   Llama2-C   Llama2   vicuna   Average   (c)   Figure 3: (a) shows grounding performance for each model size in Original-Gold. The performance tends to depend more   heavily on how the model was tuned rather than the model size. (b) shows RULES performance and grounding performance.   There is a weak correlation between instruction-following ability and grounding performance. (c) shows details of grounding   performance by the characteristics of queries and contexts in Original-Gold. Llama2 and Vicuna are 13B, Falcon is 40B model.   with more prevalent contexts (high). This resonates   with Mallen et al. (2022), underlining a model\u2019s   propensity to lean on provided data when faced   with less familiar content. For F2, queries demanding reasoning across multiple contexts (multi) show   lower grounding performance than those confined   to a single context (single). The grounding challenges likely arise from the extended context length   in multiple scenarios and the added reasoning complexity to extract all relevant atomic facts. Lastly,   for F3, questions with predetermined answers (definite) tend to achieve better grounding than openended answers (free-form). This divergence largely   stems from recall metrics as free-form instances   contain more necessary knowledge (gold atomic   facts) compared to definite instances, it is more difficult to find all. We could see that the trend holds   for all four dataset settings in Appendix D.2.   High answer accuracy does not ensure high   grounding performance   Answer accuracy is a   common metric used for measuring the grounding   ability of a model. However, though there is a correlation between grounding performance (Table 1)   and answer accuracy (Table 13), high answer accuracy does not ensure high grounding performance   as grounding performance in the same range of answer accuracy highly diverges. For example, the   answer accuracy of Llama2-13b-chat (84.79) and   Llama2-13b (81.56) only show a marginal difference of 3.23 compared to the difference of 29.82   (55.91, 26.09) in grounding performance. This discrepancy is attributed to Llama2-13b\u2019s tendency   to generate lengthy responses with relevant information drawn not only from the provided context   but also its internal parameters, leading to lower   grounding scores despite high answer accuracy.   Smaller models tend to show a higher reduction   rate by DPO training   Table 3 shows the degradation rate from T\u00dcLU2 to those trained with DPO.   Smaller models tend to show a higher degradation   rate in grounding performance by DPO training.   The degradation rate tends to come from its verbosity, aligning with the findings from Ivison et al.   (2023). Moreover, the results of Zephyr, a 7B size   model further trained with DPO on top of Mistral,   in Table 1 show similar results; high degradation   2443   T\u00dcLU2   + DPO   deg.rate (%)   T\u00dcLU2   + DPO   deg.rate (%)   Original-Gold   Revised-Gold   7B   56.2   51.5   8.5   54.9   51.4   6.4   13B   62.3   60.1   3.5   61.9   58.0   6.3   70B   59.6   58.0   2.7   59.9   58.1   3.1   Original-Dist   Revised-Dist   7B   54.9   45.3   17.6   47.9   41.4   13.5   13B   55.3   54.0   2.3   50.4   54.2   -7.5   70B   53.4   55.4   -3.7   52.4   55.1   -5.1   Table 3: Grounding performance of T\u00dcLU and those trained   with DPO (+DPO). deg.rate column shows the degradation   rate from T\u00dcLU to those trained with DPO.   rate by DPO training.   0   2000   4000   6000   8000   10000   Length of Input Contexts   38   39   40   41   42   43   44   45   Grounding Performance   5.86   3.86   3.55   2.00   1.97   Figure 4: Grounding performance of Vicuna-13B-16k as   length of input contexts increases.   Performance degradation is more influenced by   the distraction level of the contexts rather than   the length of distractor contexts   Figure 4 illustrates that as the input context length increases,   the grounding performance of Vicuna-13b-16k, capable of handling extensive inputs, varies significantly. Please note that the input contexts differ   by the length of distractor contexts as the length   of gold contexts is the same. Notably, grounding   performance deteriorates more rapidly at the initial   points (5.86 at the initial point and 1.97 at the end   point of the plot). This is because we add distractor contexts in the order of those in high rank by   contriever (Izacard et al., 2022), which indicates   that contexts with high distraction levels are added   at the initial points, causing stronger distractions.   Such a result indicates that the performance decline   is more influenced by the relevance and distraction   level of the contexts, rather than the sheer number   of distractors. The drop rate is mostly from the   model\u2019s recall ability, highlighting its struggle to   accurately identify all essential facts from the given   contexts. This tendency shows a high correlation   with a common challenge in retrieval models; performance decreases as they deal with larger data   GPT   GPT_I   Llama2   Vicuna Llama2_C   Falcon Falcon_I   35   30   25   20   15   10   5   0   Figure 5: Reduction rate in Original-Dist performance   from Original-Gold. Models with the same base model are   in the same color. Models that are instruction tuned (falcon_I, GPT_I, Vicuna) or underwent RLHF (Llama2_C)   show higher degradation when distractor contexts are   added. Vicuna and Llama2 are 13B and Falcon is 40B   model.   sets and encounter numerous query-relevant contexts within those sets (Zhong et al., 2023).   Impact of gold contexts position on grounding performance: optimal position at the end   We could see that the position of gold contexts   within multi-document settings significantly influences grounding performance, aligning with   the findings from Liu et al. (2023a).   Experiment with Vicuna-13b-16k, input context length   of 4096 over Original-dist show the highest performance when gold contexts are positioned at the   end and the lowest when positioned in the middle (end-43.37, beginning-39.32, random-39.45,   middle-39.32). The trend also holds for Conflictdist: end-43.53, beginning-41.28, random-39.10,   middle-38.30. Such results emphasize the importance of where you put the gold contexts in a multidocument setting for high grounding performance.   Instruction-tuned models show higher degradation with distractor contexts   Figure 5 demonstrates while models fine-tuned with instruction   show higher absolute grounding performance, they   show a notably greater decrease in performance   when faced with distractor contexts. This trend is   even more evident in models that underwent RLHF.   We hypothesize that this decline in performance   is likely a consequence of their tuning methods.   During instruction tuning and RLHF, the models   are trained to consider all input texts as relevant to   their output generation. Consequently, they tend   to incorporate distracting inputs when encountered.   A closer examination of the metrics reveals a more   pronounced drop in precision rather than recall.   This suggests that in the presence of distractor   contexts, these models are more inclined to use   2444   knowledge beyond the gold contexts, supporting   our hypothesis. Thus, for instruction-tuned models,   providing only the gold contexts without distractor   contexts is crucial to maintain their high grounding   performance.   Performance of answer accuracy   Table 13 in   Appendix D.6 shows the answer accuracy of models across five settings. A key notable finding is   that large-parameter models, like Falcon-40b, excel   without contexts due to their inherent knowledge   but see reduced gains with external contexts added   as input. Also, without external contexts, highpopularity questions achieve a 32.6% accuracy,   outpacing low-popularity ones at 26.8%. However, when with gold contexts: low-popularity questions slightly edge out at 83.4% over the 83.2%   for high-popularity ones. We further analyze the   generated response, we measure the fluency using   G-EVAL (Liu et al., 2023c) in Appendix D.7.   5   Conclusion   In this paper, we introduce a strict definition of   \u201cgrounding\u201d to external contexts when given as input. To evaluate and analyze grounding performance under the definition, we propose a new   dataset and grounding metric. In our extensive   evaluation of 25 LLMs across four dataset scenarios, we observed various insights. Rather than   model size, various training techniques and base   models tend to affect more on grounding performance. Models find it challenging to utilize all   necessary knowledge when generating a response.   By presenting the performance of various models   on different dataset settings, we provide valuable   perspectives to the ongoing discourse on enhancing   LLM grounding abilities and practical guidance   for choosing suitable models for applications that   require generating response by truly grounding on   a given context.   6   Limitations   To construct a dataset with the specific requirements, all the contexts we utilize are sourced from   Wikipedia, which is likely to be used as a source   during pretraining LLMs. Therefore, to follow   cases where private contexts (contexts that the   model is likely to not have seen during training) we   collect a modified version of the dataset, which also   allows us to clearly differentiate between knowledge derived from the provided context and that   inherent in the model\u2019s parameters. We leave collecting datasets with private contexts and evaluating the dataset as future work. As we modified   the existing dataset, the contexts we provide may   distract people.   While we have observed a high correlation with   human judgments in our assessments, it\u2019s important   to note that since our evaluation metric involves   a model-based approach, the performance of the   prediction model (Mpred) could be influenced by   the performance of the evaluation model (Meval).   Therefore, the accuracy and reliability of Meval are   critical, as any limitations or biases within it could   potentially affect the outcome of our performance   evaluations for Mpred. Additionally, while decomposing context into atomic facts also aligns well   with human judgment, we note several failure cases   attributable to model involvement, which further   impacts grounding performance.   Acknowledgements   This work was partly supported by Kakao Brain   grant (2023, Aligning Language Model with   Knowledge Module, 80%) and Institute of Information & communications Technology Planning &   Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00113, Developing   a Sustainable Collaborative Multi-modal Lifelong   Learning Framework, 20%).   We thank Seonghyeon Ye, Sewon Min, Yoonjoo   Lee, Hanseok Oh, and Seungone Kim for helpful   discussions and constructive feedback. We also   thank Jonghyeon Kim, Daeyang Oh, Jungeun Lee,   and Hyungyu Chae for annotating the data.", "conf": "NAACL", "year": "2024", "index": 170}, {"title": "ScanReason: Empowering 3D Visual Grounding   with Reasoning Capabilities   Chenming Zhu1,2, Tai Wang2, Wenwei Zhang2, Kai Chen2, and Xihui Liu1\u2020   1 The University of Hong Kong   2 Shanghai AI Laboratory   chaimzhu@connect.hku.hk   https://zcmax.github.io/projects/ScanReason", "abstract": ". Although great progress has been made in 3D visual grounding, current models still rely on explicit textual descriptions for grounding   and lack the ability to reason human intentions from implicit instructions. We propose a new task called 3D reasoning grounding and introduce a new benchmark ScanReason which provides over 10K questionanswer-location pairs from five reasoning types that require the synerization of reasoning and grounding. We further design our approach, ReGround3D, composed of the visual-centric reasoning module empowered   by Multi-modal Large Language Model (MLLM) and the 3D grounding   module to obtain accurate object locations by looking back to the enhanced geometry and fine-grained details from the 3D scenes. A chainof-grounding mechanism is proposed to further boost the performance   with interleaved reasoning and grounding steps during inference. Extensive experiments on the proposed benchmark validate the effectiveness   of our proposed approach.   Keywords: 3D reasoning grounding \u00b7 3D visual grounding \u00b7 Multimodal large language models   1", "content": "Understanding and reasoning in the 3D visual world is critical for applications   such as robotics and AR, where embodied agents are expected to understand the   3D layout and predict the 3D locations of objects based on human instructions.   The example in Fig. 1 demonstrates a scenario where the question can only be   solved with a comprehensive understanding of the 3D scene and joint reasoning   of the question and the 3D environment. However, current 3D visual grounding   models [24,39,43,50] trained on [1,7] localize objects based on explicit descriptions of the object category, attribute, and 3D spatial relationships, and lack the   ability to reason the user intentions and predict object locations with implicit   human instructions such as \u201cI am thirsty, can I have something to drink?\u201d.   To bridge the aforementioned gap and to push the boundaries of what embodied agents can understand and how they can interact with the 3D world, we   \u2020 Corresponding author: xihuiliu@eee.hku.hk   2   C. Zhu et al.   Fig. 1: For an embodied agent, they not only need to be able to understand the 3D   environment and complex human instructions but also localize the target objects for   interaction and navigation. Although GPT-4 (GPT-4V) have strong text (multi-modal)   reasoning abilities, they lack the ability to directly perceive the 3D scene, understand   the 3D spatial relationships, and output corresponding target object locations. Instead,   our proposed method ReGround3D has the 3D perception, reasoning, and grounding   capabilities in the real-world 3D environment.   propose a new task of 3D reasoning grounding and introduce a new benchmark   named ScanReason. The task requires the model to conduct joint reasoning on   the question and the 3D environment before predicting the 3D locations of target   objects. We define five categories of 3D reasoning: spatial reasoning, functional   reasoning, logical reasoning, emotional reasoning, and safety reasoning. The first   two categories focus on the fundamental understanding of the 3D physical world,   while the last three categories are built upon fundamental abilities to address   user-centric real-world challenges. The benchmark comprises more than 10K   question-answer-3D bounding box pairs from 2K scenes belonging to the five   reasoning types mentioned above. The GPT-4-assisted data annotation process   largely increases the efficiency of curating such a dataset.   We propose ReGround3D as an initial attempt to the new task of 3D Reasoning Grounding. Intuitively, for 3D grounding with implicit instructions, we   need to first conduct reasoning on the language instructions and the coarse visual environment. Then with the idea of which object we want to find in mind,   we look back to the 3D scene and ground the target object. For complex instructions, we may need to alternate the reasoning and look-back process for   multiple iterations. Inspired by this intuition, our framework is composed of   a visual-centric reasoning module and 3D grounding with geometry-enhanced   look-back module, with a Chain-of-Grounding mechanism during inference to   ScanReason   3   alternately conduct reasoning and grounding for multiple rounds. Specifically,   the visual-centric reasoning module conducts joint reasoning of the 3D scene and   instructions with an MLLM. This module predicts a special token representing   the semantic and location information of the target object, which is used for the   grounding module. The 3D grounding module uses the output token embedding   from the previous reasoning module to locate the target object by looking back   at the fine-grained 3D scene representation. Unlike previous MLLMs attempting   to directly predict bounding box coordinates, our look-back mechanism enables   the model to capture more comprehensive 3D geometry and fine-grained object details for accurate 3D grounding. The Chain-of-Grounding mechanism is   proposed to synergize reasoning and grounding, which allows multiple rounds   alternating between reasoning and grounding during inference.   In summary, our contributions are threefold: 1) We propose a new task of 3D   reasoning grounding which requires the model to synergize reasoning and grounding. We further introduce a new benchmark ScanReason, which comprises five   reasoning types (spatial reasoning, functional reasoning, logical reasoning, emotional reasoning, and safety reasoning) for the task of 3D reasoning grounding in   3D scenes. 2) We design a new framework ReGround3D with a visual-centric reasoning module and a 3D grounding module with geometry-enhanced look-back.   We further introduce a Chain-of-Grounding mechanism to boost the 3D reasoning grounding ability with a chain of interleaved reasoning and grounding steps.   3) Extensive experiments demonstrate the effectiveness of the our ReGround3D   on the ScanReason benchmark for 3D reasoning grounding.   2   Related Work   3D Vision and Language Learning 3D Vision-language learning (3D-VL) is   garnering increasing attention, with many 3D-VL tasks focusing on how to connect the 3D world with natural language. Among them, 3D Question Answering   (3D QA) [2,45] aims to enable models to provide text answers based on natural   language questions. Situation Question Answering in 3D Scenes (SQA3D) [29]   requires an agent to first understand its location based on a text description,   then provide a reasonable answer based on the surrounding environment, which   can be seen as an extension of 3D QA in the embodied AI area. 3D Visual   Grounding [1, 4, 7, 28, 31, 39] demands that models identify and locate target   objects in a 3D scene based on given descriptions, outputting the objects\u2019 coordinates and 3D bounding boxes. These descriptions usually explicitly rely   on the objects\u2019 attributes and their spatial relationships. 3D Dense Captioning [5, 8, 14, 16, 25, 36, 46, 49] requires models to output a series of object coordinates and corresponding scene-based descriptions based on a given scene. Different from these 3D-VL tasks, the questions in 3D reasoning grounding could   be more implicit and complex.   3D Visual Grounding The task of 3D visual grounding is aimed at localizing the objects that are explicitly referred to by free-form guided language   4   C. Zhu et al.   expressions in the 3D scene. Inspired by the success of transformers in natural   language processing, recent 3D visual grounding approaches [12,17,24,39,50,53]   have started to adopt transformer [35] architectures for handling the complex   relationships between language descriptions and 3D visual data. These methods leverage the self-attention mechanism of transformers to dynamically weigh   the importance of different parts of the input data, facilitating a more effective   grounding of textual descriptions in the 3D environment. Recent method [3] proposes a Chain-of-Thoughts module that predicts a chain of anchor objects that   are subsequently utilized to localize the final target object. Compared with 3D   visual grounding, our proposed 3D reasoning grounding requires the model to   reason the complex question, ground target objects, and give the explanation at   the same time.   Multi-modal Large Language Models Recently, there has been an increasing effort to extend the powerful complex reasoning and world knowledge capabilities of LLMs [18,34,47] to other modalities [6,9,10,19,26,44,51,52]. Among these   works, some have aimed to enable LLMs to understand the 3D world. [20, 40]   focus on delving into LLMs\u2019 ability to comprehend 3D objects, which can not   be directly applied to 3D scenes. 3D-LLM [21] is the pioneering work that incorporates the 3D scene into LLM to carry out general 3D understanding tasks.   However, by using 3D features constructed through projecting the 2D features of   multi-view 2D images extracted by pre-trained 2D Vision-Language Models into   3D space, 3D-LLM struggles to directly capture the complex spatial relationships between objects and the structure of 3D scenes. [13,27] directly extract 3D   features from the reconstructed 3D point cloud and support multi-modal visual   prompts (text, images, 3D objects) in an MLLM. To alleviate the difficulty LLMs   face in understanding complex 3D scenes, [22] choose to first explicitly segment   the objects in the 3D scene and then perform multi-stage object-aware scenetext alignment to achieve 3D scene understanding. However, due to the lack of   large-scale 3D-language alignment data and the intricate content of 3D scenes,   although current MLLMs can achieve favorable performance in 3D scene understanding tasks, their localization performance is still significantly behind the 3D   localization specialists. Our approach seeks to address this issue by introducing   a 3D grounding model to enhance the localization capability of MLLMs.   3   ScanReason Benchmark   3.1   3D Reasoning Grounding Task   Given a 3D scene and an implicit free-form query, 3D reasoning grounding requires the model to predict the 3D bounding boxes of the target objects as well   as the textual answers and explanations. As shown in Fig. 2, different from traditional 3D visual grounding, the queries of 3D reasoning grounding are implicit   and complex, requiring strong reasoning, commonsense, and world knowledge.   The number of target objects in 3D reasoning grounding is flexible and any   object satisfying the requirements should be considered as the target object.   ScanReason   5   Fig. 2: The left side figure shows the overall of our ScanReason dataset. For each reasoning category, we designed different prompts to generate corresponding questions.   And the right side figure shows the differences between the traditional 3D visual grounding task and our proposed 3D reasoning grounding task.   3.2   Question Types   To comprehensively evaluate the 3D reasoning grounding abilities, we define 5   types of questions depending on which type of reasoning is required. Spatial reasoning and functional reasoning require a fundamental understanding of the 3D   physical world, while logical reasoning, emotional reasoning, and safety reasoning   are high-level reasoning skills built upon the two fundamental reasoning abilities   to address user-centric real-world applications, as shown in Fig. 2.   Spatial Reasoning measures models\u2019 understanding of 3D spatial relationships   among objects in 3D scene. It encompasses the ability to comprehend the layout   of the 3D scene and the 3D location of objects within it, which could serve as   the foundation for navigating or planning movements in the 3D environment.   Functional Reasoning involves understanding and inferring the purpose, function, or affordance of objects within the 3D scene. For example, functional reasoning allows an embodied agent to recognize that a chair is for sitting, a lamp   is for lighting, and a refrigerator is for storing food at low temperatures. Such   understanding enables the embodied agent to assist users and to perform complex tasks more effectively (e.g., turning on a lamp when the room gets dark, or   navigating to a refrigerator to fetch a drink).   Logical Reasoning allows an embodied agent to not only understand its environment but also to interact with it in a goal-directed manner. For example,   given a question shown in Fig. 2: \u201cIf I\u2019m cooking dinner in the kitchen, where is   the nearest place to throw the rubbish?\u201d, an agent needs to use such reasoning   ability to infer the location of objects (in this question, a rubbish bin) based on   their function and spatial relationships under the specific setting (the kitchen).   Emotional Reasoning plays a critical role in human-robot interaction, where   the target objects are determined by understanding human emotions, p", "conf": "ECCV", "year": "2024", "index": 330}, {"title": "GLIGEN: Open-Set Grounded Text-to-Image Generation   Yuheng Li1\u00a7, Haotian Liu1\u00a7, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,   Chunyuan Li3\u00b6, Yong Jae Lee1\u00b6   1University of Wisconsin-Madison   2Columbia University   3Microsoft   https://gligen.github.io/   Caption: \u201cA woman sitting in a restaurant with a pizza in front of her \u201d   Grounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup   Caption: \u201ca baby girl / monkey / Hormer Simpson / is scratching her/its head\u201d   Grounded keypoints: plotted dots on the left image    Caption: \u201cA dog / bird / helmet / backpack is on the grass\u201d   Grounded image: red inset   Caption: \u201cElon Musk and Emma Watson on a movie poster\u201d   Grounded text: Elon Musk, Emma Watson; Grounded style image: blue inset   Caption: \u201cA vibrant colorful bird sitting on tree branch\u201d   Grounded depth map: the left image   Caption: \u201cA young boy with white powder on his face looks away\u201d   Grounded HED map: the left image   Caption: \u201cCars park on the snowy street\u201d   Grounded normal map: the left image   Caption: \u201cA living room filled with lots of furniture and plants\u201d   Grounded semantic map: the left image   (a)   (c)   (b)   (e)   (g)   (d)   (f)   (h)   Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding   conditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) keypoints, (e) depth map, (f)   edge map, (g) normal map, and (h) semantic map.", "abstract": "Large-scale text-to-image diffusion models have made   amazing advances.   However, the status quo is to use   text input alone, which can impede controllability. In this   work, we propose GLIGEN, Grounded-Language-to-Image   Generation, a novel approach that builds upon and extends   the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on   grounding inputs. To preserve the vast concept knowledge of   the pre-trained model, we freeze all of its weights and inject   the grounding information into new trainable layers via a   gated mechanism. Our model achieves open-world grounded   text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to   novel spatial configurations and concepts. GLIGEN\u2019s zeroshot performance on COCO and LVIS outperforms existing   supervised layout-to-image baselines by a large margin.   \u00a7 Part of the work performed at Microsoft; \u00b6 Co-senior authors   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   22511   1.", "content": "Image generation research has witnessed huge advances   in recent years. Over the past couple of years, GANs [13]   were the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [42, 54] and generation [25, 27, 41, 75]. Text conditional autoregressive [46,67] and diffusion [45,50] models   have demonstrated astonishing image quality and concept   coverage, due to their more stable learning objectives and   large-scale training on web image-text paired data. These   models have gained attention even among the general public   due to their practical use cases (e.g., art design and creation).   Despite exciting progress, existing large-scale text-toimage generation models cannot be conditioned on other   input modalities apart from text, and thus lack the ability to   precisely localize concepts, use reference images, or other   conditional inputs to control the generation process. The current input, i.e., natural language alone, restricts the way that   information can be expressed. For example, it is difficult to   describe the precise location of an object using text, whereas   bounding boxes / keypoints can easily achieve this, as shown   in Figure 1. While conditional diffusion models [9,47,49]   and GANs [24,33,42,64] that take in input modalities other   than text for inpainting, layout2img generation, etc., do exist,   they rarely combine those inputs for controllable text2img   generation.   Moreover, prior generative models\u2014regardless of the   generative model family\u2014are usually independently trained   on each task-specific dataset. In contrast, in the recognition   field, the long-standing paradigm has been to build recognition models [29,37,76] by starting from a foundation model   pretrained on large-scale image data [4,15,16] or image-text   pairs [30,44,68]. Since diffusion models have been trained   on billions of image-text pairs [47], a natural question is:   Can we build upon existing pretrained diffusion models and   endow them with new conditional input modalities? In this   way, analogous to the recognition literature, we may be able   to achieve better performance on other generation tasks due   to the vast concept knowledge that the pretrained models   have, while acquiring more controllability over existing textto-image generation models.   With the above aims, we propose a method for providing   new grounding conditional inputs to pretrained text-to-image   diffusion models. As shown in Figure 1, we still retain the   text caption as input, but also enable other input modalities   such as bounding boxes for grounding concepts, grounding   reference images, grounding part keypoints, etc. The key   challenge is preserving the original vast concept knowledge   in the pretrained model while learning to inject the new   grounding information. To prevent knowledge forgetting,   we propose to freeze the original model weights and add   new trainable gated Transformer layers [61] that take in the   new grounding input (e.g., bounding box). During training,   we gradually fuse the new grounding information into the   pretrained model using a gated mechanism [1]. This design   enables flexibility in the sampling process during generation   for improved quality and controllability; for example, we   show that using the full model (all layers) in the first half of   the sampling steps and only using the original layers (without   the gated Transformer layers) in the latter half can lead   to generation results that accurately reflect the grounding   conditions while also having high image quality.   In our experiments, we primarily study grounded   text2img generation with bounding boxes, inspired by the   recent scaling success of learning grounded language-image   understanding models with boxes in GLIP [31]. To enable our model to ground open-world vocabulary concepts [29,31,69,72], we use the same pre-trained text encoder   (for encoding the caption) to encode each phrase associated   with each grounded entity (i.e., one phrase per bounding   box) and feed the encoded tokens into the newly inserted   layers with their encoded location information. Due to the   shared text space, we find that our model can generalize to   unseen objects even when only trained on the COCO [36]   dataset. Its generalization on LVIS [14] outperforms a strong   fully-supervised baseline by a large margin. To further improve our model\u2019s grounding ability, we unify the object   detection and grounding data formats for training, following   GLIP [31]. With larger training data, our model\u2019s generalization is consistently improved.   Contributions.   1) We propose a new text2img generation method that endows new grounding controllability over   existing text2img diffusion models. 2) By preserving the pretrained weights and learning to gradually integrate the new   localization layers, our model achieves open-world grounded   text2img generation with bounding box inputs, i.e., synthesis   of novel localized concepts unobserved in training. 3) Our   model\u2019s zero-shot performance on layout2img tasks significantly outperforms the prior state-of-the-art, demonstrating   the power of building upon large pretrained generative models for downstream tasks.   2. Related Work   Large scale text-to-image generation models.   State-ofthe-art models in this space are either autoregressive [12,46,   62,67] or diffusion [39,45,47,50,74]. Among autoregressive models, DALL-E [46] is one of the breakthrough works   that demonstrates zero-shot abilities, while Parti [67] demonstrates the feasibility of scaling up autoregressive models.   Diffusion models have also shown very promising results.   DALL-E 2 [45] generates images from the CLIP [44] image   space, while Imagen [50] finds the benefit of using pretrained   language models. The concurrent Muse [6] demonstrates   that masked modeling can achieve SoTA-level generation   performance with higher inference speed. However, all of   these models usually only take a caption as the input, which   22512   can be difficult for conveying other information such as the   precise location of an object. Make-A-Scene [12] also incorporates semantic maps into its text-to-image generation, by   training an encoder to tokenize semantic masks to condition   the generation. However, it can only operate in a closed-set   (of 158 categories), whereas our grounded entities can be   open-world. A concurrent work eDiff-I [3] shows that by   changing the attention map, one can generate objects that   roughly follow a semantic map input. However, we believe   our interface with boxes is simpler, and more importantly,   our method allows other conditioning inputs such as keypoints, which are hard to manipulate through attention.   Image generation from layouts.   Given bounding boxes   labeled with object categories, the task is to generate a corresponding image [22,34,55\u201357,65,71], which is the reverse   task of object detection. Layout2Im [71] formulated the   problem and combined a VAE object encoder, an LSTM [20]   object fuser, and an image decoder to generate the image, using global and object-level adversarial losses [13] to enforce   realism and layout correspondence. LostGAN [55,56] generates a mask representation which is used to normalize features, taking inspiration from StyleGAN [26]. LAMA [34]   improves the intermediate mask quality for better image   quality. Transformer [60] based methods [22,65] have also   been explored. Critically, existing layout2image methods   are closed-set, i.e., they can only generate limited localized   visual concepts observed in the training set such as the 80   categories in COCO. In contrast, our method represents the   first work for open-set grounded image generation. A concurrent work ReCo [66] also demonstrates open-set abilities   by building upon a pretraned Stable Diffusion model [47].   However, it finetunes the original model weights, which has   the potential to lead to knowledge forgetting. Furthermore,   it only demonstrates box grounding results whereas we show   results on more modalities as shown in the Figure 1.   3. Preliminaries on Latent Diffusion Models   Diffusion-based methods are one of the most effective   model families for text2image tasks, among which latent   diffusion model (LDM) [47] and its successor Stable Diffusion are the most powerful models publicly available to   the research community. To reduce the computational costs   of vanilla diffusion model training, LDM proceeds in two   stages. The first stage learns a bidirectional mapping network to obtain the latent representation z of the image x.   The second stage trains a diffusion model on the latent z.   Since the first stage model produces a fixed bidirectional   mapping between x and z, from hereon, we focus on the   latent generation space of LDM for simplicity.   Training Objective. Starting from noise zT , the model   gradually produces less noisy samples zT \u22121, zT \u22122, \u00b7 \u00b7 \u00b7 , z0,   conditioned on caption c at every time step t. To learn such   Text    encoder    Text    encoder    +   +   +   =   =   =   Caption   Tokens   Grounding   Tokens   bride   a bride and groom    are about to cut    their wedding cake   \u2026   groom   wedding cake   box   box   box   Figure 2. Illustration of grounding token construction process for   the bounding box with text case.   a model f\u03b8 parameterized by \u03b8, for each step, the LDM   training objective solves the denoising problem on latent   representations z of the image x:    \\l   a bel { eq:ldm _loss}    \\   mi n  _{\\th et av }       \\   mathcal {L}_{\\text {LDM}} = \\mathbb {E}_{\\zv , \\epsilonv \\sim \\mathcal {N}(\\mathbf {0}, \\mathbf {I}), t} \\big [ \\| \\epsilonv - f_{\\thetav }(\\zv _t, t, \\cv ) \\|^2_2 \\big ],   (1)   where t is uniformly sampled from time steps {1, \u00b7 \u00b7 \u00b7 , T},   zt is the step-t noisy variant of input z, and f\u03b8(\u2217, t, c) is the   (t, c)-conditioned denoising autoencoder.   Network Architecture. The core of the network architecture is how to encode the conditions, based on which a   cleaner version of z is produced. (i) Denoising Autoencoder. f\u03b8(\u2217, t, c) is implemented via UNet [48]. It takes in   a noisy latent z, as well as information from time step t and   condition c. It consists of a series of ResNet [17] and Transformer [61] blocks. (ii) Condition Encoding. In the original   LDM, a BERT-like [8] network is trained from scratch to   encode each caption into a sequence of text embeddings,   ftext(c), which is fed into (1) to replace c. The caption feature is encoded via a fixed CLIP [44] text encoder in Stable   Diffusion. Time t is first mapped to time embedding \u03d5(t),   then injected into the UNet. The caption feature is used in   a cross attention layer within each Transformer block. The   model learns to predict the noise, following (1).   With large-scale training, the model f\u03b8(\u2217, t, c) is well   trained to denoise z based on the caption information only.   Though impressive language-to-image generation results   have been shown with LDM by pretraining on internet-scale   data, it remains challenging to synthesize images where   additional grounding input can be instructed, and is thus the   focus of our paper.   4. Open-set Grounded Image Generation   4.1. Grounding Instruction Input   For grounded text-to-image generation, there are a variety of ways to ground the generation process via an additional condition. We denote the semantic information of the   grounding entity as e, which can be described either through   text or an example image; and as l the grounding spatial   configuration described with e.g., a bounding box, a set of   keypoints, or an edge map, etc. Note that in certain cases,   both semantic and spatial information can be represented   22513   with l alone (e.g., edge map), in which a single map can represent what objects may be present in the image and where.   We define the instruction to a grounded text-to-image model   as a composition of the caption and grounded entities:    \\label {eq: d a ta_ inp   ut}    ~~~   \\text {I n s truc t i o n : }   ~&    \\yv = (\\cv  ,  \\ev ), ~ ~ ~ \\ t ext {with }~ \\\\ \\label {eq:data_input_caption} \\hspace {-2mm} ~~~\\text {Caption: }~ & \\cv = [c_1, \\cdots ,c_L] \\\\ \\label {eq:data_input_grounding} ~~~\\text {Grounding: }~ & \\ev = [(e_1, \\lv _1), \\cdots , (e_N, \\lv _N)]   (4)   where L is the caption length, and N is the number of entities   to ground. In this work, we primarily study using bounding   box as the grounding spatial configuration l, because of its   large availability and easy annotation for users. For the   grounded entity e, we mainly focus on using text as its   representation due to simplicity. We process both caption   and grounding entities as input tokens to the diffusion model,   as described in detail below.   Caption Tokens. The caption c is processed in the same   way as in LDM. Specifically, we obtain the caption feature sequence (yellow tokens in Figure 2) using hc =   [hc   1, \u00b7 \u00b7 \u00b7 , hc   L] = ftext(c), where hc   \u2113is the contextualized text   feature for the \u2113-th word in the caption.   Grounding Tokens. For each grounded text entity denoted   with a bounding box, we represent the location information   as l = [\u03b1min, \u03b2min, \u03b1max, \u03b2max] with its top-left and bottomright coordinates. For the text entity e, we use the same pretrained text encoder to obtain its text feature ftext(e) (light   green token in Figure 2), and then fuse it with its bounding   box information to produce a grounding token (dark green   token in Figure 2 ):     \\ label {eq:bbo x_token} h^e = \\text {MLP}(f_{\\text {text}}(e), \\text {Fourier}(\\lv ) )    (5)   where Fourier is the Fourier embedding [38], and MLP(\u00b7, \u00b7)   is a multi-layer perceptron that first concatenates the two   inputs across the feature dimension. The grounding token   sequence is represented as he = [he   1, \u00b7 \u00b7 \u00b7 , he   N]   From Closed-set to Open-set.   Note that existing layout2img works only deal with a closed-set setting (e.g.,   COCO categories), as they typically learn a vector embedding u per entity, to replace ftext(e) in (5). For a closed-set   setting with K concepts, a dictionary of with K embeddings   are learned, U = [u1, \u00b7 \u00b7 \u00b7 , uK]. While this non-parametric   representation works well in the closed-set setting, it has   two drawbacks: (1) The conditioning is implemented as a   dictionary look-up over U in the evaluation stage, and thus   the model can only ground the observed entities in the generated images, lacking the ability to generalize to ground new   entities; (2) No word/phrase is ever utilized in the model   condition, and the semantic structure [21] of the underlying   language instruction is missing. In contrast, in our open-set   design, since the noun entities are processed by the same text   encoder that is used to encode the caption, we find that even   when the localization information is limited to the concepts   in the grounding training datasets, our model can still generalize to other concepts as we will show in our experiments.   Extensions to Other Grounding Conditions. Note that   the proposed grounding instruction in Eq (4) is in a general   form, though our description thus far has focused on the case   of using text as entity e and bounding box as l (the major   setting of this paper). To demonstrate the flexibility of the   GLIGEN framework, we also study additional representative   cases which extend the use scenario of Eq (4).   \u2022 Image Prompt. While language allows users to describe   a rich set of entities in an open-vocabulary manner, sometimes more abstract and fine-grained concepts can be   better characterized by example images. To this end,   one may describe entity e using an image, instead of   language. We use an image encoder to obtain feature   fimage(e) which is used in place of ftext(e) in Eq (5) when   e is an image.   \u2022 Keypoints. As a simple parameterization method to specify the spatial configuration of an entity, bounding boxes   ease the user-machine interaction interface by providing   the height and width of the object layout only. One may   consider richer spatial configurations such as keypoints   for GLIGEN, by parameterizing l in Eq (4) with a set   of keypoint coordinates. Similar to encoding boxes, the   Fourier embedding [38] can be applied to each keypoint   location l = [x, y].   \u2022 Spatially-aligned conditions.   To enable more finegrained controlability, spatially-aligned condition maps   can be used, such as edge map, depth map, normal map,   and semantic map. In these cases, the semantic information e is already contained within each spatial coordinate   l of the condition map. A network (e.g. conv layers) can   be used to encode l into h \u00d7 w grounding tokens. We   also notice that additionally feeding l into the first conv   layer of the UNet can accelerate training. Specifically,   the input to the UNet is CONCAT(fl(l), zt) where fl is a   simple downsampling network to reduce l into the same   spatial resolution as zt. In this case, the first conv layer   of the UNet needs to be trainable.   Figure 1 shows generated examples for these other grounding   conditions. Please refer to the supp for more details.   4.2. Continual Learning for Grounded Generation   Our goal is to endow new spatial grounding capabilities to   existing large language-to-image generation models. Large   diffusion models have been pre-trained on web-scale imagetext to gain the required knowledge for synthesizing realistic   images based on diverse and complex language instructions.   Due to the high pre-training cost and excellent performance,   it is important to retain such knowledge in the model weights   while expanding the new capability. Hence, we consider to   lock the original model weights, and gradually adapt the   22514   Visual   Caption    Grounding   Gated Self-Attention   Self-Attention   Cross-Attention   Figure 3. For a pretrained text2img model, the text features are fed   into each cross-attention layer. A new gated self-attention layer is   inserted to take in the new conditional localization information.   model by tuning new modules.   Gated Self-Attention.   We denote v = [v1, \u00b7 \u00b7 \u00b7 , vM] as   the visual feature tokens of an image. The original Transformer block of LDM consists of two attention layers: The   self-attention over the visual tokens, followed by crossattention from caption tokens. By considering the residual   connection, the two layers can be written:     &  \\vv = \\vv +    \\t   e x t  {SelfAttn}(\\ vv ) \\label {eq:ldm_sa}\\\\ \\hspace {-2mm} & \\vv = \\vv + \\text {CrossAttn}(\\vv , \\hv ^c) \\label {eq:ldm_ca}   (7)   We freeze these two attention layers and add a new gated   self-attention layer to enable the spatial grounding ability;   see Figure 3. Specifically, the attention is performed over   the concatenation of visual and grounding tokens [v, he]:     \\ l a b el {eq: g ated-self-atten tion} \\vv = \\vv + \\beta \\cdot \\tanh (\\gamma ) \\cdot \\text {TS}(\\text {SelfAttn}([\\vv , \\hv ^e]))    (8)   where TS(\u00b7) is a token selection operation that considers   visual tokens only, and \u03b3 is a learnable scalar which is initialized as 0. \u03b2 is set as 1 during the entire training process   and is only varied for scheduled sampling during inference   (introduced below) for improved quality and controllability.   Note that (8) is injected in between (6) and (7). Intuitively,   the gated self-attention in (8) allows visual features to leverage bounding box information, and the resulting grounded   features are treated as a residual, whose gate is initially set to   0 (due to \u03b3 being initialized as 0). This also enables more stable training. Note that a similar idea is used in Flamingo [1];   however, it uses gated cross-attention, which leads to worse   performance in our ablation study.   Learning Procedure.   We adapt the pre-trained model such   that grounding information can be injected while all the original components remain intact. By denoting all the new   parameters as \u03b8\u2032, including all gated self-attention layers   in (8) and MLP in (2), we use the original denoising objective as in (1) for model continual learning, based on the   grounding instruction input y:    \\l   ab el {eq:gro u nding_ ldm_los   s   } \\ small \\min _{ \\the   t   a   v '} \\mathcal {L}_{\\text {Grounding}} = \\mathbb {E}_{\\zv , \\epsilonv \\sim \\mathcal {N}(\\mathbf {0}, \\mathbf {I}), t} \\big [ \\| \\epsilonv - f_{\\{\\thetav , \\thetav '\\}}(\\zv _t, t, \\yv ) \\|^2_2 \\big ]. (9)   Why should the model try to use the new grounding information? Intuitively, predicting the noise that was added to   a training image in the reverse diffusion process would be   easier if the model could leverage the external knowledge   about each object\u2019s location. Thus, in this way, the model   learns to use the additional localization information while   retaining the pre-trained concept knowledge.   Scheduled Sampling in Inference.   The standard inference scheme of GLIGEN is to set \u03b2 = 1 in (8), and the   entire diffusion process is influenced by the grounding tokens. This constant \u03b2 sampling scheme provides overall   good performance in terms of both generation and grounding, but sometimes generates lower quality images compared   with the original text2img models (e.g., as Stable Diffusion   is finetuned on high aesthetic scored images). To strike a better trade-off between generation and grounding for GLIGEN,   we propose a scheduled sampling scheme. As we freeze   the original model weights and add new layers to inject new   grounding information in training, there is flexibility during   inference to schedule the diffusion process to either use both   the grounding and language tokens or use only the language   tokens of the original model at anytime, by setting different \u03b2 values in (8). Specifically, we consider a two-stage   inference procedure, divided by \u03c4 \u2208[0, 1]. For a diffusion   process with T steps, one can set \u03b2 to 1 at the first \u03c4 \u2217T   steps, and set \u03b2 to 0 for the remaining (1\u2212\u03c4)\u2217T steps:        \\   be   t a  =     \\ left \\{\\ begin {ma trix}    1   ,  &  t    \\le \\tau  * T ~~~\\ text {\\# Grounded inference stage}~ \\\\ 0, & t > \\tau * T ~~~\\text {\\# Standard inference stage}~~ \\end {matrix}\\right . \\label {eq:beta_cyclic}   (10)   The major benefit of scheduled sampling is improved   visual quality as the rough concept location and outline are   decided in the early stages, followed by fine-grained details   in later stages. It also allows us to extend the model trained   in one domain (human keypoint) to other domains (monkey,   cartoon characters) as shown in Figure 1.   5. Experiments   We evaluate our model\u2019s grounded text2img generation   in both the closed-set and open-set settings, and show extensions to other grounding modalities. We conduct our main   quantitative experiments by building upon a pretrained LDM   on LAION [51], unless stated otherwise.   22515   Model   Generation: FID (\u2193)   Grounding: YOLO (\u2191)   Fine-tuned   Zero-shot   AP/AP50/AP75   CogView [10]   27.10   KNN-Diffusion [2]   16.66   DALL-E 2 [45]   10.39   Imagen [50]   7.27   Re-Imagen [7]   5.25   6.88   Parti [67]   3.20   7.23   LAFITE [75]   8.12   26.94   LAFITE2 [73]   4.28   8.42   Make-a-Scene [12]   7.55   11.84   N \u00a8UWA [62]   12.90   Frido [11]   11.24   XMC-GAN [70]   9.33   AttnGAN [63]   35.49   DF-GAN [59]   21.42   Obj-GAN [32]   20.75   LDM [47]   12.63   LDM*   5.91   11.73   0.6 / 2.0 / 0.3   GLIGEN (COCO2014CD)   5.82   21.7 / 39.0 / 21.7   GLIGEN (COCO2014D)   5.61   24.0 / 42.2 / 24.1   GLIGEN (COCO2014G)   6.38   11.2 / 21.2 / 10.7   Table 1. Evaluation of image quality and correspondence to layout   on COCO2014 val-set. All numbers are taken from corresponding   papers, LDM* is our COCO fine-tuned LDM baseline. Here GLIGEN is built upon LDM.   5.1. Closed-set Grounded Text2Img Generation   We first evaluate the generation quality and grounding   accuracy of our model in a closed-set setting. For this, we   train and evaluate on the COCO2014 [36] dataset, which is   a standard benchmark used in the text2img literature [45,50,   59,63,75], and evaluate how the different types of grounding   instructions impact our model\u2019s performance.   Grounding instructions.   We use the following grounding   instructions to train our model: 1) COCO2014D: Detection Data. There are no caption annotations so we use a   null caption input [19]. Detection annotations are used as   noun-entities. 2) COCO2014CD: Detection + Caption Data.   Both caption and detection annotations are used. Note that   the noun entities may not always exist in the caption. 3)   COCO2014G: Grounding Data. Given the caption annotations, we use GLIP [31], which detects the caption\u2019s noun   entities in the image, to get pseudo box labels. Please refer   to supp for more details about these three types of data.   Baselines.   Baseline models are listed in Table 1. Among   them, we also finetune an LDM [47] pretrained on LAION   400M [51] on COCO2014 with its caption annotations,   which we denote as LDM*. The text2img baselines, as   they cannot be conditioned on box inputs, are trained on   COCO2014C: Caption Data.   Evaluation metrics.   We use the captions and/or box annotations from 30K randomly sampled images to generate 30K   images for evaluation. We use FID [18] to evaluate image   quality. To evaluate grounding accuracy (i.e. correspondence   between the input bounding box and generated entity), we   use the YOLO score [35]. Specifically, we use a pretrained   YOLO-v4 [5] to detect bounding boxes on the generated   images and compare them with the ground truth boxes using   A blue jay is standing on a branch in the woods near us   a croissant is placed in a brown wooden table   a hello kitty is holding a laundry basket   Figure 4. Our model can generalize to open-world concepts even   when only trained using localization annotation from COCO.   average precision (AP). Since prior text2img methods do   not support taking box annotations as input, it is not fair to   compare with them on this metric. Thus, we only report   numbers for the fine-tuned LDM as a reference.   Results.   Table 1 shows the results. First, we see that the   image synthesis quality of our approach, as measured by FID,   is better than most of the state-of-the-art baselines due to rich   visual knowledge learned in the pretraining stage. Next, we   find that all three grounding instructions lead to comparable   FID to that of the LDM* baseline, which is finetuned on   COCO2014 with caption annotations. Our model trained   using detection annotation instructions (COCO2014D) has   the overall best performance. However, when we evaluate   this model on COCO2014CD instructions, we find that it   has worse performance (FID: 8.2) \u2013 its ability to understand   real captions may be limited as it is only trained with the   null caption. For the model trained with GLIP grounding   instructions (COCO2014G), we actually evaluate it using   the COCO2014CD instructions since we need to compute   the YOLO score which requires ground-truth detection annotations. Its slightly worse FID may be attributed to its   learning from GLIP pseudo-labels. The same reason can   explain its low YOLO score (i.e., the model did not see any   ground-truth detection annotations during training).   Overall, this experiment shows that: 1) Our model can   successfully take in boxes as an additional condition while   maintaining image generation quality. 2) All grounding   instruction types are useful, which suggests that combining   their data together can lead to complementary benefits.   Comparison to Layout2Img generation methods.   Thus   far, we have seen that our model correctly learns to use the   grounding condition. But how accurate is it compared to   methods that are specifically designed for layout2img generation? To answer this, we train our model on COCO2017D,   which only has detection annotations. We use the 2017 splits   (instead of 2014 as before), as it is the standard benchmark   22516   Ours-sample1   Ours-sample2   Ours-sample3   Stable diffusion   Caption: \u201ca hen is hatching a huge egg\u201d   Grounded text: hen, egg   Caption: \u201cgolden hour, a pekingese is on the beach with an umbrella\u201d   Grounded text: Pekingese, umbrella, sea    Figure 5. Grounded text2image generation. The baseline lacks grounding ability and can also miss objects e.g. \u201cumbrella\u201d in a sentence   with multiple objects due to CLIP text space, and it also struggles to generate spatially counterfactual concepts.   Model   FID (\u2193)   YOLO score (AP/AP50/AP75) (\u2191)   LostGAN-V2 [56]   42.55   9.1 / 15.3 / 9.8   OCGAN [58]   41.65   HCSS [23]   33.68   LAMA [35]   31.12   13.40 / 19.70 / 14.90   TwFA [64]   22.15   - / 28.20 / 20.12   GLIGEN-LDM   21.04   22.4 / 36.5 / 24.1   Table 2. Image quality and correspondence to layout are compared   with baselines on COCO2017 val-set.   Model   Training data   AP   APr   APc   APf   LAMA [35]   LVIS   2.0   0.9   1.3   3.2   GLIGEN-LDM   COCO2014CD   6.4   5.8   5.8   7.4   GLIGEN-LDM   COCO2014D   4.4   2.3   3.3   6.5   GLIGEN-LDM   COCO2014G   6.0   4.4   6.1   6.6   GLIGEN-LDM   GoldG,O365   10.6   5.8   9.6   13.8   GLIGEN-LDM   GoldG,O365,SBU,CC3M   11.1   9.0   9.8   13.4   GLIGEN-Stable   GoldG,O365,SBU,CC3M   10.8   8.8   9.9   12.6   Upper-bound   25.2   19.0   22.2   31.2   Table 3. GLIP-score on LVIS validation set. Upper-bound is   provided by running GLIP on real images scaled to 256 \u00d7 256.   in the layout2img literature. In this experiment, we use the   exact same annotation as all layout2img baselines.   Table 2 shows that we achieve the state-of-the-art performance for both image quality and grounding accuracy. We   believe the core reason is because previous methods train   their model from scratch, whereas we build upon a largescale pretrained generative model with rich visual semantics.   Qualitative comparisons are in the supp.   5.2. Open-set Grounded Text2Img Generation   COCO-training model. We first take GLIGEN trained only   with the grounding annotations of COCO (COCO2014CD),   and evaluate whether it can generate grounded entities beyond the COCO categories. Figure 4 shows qualitative   results, where GLIGEN can ground new concepts such as   \u201cblue jay\u201d, \u201ccroissant\u201d or ground object attributes   such as \u201cbrown wooden table\u201d, beyond the training   categories. We hypothesize this is because the gated selfattention of GLIGEN learns to re-position the visual features   corresponding to the grounding entities in the caption for   the ensuing cross-attention layer, and gains generalization   ability due to the shared text spaces in these two layers.   We also quantitatively evaluate our model\u2019s zero-shot   generation performance on LVIS [14], which contains 1203   long-tail object categories. We use GLIP to predict bounding   boxes from the generated images and calculate AP, thus we   name it as GLIP score. We compare to a state-of-the-art   model designed for the layout2img task: LAMA [35]. We   train LAMA using the official code on the LVIS training set   (in a fully-supervised setting), whereas we directly evaluate   our model in a zero-shot task transfer manner, by running   inference on the LVIS val set without seeing any LVIS labels.   Table 3 (first 4 rows) shows the results. Surprisingly, even   though our model is only trained on COCO annotations,   it outperforms the supervised baseline by a large margin.   This is because the baseline, which is trained from scratch,   struggles to learn from limited annotations (many of the rare   classes in LVIS have fewer than five training samples). In   contrast, our model can take advantage of the pretrained   model\u2019s vast concept knowledge.   Scaling up the training data.   We next study our model\u2019s   open-set capability with much larger training data. Specifically, we follow GLIP [31] and train on Object365 [52]   and GoldG [31], which combines two grounding datasets:   Flickr [43] and VG [28]. We also use CC3M [53] and   SBU [40] with grounding pseudo-labels generated by GLIP.   Table 3 shows the data scaling results. As we scale up   the training data, our model\u2019s zero-shot performance increases, especially for rare concepts. We also try to finetune   the model pretrained on our largest dataset on LVIS and   demonstrate its performance in the supp. To demonstrate   the generality of our method, we also train our model based   on the Stable Diffusion model checkpoint using the largest   data. We show some qualitative examples in Figure 5 using   22517   this model. Our model gains the grounding ability compared   to vanilla Stable Diffusion. We notice that Stable Diffusion   model may overlook certain objects (\u201cumbrella\u201d in the   second example) due to its use of the CLIP text encoder   which tends to focus on global scene properties, and may   ignore object-level details [3]. It also struggles to generate   spatially counterfactual concepts. By explicitly injecting   entity information through grounding tokens, our model can   improve the grounding ability in two ways: the referred objects are more likely to appear in the generated images, and   the objects reside in the specified spatial location.   5.3. Beyond Text Modality Grounding   Image grounded generation. One can also use a reference   image to represent a grounded entity as discussed previously.   Fig. 1 (b) shows qualitative results, which demonstrate that   the visual feature can complement details that are hard to   describe by language.   Text and image grounded generation.   Besides using either text or image to represent a grounded entity, one can also   keep both representations in one model for more creative   generation. Fig. 1 (c) shows text grounded generation with   style / tone transfer. For the style reference image, we find   that grounding it to an image corner or its edge is sufficient.   Since the model needs to generate a harmonious style for   the entire image, we hypothesize the self-attention layers   may broadcast this information to all pixels, thus leading to   consistent style for the entire image.   Keypoints grounded generation.   We also demonstrate   GLIGEN using keypoints for articulate objects control as   shown in the Fig. 1 (d). Note that this model is only trained   with human keypoint annotations; but it can generalize to   other humanoid object due to the scheduled sampling technique we proposed. We also quantitatively study this grounding condition in the supp.   Spatially-aligned condition map grounded generation.   Fig. 1 (e-h) demonstrate results for depth map, edge map,   normal map, and semantic map grounded generation. These   types of conditions allow users to have more fine-grained   generation control. See supp for more qualitative results.   5.4. Scheduled Sampling   As stated in Eq. (8) and Eq. (10), we can schedule inference time sampling by setting \u03b2 to 1 (use extra grounding   information) or 0 (reduce to the original pretrained diffusion model). This can make our model exploit different   knowledge at different stages.   Fig. 6 qualitatively shows the benefits of our scheduled   sampling by setting \u03c4 to be 0.2. The images in the same row   share the same noise and conditional input. The first row   shows that scheduled sampling can be used to improve image   quality, as the original Stable Diffusion model is trained with   Caption: \u201ca cute low poly Shiba Inu\u201d   Grounded text: Shiba Inu   \ud835\udf0f= 1   Caption: \u201ca robot is sitting on a bench\u201d   Grounded keypoints: plotted dots on the left figure   \ud835\udf0f= 0.2   Figure 6. Scheduled Samping. It can improve visual or extend a   model trained in one domain (e.g., human) to the others.   high quality images. The second row shows a generation   example by our model trained with COCO human keypoint   annotations. Since this model is purely trained with human   keypoints, the final result is biased towards generating a   human even if a different object (i.e., robot) is specified in   the caption. However, by using scheduled sampling, we can   extend this model to generate other objects with a humanlike shape.   6. Conclusion   We proposed GLIGEN for expanding pretrained text2img   diffusion models with grounding ability, and demonstrated   open-world generalization using bounding boxes as the   grounding condition. Our method is simple and effective,   and can be easily extended to other conditions such as keypoints, reference images, spatially-aligned conditions (e.g.,   edge map, depth map, etc). One limitation we noticed is that   the generated style or aesthetic distribution can shift after   adding the new gated self-attention layers (e.g., the model   sometimes struggles to generate graphics style images when   \u03c4 is set to 1), which is probably due to the grounding training   data being all natural images. Adding images from more   diverse style distributions or further finetuning the model   with highly aesthetic images could help alleviate this issue.   Acknowledgement.   This work was supported in part by   NSF CAREER IIS2150012, NASA 80NSSC21K0295, and   Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022- 0-00871, Development of AI   Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of   Large Korean Language Model Technology for Efficient   Pre-training), and Adobe Data Science Research Award.   22518", "conf": "CVPR", "year": "2023", "index": 460}, {"title": "Cycle-Consistency Learning for Captioning and Grounding   Ning Wang1, Jiajun Deng2, Mingbo Jia1   1Huawei Inc.   2University of Adelaide, Australian Institute for Machine Learning   wn6149@mail.ustc.edu.cn, jiajun.deng@adelaide.edu.au, jiamingbo@huawei.com", "abstract": "We present that visual grounding and image captioning,   which perform as two mutually inverse processes, can be   bridged together for collaborative training by careful designs.   By consolidating this idea, we introduce CyCo, a cyclicconsistent learning framework to ameliorate the independent   training pipelines of visual grounding and image captioning.   The proposed framework (1) allows the semi-weakly supervised training of visual grounding; (2) improves the performance of fully supervised visual grounding; (3) yields a general captioning model that can describe arbitrary image regions. Extensive experiments show that our fully supervised   grounding model achieves state-of-the-art performance, and   the semi-weakly supervised one also exhibits competitive   performance compared to the fully supervised counterparts.   Our image captioning model has the capability to freely describe image regions and meanwhile shows impressive performance on prevalent captioning benchmarks.", "content": "The recent decades have witnessed the great success in   vision-language (VL) related \ufb01elds. Based on the primary   target of bridging the modality gap between vision and language, deep neural networks addressing VL tasks generally   share the pre-training objective, model structure, large-scale   training corpus, etc. However, by the time of downstream   \ufb01ne-tuning, these tasks are typically individually tackled or   simply combined in a multi-task training paradigm.   In this work, we devote our efforts to two VL downstream   tasks including image captioning (Vinyals et al. 2015; Anderson et al. 2018) and visual grounding (Mao et al. 2016;   Yu et al. 2016), and explore their inherent relationships to   enable effective joint training. To match the granularity of   visual parts in visual grounding, we \ufb01rst extend image captioning to a more general scenario, where the model is intended to describe a given region. We de\ufb01ne this generalized   task as regional image captioning, which is similar to dense   captioning task (Johnson, Karpathy, and Fei-Fei 2016) but is   free of the requirement of object detection. Particularly, the   conventional task de\ufb01nition of image captioning (Vinyals   et al. 2015) is a special case of regional image captioning   Copyright \u00a9 2024, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   Visual    Grounding   Image   Captioning   CycleConsistency   Loss   Visual    Grounding   Image   Captioning   right pink and    blue person   CycleConsistency   Loss   Predicted Caption   Initialization Caption   Predicted Caption   Initialization    Box   Predicted    Box   Predicted    Box   (a) Captioning-to-Grounding Cycle-Consistency Framework   (b) Grounding-to-Captioning Cycle-Consistency Framework   a woman in a    pink snowsuit   a woman in a    pink snowsuit   Figure 1: The proposed framework jointly optimizes image captioning and visual grounding models via cycleconsistency learning. In (a), our method computes the region (box) consistency in the captioning-to-grounding optimization cycle. In (b), our framework measures the caption   consistency in the grounding-to-captioning cyclic process.   that regards the whole image as a region. For visual grounding, which is also known as referring expression comprehension (Mao et al. 2016; Yu et al. 2016) and phrase localization (Kazemzadeh et al. 2014; Plummer et al. 2015) in the   literature, we maintain the original task target to localize the   corresponding region (generally denoted by a bounding box)   described by a given language expression.   Despite achieving inspiring progress, image captioning   and visual grounding still suffer from several limitations.   For visual grounding, this task simultaneously requires text   descriptions and accurate object bounding boxes for model   optimization. These \ufb01ne-grained image-text-box triplets are   rather laborious to collect. How to optimize the grounding   model using limited data annotations has received considerable attention (Liu et al. 2021; Wang et al. 2021a). As for   image captioning, existing methods typically focus on describing the whole image. The capabilities of modeling region relationships and properly describing them are largely   overlooked in existing algorithms. We argue that a robust   image captioner should be quali\ufb01ed to freely describe the   image, from an arbitrary region to the whole image. SimiThe Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5535   lar to the grounding task, obtaining the regional captioning   ability also requires suf\ufb01cient image-text-box data, increasing the training cost.   In this paper, we introduce a joint learning framework,   namely CyCo, to ameliorate the training pipelines of image   captioning and visual grounding via cyclic consistency. Our   core motivation is that visual grounding and regional image captioning can be regarded as an inverse process of each   other. Speci\ufb01cally, a visual grounding model takes an image and a region-speci\ufb01c description as inputs to predict the   position of the corresponding bounding box, while regional   image captioning receives the location of a region to produce   a region-aware caption. When taking one\u2019s output as the input of the other, these two tasks naturally establish a cyclic   structure. As shown in Figure 1 (a), the generated bounding   box from the grounding model is expected to be consistent   with the input box of the regional image captioner. In this   process, the whole framework merely needs an initialized   bounding box for training. As depicted in Figure 1 (b), the   produced text description from the regional image captioner   is expected to be consistent with the input referring text of   the grounding model. As a result, the models in this cycle   are free of the bounding box annotations.   The proposed cycle-consistency learning framework enjoys the following merits. (1) We bridge two independent   VL tasks in a uni\ufb01ed framework. To this end, our method can   potentially absorb the training data from both tasks and share   the model parameters for collaborative training. (2) Thanks   to the joint training and online data augmentation (iterative   pseudo label generation) in the cyclic learning process, under the same training data, our framework further improves   the performance of the supervised grounding model. (3) After collaborative training, our framework yields a strong image captioning model that can describe the visual contents   in different spatial levels, i.e., from a subregion to the global   image. (4) The proposed framework allows the semi-weakly   supervised training of the grounding model, which merely   needs limited fully-annotated images and many more images with only bounding box annotations or only language   expression labels for model training.   In summary, we make three-fold contributions:   \u2022 We present a novel cycle-consistency learning framework to bridge two independent vision-language tasks   including image captioning and visual grounding.   \u2022 We design a simple regional image captioner and a   Transformer-based grounding model. We further organize them in a uni\ufb01ed framework with weight-sharing   architecture for ef\ufb01cient end-to-end learning.   \u2022 Extensive experiments validate the effectiveness of our   proposed cycle-consistency learning framework.   Related Work   Vision-language Pre-training. Vision-language (VL) pretraining algorithms (Radford et al. 2021; Li et al. 2020,   2022) aim to bridge the domain gap between vision and   language representations. The recent dual-encoder methods   such as CLIP (Radford et al. 2021) align the representations   using contrastive learning. Despite the outstanding performance, their light interaction manner fails to deeply fuse VL   representations for generation tasks. In contrast, recent VL   pre-training approaches (Zhou et al. 2020; Li et al. 2020,   2021, 2022) adopt a relatively heavy Transformer architecture (Vaswani et al. 2017) to achieve the deeper multi-modal   interaction. Inspired by the success of previous arts, we also   conduct the cross-modal pre-training to prompt the downstream VL tasks.   Visual Grounding. Traditional visual grounding methods   typically follow a two-stage pipeline, which generates plentiful region proposals in the \ufb01rst stage and selects the most   matched one via language expression in the second stage   (Yang, Li, and Yu 2019; Liu et al. 2019). Recently, onestage visual grounding approaches gain increasing attention.   They generally embed the linguistic information into the   one-stage object detector (Yang et al. 2019) or model multimodal representations via Transformer (Deng et al. 2021,   2023) for ef\ufb01cient visual grounding.   Different from the above algorithms based on supervised   training, weakly supervised grounding models learn the   region-phrase correspondence with only language expressions. These methods \ufb01rst obtain a set of ROIs (Region of   Interest) using object detectors, and then mine the correspondence between ROIs and query expressions for model   training (Wang et al. 2021a; Liu et al. 2021). In this work,   we train the grounding model in a semi-weakly supervised   manner with the help of a captioning model. Our framework   is free of external object detectors for data pre-processing.   Image Captioning. Image captioning aims to generate a   human-readable sentence to describe the image contents.   Captioning algorithms (Huang et al. 2019; Anderson et al.   2018; Hu et al. 2021b) typically utilize the object detectors   to extract ROI features or simply exploit the grid features   for ef\ufb01cient visual representation modeling (Wang et al.   2021b; Li et al. 2022). After visual feature extraction, captioning models utilize a decoder such as Transformer to generate the sentence (Wang, Xu, and Sun 2022; Wang et al.   2023a,b). Previous algorithms exploit the region information to facilitate the image captioning (Chen et al. 2020a;   Kinghorn, Zhang, and Shao 2018; Cornia, Baraldi, and Cucchiara 2019), but they still focus on describing the global   image content. The dense captioner (Johnson, Karpathy, and   Fei-Fei 2016) mainly focuses on detecting and describing local ROI regions. In contrast, our image captioner is designed   to freely describe both global and regional contexts.   Recently, some methods (Wang et al. 2022; Yu et al. 2017)   jointly train the captioning and grounding models in a multitask fashion. Mao et al. (Mao et al. 2016) utilize the grounding model as a veri\ufb01cation module to push the generated referring expressions to be unambiguous. Different from previous arts, our method bridges the captioning and grounding models in a cyclic framework and explores two different   cycle-consistency constraints for collaborative training.   Cycle-Consistency Learning. To bridge one modality or   domain to the other, cycle-consistency learning has been   explored extensively in visual object tracking (Wang et al.   2019; Wang, Jabri, and Efros 2019), machine translation (He   et al. 2016), unpaired image-to-image translation (Zhu et al.   2017), visual question answering (Shah et al. 2019), image   captioning (Guo et al. 2019), etc. Different from the previThe Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5536   [DEC]   Add & Norm   Feed    Forward   Self  Atten   Add & Norm   Project     Layer    N\ue000   Visual Encoder (ViT)   Add & Norm   Add & Norm   Feed    Forward   Add & Norm   Cross Atten   N\ue000   Causal   Self  Atten   Add & Norm   Add & Norm   Feed    Forward   Add & Norm   Cross Atten   N\ue000   Bi-direct   Self  Atten   Regression    Head   (x, y, w, h)   (x, y, w, h)   Box Project   Layer    Classification    Head   Caption   \u2018right pink and    blue person\u2019   Vision    Tokens   Regional Image Captioning Model   Visual Grounding Model   (a) Model Architecture   (b) Cycle-Consistency Learning Framework   Regional    Image    Captioning   Visual    Grounding   CycleConsistency   Constraint   Language    Expression   Predicted   Expression   Bounding   Box   Visual    Grounding   Predicted   Box   Regional    Image    Captioning   Language    Expression   Bounding   Box   Grounding-to-Captioning Cycle-Consistency   Captioning-to-Grounding Cycle-Consistency   Pseudo Label   Pseudo Label   Input Image   Input Image   Partial   Weight-sharing   Partial   Weight-sharing   CycleConsistency   Constraint   Figure 2: In (a), we exhibit the model architecture of our proposed joint training framework of visual grounding and regional   image captioning. Two models share a visual encoder (ViT) and leverage different Transformer blocks for individual tasks.   In (b), we show the cyclic consistency learning processes of our framework including a grounding-to-captioning cycle and a   captioning-to-grounding cycle.   ous arts that explore the consistency within a single modality   or a single task, we jointly optimize two VL tasks to form   different cycles. To our knowledge, the cyclic consistency   between image captioning and visual grounding has rarely   been touched in the literature. Further, we explore the potential of our framework in different training scenarios including semi-weakly supervised and fully supervised training.   Methodology   Our method follows the pretrain-and-\ufb01netune paradigm.   At the pre-training stage, we leverage the widely-adopted   training objectives (i.e., image-text contrastive loss, matching loss, and language modeling loss) to align and fuse   the visual and linguistic representations. At the \ufb01ne-tuning   stage, both visual grounding and image captioning models reuse the pre-trained model architecture, while capitalizing on task-speci\ufb01c head networks. During \ufb01ne-tuning, we   further develop the cycle-consistency learning framework   to jointly optimize the grounding and captioning models.   The detailed model architecture and our proposed cycleconsistency learning framework are illustrated in Figure 2.   Revisiting Model Pre-training   Our pre-trained vision-language model follows the BLIP approach (Li et al. 2022). We brie\ufb02y review its vision encoder,   image-grounded text encoder, and image-grounded text decoder, which are highly related to our downstream tasks.   Vision Encoder. We exploit the commonly used ViT-B/16   network (Dosovitskiy et al. 2020) as the vision encoder. ViTB/16 is composed of a stack of 12 Transformer encoder layers with 12 heads in each multi-head attention layer. Given   an image, we \ufb01rst split it into small patches with equally   16 \u00d7 16 size, and project them into feature vectors, which   are known as vision tokens. We denote the \ufb01nal encoded vision tokens as v.   Image-Grounded Text Encoder. In this block, we \ufb01rst append a special [ENC] token at the beginning of the text sequence. Then the text tokens are converted to embeddings   via a word embedding layer. This text encoder leverages   the bi-directional self-attention to further encode the text   embeddings, and aggregate the visual information through   cross-attention with vision tokens for multi-modal fusion.   The output embedding of [ENC] contains the rich multimodal representation of the image-text pair, which is exploited to compute the image-text matching (ITM) loss.   Speci\ufb01cally, we add a binary classi\ufb01cation head on top of   it to predict whether an image-text pair is matched.   Image-Grounded Text Decoder. This text decoder block is   similar to the above image-grounded text encoder, while replacing the bi-directional self-attention with the causal selfattention to facilitate the text generation. A special [DEC]   token is used as the beginning signal of a sequence.   The image-grounded text decoder is optimized by the language modeling (LM) loss, which maximizes the likelihood   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5537   of the text in an autoregressive manner.   Cycle-Consistency Learning Framework   In this section, we \ufb01rst introduce how to transfer the pretrained model to downstream visual grounding and regional   image captioning tasks. Then, we exhibit how to jointly optimize them by virtue of the cyclic constraints.   Visual Grounding (VG). To facilitate the following introduction of cycle-consistency learning, we depict the visual   grounding model in a holistic view, which takes the visual   features v and description tokens x as its inputs, and outputs a predicted bounding box bpred as follows.   bpred = ModelVG(v, x).   (1)   In our framework, visual grounding model ModelVG(\u00b7, \u00b7)   reuses the image-grounded text encoder block, which fuses   the visual and text features to predict the region localization.   Following the setting in the pre-training stage, we also add   a special [ENC] token at the beginning of the tokenized tokens. Then, the bi-directional self-attention encodes the text   tokens and cross-attention injects visual information into the   text tokens. After multi-layer visual-linguistic fusion via the   Transformer structure, the output embedding of the [ENC]   token contains rich cross-modal contexts, which is leveraged for bounding box regression via a regression head. The   regression head is a simple three-layer multi-layer perceptron (MLP) with ReLU activations between layers. The output of the box regression head is the 4-dim box coordinates   bpred = (x, y, w, h).   In the model \ufb01ne-tuning stage, we normalize the groundtruth region bounding box by the scale of the image to   obtain bgt = (\u02c6x, \u02c6y, \u02c6w, \u02c6h), and combine the generalized   IoU (GIoU) loss LGIoU(bpred, bgt) (Rezato\ufb01ghi et al. 2019)   and smooth L1 loss Lsmooth-L1(bpred, bgt) to optimize the   grounding model.   Regional Image Captioning (IC). Similar to the visual   grounding, we also formulate the regional image captioning   model ModelIC(\u00b7, \u00b7) as a black box, which takes image features v and a speci\ufb01c region (denoted by the box coordinate   b) as inputs to predict the region-related language expression   xpred as follows.   xpred = ModelIC(v, b).   (2)   This regional image captioner mainly reuses the imagegrounded text decoder in the pre-training stage to generate   the language expression. After model pre-training with language modeling (LM) loss, the text decoder already has the   zero-shot captioning capability to some extent. Nevertheless, different from the classic image captioner, our model is   required to be region-aware. To this end, in the \ufb01ne-tuning   stage, we project the box coordinate b to the regional embedding via a fully-connected layer: ebox = FCbox(b). This   regional embedding is added to the vision tokens to obtain   the region-aware visual representations v\u22c6:   v\u22c6= v + ebox.   (3)   Different from the bi-directional self-attention in visual grounding, the captioning model utilizes causal selfattention to facilitate text generation. As shown in Figure 2 (a), a classi\ufb01cation head upon Transformer is used   to generate tokens over the vocabulary. For a text sequence   x = {x1, x2, \u00b7 \u00b7 \u00b7 , xn}, image captioner generates token xt   based on the previous tokens x<t in an auto-regressive fashion. We train the captioning model using unidirectional language modeling objective by maximizing the negative loglikelihood of the text sequence: \u2212P   t logP\u03b8 (xt|v\u22c6, x<t),   where \u03b8 denotes the trainable model parameters.   VG \u2192IC Cycle-Consistency Learning. As shown in Eq. 1   and Eq. 2, visual grounding and regional image captioning   perform as the inverse process of each other. Consequently,   we can organize them in a cyclic framework for joint optimization using consistency constraints. In the \ufb01ne-tuning   stage, inspired by BLIP (Li et al. 2022), grounding and   captioning branches share the model parameters of crossattention and feed-forward network (FFN) in their Transformer architectures to tightly bridge two individual tasks.   This weight-sharing mechanism not only improves training ef\ufb01ciency but also enjoys multi-task learning for mutual   prompting.   We \ufb01rst start the cycle-consistency learning from visual   grounding (VG) to image captioning (IC), and leverage the   training objective of image captioning to optimize two tasks.   This process merely needs the language expression xinit   and visual features v as the inputs, without requiring any   bounding box labels as follows.   VG: bpred = ModelVG(v, xinit),   (4)   IC: xpred = ModelIC(v, bpred),   (5)   Loss: LVG\u2192IC = LXE(\u02dcxinit, \u02dcxpred),   (6)   where LXE(\u00b7, \u00b7) denotes the cross-entropy loss, \u02dcxinit represents the one-hot vocabulary distribution of the input expression xinit, xpred denotes the predicted text sequence   and \u02dcxpred is the corresponding token prediction probability. In this cycle-consistency learning process, we \ufb01rst utilize   the grounding model ModelVG(\u00b7, \u00b7) to generate the bounding box coordinate bpred, which serves as the pseudo label   of captioning model ModelIC(\u00b7, \u00b7). Then we can utilize the   initial language expression xinit as the supervision signal of   the generated caption xpred to form the cyclic supervision   constraint. In this way, we can optimize the model without   providing any bounding box annotations.   IC \u2192VG Cycle-Consistency Learning. We can also build   the cycle-consistency learning from image captioning (IC)   to visual grounding (VG). This cycle merely requires the visual feature and an initial bounding box binit as the inputs:   IC: xpred = ModelIC(v, binit),   (7)   VG: bpred = ModelVG(v, xpred),   (8)   Loss: LIC\u2192VG = LGIoU+L1(binit, bpred),   (9)   where LGIoU+L1(\u00b7, \u00b7) denotes the combination of generalized   IoU loss (Rezato\ufb01ghi et al. 2019) and smooth L1 loss. Ideally, the generated bounding box bpred should be consistent   with the initial bounding box binit. To this end, we can optimize the whole network via the visual grounding loss.   Model Training   Both visual grounding and regional image captioning models require the image-text-box triplets for training. Labeling   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5538   such well-annotated data is rather time-consuming. However, the aforementioned two learning cycles enable us to   optimize the models in a semi-weakly supervised manner,   where either the ground-truth language expression or bounding box can be omitted. Besides, based on the same data,   we observe that adding cycle-consistency constraints to the   classic fully supervised training paradigm can further boost   the performance, which shows our cycle-consistency learning framework can better exploit the training data.   Fully Supervised Training. In the supervised training, except for the individual training objectives for grounding and   captioning models, we also add the cycle-consistency losses   (i.e., LVG\u2192IC and LIC\u2192VG) to regularize the models.   The effectiveness of our framework can be explained in   an online data augmentation view. It is well recognized that   a picture is worth thousands of words. The proposed cycleconsistency learning framework can be regarded as an incremental training process that iteratively predicts pseudo   labels to augment the training data (e.g., diverse referring   expressions for a region), and thus further boosts the performance of supervised training.   Semi-weakly Supervised Training. We also validate the   potential of our framework by conducting semi-weakly supervised training, where only limited fully-annotated images   (with both referring expression and bounding box) are available. In the experiments, we empirically set the percentage   of fully-annotated data as 20%, and the rest 80% images are   annotated with only language expressions or only bounding   boxes. For fully-annotated images, we compute the standard   fully-supervised losses for the 20% image-text-box data. As   for the partially labeled data, we compute the IC \u2192VG   consistency loss for box-only images, and compute the VG   \u2192IC consistency loss for text-only images. All the losses   are gathered as the total loss of a batch of training samples.   By virtue of VG \u2192IC and IC \u2192VG, the grounding and   captioning models mutually annotate the weakly-annotated   data for collaborative training. Note that the captioning and   grounding models share the same visual encoder (ViT) and   most blocks (cross-attention and FFN) of their Transformer   blocks. To this end, they both bene\ufb01t from the VG \u2192IC and   IC \u2192VG cycle-consistency training.   Experiments   Datasets and Metrics   Pre-training Data. In the pre-training stage, we collect the   image-text pairs from Visual Genome (Krishna et al. 2017),   COCO (Lin et al. 2014), SBU (Ordonez, Kulkarni, and Berg   2011), Conceptual 3M (Sharma et al. 2018), and a \ufb01ltered   version of LAION (115M images) (Schuhmann et al. 2021).   RefCOCO, RefCOCO+, and RefCOCOg. We evaluate   the visual grounding performance on three prevalent benchmarks including RefCOCO (Yu et al. 2016), RefCOCO+   (Yu et al. 2016), and RefCOCOg (Mao et al. 2016). Following the of\ufb01cial setting, RefCOCO and RefCOCO+ are   split into the train set, validation set, testA set, and testB set.   RefCOCOg includes the train set, validation set, and test set.   We consider a referring expression grounded correctly   when its predicted region has at least 0.5 Intersection-overUnion (IoU) with the ground-truth box. We measure the visual grounding performance in terms of top-1 accuracy.   COCO Caption. We evaluate the image captioning performance on the COCO Karpathy split dataset (Lin et al. 2014;   Karpathy and Fei-Fei 2015). To evaluate the performance,   we leverage standard metrics in the captioning task including BLEU@4 (Papineni et al. 2002), METEOR (Banerjee   and Lavie 2005), CIDEr (Vedantam, Lawrence Zitnick, and   Parikh 2015), and SPICE (Anderson et al. 2016).   Implementation Details   In our framework, the image encoder is initialized from ViTB/16 pre-trained on the ImageNet (Dosovitskiy et al. 2020),   and the text encoders of both visual grounding and image   captioning branches are initialized from the of\ufb01cial BERTbase (Devlin et al. 2018). In the pre-training stage, the model   is trained on 32 V100 GPUs for 20 epochs using a batch   size of 2880. We use AdamW optimizer (Loshchilov and   Hutter 2017) with a weight decay of 0.05. The learning rate   is warmed-up to 3\u00d710\u22124 and decayed linearly with a rate of   0.85. We take random image crops of resolution 224 \u00d7 224   during pre-training.   In the \ufb01ne-tuning stage, we train the model using a small   learning rate of 1\u00d710\u22125 and linearly decay it. The additionally added blocks that are not included in the pre-training   stage (i.e., box regression head and box project layer) are   randomly initialized. For fair comparisons, following (Deng   et al. 2021; Li et al. 2022), the input image resolutions are set   to 640 \u00d7 640 and 384 \u00d7 384 when evaluating grounding and   captioning tasks, respectively. When combining different   datasets, we carefully check the train/test sets to avoid image overlap. The captioning model adopts the beam search   strategy (beam size = 3) in all experiments. The proposed   cycle-consistency model is \ufb01ne-tuned for 20 epochs.   In the following experiments, our Cycle-Consistency   learning of captioning and grounding framework is denoted   as CyCo.   Ablation Study   Semi-weakly Supervised Training. Thanks to the cycleconsistency design, our approach is able to train the grounding model using weakly labeled data. As shown in Table 1   (top), the grounding performance is poor when only 20%   training data is available. The performance gap between the   models trained using 20% data and 100% data is considerable, e.g., 13.4% gap on RefCOCOg test set. In \u201cpseudo   label\u201d setting in Table 1, we train a grounding/captioning   model using the 20% fully-annotated data and leverage this   model to label the rest 80% images. Then, all the 100% data   are used to train the model, whose performance can be improved but is still unsatisfactory. Further, by annotating the   rest 80% training data in an online manner using our CyCo   framework, the performance gap is signi\ufb01cantly reduced. Finally, our semi-weakly supervised CyCo model only has a   minor gap in comparison to the fully-supervised model.   In Table 1 (bottom), we further ablative the regional image captioning task, which requires the model to describe   a speci\ufb01c region. Using only 20% full-annotated data, our   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5539   Ablation Study of Visual Grounding   Training Data   RefCOCO   RefCOCOg   20% data   80% data   val   testA   testB   val   test   Fully-anno   \u0017   82.07 87.33 73.20 68.08 66.65   Fully-anno   Pseudo labels   84.13 88.36 76.58 74.64 75.10   Fully-anno Weakly (CyCo) 86.73 90.27 81.87 76.85 77.81   Fully-anno   Fully-anno   88.73 91.07 84.27 80.23 80.06   Ablation Study of Regional Image Captioning   Training Data   RefCOCO testA   RefCOCO testB   20% data   80% data   M   C   M   C   Fully-anno   \u0017   29.6   77.5   34.0   131.8   Fully-anno   Pseudo labels   31.3   87.4   35.2   143.5   Fully-anno Weakly (CyCo) 33.7   98.5   36.6   151.6   Fully-anno   Fully-anno   34.8   104.2   37.9   156.6   Table 1: Comparison results of semi-weakly supervised and   fully supervised models. Top: comparison of visual grounding performance in terms of top-1 accuracy. Bottom: comparison of regional image captioning performance in METEOR (M) and CIDEr (C).   Supervised   Cycle   RefCOCO   RefCOCO+   RefCOCOg   testA   testB   testA   testB   val   test   \u2713   91.07   84.27   84.40   68.20   80.23   80.06   \u2713   \u2713   91.87   85.33   87.07   69.87   81.31   81.04   Table 2: Performance study by adding cycle-consistency   learning to the naive supervised VG model. Our \u201csupervised+cycle\u201d utilizes the same training splits but adds cycleconsistency losses as the additional regularizations, which   steadily improves the results.   semi-weakly supervised captioning model signi\ufb01cantly reduces the performance gap in comparison to the supervised   counterparts. Our CyCo framework has the potential of absorbing more weakly-labeled data such as object detection   images with only box labels or image-text pairs with only   caption annotations to further improve the performance.   Fully Supervised Training. We further combine this cycleconsistency learning pipeline with the standard supervised   learning. In each training iteration, we not only use the   ground-truth labels to supervise the grounding model, but   also utilize the VG \u2192IC and IC \u2192VG cycles to regularize   the training. As shown in Table 2, based on the same training   data, adding cycle-consistency learning to the classic supervised learning can further boost performance. This reveals   that our framework can better exploit the training data by   online data augmentation.   Adding More Training Data For VG. Since the proposed   framework jointly optimizes visual grounding and image   captioning within a uni\ufb01ed framework, we can merge the   datasets of both tasks for optimization. As shown in Table 3,   adding captioning data further improves the visual grounding results. Note that COCO Karpathy split lacks the region/box annotation, which can be regarded as the weaklylabeled data compared to the image-text-box triplet. These   results justify the potential of our framework of absorbing   more cheap data to achieve superior performance.   Ground Data Caption Data   RefCOCO   RefCOCOg   val   testA   testB   val   test   \u2713   89.47 91.87 85.33 81.31 81.04   \u2713   \u2713   89.78 92.05 85.63 82.24 82.20   Table 3: Performance study by adding more training data.   Adding weakly labeled images (e.g., COCO-caption without   box annotations) improves the grounding performance.   Training Data   RefCOCOg   COCO-caption   B@4 / CIDEr   B@4 / CIDEr   BLIP   only COCO   24.1 / 74.6   39.7 / 133.3   BLIP   COCO & RefCOCOg   30.8 / 96.2   38.4 / 130.2   CyCo   COCO & RefCOCOg   36.6 / 128.5   40.6 / 133.9   Table 4: Performance study on image captioning. Compared   to our baseline, CyCo can better leverage the datasets from   different domains (COCO-caption and RefCOCOg).   Adding More Training Data For IC. In Table 4, we exhibit the image captioning potentials of our CyCo compared   to the baseline BLIP. Without the design of regional embedding, BLIP is not aware of the local region. To this end, combining both COCO-caption and RefCOCOg datasets fails to   improve its performance. In contrast, our model can freely   switch between the local and global captions (Figure 3) and   absorb more (fully- or partially-annotated) data to further   boost the performance.   State-of-the-art Comparison   Evaluation on RefCOCO/RefCOCO+/RefCOCOg. Table 5 reports the comparison results of state-of-the-art   methods on the RefCOCO, RefCOCO+, and RefCOCOg   datasets. As ablated in Table 3, adding more captioning data   can steadily boost the grounding performance. For fairness,   when \ufb01ne-tuning the visual grounding model, we only use   the standard training splits of the above benchmarks without   involving other training data. Thanks to the vision-language   pre-training, our approach signi\ufb01cantly outperforms the   classic grounding methods without pre-training. We also include some representative pre-training-based approaches including UNITER, VILLA, ERNIE-ViL, and BLIP to justify   the superior performance of our approach. For example, our   method surpasses VILLA and ERNIE-ViL on the RefCOCO   and RefCOCO+. On the RefCOCOg dataset with longer expressions, our method also exhibits satisfactory results. In   Table 5, we also present some high-performance methods   (e.g., MDETR, OFA, and FIBER) that additionally leverage   the expensive image-text-box data for pre-training. Besides,   FIBER adopts a strong visual backbone and higher image   resolution (e.g., 800\u00d71, 333), exhibiting the leading performance. We can also improve our performance by adopting a   stronger backbone and higher image resolution, which leave   as our future work.   Furthermore, our framework enables model training using   partially labeled data. In the semi-weakly supervised setting   (e.g., 20% fully-annotated data and 80% weakly-annotated   data), it is worth noting that our method also steadily outThe Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5540   Method   Backbone   Supervised   Data   RefCOCO   RefCOCO+   RefCOCOg   I-T   I-T-B   val   testA   testB   val   testA   testB   val   test   w/o Pre-training   MCN (Luo et al. 2020)   DarkNet-53   \u2713   80.08   82.29   74.98   67.16   72.86   57.31   66.46   66.01   QRNet (Ye et al. 2022)   Swin-S   \u2713   84.01   85.85   82.34   72.94   76.17   63.81   73.03   72.52   VLTVG (Yang et al. 2022)   ResNet-101   \u2713   84.77   87.24   80.49   74.19   78.93   65.17   76.04   74.18   VG-LAW (Su et al. 2023)   ViT-B   \u2713   86.62   89.32   83.16   76.37   81.04   67.50   76.90   76.96   TransVG++ (Deng et al. 2023)   ViT-B   \u2713   86.28   88.37   80.97   75.39   80.45   66.28   76.18   76.30   Img-Text Pre-training   UNITER-B (Chen et al. 2020b)   ResNet-101   \u2713   \u2713   81.24   86.48   73.94   75.31   81.30   65.58   74.31   74.51   VILLA-B (Gan et al. 2020)   ResNet-101   \u2713   \u2713   81.65   87.40   74.48   76.05   81.65   65.70   75.90   75.93   ERNIE-ViL-B (Yu et al. 2020)   ResNet-101   \u2713   \u2713   74.02   80.33   67.74   BLIP-B\u22c6(Li et al. 2022)   ViT-B   \u2713   \u2713   88.73   91.07   84.27   79.53   84.40   68.20   80.23   80.06   Img-Text-Box Pre-training   MDETR (Kamath et al. 2021)   ResNet-101   \u2713   \u2713   \u2713   86.75   89.58   81.41   79.52   84.09   70.62   81.64   83.31   OFA-L (Wang et al. 2022)   BART-L   \u2713   \u2713   \u2713   90.05   92.93   85.26   84.49   90.10   77.77   84.54   85.20   FIBER-B (Dou et al. 2022)   Swin-B   \u2713   \u2713   \u2713   90.68   92.59   87.26   85.74   90.13   79.38   87.11   87.32   CyCo (semi-weakly)   ViT-B   \u0017   \u2713   86.73   90.27   81.87   72.53   82.00   61.33   76.85   77.81   CyCo (fully supervised)   ViT-B   \u2713   \u2713   89.47   91.87   85.33   80.40   87.07   69.87   81.31   81.04   Table 5: Comparisons with state-of-the-art methods on RefCOCO, RefCOCO+, and RefCOCOg in terms of top-1 accuracy   (%). The BLIP-B\u22c6denotes our implemented BLIP-B model on the visual grounding task, which is not included in the original   BLIP framework (Li et al. 2022). In the pre-training stage, we only use the image-text (I-T) pairs without any image-text-box   (I-T-B) triplets. We report the performance of our semi-weakly supervised model as well as the supervised model.   Method   Cross-Entropy   B@4   M   C   S   w/o Pre-training   AoANet (Huang et al. 2019)   37.2   28.4   119.8   21.3   X-LAN (Pan et al. 2020)   38.2   28.8   122.0   21.9   w/ Pre-training   Oscar-B (Li et al. 2020)   36.5   30.3   123.7   23.1   VinVL-B (Zhang et al. 2021)   38.2   30.3   129.3   23.6   LEMON-B (Hu et al. 2021a)   40.3   30.2   133.3   23.3   BLIP-B (Li et al. 2022)   39.7   133.3   23.3   SimVLM-B (Wang et al. 2021b)   39.0   32.9   134.8   24.0   CyCo (Ours)   40.6   31.2   133.9   24.4   Table 6: Performance comparisons on the COCO-caption   Karpathy test split, where B@4, M, C, S denote BLEU@4,   METEOR, CIDEr, and SPICE scores, respectively.   performs the previous fully supervised counterparts such as   UNITER and VILLA on the RefCOCO and RefCOCOg.   Evaluation on COCO Caption. Mainstream image captioning methods focus on describing the global image context. However, optimizing the proposed framework only on   the grounding dataset will overlook the captioning capability of the global content. To this end, we merge two datasets   including COCO Karpathy train split and RefCOCOg train   split to jointly optimize our cycle-consistency framework.   We de\ufb01ne the referred region of COCO-caption dataset as   the whole image. After joint training, we assess the global   captioning performance on the COCO Karpathy test split.   In Table 6, compared with the state-of-the-art captioning   methods with pre-training such as Oscar and VinVL, our   method shows better results. Our method is even comparable   with the recent SimVLM trained with 1.8 billion image-text   pairs, which is 10\u00d7 larger than ours. The proposed method   Local Caption: a green    plant in a brown pot   Global Caption:  a living    room filled with furniture    and a flat screen tv   Local Caption: the half of    a banana on the left side    of the plate   Global Caption:  a white    plate topped with food on    top of a wooden table    Local Caption: the giraffe    on the far left   Global Caption:  a group    of giraffe standing next    to each other   Figure 3: Our captioner can describe both the local (red region) and global (blue region) contexts.   slightly outperforms our baseline BLIP.   It is worth noting that our captioner focuses on a more   challenging scenario and can describe arbitrary image regions, which is infeasible for all the comparison methods.   Figure 3 showcases some examples. When describing a speci\ufb01c region, our captioner captures its relationship with surrounding objects to avoid ambiguous expressions. When describing the global image content, the proposed method performs in a similar way as the general image captioners.   Conclusion   In this paper, we bridge two vision-language tasks including   visual grounding and regional image captioning in a cyclic   training pipeline. By cycle-consistency constraints, the proposed framework can exploit the weakly-annotated data for   model training and augment the training samples to further   boost the fully supervised training. Extensive experiments   on image captioning and visual grounding datasets verify   the effectiveness of our framework.   The Thirty-Eighth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-24)   5541", "conf": "AAAI", "year": "2024", "index": 730}, {"title": "What, when, and where? Self-Supervised Spatio-Temporal Grounding   in Untrimmed Multi-Action Videos from Narrated Instructions   Brian Chen 1   Nina Shvetsova2,3   Andrew Rouditchenko4   Daniel Kondermann 5   Samuel Thomas6,7   Shih-Fu Chang1   Rogerio Feris6,7   James Glass4   Hilde Kuehne2,3,7   1Columbia University, 2Goethe University Frankfurt, 3University of Bonn, 4MIT CSAIL   5Quality Match GmbH, 6IBM Research AI, 7MIT-IBM Watson AI Lab   {bc2754,sc250}@columbia.edu, shvetsov@uni-frankfurt.de, {roudi,glass}@mit.edu   dk@quality-match.com, {sthomas,rsferis}@us.ibm.com, kuehne@cs.uni-bonn.de", "abstract": "Spatio-temporal grounding describes the task of localizing events in space and time, e.g., in video data, based   on verbal descriptions only. Models for this task are usually trained with human-annotated sentences and bounding box supervision. This work addresses this task from   a multimodal supervision perspective, proposing a framework for spatio-temporal action grounding trained on loose   video and subtitle supervision only, without human annotation. To this end, we combine local representation learning, which focuses on leveraging \ufb01ne-grained spatial information, with a global representation encoding that captures higher-level representations and incorporates both in   a joint approach. To evaluate this challenging task in a reallife setting, a new benchmark dataset is proposed, providing dense spatio-temporal grounding annotations in long,   untrimmed, multi-action instructional videos for over 5K   events. We evaluate the proposed approach and other methods on the proposed and standard downstream tasks, showing that our method improves over current baselines in various settings, including spatial, temporal, and untrimmed   multi-action spatio-temporal grounding.   1.", "content": "Spatio-temporal grounding (STG) describes the challenging   task of locating events in space and time within video data   based on text referential expressions. Methods in this \ufb01eld   usually rely on a combination of spatio-temporal bounding box annotation, together with a human-generated caption, describing the visual content of the bounding box   [23, 54], which limits their generalizability beyond the   given training scenario. Compared to that, as a second line   of work, multimodal self-supervised learning tries to leverage \u201cfree\u201d data sources, such as video and automatic speech   Task: Spatio-Temporal Grounding - Find the temporal boundary of a    queried action in an untrimmed video and spatially localize the action.    \"Crack egg\"   \u2026   background   background   \u2026   \u2026   \u2026   Evaluation Setup: Referential queries - \"Crack egg\", \"Mix egg\", etc.   Training Setup:  Unlabeled videos with narrated instructions   \"To this we will add    one carrot chopped    into jong jullien \u2026\"   \"\u2026 you can use the    forks to pull it a bit    appart\u2026\"   \"\u2026 stir it a bit so it mixes    well\u2026\"   background   \"Mix egg\"   Figure 1. Learning Spatio-temporal grounding in untrimmed   videos. In training, we learn from unlabeled videos without human   annotation. In evaluation, we perform spatio-temporal grounding   using an action description such as \u201ccrack egg\u201d as a query. The   model needs to localize both the action\u2019s temporal boundary and   spatial region in the long untrimmed video. We visualize the heatmap from the annotation points as well as derived bounding boxes.   recognition (ASR) captions from large-scale instructional   videos to learn representations without human annotation   [3, 4, 8, 35, 36]. The resulting models achieve state-ofthe-art performance on zero-shot tasks such as cross-modal   video retrieval or classi\ufb01cation and also for zero-shot temporal action segmentation and detection based on free text   queries [8, 28, 42, 47, 66], but usually lack spatial localization abilities.   A third line of work focuses on labelfree spatial grounding, e.g. by training on image-caption   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   18419   [2, 31, 50, 55, 62] or video-caption pairs [41, 46].   The   goal is to correctly localize a referential expression in an   image or each video frame, e.g., via a bounding box or a   heatmap. However, those methods are not optimized to detect whether an event is present in a video. The assumption   is thus that the evaluated expression is visible in the image   or in all video frames.   The following work aims to bring together those ideas to   address the task of spatio-temporal action grounding from   multimodal supervision in untrimmed videos. We propose   a grounding approach that uses video-text pairs based on   ASR transcripts in instructional videos and learns the spatial   representation of free-text events as well as their temporal   extent, as shown in Figure 1. To this end, we leverage two   different representations of the visual data: a global feature   representation based on full-frame information to de\ufb01ne the   temporal extent of an event and a local representation based   on frame-wise grid features for spatial localization. The   motivation for this dualism is that while the local representation captures the spatial correlations between vision and   text input, this can be too \ufb01ne-grained to learn a holistic   representation of the frame, while the global representation   can be assumed to capture a more compact, aggregated view   compared to local data and thus to provide a more reliable   cue for the task of temporal localization. However, compared to the hand-annotated video-caption setup of most   spatio-temporal grounding methods, the ASR text can be   noisy as not all comments refer to visible events. Further, as   there is only a loose temporal correlation, the described activities might not be precisely aligned, can be scattered over   multiple frames, or not be present at all [18, 36]. Therefore,   we propose to speci\ufb01cally select frames to capture only   those useful for training. To this end, we look for frames   that match the vocabulary of the respective text, leveraging a selection strategy by Sinkhorn optimal transport [11].   This allows us to train a model that can localize actions in   space and time within videos without labeling supervision.   To evaluate spatio-temporal grounding in untrimmed   videos, a new benchmark, GroundingYouTube, is proposed.   It is based on the existing MiningYouTube dataset [28]   and extended with spatio-temporal localization information.   This setup differs from other benchmarks such as   [10, 46, 63] in two ways: \ufb01rst, by using multiple center   point annotations, it focuses on the grounding of referential actions itself instead of interacting humans or objects   which are usually labeled; second, the dense annotations of   multiple actions in the video allow us to benchmark action   grounding in long, realistic untrimmed videos compared to   existing, often pre-clipped benchmarks[10, 59]. The benchmark provides queries for 512 different event types and over   5K spatio-temporal annotations, as shown in Figure 1. A   comparison of current datasets is shown in Table 1.   To evaluate the proposed approach as well as the new   Dataset   Annotation   Spatial   Temporal   V-HiCo   object bb + human bb   AVA-Kinetics   object bb + human bb   ActivityNet entities   object bb + human bb   THUMOS14   action boundaries   ActivityNet   action boundaries   HACSSegment   action boundaries   YouCook II   multi-action boundaries   Cross-Task   multi-action boundaries   COIN   multi-action boundaries   EPIC KITCHENS-100   multi-action boundaries   Ego4D   multi-action boundaries   JHMDB51-21   human tubes   UCF101-24   human tubes   action boundaries   Daly   human tubes   action boundaries   Vid-STG   human tubes   action boundaries   HC-STVG   human tubes   action boundaries   AVA   human tubes   action boundaries   YouCook-Interactions   action bb   GroundingYoutube (ours)   action bb + center points   multi-action boundaries   Table 1. Comparison of spatial, temporal, and spatio-temporal   grounding datasets.   V-HiCo [32], AVA-Kinetics [30], THUMOS14 [20], ActivityNet [6], ActivityNet entity [65], HACSSegment [60], YouCook II [64], Cross-Task [66], COIN [47], EPIC   KITCHENS-100 [12], Ego4D [14], JHMDB51-21 [21], UCF10124 [44] Daly [51], Vid-STG [59], HC-STVG [48], AVA [15],   YouCook-Interactions [46].   benchmark, the system is trained on the HowTo100M   dataset [35] and compared to state-of-the-art methods based   on full, weak, and self-supervision for spatial and temporal,   as well as combined spatio-temporal grounding tasks. It   shows that existing methods usually do well in one of the   two aspects, spatial or temporal grounding. In contrast, the   proposed method can combine spatial and temporal aspects.   We summarize the contributions of this work as follows1:   (1) We propose a framework for spatio-temporal grounding in untrimmed videos based on weakly aligned multimodal supervision without human annotation, employing a   combination of global and local representation learning to   learn the spatio-temporal extent of actions. (2) To facilitate   this task, we propose a frame selection strategy based on   Sinkhorn-Knopp Optimal transport that improves the quality of the acquired learning samples, leading to more effective supervision. (3) We provide a new benchmark and annotations to evaluate this challenging problem on real-world   multi-action instructional video data.   2. Related Work   Supervised   Spatio-temporal   Grounding.   Spatiotemporal Grounding refers to the problem of localizing   a sequence of bounding boxes (a spatio-temporal tube)   for a target object described by an input text.   This   problem has been addressed by various approaches TubeDETR [54], STCAT [23], STVGBert [45], STVGBert [45],   1We will make the code and the annotations publicly available.   18420   STGVT [48], STGRN [59], Visil [27].   These methods   rely on proposal networks such as Faster R-CNN [39]   or MDETR [25] to predict bounding box coordinates for   learning text-to-region interaction.   All those approaches   rely on supervised training with the human-annotated   sentence and bounding box supervision, provided, e.g., by   datasets such as VidSTG [59], HC-STVG [10]. While those   datasets provide a temporal aspect, temporal detection is   usually limited to identifying the start and end frame of a   single action in a video. Compared to that, an untrimmed   setting usually comprises multiple actions in a longer video   that can be separated by longer background sequences.   This conceptually differs from previous works [10] that   typically use short videos of around 5-10 seconds. Other   datasets such as ActivityNet entities [65] provide only   bounding boxes for noun phrases in the captions, namely   the objects, which is related to object detection task and   does not capture any spatial or temporal extent of actions.   Multimodal Self-supervised Learning.   The \ufb01eld of   multimodal self-supervised learning aims to learn data representations by leveraging large amounts of unlabeled data   with multiple modalities. Early works [13, 52] started by   projecting images and text into a joint visual-language embedding space, where embeddings of semantically similar   pairs are close. Those ideas have now grown into systems   such as MIL-NCE [36] using the HowTo100M dataset [35]   to train a video-language embedding space from 1.2 million   instructional videos paired with text descriptions from ASR.   Follow-up works, including [3, 4, 8, 40, 42] show that using   videos without annotation enables an effective multimodal   embedding space via contrastive learning.   Based on those advantages, approaches started to address the problem of Spatial Video Grounding from multimodal self-supervised aiming to identify spatial locations   in a trimmed video based on text descriptions without the   need for bounding box annotation during training. One of   the early works studied this task in the context of weakly   supervised learning where we learn grounding with humanannotated captions of the video [63].   In this context,   works [41, 46] have focused on object grounding benchmarks such as YouCook2-BoundingBox [64], which provides bounding box annotations for visible objects in cooking videos.   Other works such as GLIP [31], RegionCLIP [62], and others [56, 58] combine the principles of   large-scale vision language training with bounding box \ufb01netuning on object detection datasets [16, 34]. Recently, the   YouCook-Interactions dataset [46] and CoMMA [46] have   been proposed for the spatial grounding of objects and actions with multimodal self-supervision from HowTo100M   videos. These works assume that the video is temporally   clipped with respect to the grounding phrase.   Compared to that, Temporal Video Grounding aims to   determine the set of consecutive frames corresponding to a   text query in an untrimmed video [9, 22, 43], thus predicting   temporal boundaries of action instances. Recent work such   as MIL-NCE [36], MCN [8], and VideoCLIP [33] utilize   large-scale pretraining for grounding actions temporally via   text-to-frame similarity on video datasets such as MiningYouTube [28] or CrossTask [66] without proposals. However,   these methods lack spatial localization ability [57, 61].   3. A Global-Local Framework for SpatioTemporal Grounding   3.1. General setup   The goal of the proposed method is to temporally and   spatially localize actions based on free-text queries in   untrimmed videos.   To this end, two representations are   learned, a local and a global one. We start with narrated   video clips, each associated with a corresponding visual   representation and text narration.   Namely, for each clip   X = {V, S}, let V stand for the video clip and S for the   text narration sentence generated by the automatic speech   recognition (ASR) system. Each clip V consists of U \u21e5N   spatio-temporal tokens {vu,n}, where u 2 {1, ..., U} represents the number of frames in the video and n 2 {1, ..., N}   represents the number of spatial grid region tokens or features in a frame. The text sentence S consists of K words   {s1, ..., sK}. We represent localized features by the tokens   from each modality, and the global features {V, S} are acquired either by mean-pooling over the local features (S3D)   or by using the [CLS] token from the transformer (CLIP) as   in Radford et al. [38]. We learn transformations f : V !   Rd to a d-dimensional representation f(V ) 2 Rd from the   global representation V , and g : S ! Rd, to produce similar d-dimensional text global embeddings: g(S) 2 Rd.   Similar to {f, g}, we note {f   0, g   0} to be the transform for   localized features, where local features {v, s} are also projected as d-dimensional representations.   3.2. Representation guided frame sampling   Learning from multimodal self-supervision is challenging   since the narration is likely to be noisy, thus containing   more information than the actual task descriptions due to   poor temporal alignment or cut scenes [18], which is the   key differences between weakly supervised vision-caption   grounding and multimodal self-supervised grounding. This   work pursues a frame selection strategy to improve object   grounding and temporal alignment during training. We start   from a longer sequence U, where U > T, which includes   the video frames before and after the ASR boundaries that   could contain actions or objects in the sentence. Our goal is   to \ufb01nd T frames out of the U frames that are most relevant   to the actions and objects in the sentence S. We formalize   it as an optimal transport problem utilizing the SinkhornKnopp algorithm [11].   18421   Video Projection*   \u2026   \u2026   Text  Projection**    CLS   CLS   CLS   CLS   CLS   CLS   CLS   CLS   CLS   CLS   Input: Video frames and text   need    CLS   you    to   pick   two   eggs   mix   (b) Sinkhorn-Knopp Optimal Transport     Frame Selection   CLS   CLS   CLS   CLS   (c) Global representation learning   Text   Projection**   Similarity   Video   Projection*   Mean    Pooling   Similarity   pick   two   \u2026 \u2026   Mean Pool   Mean Pool   (d) Local representation Learning   Cross-Attention   Self-Attention   Cross-Attention   need    you    to   MLP   MLP   Cross-modal    Projection   \u2026 \u2026   \u2026   (a) Video-ASR Assignment Matrix Q   CLS   CLS   CLS   \u2026   \u2026   Selected   Frames   [CLS] or Spatio-temporal token   [CLS] or    word    Figure 2. Spatio-temporal grounding approach. (a) We aim to select frames with groundable objects and actions. To this end, projected   text features are matched with respective frame features. (b) Sinkhorn optimal transport is then leveraged to optimize the selected frames   wrt. the text input. (c) Based on the selected frames, a global representation is learned to allow for temporal localization, as well as (d) a   local representation to ground the action description in the spatial region.   Optimal transport for text-to-frame assignment. To acquire the optimal assignment from word features to video   frames, an assignment matrix Q is computed from each   video and ASR pair as shown in Figure 2(a). This crossmodel optimal transport mechanism is applied to assignment Q from the projected cross-model similarity P between word tokens and each video frame, where P =   g(S) N f(V)> 2 RK\u21e5U. To compute the assignment matrix, the text and video projection layers from the global   representation in Figure 2(c) are used to project multimodal   features into a common space for feature similarity calculation. We investigate various granularities of the features   where we compute the similarity between the text features   at the word (local) / sentence (global) level and the visual   feature at frames (global) / spatiotemporal tokens (local)   level to acquire P, as shown in Table 5. To ensure that   the word-to-frame assignment contains more diversity instead of just saturated assignments to a single video frame,   we add a constraint by Sinkhorn that requires label assignments to be equally distributed across various video frames   representing diverse object/action concepts. Details of the   Sinkhorn optimal transport are included in the appendix 7.1.   3.3. Local representations for spatial localization   To capture multimodal interaction with \ufb01ner granularity,   we need to learn the projection between tokenized features as shown in Figure 2(d). We extract spatio-temporal   region features vtn from the video.   Also, we extract   word features sk which represents the feature from word   k.   All tokenized features are projected through a linear   layer.   To compute attention between the tokenized features, we stacked two cross-modal attention layers with a   self-attention layer in the middle, as illustrated in Figure 2   (d). Cross-modal attention is computed similar to the standard attention mechanism [29]. Given a spatio-temporal token vtn from a video, we compute the attention score to   all of the words sk, where k 2 {1, ..., K} in the ASR sentence S by \u21b5tnk =   exp(etnk)   PK   k=1 exp(etnk) in the same video clip,   where etnk = cosine(vtn, sk). We then acquire a contextual video token feature \u00afvtn = PK   k=1 \u21b5tnksk, which   encoded text contextual information.   Note that the contextual vector is represented by aggregating the representations from the other modality. Follow the standard selfattention computation [49] K, Q, V represent the features   for the keys, queries, and values as: Attn(K, Q, V ) =   softmax   \u2713   (Q>K)   pdk   \u25c6   V where dk is the dimension of the key.   In our case, we feed each contextual feature {\u00afvtn,\u00afsk} right   after the \ufb01rst cross-attention layer to the K, Q, V to acquire its self-attended representation. The localized attention model was trained using contrastive loss.To represent   the video clip V and ASR sentence S, we mean-pool over   the spatio-temporal tokens in video \u00afV =   1   T N   PT N   r=1 \u00afvr, and   words \u00afS =   1   K   PK   k=1 \u00afsk respectively. Let   % \u00afV (l), \u00afS(l)&   be   the l-th training example pair. We adopt the Noise Contrastive Estimation (NCE) loss [17] and the localized attention losses LLocal :   \u22121   B   B   P   l=1   \"    log   e   \u00afVl\u00b7 \u00afSl\u2212\u03b4   e   \u00afVl\u00b7 \u00afSl\u2212\u03b4 +   B   P   k=1   k6=l   e \u00afV imp   k   \u00b7 \u00afSl   !   +       log   e   \u00afVl\u00b7 \u00afSl\u2212\u03b4   e   \u00afVl\u00b7 \u00afSl\u2212\u03b4 +   B   P   k=1   k6=l   e \u00afVl\u00b7 \u00afSimp   k   )   !#   (1)   where B is the batch. \u00afV imp   k   and \u00afSimp   k   represent imposter   samples, and \u03b4 is a margin hyperparameter.   3.4. Global representations for temporal   We learn the global representation of a video clip and a sentence by contrastive loss, as shown in Figure 2(c). We again   use the NCE loss function [17]. The global contrastive loss   LGlobal follows the formulation as Equation 1 while using   the global representations V and S, which are the [CLS] tokens or mean-pooled features from both modalities, instead   of the local representations. Projecting the global features   to the same space ensures that the features across different   18422   modalities are comparable. Since global representations encode information from the entire video, it is essential in encoding temporal information for the later downstream tasks.   The \ufb01nal model is optimized by the sum of both losses as   LF inal = LLocal + LGlobal.   CLS CLS CLS CLS   CLS   Crack egg   Pour eggs   CLS   Beat mixture   CLS   Tilt pan    CLS   Similarities   (a) Temporal grounding   (b) Spatial grounding   crack   egg   Cross-Attention   Self-Attention   Cross-Attention   MLP   MLP   Attention Rollout   \u2026   Attention    Scores   0.0   0.1   0.7   0.0   0.1   0.5   0.0   0.0   ...   Projection   Projection   Threshold   Similarity   High   Threshold   CLS   Figure 3. Spatio-temporal inference. Both representations are   used for spatio-temporal grounding: Starting by predicting the   action boundary, spatial grounding is performed on the selected   frames using the predicted label to \ufb01nd corresponding regions.   3.5. Inference for spatio-temporal grounding.   To perform spatio-temporal grounding on untrimmed   videos, we start from temporal action detection as shown in   Figure 3. Given a pool of possible action descriptions on the   left and an untrimmed video, we perform feature similarity   matching using the global representation ([CLS] token or   mean-pooled feature) per frame with a threshold \u2327to \ufb01lter   backgrounds. We pick the action class with the largest similarity score per frame. Later, we use the predicted action   class and feed it into the local representation branch to compute spatial grounding. We follow [1] to compute feature   similarity between visual tokens and text tokens through the   cross-attention and self-attention. In the end, we acquire an   attention heatmap for later downstream evaluation. More   information on inferencing are in appendix 8.3.   4. GroundingYoutube Benchmark   Current downstream datasets either provide spatial [46],   or temporal annotation [28], or spatio-temporal annotation [59] but only for short video clips with few frames   before and after the action takes place. These datasets do   not provide the opportunity to evaluate both aspects, spatial   and temporal grounding, in an untrimmed long video manner. We, therefore, extend one of the current benchmarks,   MiningYouTube [28], with 250 untrimmed videos with a   duration of around 3-5 minutes and an average of 41.6 labeled action instances per video. Note that each video contains various classes. As the dataset already provides dense   temporal annotations, we annotate the respective temporal   action segments in the dataset with spatial information.   Annotating the spatio-temporal extent of actions can be   challenging as there is no clear visible outline as, e.g., in object annotation, nor is there a unique signal to indicate the   Figure 4. Visualization of the point annotation and automatic   bounding box generation from points. The red point represents   the mean of the \ufb01ve annotation points. The points annotation captures diverse patterns in various action types.   temporal begin and end points. Similarly, grounding systems do not produce pixel-exact bounding boxes, but rather   indicate regions of interest. Detector-free spatial grounding models [5] address this fuzziness by relying on pointing game accuracy, thus only using the center point of the   heat map for evaluation. Lending on this idea, annotators   were asked to mark the presumed center point of the action.   Compared to bounding boxes, center point annotation can   be advantageous because annotators are not visually distracted by object outlines, so it is more likely that the most   important region will be selected. We capture \ufb01ve annotations per frame, resulting in a density-based heatmap.   Starting from 5,091 clips showing one of the 512 action   classes, we adopt the methodology used for temporal action   localization developed in [15] and label one frame per second, resulting in 26, 987 labeled frames. We annotated all   frames with \ufb01ve repeats per image, resulting in \ufb01ve annotations per frame and 134, 935 point labels in total. To evaluate using bounding boxes [24], we get the union of all annotated points with additional distance to construct the bounding box as shown in Figure 4. More information on the   annotation process, bounding box derivation, and dataset   analysis is provided in the appendix 10.   5. Experiments   5.1. Datasets   Training Data: HowTo100M dataset contains 1.2M instructional videos along with their corresponding automatically generated speech (ASR). The narrations may be inaccurate and do not always accurately depict the video scene.   18423   GroundingYoutube   Method   Backbone   DataSet   Supervision   Modality   IoU+Point   mAP   0.1   0.2   0.3   0.4   0.5   0.1:0.5   CoMMA\u2020 [46]   S3D   HT250K   Self   VT   1.02   2.18   1.72   1.11   0.93   0.37   1.26   MIL-NCE [36]   S3D*   HT100M   Self   VT   4.67   33.94   25.16   12.65   3.42   0.41   15.11   Ours   S3D   HT100M   Self   VT   9.12   42.70   35.49   25.16   16.22   10.05   25.92   GLIP [31]   Swin-L*   Cap24M   Weak   IT   1.24   2.83   2.10   1.52   0.96   0.37   1.56   CoMMA\u2021 [46]   CLIP   HT100M   Self   VT   1.68   3.51   2.32   1.88   0.99   0.40   1.82   CLIP [38]   CLIP   HT100M   Self   IT   3.59   29.54   22.15   9.16   2.48   0.39   12.74   RegionCLIP [62]   ResNet-101*   CC3M   Weak   IT   5.65   35.65   27.43   15.69   4.31   0.86   16.78   Ours   CLIP   HT100M   Self   VT   10.09   42.81   36.05   25.84   17.10   11.35   26.63   Ours   CLIP*   HT100M   Self   VT   11.53   43.64   36.94   26.78   19.45   14.61   28.26   MIL-NCE(temp.)+RegionCLIP(spa.)   VT   9.21   40.54   34.97   22.38   13.79   9.18   22.33   Table 2. Spatio-temporal grounding on GroundingYouTube full videos. The proposed model learns global representations encoding   global information and spatial correspondences across modalities, achieving a better performance in spatio-temporal evaluation compared   to models trained on only spatial or temporal grounding. (V: video, I: image, T: text.) \u21e4indicates \ufb01netuned backbone.   Downstream Datasets:   GroundingYoutube (GYT) is   used to evaluate the task of multi-action spatio-temporal   grounding as described in Section 4.   MiningYoutube   (MYT) [28] provides temporal annotation and is limited   to the domain of cooking instruction videos. YouCookInteraction (YC-Inter) [46] is an extension of the   YouCook2 dataset [64] for cooking instruction providing   bounding boxes for 6K selected frames.   The bounding   boxes usually comprise the hand and the tool mentioned in   the respective sentence-wise annotation. To further benchmark on general video domains on the V-HICO dataset [32]   with 6.5k videos with human-object interaction bounding   boxes annotations, and Daly action dataset [51], featuring   videos consisting of daily actions such as \u201cbrushing teeth\u201d.   5.2. Baseline methods   Temporal:   MIL-NCE   [36]   utilizes   S3D   [53]   and   word2vec [37].   CLIP [38], an image-text model with   transformer.   Spatial:   CoMMA [46], SSL model (\u2020   for weights shared by the author2 \u2021 trained with CLIP);   GLIP [31], RegionCLIP [62], SOTA weakly supervised   grounding model. Spatio-temporal: We construct MILNCE+RegionCLIP following the inference pipeline in Figure 3. TubeDETR [54] and STCAT [23] are supervised.   More descriptions of the baselines are given in the Appendix 8.1. Details of the implementation and experimental   settings can be found in the appendix 8.2. Inference setups   for each baseline are described in Section 8.3.   5.3. Downstream Tasks   We considered the following downstream tasks to evaluate   spatio-temporal grounding abilities of various models (detailed description is included in the appendix 8.4):   (i) Spatio-temporal grounding in untrimmed video is   evaluated on the proposed Grounding Youtube dataset. The   2We thank the authors for providing code and weights.   entire video and the respective pool of action instructions   were provided. The model needs to localize each action   step in time (start-time/end-time) and space (location in the   video) as described in Figure 3. We evaluate in two metrics:   IoU+Pointing game combines the evaluation setting from   the spatial grounding [2] and temporal grounding [28] metrics. For each video frame, the prediction is correct when   the model predicts the correct action for the frame. Also,   given the predicted action as a query, the maximum point   of the heatmap aims to lie within the desired bounding box.   We then compute the Intersection over Union (IoU) over   all the predictions with the GT to acquire the \ufb01nal score.   We also compute video mAP following previous evaluation   [15], where we set IoU threshold between GT and predicted   spatio-temporal tubes. A prediction is correct when it surpasses the IoU threshold. We then compute the mAP over   all classes. We form a 3D prediction mask following Figure   3 and compute IoU between our 3D heatmap and 3D tube.   (ii) Spatial grounding is given a text description to localize the region in the trimmed video. It is evaluated using   the pointing game accuracy. If the predicted point lies   in the ground truth bounding box, the result counts as a   \u201chit\u201d and counts as \u201cmiss\u201d otherwise. The \ufb01nal accuracy   is calculated as a ratio between hits to the total number of   predictions   # hits   # hits+# misses. We also report the mean average   precision (mAP) following the settings from V-HICO [32].   (iii) Temporal grounding provides videos with the respective actions and their ordering, including the background.   The goal is to \ufb01nd the correct frame-wise segmentation   of the video. We follow the inference procedure in [28]   to compute the alignment given the similarity input matrix. The task is evaluated by intersection over detection   (IoD), de\ufb01ned as G\\D   D   the ratio between the intersection   of ground-truth action G and prediction D to prediction D,   and the Jaccard index, which is an (IoU) given as G\\D   G[D.   18424   YC-Inter   GroundingYT   V-HICO   Daly   Method   Backbone   Data   Super.   Mod.   Acc   Acc   mAP   Acc   mAP   Acc   mAP   MIL-NCE [36]   S3D*   HT100M   Self   VT   23.67   27.45   8.21   12.65   11.23   13.84   24.23   CoMMA\u2020 [46]   S3D   HT250K   Self   VT   48.63   47.68   23.38   40.97   21.45   54.48   33.39   Ours   S3D   HT100M   Self   VT   53.98   60.62   44.93   44.32   24.31   66.35   45.93   CLIP [38]   CLIP   HT100M   Self   IT   14.10   12.50   3.49   29.23   12.51   18.02   27.28   CoMMA\u2021 [46]   CLIP   HT100M   Self   VT   52.65   47.56   36.42   55.20   34.54   61.06   44.37   RegionCLIP [62]   RN50x4*   CC3M   Weak   IT   51.56   52.84   23.42   57.92   37.82   67.12   48.62   GLIP [31]   Swin-L*   Cap24M   Weak   IT   52.84   53.62   24.73   66.05   41.17   Ours   CLIP   HT100M   Self   VT   57.10   55.49   43.12   60.71   39.28   70.08   50.56   Ours   CLIP*   HT100M   Self   VT   58.35   56.98   45.32   62.34   41.56   71.35   52.78   TubeDETR [54]   MDETR   Vid-STG   Full   VT   51.63   53.24   41.76   63.23   40.87   84.21   62.98   STCAT [23]   ResNet-101   Vid-STG   Full   VT   54.47   55.90   44.21   65.34   41.10   85.42   63.94   Table 3. Video spatial grounding. We evaluate the accuracy of the pointing game and the mean average precision. We listed CNN-based   methods on top and transformer-based methods in the middle. Models learning global representations (MIL-NCE, CLIP) don\u2019t perform   well on localization tasks, while our model outperforms other grounding methods. \u21e4indicates \ufb01netuned backbone.   Method   Backbone   Data   Super.   IoU   IoD   Mining: MLP [28]   TSM   MiningYT   Weak   9.80   19.20   CoMMA* [46]   S3D   HT250K   Self   2.05   5.63   RegionCLIP [62]   RN50x4*   CC3M   Weak   10.96 16.86   MIL-NCE [36]   S3D*   HT100M   Self   18.69 26.74   Ours   S3D   HT100M   Self   19.18 27.65   Ours   CLIP   HT100M   Self   19.88 28.50   Ours   CLIP*   HT100M   Self   20.33 29.67   Table 4. Temporal Grounding on MiningYoutube. \u21e4indicates   \ufb01netuned backbone. Spatial-focused model CoMMA is not trained   for temporal detection, which results in lower performance. In   contrast, the proposed model combines global and local representation, resulting in better temporal localization than one alone.   5.4. Comparison with state-of-the-art methods   (i) Spatio-temporal grounding in untrimmed video: We   \ufb01rst compare the proposed method with other approaches   designed for spatial or temporal grounding in Table 2. It   shows that models without speci\ufb01c loss designs for spatial grounding (MIL-NCE [36], CLIP [38]) show good   mAP scores but lower pointing game accuracy. Out of the   two weakly supervised methods, GLIP [31] and RegionCLIP [62]), trained with aligned image-text, RegionCLIP   show signi\ufb01cantly better performance in this setting, while   both perform in a similar range in the spatial grounding scenario (see Table 3). We attribute this behavior to the fact   that RegionCLIP distinguishes frames with relevant queries   better from background than GLIP, leading to better temporal localization. We \ufb01nally compare the strong baseline   MIL-NCE+RegionCLIP, which combines two approaches   specialized in temporal and spatial aspects, to our task. It   shows that the proposed method improves over all other   baselines underlining the need to incorporate global (temporal) and local (spatial) representations.   (ii) Spatial grounding:   Second, we compare the performance of the proposed framework to other methods on the   task of spatial grounding, including models with weak supervision, as well as models trained in a fully supervised   setting in Table 3. In the instruction video domain (GYT   and YC-Inter), the proposed approach achieves the best result among all weakly and self-supervised trained methods.   In the general domain (V-HICO and Daly), the method also   achieves competitive results, showing the generalizability   of the model to other domains. Note that in the Daly dataset,   the classes are verbs, which are not detectable by the objectfocused model GLIP. Compared to their weakly trained   counterparts, fully-supervised model (TubeDETER [54],   STCAT [23]) achieve competitive performance in the general domain (V-HICO, Daly) and slightly lower performance in instruction domain (GYT, YC-Inter) due to the   domain gap with respect to the training data.   (iii) Temporal grounding: We evaluate temporal grounding in Table 4. Here, it shows that global representations   also pro\ufb01t from local representation learning.This hypothesis is further validated in the ablation studies in Table 6,   where we ablate both losses for all three settings and show   a consistent improvement in the joint loss formulation.   5.5. Ablation study   We perform ablation studies with respect to all three settings, spatio-temporal grounding, as well as spatial and temporal grounding alone, reporting performance for spatiotemporal grounding on GroundingYT using mAP with   IoU@0.4, on temporal grounding using MiningYT IoU, and   on spatial grounding using YC-Inter. pointing game. Additional ablation are in appendix 9.3.   Frame selection strategy. We perform an ablation on the   possible frame selection strategies for our method (Figure   2(b) and Section 3.2). In Table 5, None uses all frames   within the ASR boundary (U = T) as our video training   data. Global represents the [CLS] token in text and video.   Local uses the words and spatio-temporal tokens. In the   setting Sinkhorn was not applied, the top T frames with   the highest similarity score were selected. When we set   18425   GroundingYT   MiningYT   YC-Inter.   GroundingYT   MiningYT   YC-Inter.   Spatio-temporal   Temporal   Spatial   Spatio-temporal   Temporal   Spatial   w/o Sinkhorn   with Sinkhorn   None   17.43   18.34   57.42   Global T - Global V   18.24   19.38   56.31   18.96   19.89   57.34   Global T - Local V   18.01   19.31   57.56   18.53   19.35   57.67   Local T - Global V   18.05   19.85   57.34   19.32   20.36   58.13   Local T - Local V   18.31   19.48   57.86   19.43   20.16   58.51   Average over last two   18.36   19.68   57.77   19.45   20.33   58.35   Table 5. Frame selection: (a) Sinkhorn selection results in better supervision. (b) We investigate all possible combinations of global and   local representations for frame selection similarity matching. We found keeping the local text representation is crucial, and a combination   of local and global representation leads to the best spatio-temporal grounding result.   Figure 5. Visualization on GroundingYoutube. Red box: annotation. Heatmap: prediction. Baseline GLIP focuses on objects such as   power, mixer, and pancake. Our method focuses on actions such as whisk and serve.   GroundingYT   MiningYT   YC-Inter.   Spatio-temp   Temporal   Spatial   only Local loss   7.29   5.23   55.29   only Global loss   9.28   19.12   36.23   w/ Both loss   19.45   20.33   58.35   Table 6. Loss ablations: both losses contribute to the \ufb01nal loss,   and the existence of global loss helps the localization task.   spatio-temporal tokens as the selection target, we sum over   the scores with respect to each frame to acquire the frame   similarity score. It shows that selecting frames based on   Sinkhorn selection leads to consistently better results as it   enforces more variety of visual concepts but also captures   frames with possible groundable objects. It further shows   that word tokens are more suitable than the global text CLS   token for frame selection. Finally, we see that depending on   the task (spatial vs. temporal), a local resp. global representation is better, and a combination of both works best for   spatio-temporal grounding. We provide runtime analysis of   such frame selection strategy in the appendix 9.1.   Global and local loss. As mentioned in the spatio-temporal   evaluation, both features contribute to the \ufb01nal grounding   result. We test the model by ablating out each loss. Table   6 shows that each loss not only contributes to the spatiotemporal grounding on the GYT, but also that the whole is   more than the sum of its parts (losses) since this task requires both spatial and temporal detection. The reduced impact of the global loss in the case of YC-Inter is that this   is a pure spatial grounding dataset (no background frames)   without temporal detection, and the local loss plays a more   critical role. We observe the same patterns in the temporal   grounding result for MYT, where spatial localization is not   directly contributing to the \ufb01nal performance. We tried out   the same ablation using in the S3D backbone in supplement.   5.6. Qualitative results   We visualize our spatio-temporal result in Figure 5. For the   GLIP model, we output the bounding box with the highest   con\ufb01dence score and visualize its center point. We found   GLIP model focuses on the salient object while our model   focuses more on human-object interaction.   6. Conclusion   We presented an approach for learning spatio-temporal   grounding with self-supervision and a new dataset: GroundingYoutube annotations, where we densely annotate spatiotemporal points/boxes from untrimmed multi-action videos.   Our method includes a frame selection mechanism that   identi\ufb01es frames with groundable objects to adapt the learning process for untrimmed videos. Furthermore, we jointly   learn global representations, which capture temporal information, and local representations learning \ufb01ne-grained multimodal interactions between video and text. We conducted   extensive experiments and our approach shows state-of-theart performance in spatio-temporal grounding, as well as   temporal and spatial grounding alone.   Acknowledgments: Nina Shvetsova is supported by the German Federal Ministry of   Education and Research (BMBF) project STCL-01IS22067. Andrew Rouditchenko   is supported by an NDSEG Fellowship and by the MIT-IBM Watson AI Lab. We   thank the MIT-Satori cluster team for their technical support.   18426", "conf": "CVPR", "year": "2024", "index": 701}, {"title": "Refer-it-in-RGBD: A Bottom-up Approach for   3D Visual Grounding in RGBD Images   Haolin Liu1,2   Anran Lin1   Xiaoguang Han1,2,*   Lei Yang4   Yizhou Yu3,4   Shuguang Cui1,2   1SRIBD, CUHK-Shenzhen\u2020   2FNii, CUHK-Shenzhen\u2021   3Deepwise AI Lab   4The University of Hong Kong", "abstract": "Grounding referring expressions in RGBD image has   been an emerging \ufb01eld. We present a novel task of 3D visual   grounding in single-view RGBD image where the referred   objects are often only partially scanned due to occlusion.   In contrast to previous works that directly generate object   proposals for grounding in the 3D scenes, we propose a   bottom-up approach to gradually aggregate content-aware   information, effectively addressing the challenge posed by   the partial geometry.   Our approach \ufb01rst fuses the language and the visual features at the bottom level to generate a heatmap that coarsely localizes the relevant regions   in the RGBD image. Then our approach conducts an adaptive feature learning based on the heatmap and performs   the object-level matching with another visio-linguistic fusion to \ufb01nally ground the referred object. We evaluate the   proposed method by comparing to the state-of-the-art methods on both the RGBD images extracted from the ScanRefer   dataset and our newly collected SUNRefer dataset. Experiments show that our method outperforms the previous methods by a large margin (by 11.2% and 15.6% Acc@0.5) on   both datasets.   1.", "content": "Localizing objects described by referring expressions   in vision signals, also known as visual grounding, has   long been a major motive for robotics and embodied vision.   So far, we have seen growing efforts devoted to   visual grounding in images [17, 36, 13, 40, 24, 29, 33,   5, 41, 11, 42, 10, 9, 12, 19, 47, 18, 35, 38, 39, 20] and   videos [46, 45, 43, 37, 30, 31, 44]. Suppose that a robot   is going to take \u2018the spoon on the table in the kitchen\u2019   following your command [14, 23]; this would require a   *Corresponding Email: hanxiaoguang@cuhk.edu.cn   \u2020Shenzhen Research Institute of Big Data   \u2021The Future Network of Intelligence Institute, The Chinese University   of Hong Kong, Shenzhen   Query : This is a brown armchair. It is in a corner of the room.   Query : There is a gray trash can at the corner of the room. It    is located left to the table legs and below the table.   Figure 1: We present a novel task of 3D visual grounding in   single-view RGBD images given a referring expression, and   propose a bottom-up neural approach to address it. Our goal   is to estimate the bounding box that encloses the full shape   of the referred object even this object is only partially observed (top-left). Predicted bounding boxes of the referred   objects are in green.   more accurate localization result, preferable the 3D coordinate of the referred object rather than a 2D bounding box.   Recent works [4, 1] extend the visual grounding task to   3D scenes [7] and localize the object referred by a natural language expression. While promising results are produced, these methods can only perform 3D visual grounding in complete scenes that are reconstructed and/or segmented [4, 1] in advance. Thus, they are not readily applicable to single-view RGBD images with partial observation,   RGBD streaming data, or any dynamically changing environments.   To this end, we propose a novel task for 3D visual   grounding: Given a single-view RGBD image of a scene,   6032   we aim at estimating the 3D bounding box of the full target   object described by a given referring expression. While this   novel task opens up many promising possibilities, it also   poses a major challenge due to the nature of single-view   RGBD images that they contain only incomplete information about the scene and often partial observation of the referred object. Compared to 2D visual grounding on image   and 3D visual grounding on complete scene where geometry information is complete in 2D and 3D space, various   occlusion cases in our task require a holistic understanding   of the object geometry to infer the 3D bounding boxes enclosing the full target object.   Image-based grounding methods may be applied to   RGBD images but require an extra effort to lift the produced   2D bounding boxes to 3D. Chen et al. [4] proposed a onestage search and match strategy for 3D grounding. However, it fails to handle single-view RGBD images where   the referred objects are partially observed. The reason is   two-fold: First, it is inadequate to directly match between   features of these object proposals and the referring expression to achieve reliable grounding, as each proposal contains only incomplete information due to the partial observation. Moreover, this is worsened by the fact that only   content-free object proposals are generated by a detection   network that searches the scene globally, thus failing to accumulate useful information about the referred object for   grounding.   With these observations, we propose a novel, bottomup matching approach for \ufb01ne-grained grounding of the   referred objects in given single-view RGBD images. To   this end, our approach \ufb01rst matches the query expression   to the input RGBD image and generates a content-aware   heatmap on the voxel domain converted from the RGBD   image. This bottom-level matching amounts to coarse localization of regions, which are relevant to the referred object. Then, based on the content-aware coarse localization,   an adaptive search-and-match strategy is employed. This   enables our network to conduct \ufb01ne-grained search in the   relevant regions and generate visio-linguistic features by   fusing the query with more informative features from the   visual modality. These fused features are used to generate and re\ufb01ne the 3D object proposals to the \ufb01nal bounding   box enclosing the target object in the given referring expression. Compared to previous works, our bottom-up approach   exploits the language features at different levels, and thus   enables our network to be content-aware during searching   and matching stages. The adaptive search guided by the   content-aware heatmap also ensures the feature learning to   be concentrated in the relevant regions, mitigating the challenge posed by the partial geometry.   Mauceri et al. [22] present the SUN-Spot dataset that   provides spatial referring expressions to raw single-view   RGBD images in the SUNRGBD dataset[32]. However, the   amount of language annotations as well as their linguistic   variations (only spatial", "conf": "CVPR", "year": "2021", "index": 157}, {"title": "Multi3DRefer: Grounding Text Description to Multiple 3D Objects   Yiming Zhang1   ZeMing Gong1   Angel X. Chang1,2   Simon Fraser University1   Alberta Machine Intelligence Institute (Amii)2   {yza440, zmgong, angelx}@sfu.ca   https://3dlg-hcvc.github.io/multi3drefer/", "abstract": "We introduce the task of localizing a flexible number of   objects in real-world 3D scenes using natural language descriptions.   Existing 3D visual grounding tasks focus on   localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement).   To address this setting we propose   Multi3DRefer, generalizing the ScanRefer dataset and task.   Our dataset contains 61926 descriptions of 11609 objects,   where zero, single or multiple target objects are referenced   by each description. We also introduce a new evaluation   metric and benchmark methods from prior work to enable   further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging   2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state   of the art on the ScanRefer benchmark.   1.", "content": "There is growing interest in multi-modal methods that   connect language and vision, tackling tasks such as image captioning, visual question answering, text-to-image   retrieval, and generation. One fundamental task is visual   grounding where natural language text queries are linked to   regions of an image or 3D scene. While the problem of visual grounding has been well studied in 2D images, there   are fewer datasets and methods studying the problem in 3D   scenes. Being able to indicate the object that the text \u201cthe   first of four stools\u201d", "conf": "ICCV", "year": "2023", "index": 209}, {"title": "Can I Trust Your Answer? Visually Grounded Video Question Answering   Junbin Xiao   Angela Yao*   Yicong Li   Tat-Seng Chua   Department of Computer Science, National University of Singapore   {junbin,ayao,chuats}@comp.nus.edu.sg, liyicong@u.nus.edu   Q1. Why did the baby pick up one present from    the group of  them and move to the sofa?    Q2. Why was there torn wrapping paper on the    ground near the sofa at the end of  the video?    Unwrap it.    (Prediction)   Man tears it.   (Prediction)    Boy threw it there.    (Ground Truth)    Unwrap it.    (Ground Truth)   \u2026   \u2026   \u2026   BlindQA   SigFQA   SoTA   45.0   47.5   50.0   52.5   55.0   57.5   60.0   62.5   65.0   Accuracy (%)   49.3   60.6   63.2   QA Performance   26.9%   13.4%   10.6%   15.3%   8.7%   22.2%   53.8%   BlindQA   SigFQA   SoTA   Overall Predictions   4.1%   4.2%   3.8%   6.8%   3.7%   14.9%   37.8%   BlindQA   SigFQA   SoTA   Correct Predictions   14.9%   3.7%   6.8%   3.8%   4.2%   4.1%   24.8%   BlindQA   SigFQA   SoTA   Wrong Predictions   Figure 1. Top: Real predictions of VQA models (BlindQA, SigFQA and SoTA) on NExT-QA [55]. All the models correctly answer Q1 but   wrongly answer Q2, albeit the two questions sharing visual evidence (the boy unwraps the present and throws the wrapping paper). Bottom:   Overlap in model predictions. BlindQA: A pure language model (i.e. RoBERTa [36]) fine-tuned with question-answer text. SigFQA: An   image-text model (i.e. CLIP [44]) using only the center video frame. SoTA: Temp[CLIP] (Sec. 5) model using 32 video frames. The analyses   indicate that the models may not learn from causal visual content but more likely from language short-cut and irrelevant visual context.", "abstract": "We study visually grounded VideoQA in response to the   emerging trends of utilizing pretraining techniques for videolanguage understanding. Specifically, by forcing visionlanguage models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain   the extent to which the predictions of such techniques are   genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context.   Towards this, we construct NExT-GQA \u2013 an extension of   NExT-QA with 10.5\ud835\udc3etemporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we   scrutinize a series of state-of-the-art VLMs. Through posthoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their   strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy,   we further explore and propose a grounded-QA method via   Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this   grounding mechanism improves both grounding and QA.   With these efforts, we aim to push towards trustworthy VLMs   in VQA systems. Our dataset and code are available at   https://github.com/doc-doc/NExT-GQA.   1.", "content": "Video Question Answering (VideoQA) has recently   emerged as a golden testbed to develop vision-language   models (VLMs), especially foundation VLMs pretrained at   scale on multi-modal web corpora [1,11,24,29,51,61,63,66].   Despite significant advancements in QA performance, a fundamental concern arises \u2013 whether or to what extent are   the answers of such techniques grounded on the relevant   visual content? Alternatively, are they relying on the language short-cut for the use of powerful language models   [20,34,42,52,62,64,69,70,73] or spurious vision-language   correlation captured via cross-modal pretraining [45,60]?   For example, Fig. 1(Top) shows that existing VLMs   are inclined to answer questions with language-biased preThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   13204   dictions, e.g., \u201cunwrap (Q1:   present)\u201d and \u201ctear   (Q2:   paper)\u201d. Fig. 1(Bottom) shows that the overall   predictions of SoTA VLMs overlap the predictions of standalone language models (BlindQA), i.e., models without   visual inputs, by 62.5%. In fact, the BlindQA counterpart   shares 66% of correct predictions and also 79% of the wrong   predictions in SoTA VLMs. The overlap increases by injecting a coarse visual signal from a single frame [3,23] into the   language model; as our later analysis will show, this frame   often lies outside the key moments of the correct answers.   Given these findings, a natural question arises \u2013 to what   extent are the predictions of current VLMs grounded on   the video content, and more precisely on the relevant parts?   To answer this, we propose to study visually grounded   VideoQA. Grounded VQA requires VLMs to answer the   questions and simultaneously output the relevant video moments to support the answers. Earlier works have explored   grounded QA under full supervision [25,26], but we target   visual explanability in VideoQA and thus define the task   under weak-supervision, which is the first of its kind.   To accomplish the goal, we construct the NExT-GQA   (short for Grounded) dataset by extending the NExT-QA   dataset [55] with 10.5\ud835\udc3etemporal labels of start and end   timestamps for the QA pairs in the validation and test sets.   The labels are manually annotated and checked to be key for   comprehending the questions and determining the correct   answers. With NExT-GQA, we examine a series of recent   high-performing VLMs, including task-specific architectures   without pretraining [57] and pretrained models with either   image-text or video-text data [11,44] and those using frozen   large language models (LLMs) [62,64]. Our findings reveal   that all these models struggle to predict visually grounded   answers, despite their strong QA performance. For example,   the SoTA model [62] achieves QA accuracy of 69%, but only   16% of the correctly predicted answers are grounded in the   video. In contrast, humans can ground 82% out of the 93%   of the correctly answered questions. Such clear discrepancy   underscores the need for continued research efforts.   As a pioneering solution, we propose a temporal grounding approach which can be easily applied to existing VLMs   for visually grounded VideoQA. Specifically, our approach   learns differentiable Gaussian masks along the temporal   dimension of the videos, by optimizing light-weight transformer layers under both VQA and question-video (QV)   supervisions, without the need for temporal labels. Experiments with different QA backbones demonstrate that our   approach effectively improves video grounding and question   answering as well. The improvement is especially significant   on a subset of questions that necessitate video understanding   and temporal grounding.   To summarize our contributions: 1) we conduct the first   study of weakly grounded VideoQA, and release the NExTGQA benchmark, to facilitate research on more trustworthy   VLMs; 2) we comprehensively analyze a wide range of   advanced VLMs and reveal their limitation in performing   visually grounded QA; 3) we propose a simple yet effective   grounding mechanism which not only enhances existing   VLMs in visual grounding but also contributes to new SoTA   QA performance, e.g., 73.1% on NExT-QA test set.   2. Related Work   Benchmarks   Grounded VQA with full supervision has   been studied in both image [4, 77] and video [25, 26] domains. Recently, weakly-supervised grounding has received   increasing attention in ImageQA [18,19] and video grounding [12, 39]. Nonetheless, to our best knowledge, there is   no work for weakly-grounded VideoQA. Also, existing supervised benchmarks are either biased towards localizing   subtitles in TV shows (e.g. TVQA [25]) or limited to few   objects (e.g. VidSTG [72]). Thus, they are not ideal benchmarks for visual evidence grounding.   Techniques   Strong VideoQA methods are predominantly   banked on transformer [49] and pre-training [44]. The popular transformer architectures follow either shared [24,50],   dual [57, 58, 60, 61] or stacked [11, 29, 62] implementations, and pre-training is done with image-text [24], videotext [29,61,65,66] or both [11] forms of data. Notably, all   these VLMs use powerful language models (e.g., BERT [8],   T5 [45], GPT [2], LLaMA [48] or their successors) for text   encoding and focus on improving QA while ignoring visual   evidence grounding. Some recent works [5,30\u201333,43,64]   have begun to ground key frames or objects for VideoQA.   Yet, they still aim to improve QA accuracy, and thus the   grounded contents may not be the actual evidences since they   do not evaluate grounding. For weakly-supervised video   grounding, typical approaches extract temporal proposals   and rank the proposals according to their similarities with the   language query [13,35,39,71]. Despite their effectiveness,   these two-stage approaches are notorious for inefficient and   sub-optimal for multi-granular temporal modelling. More recent research [74,75] points to the superiority of end-to-end   Gaussian mask learning. In light of this, we design a simple   yet effective Gaussian mask learning module for grounding in VideoQA. Unlike previous works [74,75] that design   small transformer models and hand-craft negative visual proposals for contrastive learning, we integrate Gaussian mask   learning into large VLMs and optimize its parameters via   question-answering and video-question grounding.   Language Priors   Our work is also related to efforts in   preventing language priors and other spurious correlations.   Goyal et al. [14] construct VQAv2 to prevent language priors in VQA by pairing the questions with additional images   that carry similar contents but with different answers. Niu   et al. [40] and Guo et al. [15] alleviate language priors by   regularizing the prediction scores. Zheng et al. [67] develop   13205   Table 1. Statistics of NExT-GQA.   Split   #Vid.   #Que.   #Seg.   Seg. Dur.(s)   Vid. Dur.(s)   Ratio (S./V.)   Train   3,860   34,132   44.9   Val   567   3,358   3,931   7.3   42.2   0.2   Test   990   5,553   6,600   6.7   39.5   0.2   X2-VLM via multi-grained vision-language pretraining for   better spatial grounding. These works discourage short-cut   learning in the image side by either collecting new data,   designing tailored-made learning methods, or focusing on   spatial grounding. Our primary contribution lies in defining the weakly-grounded VideoQA task to encourage more   interpretable and trustworthy techniques.   3. NExT-GQA Dataset   3.1. Dataset Construction and Analysis   Data Source.   We choose NExT-QA [55] as our   data source to augment with temporal labels.   Most   of the other VideoQA datasets [17, 59] are not suitable because they feature short videos (3 \u223c15s) already   trimmed around the relevant content.   NExT-QA has   three different types of questions: Causal (\u201cwhy/how\u201d),   Temporal (\u201cbefore/when/after\u201d) and Descriptive   (\u201cwhat/who/where\u201d). We exclude the descriptive questions because they mostly pertain to global video content   (e.g., \u201cwhat event?\u201d) or their answers can be found almost throughout the whole video (e.g., \u201cwhere is?\u201d). In   addition, we only label the validation and test sets since we   aim for a weakly-supervised setup. As a result, 11,378 QA   pairs drawn from 1,570 videos [47] are to be annotated.   Label Collection. We invite undergraduate students for   annotation (using Elan [10]) and train them with our demo   annotations together with some trial examples following specific criteria (see Appendix A.1) before the actual annotation   exercise. To guarantee quality and reduce subjectiveness,   each QA pair is annotated by at least two people. The final temporal label is determined by an additional check and   refinement of the two accepted annotations. The entire exercise lasted around 2 months with a team of 30 annotators.   Eventually, we collect 10,531 valid temporal segments corresponding to 8,911 QA pairs and 1,557 videos. Detailed   statistics are presented in Tab. 1.   Label Analysis. Fig. 2a(left) shows that most of the   segments last for less than 15s, and with an average duration   of 7s (Tab. 1) which is short compared to the video length   (\u223c40s). In fact, the ratio reflected in Fig. 2a(right) shows   that most of the segments occupy less than half (0.5) length   of the videos, and the average ratio is merely 0.2 (Tab. 1.   This ratio is slightly low, compared to that of 0.3 for both   ActivityNet-Caption [21] and Charades-STA [12]. Moreover,   Fig. 2b(1) shows that the segments are evenly distributed   in the left, middle and right parts of the video. Fig. 2b(2)   shows that near 90% of the QAs ground on a single temporal   1   2   3   4   5   6   7   8   9 10 11 12 13 14 15 16 17 18 19>=20   Segment length in seconds   3   4   5   6   7   Number in log view   5.5   5.7   6.4   7.0   6.4   7.1   6.3   7.0   5.9   6.4   5.8   6.2   5.3   5.8   5.2   5.6   4.7   5.1   4.6   5.2   4.4   4.54.5   4.8   4.3   4.3   3.9   4.2   3.4   3.8   3.5   4.0   3.2   3.23.4   3.7   3.0   3.4   5.5   5.9   Distribution of segment length   Test   Val   0.10.20.30.40.50.60.70.80.91.0   Ratio of segment w.r.t entire video   3   4   5   6   7   8   Number in log view   6.4   6.8   7.4   8.0   6.6   7.2   5.7   6.4   5.1   5.7   4.9   5.1   4.5   4.9   4.2   4.4   3.9   4.2   4.5   4.9   Distribution of segment ratio   Test   Val   (a)   33.9%   35.3%   30.8%   Left Middle   Right   (1) Segment positions.   89.9%   10.1%   1   >1   (2) #Segments w.r.t each QA.   46.4%   30.3%   14.0%   9.3%   1   2   3   >3   (3) #QAs w.r.t each segment.   (b)   Figure 2. Distribution of temporal segments.   segment. Conversely, Fig. 2b(3) shows that each segment   often corresponds to 1 or 2 QAs (Here two segments are   considered the same if their IoU > 0.5). To better understand   the dataset, we show two examples in Appendix Fig. 6.   3.2. Comparison with Existing Benchmarks   We highlight the uniqueness of NExT-GQA by comparing   it with other relevant benchmarks in Tab. 2.   NExT-GQA vs. NExT-QA. NExT-QA [55] targets the   prediction of text answers. NExT-GQA differs in two major aspects: 1) it provides visual evidence to support the   answers, and 2) it extends the VQA setup by allowing visual answers. This satisfies more real-world applications   and additionally helps to better diagnose model performance.   For example, is a prediction wrong because the model failed   to localize the relevant video contents, or because it could   not convert the localized video contents into a text answer?   NExT-GQA is also more challenging, because: 1) the models need to achieve multiple goals (i.e., grounding and QA)   and maintain their consistency, and 2) the set of questions   are relatively harder to answer by focusing on local video   moments in untrimmed long videos. This also differs from   major VideoQA benchmarks that focus on trimmed (short)   video understanding [17,53,59].   NExT-GQA vs. Video Grounding (VG) Benchmarks.   Video grounding [12,21] aims to find a video moment described by a declarative sentence. NExT-GQA shares core   challenges, i.e., cross-modal correspondence learning and   multi-granular temporal modelling, while featuring some   unique aspects. First, the questions feature visual content for   grounding which is not explicitly stated in the text, such as   \u201ca baby falls and cries.\u201d vs. \u201cwhy did the   baby cry?\u201d. To answer the questions, the models not   only need to find the described video moments (e.g., \u201cbaby   cries\u201d) but also should be capable of refining the moment   13206   Table 2. Benchmark comparison. GD: Grounding. MM: Multimodal. Acc: Accuracy. IoU/P: Intersection over Union/Prediction.   Datasets   GD   QA   Weak Sup.   Goal   Eval   ActNet-Cap [21]   \u221a   \u00d7   \u221a   VG   IoU   Cha-STA [12]   \u221a   \u00d7   \u221a   VG   IoU   TVQA [25]   \u221a   \u221a   \u00d7   MMVQA   Acc, IoU   VidSTG [72]   \u221a   \u221a   \u00d7   VG   IoU   NExT-QA [55]   \u00d7   \u221a   \u00d7   VQA   Acc   NExT-GQA   \u221a   \u221a   \u221a   Trust VQA   Acc, IoP, IoU   to enclose the answer (e.g., \u201cbaby falls\u201d). This may   ask for temporal and causal relationship reasoning. Second, the video backgrounds are relatively monotonous with   little scene change. Accordingly, the temporal segments   of QA pairs are often more fine-grained than those in VG   benchmarks. Notably, NExT-GQA prioritizes finding visual   evidence to support the answers. This means that any individual frame or moment that sufficiently tells the answer should   be considered as a valid grounding instead of retrieving all   of the video contents that match the query. This is reflected   in our selection of intersection over prediction (IoP) as an   evaluation criterion. That is, a correct grounding depends on   whether the predicted segment falls into the labelled segment   but is not necessarily an exact match.   NExT-GQA vs.   Supervised benchmarks.   Fullysupervised benchmarks [25, 26] provide temporal annotations for training data; the labels can resolve reference ambiguities in the questions or improve QA performance with   well-localized visual inputs. NExT-GQA differs from them   by seeking to identify visual evidence that explains the answers with QA supervision alone. It is worth mentioning   that directly applying the fully-supervised benchmarks for   weakly grounding does not suit our goal, because these   benchmarks are either biased to text localization [25] or   the answers are a limited set of, e.g. 80 objects [72]. Additionally, we focus on weakly-supervised temporal grounding and leave spatio-temporal grounding for future exploration. Our consideration is that fine-grained spatio-temporal   grounding [72] is currently more challenging than questionanswering, especially in the weak supervision setting [54],   and would derail the main goal of VQA.   4. Weakly-Supervised Grounding in VideoQA   VideoQA. We first give an overview of the typical approaches to VideoQA, focusing on transformer-based methods due to their superior performance. Given a video \ud835\udc63and   a question \ud835\udc5e, the goal of VideoQA is to predict a correct   answer \ud835\udc4e* from a set of candidate answers \ud835\udc34. Depending   on the task setting, \ud835\udc34can be given by multiple choices accompanying each question [55,56] (multi-choice), or by a   global answer set [59] to all questions (open-ended). Note   that SoTA transformer-methods [11, 57, 61, 62] formulate   and solve both multi-choice QA and open-ended QA in a   unified formulation:   \ud835\udc4e* = argmax\ud835\udc4e\u2208\ud835\udc34\u03a8(\ud835\udc4e|\ud835\udc63, \ud835\udc5e, \ud835\udc34),   (1)   in which the mapping \u03a8 is typically realized as either shared   [24,50], stacked [11,29,62] or dual [44,57,61] transformer.   In this work, we primarily study the behaviour of the stacked(Fig. 3a) and dual-transformer (Fig. 3b) architectures for   their relatively better performance.   Weakly Grounded VideoQA. Aside from answering   questions, weakly-grounded VideoQA requires the models   to explicitly estimate a QA-relevant video segment to serve   as visual evidence. We introduce below three model-agnostic   solutions to achieve this goal:   Post-hoc (PH). Intuitively, relevant temporal segments   can be found through a post-hoc analysis of the temporal   attention, i.e., identifying the segment or frame with the   maximal attention value and then thresholding around it to   obtain a time interval. To that end, we use attention-pooling   to summarize the outputs from the temporal transformers for   dual architectures. For stacked architectures, we directly return the averaged multi-head attention values corresponding   to the prediction token.   Naive Gaussian (NG). The post-hoc approach is designed to analyze the models, but not influence their predictions. More favourably, we propose to explicitly incorporate   a video grounding mechanism into VideoQA. We illustrate   the framework in Fig. 4a, and reformulate Eqn. 1 as   \ud835\udc4e*, \ud835\udc61* = argmax\ud835\udc4e\u2208\ud835\udc34\u03a8(\ud835\udc4e|\ud835\udc63\ud835\udc61, \ud835\udc5e, \ud835\udc34)\u03a6(\ud835\udc61|\ud835\udc63, \ud835\udc5e),   (2)   in which the grounding module \u03a6 firstly estimates the key   moment specified by \ud835\udc61and thereafter the QA module \u03a8 takes   the more localized video content \ud835\udc63\ud835\udc61for answer prediction.   To enable end-to-end learning, \ud835\udc61is represented by differentiable Gaussian weights over the entire video sequence, i.e.,   \ud835\udc61\u223c\ud835\udc41(\ud835\udf07, \ud835\udf0e2), where \ud835\udf07, \ud835\udf0e\u2208[0, 1] are two learnable Gaussian parameters corresponding to the mean and standard   deviation. During inference, the grounding can be achieved   by the confidence interval \ud835\udc61= (\ud835\udf07\u2212\ud835\udefe\ud835\udf0e, \ud835\udf07+ \ud835\udefe\ud835\udf0e) * \ud835\udc51, where   \ud835\udefeis a hyper-parameter to control the width of the confidence   interval and \ud835\udc51denotes the duration of the video.   Fig. 3c shows a dual transformer instantiation of this   naive solution. The difference with respect to the original   VideoQA counterpart (Fig. 3b) lies in a Gaussian mask prediction head, along with a Gaussian weighted token learning   and aggregation stage (details in Appendix A.2). We find   that this approach effectively learns and outputs grounding   information. Nevertheless, the improvements over a posthoc solution are limited due to the weak QA supervision.   NG+. In light of the naive Gaussian results, we further design an auxiliary objective with cross-modal selfsupervision to regularize the VQA objective towards more   visually grounded QA. Specifically, for each question \ud835\udc5e+,   13207   (b) Dual-style VideoQA   (c) Dual-style weakly grounded VideoQA   (a) Stacked-style VideoQA   ViT   BERT   F\ud835\udc5e\ud835\udc4e   F\ud835\udc63   Multimodal Transformer   ViT   BERT   F\ud835\udc5e\ud835\udc4e\ud835\udc58   \u00d7   \ud835\udc53\ud835\udc5e\ud835\udc4e\ud835\udc58   \ud835\udc53\ud835\udc5e\ud835\udc4e1   \u2026   F\ud835\udc63   Temporal Transformer   \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65   \ud835\udc53\ud835\udc5e   q + A{a1, a2, \u2026, ak}   F\ud835\udc34   \ud835\udc53\ud835\udc5e\ud835\udc63Gaussian    Mask   \ud835\udc53\ud835\udc5e\ud835\udc63\ud835\udc61   ViT   BERT   F\ud835\udc5e\ud835\udc4e\ud835\udc58   \u00d7   \ud835\udc53\ud835\udc5e\ud835\udc4e\ud835\udc58   \ud835\udc53\ud835\udc5e\ud835\udc4e1   \u2026   F\ud835\udc63   Temporal Transformer   \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65   q + A{a1, a2, \u2026, ak}   F\ud835\udc34   \ud835\udc53\ud835\udc5e\ud835\udc63   \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65   cls+   q + A{a1, a2, \u2026, ak}   Figure 3. Illustration of stacked (a) and dual (b) style Transformer architecture for VideoQA. (c) Our example of dual-style weakly-grounded   VideoQA. Note that the grounding part is identical for stacked-style implementation.   Q: Why did the baby pick up one present from    the group of them and move to the sofa?   \u2026   \u2026   13.8s   29.8s   Vt   Gaussian Mask   Video    Grounding   VideoQA   V:   A: Unwrap it.   (a) Framework of weakly-grounded VideoQA.   What did the boy do after   he walked towards the    woman with a present?   push away   pull close   Why did the boy return to   the Christmas tree after    unwrapping one present?   \u2026   \u2026   \u2026   pull close   (b) Cross-modal self-learning of GQA.   Figure 4. Illustration of the framework (a) and our NG+ solution (b) for weakly-grounded VideoQA.   we treat the corresponding grounding hypothesis \ud835\udc63\ud835\udc61as an anchor point and pull it close to \ud835\udc5e+ while pushing it away from   other questions \ud835\udc44\u2212in the feature space. The negative set   \ud835\udc44\u2212includes: 1) other questions defined in the same video as   hard negatives, since a large portion (near half) of questions   each invokes unique video moment for answer (Fig. 2b(3));   2) questions sampled from other videos to ensure the sufficiency and diversity of negative samples. Moreover, we   enrich 10% of the positive questions by rephrasing each   question (using GPT-4 [41]) with a maximum of 5 additional questions to form \ud835\udc44+. It is worth noting that there is   only one positive question at each training iteration and the   enriched positive questions are randomly picked to substitute the original one for data augmentation. Such form of   contrast is thus implemented as classification by also fixing   the number of negative questions to be identical to that of   distractor answers. Thereby, our final solution is:   \ud835\udc4e*, \ud835\udc61* = argmax\ud835\udc4e\u2208\ud835\udc34\u03a8(\ud835\udc4e|\ud835\udc63\ud835\udc61, \ud835\udc5e+, \ud835\udc34)\u03a6(\ud835\udc61|\ud835\udc63, \ud835\udc5e+)   \u23df    \u23de       GroundedQA   +   \ud835\udefcargmax\ud835\udc5e\u2208\ud835\udc44\u03a8(\ud835\udc5e+|\ud835\udc63\ud835\udc61, \ud835\udc44)\u03a6(\ud835\udc61|\ud835\udc63, \ud835\udc5e+)   \u23df    \u23de       Grounding   ,   (3)   where \ud835\udc44= \ud835\udc44+ \u222a\ud835\udc44\u2212which comprises both the positive   and negative questions of \ud835\udc63\ud835\udc61and \ud835\udefcis a trade-off parameter. Note that the Grounding-term coarsely identifies the   question-relevant video moment \ud835\udc61, while the GroundedQAterm not only makes the prediction but also helps to refine   the moment \ud835\udc61with answer supervision. The overall objective   thus enforces the grounded video contents to be relevant to   both the answers and the questions.   5. Experiments   5.1. Overview   Our experiments answer three research questions: Q1: To   what extent are the current VLMs\u2019 predictions grounded on   relevant video content? Q2: Does better QA performance   imply better grounding and vice versa? Q3: How effective is   our Gaussian masking mechanism? We study a wide variety   of VLMs, covering different architectures (dual and stacked   transformers), vision encoders (task-specific and pretrained   with image- or video-text data), and text encoders (BERT,   RoBERTa, DeBERTa, Flan-T5):   1. VGT [57] is a task-specific, dual-style graph transformer model. It encodes spatio-temporal object information [46] for VideoQA. We also investigate VGT   with RoBERTa [36] as suggested by [58].   2. Temp[Swin] is a dual architecture. The Swin Transformer (SWT) [37] is pre-trained on ImageNet [7].   Temp[CLIP] and Temp[BLIP] follow the same dual   architecture, but use ViT [9] pretrained by CLIP [44]   and BLIP [28] respectively as vision encoders.   3. VIOLETv2 [11] adopts a stacked transformer. It uses   video Swin Transformer [38] (VSWT) and BERT for   vision and text encoding, respectively. The model is   13208   pretrained with both image- and video-text data, and   achieves SoTA on various VL tasks.   4. FrozenBiLM [62] applies a stacked transformer. It   uses CLIP as vision encoder and highlights the strength   of adapting frozen large language models (LLMs) (e.g.,   DeBERTa-V2-XL (1B) [16]) for VideoQA.   5. IGV [31] and SeViLA [64] are additionally reproduced   for comparison.   Both works emphasize grounding   keyframes for VideoQA. IGV is built on a visual graph,   whereas SeViLA is founded on BLIP-2 [27]. It exploits   ViT-G [68] and frozen LLM (e.g., Flan-T5-XL (3B) [6])   for video localization and QA. In our implementation,   we choose the smallest time spans that can enclose the   localized keyframes as grounded moments.   Experimental Settings. For all the models, we uniformly   sample 32 frames from each video and freeze the vision   encoders. In post-hoc analysis, the temporal attention thresholds are set dynamically according to mean attention values   to maximize the grounded QA accuracy. The number of negative questions in Eqn. 3 is kept the same as the number of   distractor answers in MCQA to facilitate joint optimization.   The trade-off parameter \ud835\udefcis set to 1 and 0.1 for dual and   stacked transformers, respectively. During inference, the hyperparameter \ud835\udefefor the Gaussian confidence interval is chose   from {1, 0.8} depending on different models. Our final   results are reported based on a combination of predictions   from Gaussian and temporal attention. All hyper-parameters   are tuned on the validation set, and unless otherwise indicated, the results are reported on the test set. Other details   are described in Appendix A.2.   Evaluation. We report accuracy for QA [76], which   stands for the percentage of correctly answered questions.   For visual evidence grounding, we use intersection over   prediction (IoP) to measure whether the predicted temporal   window lies inside the ground truth. Additionally, we include   temporal IoU following video grounding benchmarks. For   both IoP and IoU, we report the mean and the values with   overlap thresholds of 0.3 and 0.5. If a QA pair involves   multiple temporal segments, we report the results based on   the one with maximal overlap with the prediction. Notably,   we define grounded QA accuracy (Acc@GQA) to inspect   the percentages of questions that are correctly answered and   also visually grounded (i.e., IoP \u22650.5).   5.2. Result and Analysis   5.2.1   Q1:Are the answers visually grounded?   We focus on Acc@QA, Acc@GQA and IoP@0.5 in the Posthoc (PH) block of Tab. 3. Generally, the existing VLMs excel   at QA but are weak in grounding the answers in the videos.   For example, all the methods exceed 50% in QA accuracy,   yet cannot reach more than 12-16% for grounded QA accuracy. In fact, the SoTA QA model (FrozenBiLM) achieves   an accuracy of 69% for QA compared to a surprisingly low   16% for GQA. The results of IoP@0.5 suggest that the large   disparity is mainly due to the models\u2019 poor performance in   temporal grounding. It is also due partly to inconsistency between grounding and QA because not all correct grounding   yields correct answers according to Acc@GQA vs. IoP@0.5.   We additionally exclude the influence of sparse video sampling by investigating the coverage of QA content w.r.t the   number of sampled video frames in Fig. 5(a). The figure   shows that the sampled 32 frames can cover almost all QA   contents. Moreover, to understand the extent of such poor   performance, we estimate the upper-bound performance via   a human study on 10% of the test data. The study shows that   the participants correctly answered 93% of the questions,   with 82% being visually grounded.   Given the above observations, we believe most of these   models\u2019 answers are not grounded on the relevant video   content. Instead, they are more likely derived from language shortcuts or spurious correlations with irrelevant visual context.   To investigate the language shortcut, we   conduct a BlindQA experiment, in which we train only the   language counterparts of the VQA models without video   inputs. Tab. 4(a) shows that BlindQA achieves 80% of the   performance of standard VQA (NormalQA), i.e., 50.3% vs.   59.4% for the dual models and 56.7% vs. 69.1% for the   stacked models. To study the spurious correlations, we test   the VLMs by directly sampling inside (PosQA) or outside   (NegQA) the ground-truth video segments. Surprisingly, the   models\u2019 QA performances remain almost unaffected compared to a normal uniform sampling (NormalQA), likely because the image representations are not fine-grained enough   to differentiate different frames. Tab. 4(a) shows that providing the ground-truth temporal segments (PosQA) brings   marginal improvement (<1%) for the dual-style models and   even hurts stacked-style transformers, likely due to a distribution shift in visual inputs. Furthermore, excluding the   temporal segments (NegQA) degenerates the performance   by less than 1% for both dual and stacked-style models. The   above studies reinforce our belief that the current VLM\u2019s   predictions are often not visually-grounded.   5.2.2   Q2: Does better QA imply better grounding?   First, by focusing on Acc@QA, mIoP and mIoU in Tab. 3.   we find that better QA is not necessarily established by better grounding, and the results vary across architectures.   For instance, by comparing across different architectures,   FrozenBiLM shows the strongest QA performance, yet with   surprisingly poor grounding, e.g., the IoP values are even   worse than those of VGT which displays the lowest QA results among other transformer models. This could be due   to FrozenBiLM\u2019s freezing of the LLMs, causing its predic13209   Table 3. Grounded QA performance on NExT-GQA test set. \u2020: original NExT-QA. D/S: Dual/Stacked. CM: Cross-modal pretrain. BT:   BERT. RBT: RoBERTa. DBT: DeBERTa-V2-XL. FT5: Flan-T5-XL. Random: always choose the same answer id and return the whole   video duration as grounding result. *: pretrain on video-language grounding dataset.   Model   D/S   CM   Vision   Text   Acc@QA   Acc@QA\u2020   Acc@GQA   mIoP   IoP@0.3   IoP@0.5   mIoU   IoU@0.3   IoU@0.5   Human   93.3   82.1   72.1   91.7   86.2   61.2   86.9   70.3   Random   20.0   20.0   1.7   21.1   20.6   8.7   21.1   20.6   8.7   IGV   N   ResNet   BT   50.1   51.3   10.2   21.4   26.9   18.9   14.0   19.8   9.6   SeViLA*   S   Y   ViT-G   FT5   68.1   71.5   16.6   29.5   34.7   22.9   21.7   29.2   13.8   PH   VGT   D   N   RCNN   BT   50.9   53.8   12.7   24.7   26.0   24.6   3.0   4.2   1.4   VIOLETv2   S   Y   VSWT   BT   52.9   57.2   12.8   23.6   25.1   23.3   3.1   4.3   1.3   VGT   D   N   RCNN   RBT   55.7   57.7   14.4   25.3   26.4   25.3   3.0   3.6   1.7   Temp[Swin]   D   N   SWT   RBT   55.9   58.7   13.5   23.1   24.7   23.0   4.9   6.6   2.3   Temp[CLIP]   D   Y   ViT-B   RBT   57.9   60.7   14.7   24.1   26.2   24.1   6.1   8.3   3.7   Temp[BLIP]   D   Y   ViT-B   RBT   58.5   61.5   14.9   25.0   27.8   25.3   6.9   10.0   4.5   Temp[CLIP]   D   Y   ViT-L   RBT   59.4   62.5   15.2   25.4   28.2   25.5   6.6   9.3   4.1   FrozenBiLM   S   Y   ViT-L   DBT   69.1   71.8   15.8   22.7   25.8   22.1   7.1   10.0   4.4   NG   Temp[CLIP]   D   Y   ViT-L   RBT   59.4   62.7   15.5   25.8   28.8   25.9   7.7   10.9   4.6   FrozenBiLM   S   Y   ViT-L   DBT   70.4   73.1   17.2   24.0   28.5   23.5   9.2   13.0   5.8   NG+   Temp[CLIP]   D   Y   ViT-L   RBT   60.2+0.8   63.3+0.8   16.0+0.8   25.7+0.3   31.4+3.2   25.5+0.0   12.1+5.5   17.5+8.2   8.9+4.8   FrozenBiLM   S   Y   ViT-L   DBT   70.8+1.7   73.1+1.4   17.5+1.7   24.2+1.5   28.5+2.7   23.7+1.6   9.6+2.5   13.5+3.5   6.1+1.7   Table 4. Performances under different settings. (+): with NG+.   VQA: Question subset that BlindQA cannot answer. GDQA: Subset   that both BlindQA and NegQA cannot answer but PosQA can.   (a)   Model   NormalQA   BlindQA   PosQA   NegQA   Post-hoc   Temp[CLIP]   59.4   50.3   59.8   59.1   FrozenBiLM   69.1   56.7   68.5   68.2   NG+   Temp[CLIP]   60.2   50.3   61.0   59.4   FrozenBiLM   70.8   56.7   70.0   69.6   (b)   Models   QA Set   Acc@QA   Acc@GQA   mIoP   mIoU   Temp[CLIP]   Whole   59.4   15.2   25.5   6.6   VQA   35.7   9.7   25.2   7.0   VQA(+)   39.4+3.7   10.6+0.9   25.5+0.3   12.2+5.2   GDQA   23.0   10.8   27.6   5.9   GDQA(+)   30.2+7.2   14.3+3.5   29.3+1.7   13.1+7.2   FrozenBiLM   Whole   69.1   15.8   22.7   7.1   VQA   47.6   11.2   22.2   6.6   VQA(+)   50.0+2.4   12.8+1.6   23.7+1.5   9.5+2.9   GDQA   42.6   14.8   24.6   7.3   GDQA(+)   44.0+1.4   16.6+1.8   27.0+2.4   13.2+5.9   tions to heavily rely on the common sense knowledge of the   LLMs rather than the provided videos (similar problem is   also found on SeViLA). In contrast, VGT is a task specific   model. It focuses on exploiting the fine-grained video information, and thus conditions better on the visual content.   By comparing among different instantiations of the same   architectures (e.g., Temp[Swin] to Temp[CLIP]) as well as   different training epochs of the same models in Fig. 5(b),   we find that the grounding performance (mIoP) improves   along with the increase of QA accuracy for dual-style   architectures yet not for stacked-style ones. Second, regarding the influence of grounding on QA, our conclusion   is that having grounding is better than not having it. Yet,   this is not controlled and opts for the underlying shortcuts   when the models are allowed to learn freely. The conclusion   is backed by the observations that PosQA always outperforms NegQA in Tab. 4(a) regardless of model architectures.   Moreover, our effort to improve grounding also brings better   QA performance (Tab. 3 & 4(b)). However, as mentioned,   correct grounding does not guarantee correct answers.   5.2.3   Q3: Is Gaussian masking solution effective?   We incorporate our Gaussian grounding mechanism (NG and   NG+) into the top-performing dual- and stacked-style models   and compare with Post-hoc baseline. 1 Tab. 3 shows that both   NG and NG+ lead to better grounding and QA performance.   Also, NG+ generally outperforms NG, especially for dualstyle architectures. Additionally, Tab. 4(b) indicates that our   superiority gets enlarged in answering the subset of questions   that necessitate videos and temporal grounding.   For better understanding, we analyze two cases in   Fig. 5(c). The top example shows that the Gaussian masks   (NG and NG+) are more focused on the relevant video moment than temporal attention, thus bringing better grounding, especially for IoU. The bottom example highlights the   strength of NG+. In this case, there are multiple visual   instances that correspond to the answer \u201cgirl stands   up\u201d.   The correct instance is the one after the \u201cgirl   takes the green ball\u201d, though the instance after   \u201ctake the red ball\u201d is more salient. Both the Posthoc and Naive methods are distracted because they are   learned via answer supervision alone. In contrast, NG+ finds   the correct grounding since it also optimizes the cross-modal   correspondence between questions and video segments.   More detailed analyses are presented in Appendix A.3.   1Despite the weaker performance, we highlight the higher efficiency   of dual-style implementation, especially in retrieval-based QA systems as   exemplified by multi-choice QA.   13210   Why does the baby extend her hand to the animal in the middle of  the video? Feed the cat.   7.4s   46.1s   60.0s   22.8s   32.8s   16.2s   25.8s   9.8s   GT:   Post-hoc:   NG:   NG+:   8.8s   16.8s   66.0s   27.7s   37.6s   9.9s   21.1s   60.2s   62.2s   GT:   Post-hoc:   NG:   NG+:   (a)   (b)   (c)   What did the girl do after she took the green ball? Stand up.    Figure 5. Analysis of visually-grounded VideoQA. (a) Coverage of QA content w.r.t. number of sampled video frames. (b) VQA and VG   results w.r.t. training epochs on NExT-GQA Val set. (c) Visualization of the prediction examples. (Please zoom in for better view.)   5.2.4   Method Comparison   Compared with a random baseline, all methods effectively   perform grounded QA (refer to Acc@GQA and IoP@0.5 in   Tab. 3). More concretely, we find that both IGV and SeViLA   obtain lower GQA accuracy than FrozenGQA though they   also incorporate a sense of grounding in their models.   The weakness manifest in both visual evidence grounding   (IoP@0.5) and QA. However, we find that SeViLA performs   much better than other methods in standalone grounding   (mIoP and mIoU). We speculate this is because SeViLA is   pretrained with localization supervisions [22]. The observations thus point to possible future improvement by pretraining with location supervisions. Furthermore, they call for   improved coordination between QA and grounding.   5.2.5   Other Observations   Tab. 3 also compares the Acc@QA performance on NExTGQA versus the full (original) NExT-QA test set. There is   a consistent 2-3% higher accuracy on the full set, suggesting that the questions rooted in local video moments are   harder to answer than those rely on overall video content.   Besides, the cross-modal pretrained representations perform better than the uni-modal pretrained ones for both   VQA and visual grounding. Also, the image-text pretrained   representations outperform those pretrained with video-text   data. Moreover, existing dual-style architectures tend to   have better grounding performance than stacked ones   (Note that FrozenBiLM\u2019s high-ranking GQA result is due   to its strong QA performance but not grounding). This is   surprising, as there is no cross-modal interaction in dualstyle implementations. We speculate that cross-modal transformers likely suffer from a uni-modal bias, which leads to   the attention being skewed towards the language side for   predicting textual answers. The findings on the one hand   consolidate the benefits of harnessing foundation VLMs or   LLMs for videoQA. On the other hand, they accentuate the   need to balance between vision fact and text knowledge.   6. Conclusion   We summarize the following points and raise them as   open challenges for the rest of the community: First, current   VLMs built on powerful language models excel in answering   visual questions. Yet, their predictions often lack a strong   connection to the pertinent visual information but instead   heavily rely on languages short-cut and irrelevant visual   context. This calls for more efforts towards the interpretability and trustability. Second, our experiments show that,   localizing the questions, especially those featuring temporal actions and events is still a difficult and open challenge.   Our studies indicate that solving this problem would largely   benefit visually-grounded VideoQA. Third, although our   solution improves grounding and QA, there is still a large   gap compared with human performance. This leaves ample   opportunity for follow-up works. Last but not least, we   highlight the significance of NExT-GQA and hope it can   contribute towards advancement in these areas.   Limitations. The NG+ method demands more memory   and time to train (Appendix A.3.3). Besides, our analyses   are focused on multi-choice QA (Appendix A.4).   Acknowledgements   This research is supported by NUS NExT++   Research Center. The research is also supported by the National Research   Foundation, Singapore under its NRF Fellowship for AI (NRF-NRFFAI12019-0001). Any opinions, findings and conclusions or recommendations   expressed in this material are those of the author(s) and do not reflect the   views of National Research Foundation, Singapore.   13211", "conf": "CVPR", "year": "2024", "index": 541}, {"title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6362\u20136371   November 7\u201311, 2021. c\u20dd2021 Association for Computational Linguistics   6362   Improving Pre-trained Vision-and-Language Embeddings for Phrase   Grounding   Zi-Yi Dou, Nanyun Peng   Department of Computer Science   University of California, Los Angeles   {zdou,violetpeng}@cs.ucla.edu", "abstract": "Phrase grounding aims to map textual phrases   to their associated image regions, which can   be a prerequisite for multimodal reasoning   and can bene\ufb01t tasks requiring identifying objects based on language.   With pre-trained   vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned   embeddings for phrase grounding without \ufb01netuning.   To this end, we propose a method   to extract matched phrase-region pairs from   pre-trained vision-and-language embeddings   and propose four \ufb01ne-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised   grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline   models in both weakly-supervised and supervised phrase grounding settings. In addition,   we evaluate the aligned embeddings on several   other downstream tasks and show that we can   achieve better phrase grounding without sacri\ufb01cing representation generality.1   1", "content": "Recent   studies   on   vision-and-language   pretraining (Tan and Bansal, 2019; Li et al., 2019;   Lu et al., 2019; Su et al., 2019; Chen et al., 2020;   Li et al., 2020b, 2021; Shen et al., 2021) demonstrate impressive performance across vision-andlanguage tasks, including image-text retrieval (Lin   et al., 2014; Plummer et al., 2015), visual entailment (Xie et al., 2019) and visual question answering (Antol et al., 2015).   However, few existing papers have paid attention to the phrase grounding ability of their pretrained embeddings, namely the ability to map natural language queries to their corresponding image   1Code   is   available   at   https://github.com/   pluslab/phrase_grounding.   regions, which can 1) bene\ufb01t tasks requiring identifying objects based on language (Deng et al., 2018);   2) be a prerequisite for advanced multimodal reasoning (Plummer et al., 2015). Among the prior   work, Li et al. (2020a) demonstrate certain grounding abilities of VisualBERT, yet their analysis is   limited to attention heads and it is unclear how VisualBERT compares with state-of-the-art grounding models. Cao et al. (2020) provide insights on   cross-modal interaction, but their analysis is primarily limited to the coreference relations between   phrases and visual tokens.   In this paper, we study the phrase grounding ability of vision-and-language embeddings pre-trained   on image-caption datasets. First, we propose a   method to extract phrase-region pairs from the   pre-trained embeddings without any \ufb01ne-tuning.   We \ufb01nd that while our method uncovers certain   grounding abilities of the pre-trained embeddings,   there is still much room for improvement. Therefore, we propose to \ufb01ne-tune models with objectives designed for better aligning word and region   representations on image-caption datasets. The   \ufb01ne-tuning objectives are designed to maximize   the symmetricity between vision and language during \ufb01ne-tuning for better phrase grounding while   maintaining the representation transferability so   that the learned representations are still useful for   other downstream tasks. Speci\ufb01cally, we \ufb01ne-tune   models with 1) a masked language modeling objective conditioned on images; 2) an adapted masked   region modeling objective with texts utilizing a dynamically constructed vision vocabulary; 3) a modi\ufb01ed object label prediction objective that explicitly   bridges the gap between vision and language; 4) a   proposed bidirectional attention optimization objective encouraging the consistency between visionto-language and language-to-vision alignments.   We   \ufb01ne-tune   pre-trained   models   on   COCO (Chen et al., 2015) and test them on   two representative phrase grounding datasets,   6363   RefCOCO+ (Kazemzadeh et al., 2014) and   Flickr30k Entities (Plummer et al., 2015). We \ufb01nd   that our \ufb01ne-tuning objectives can improve the   model grounding ability signi\ufb01cantly, improving   baseline in both weakly-supervised and supervised   phrase grounding settings. We also evaluate the   aligned representations on several downstream   tasks and show that our model can achieve better   phrase grounding without sacri\ufb01cing performance   on other types of tasks.   2   Extracting Phrase-Region Pairs from   Pre-Trained Embeddings   Formally, the phrase grounding task can be de\ufb01ned   as: given an image v consisting of multiple regions \u27e8v1, \u00b7 \u00b7 \u00b7 , vn\u27e9and its corresponding caption l   segmented into tokens \u27e8l1, \u00b7 \u00b7 \u00b7 , lm\u27e9, for each noun   phrase pi = \u27e8lix, \u00b7 \u00b7 \u00b7 , liy\u27e9, a model needs to \ufb01nd   its associated region vj.   We \ufb01rst propose a way to directly extract the   matched phrase-region pairs from pre-trained embeddings. Then, we evaluate this method on phrase   grounding tasks with several popular pre-trained   models, including LXMERT (Tan and Bansal,   2019), UNITER (Chen et al., 2020), ViLBERT (Lu   et al., 2019), VisualBERT (Li et al., 2019) and VLBERT (Su et al., 2019).   2.1   Extraction Method   We propose to directly extract phrase-region pairs   from pre-trained models based on representation   similarities. Speci\ufb01cally, given an image v and its   caption l, we feed them to a pre-trained vision-andlanguage model and obtain their representations   h(v) and h(l). Note that here representations of   the k-th model layer are taken, where k is a hyperparameter and is selected on the validation set.   Then, given a noun phrase pi, we average its   token representations and get the phrase representation h(pi) = MEAN(\u27e8h(lix), \u00b7 \u00b7 \u00b7 , h(liy)\u27e9). Afterwards, we score each candidate region vj by computing the dot product between h(pi) and h(vj).   Regions with the highest scores are selected and   we can measure the accuracy of the selected pairs.   2.2   Experiments   We evaluate the extraction method on RefCOCO+   using pre-trained models in a controlled setting (Bugliarello et al., 2020).   Model   RefCOCO+   val   testA   testB   LXMERT   13.62 (33.33)   10.41 (36.59)   16.53 (30.91)   UNITER   26.26 (43.27)   32.62 (50.90)   18.49 (35.80)   ViLBERT   14.47 (42.14)   10.79 (48.88)   18.76 (36.27)   VisualBERT   33.26 (43.88)   33.59 (52.04)   33.34 (36.56)   VL-BERT   23.52 (42.97)   32.54 (49.86)   13.93 (36.19)   Supervised   70.98   77.05   60.73   Table 1: Phrase grounding accuracy (%) of pre-trained   models investigated with our proposed method.   We   also include the performance of probing classi\ufb01ers   (numbers in parenthesis) and a supervised VisualBERT   model (\u2018Supervised\u2019) for reference.   2.2.1   Setup   We follow the setting in Bugliarello et al. (2020)   in this section. Speci\ufb01cally, all the vision-andlanguage models are pre-trained on a pruned Conceptual Captions dataset (Sharma et al., 2018), consisting of 2.77M images with weakly-associated   captions automatically collected from billions of   web pages. The image features are extracted using   a Faster R-CNN (Ren et al., 2016) with a ResNet101 backbone (Anderson et al., 2018) trained on the   Visual Genome dataset (Krishna et al., 2017) and   the vision-and-language models are trained with 36   extracted regions of interest.   2.2.2   Results   Table 1 shows the phrase grounding accuracy of   the pre-trained models using our method. To provide upper-bound performance for our extraction   method, we train a linear probing classi\ufb01er with   the frozen model embeddings as inputs (numbers   in parenthesis). We \ufb01nd that our extraction method   can better uncover the phrase grounding ability   of single-stream models (UNITER, VisualBERT,   VL-BERT), which process the vision and language   inputs jointly. On the other hand, the grounding   information in two-stream models (LXMERT, ViLBERT) can be hard to extract, probably because the   parameters of two-stream models are not shared in   the top layers and thus they are less likely to learn   aligned representations.   Also, comparing with a supervised VisualBERT   model, the pre-trained models can underperform   their supervised counterparts by a large margin,   indicating there is much room for improvement   and additional efforts are required to align the pretrained vision-and-language embeddings.   6364   3   Aligning Pre-trained   Vision-and-Language Embeddings   To improve the model phrase grounding ability,   we then propose four \ufb01ne-tuning objectives for   vision-and-language models. We assume an imagecaption dataset {\u27e8vk, lk\u27e9} is provided but no \ufb01negrained phrase-region annotations are available. A   pre-trained vision model (Anderson et al., 2018) is   used to segment images into regions and produce   region representations and object labels.   3.1   Fine-tuning Objectives   We investigate four objectives that \ufb01ne-tune pretrained vision-and-language models for phrase   grounding:   Masked Language Modeling (MLM).   MLM   with images has proven to be useful for representation learning (Li et al., 2019) and here we investigate if it is also helpful for phrase grounding.   Speci\ufb01cally, we randomly mask 15% of the tokens   l and the model is trained to reconstruct l given the   masked texts lmask and regions v:   LMLM = log p(l|[v; lmask]).   (1)   Masked Region Modeling (MRM).   Inspired by   MLM, we propose its counterpart in the vision side   to encourage the symmetricity between vision and   language. While previous work (Tan and Bansal,   2019) regress the region features, we \ufb01nd it is unhelpful in our setting (in Appendix). Instead, by   imitating MLM which uses a text vocabulary, we   create a dynamic vision vocabulary on the \ufb02y, and   the model tries to reconstruct the input regions   given the dynamically constructed vocabulary.   Concretely, at each training step, we sample a   batch of image-caption pairs {\u27e8vk, lk\u27e9}B   k=1 and randomly mask 15% of the regions, where B is the   batch size. We treat all the regions in {vk}B   k=1 as   candidate regions, and for each masked region, the   model needs to select the original region within   the set of candidate regions given masked inputs.   Denoting the pre-trained vision model representations and our model representations of {vk}B   k=1 as   {c(vk)}B   k=1 and {h(vk)}B   k=1 respectively, we can   represent the output probability at position i for the   k-th instance as:   p(vk   i |[vk,mask; lk]) =   ecos(h(vk   i ),c(vk   i ))   P   j,k\u2032 ecos(h(vk   i ),c(vk\u2032   j )) ,   where cos(\u00b7, \u00b7) refers to the cosine similarity.   The model is trained to maximize this probability   similar to noise contrastive estimation (Gutmann   and Hyv\u00e4rinen, 2010; Jozefowicz et al., 2016):   LMRM = log p(v|[vmask; l]).   (2)   Object Label Prediction (OLP).   The object labels predicted by the pre-trained vision model provide us with good anchor points to bridge the gap   between vision and language, and previous work   has tried to incorporate the information by predicting the object labels for each region (Tan and   Bansal, 2019; Chen et al., 2020). In this paper,   to better share the information between the two   modalities, we propose to 1) use simple heuristics   to convert object labels into text tokens and train   our model to predict the object labels ov with a   multi-class MLM objective; 2) share the classi\ufb01cation layer of MLM and OLP.   For example, if the object label of vi is \u201cstop   sign\u201d, we \ufb01rst tokenize it into \u201cstop\u201d and \u201csign\u201d,   the model is then trained to maximize the joint   probability of both the two tokens at vi:   LOLP = log p(ov|[v; l]).   (3)   Bidirectional Attention Optimization (BAO).   Inspired by the work on encouraging the consistency between forward and backward attentions (Cohn et al., 2016; Hu et al., 2020; Dou and   Neubig, 2021), we propose an objective to encourage the symmetricity of vision-to-language and   language-to-vision attentions. Speci\ufb01cally, after   obtaining the representations h(v) and h(l), we   compute the forward and backward attention matrices as:   ATTV L = SOFTMAX(h(v)Th(l)/   \u221a   d),   ATTLV = SOFTMAX(h(l)Th(v)/   \u221a   d),   where d denotes the feature dimension.   We then minimize the distance between them by   maximizing the trace of ATTT   V LATTLV :   LBAO = \u2212log(1 + trace(ATTT   V LATTLV )   min(|v|, |l|)   ).   (4)   Combined Objective.   Our \ufb01nal objective is a   combination of the four objectives:   L = LMLM + LMRM + LOLP + \u03b1LBAO,   (5)   where \u03b1 is selected from {0.1, 0.25, 0.5, 1.0} and   is set to 0.1 based on the validation performance.   6365   Baseline   Ours   the girl is about to    kick a soccer ball .   Figure 1: Visualizations of cosine similarities between text and image representations of the VisualBERT baseline   and our model. Our model can learn more aligned representations than the baseline.   Model   Flickr30k   RefCOCO+   testA   testB   Weakly-Supervised   MAF (Wang et al., 2020)   61.43   17.10   13.50   VisualBERT   34.53   42.19   35.44   Ours   62.10   47.89   38.20   Supervised   VisualBERT   71.33   78.31   61.98   Ours   72.49   78.64   62.86   Table 2: Accuracy (%) in weakly-supervised and supervised grounding settings. Best scores are in bold.   3.2   Experiments   We then train our model with the proposed objective and compare with several baselines.   3.2.1   Setup   Model/Datasets.   We choose VisualBERT as our   base architecture because it performs the best in   Section 2 and pre-train it on COCO (Chen et al.,   2015). We then further \ufb01ne-tune models on COCO   and evaluate them on RefCOCO+ and Flickr30k   in both weakly-supervised and supervised settings.   Details of the models and datasets are in Appendix.   Settings.   In weakly-supervised settings where   only the image-text pairs in COCO are given, we   directly extract phrase-region pairs from models   using our method in Section 2.1. In supervised   settings where phrase-region annotations in RefCOCO+ and Flickr30k are available, we add a linear layer on top of each region representation and   \ufb01ne-tune models with the cross-entropy loss.   3.2.2   Results   We \ufb01rst present the main results of the models and   some ablation studies of the training objectives.   Model   Flickr30k   RefCOCO+   val   test   val   testA   testB   Ours   59.59   62.10   42.79   47.89   38.20   -MLM   50.53   52.71   39.14   42.54   35.66   -MRM   51.21   53.49   40.58   44.53   36.87   -OLP   48.38   50.17   40.06   42.32   38.84   -BAO   57.20   59.19   41.49   44.48   37.12   Table 3: Ablation studies on each of our objectives.   We measure the accuracy (%) numbers in weaklysupervised grounding settings.   Model   Flickr   SNLI-VE   VQAv2 (VQA-score)   (Recall@1)   (Accuracy)   test-dev   test-std   VisualBERT   58.94   76.41   69.68   69.92   Ours   59.84   76.83   69.89   70.16   Table 4: We can achieve better phrase grounding abilities while maintaining the representation transferability   on other types of tasks, including image-text retrieval,   visual entailment and visual question answering.   Main Results.   In the weakly-supervised settings,   Table 2 demonstrates that our objectives can improve the model grounding ability signi\ufb01cantly, outperforming all the baselines. Moreover, we \ufb01nd   that while MAF (Wang et al., 2020) achieves strong   performance on Flickr30k, it fails on RefCOCO+.   We hypothesize that this is because MAF is based   on static word embeddings and in the RefCOCO+   setting multiple objects of the same type will typically present in one image, making MAF unable   to disambiguate the phrases. With the aligned representations, we can also achieve better grounding   ability than VisualBERT in supervised settings.   Ablation Studies.   We ablate each of our training   objective and test their contributions in Table 3. We   can see that all of the objectives are bene\ufb01cial for   phrase grounding, with OLP being the most effec6366   tive one. BAO can bring marginal improvements,   yet its contributions are still non-negligible. We   also test most existing pre-training objectives in   Appendix and show that our proposed objective   works the best.   3.2.3   Analysis   We then perform analysis to provide insights on the   \ufb01ne-tuned model representations.   Transferring to Other Tasks.   It is interesting   to see if the aligned representations are still useful for other types of tasks. In Table 4, we test   our model on image-text retrieval (Plummer et al.,   2015), visual entailment (Xie et al., 2019) and visual question answering (Goyal et al., 2017) (details in Appendix). We \ufb01nd that our model can   achieve comparable or superior performance compared with VisualBERT, especially on tasks relying   more on the model grounding ability like image   retrieval, which shows that our training paradigm   can maintain the representation generality.   Qualitative Examples.   We visualize the learned   representations in Figure 1. We \ufb01nd it hard to   observe clear patterns from the baseline representations. For example, while the token representation   of \u201cball\u201d have high similarity with its associated   region embedding, it is also close to the representation of the mascot. By contrast, our model can   clearly learn more aligned representations. It is   interesting to note that our model can learn there is   a partial correspondence between the word \u201ckick\"   and the soccer ball region, indicating that our objectives can also align verb and region representations.   4   Related Work   We overview two lines of related work in this part.   Vision-and-Language Representation Learning.   Learning multimodal representations has   been an active research area (Ngiam et al., 2011;   Silberer and Lapata, 2014; Hill and Korhonen,   2014; Hubert Tsai et al., 2017) and the progresses   on model pre-training in computer vision (Doersch et al., 2015; Pathak et al., 2016) and natural   language processing (Peters et al., 2018; Devlin   et al., 2019) have motivated research on visionand-language representation learning with pretraining (Tan and Bansal, 2019; Li et al., 2019;   Lu et al., 2019; Su et al., 2019; Chen et al., 2020;   Sun et al., 2019; Li et al., 2020b). The pretraining\ufb01netuning paradigm has proven to be effective   across tasks, such as image-text retrieval (Lin et al.,   2014; Plummer et al., 2015), visual entailment (Xie   et al., 2019) and visual question answering (Antol   et al., 2015). A few studies (Li et al., 2020a; Cao   et al., 2020) analyze the pre-trained multimodal   models, while to the best of our knowledge, no existing work have focused on the phrase grounding   ability of learned representations.   Phrase Grounding.   Many vision-and-language   tasks, such as visual question answering and visionlanguage navigation, rely on or can bene\ufb01t from   phrase grounding.   Both supervised (Rohrbach   et al., 2016a; Yu et al., 2018; Liu et al., 2020) and   weakly-supervised (Rohrbach et al., 2016b; Yeh   et al., 2018; Chen et al., 2018; Wang and Specia,   2019; Hessel et al., 2019; Wang et al., 2020) phrase   grounding approaches have been proposed. While   pre-trained vision-language models have been applied in vision grounding tasks in supervised settings (Chen et al., 2020; Li et al., 2020b), it is   unclear whether the models can perform phrase   grounding by directly using the representations   learned during pre-training.   5   Conclusion   In this paper, we \ufb01rst propose a method to extract matched phrase-region pairs from pre-trained   vision-and-language embeddings and evaluate its   performance across models. Then, we propose   several \ufb01ne-tuning objectives for phrase grounding and demonstrate their effectiveness in both   weakly-superivsed and supervised phrase grounding tasks. We also evaluate our aligned representations on other downstream tasks and show that we   can achieve better phrase grounding without sacri\ufb01cing the representation transferability to other   downstream tasks. Future directions include better   utilizing the aligned representations and incorporating our objectives into pre-training.   Acknowledgement   We would like to thank the anonymous reviewers for valuable suggestions and Liunian Harold   Li, Da Yin, Te-Lin Wu for helpful discussions.   This work is supported by the Machine Common   Sense (MCS) program under Cooperative Agreement N66001-19-2-4032 with the US Defense Advanced Research Projects Agency (DARPA). The   views and the conclusions of this paper are those   of the authors and do not re\ufb02ect the of\ufb01cial policy   or position of DARPA.   6367", "conf": "EMNLP", "year": "2021", "index": 299}, {"title": "ReGround: Improving Textual and Spatial   Grounding at No Cost   Phillip Y. Lee   Minhyuk Sung   KAIST   {phillip0701,mhsung}@kaist.ac.kr   Layout   SD   GLIGEN\u03b3=1.0   GLIGEN\u03b3=0.2   ReGround   \u201cA low poly illustration of a man holding a cell phone in hand.\u201d   \u201cAn elephant walking draped with a colorful blanket.\u201d    \u201cOne ancient vase [...] the backdrop of a serene, moonlit desert landscape.\u201d    \u201cA train blowing smoke is coming down the tracks.\u201d   person   cell    phone   elephant   vase   train   train   person   Fig. 1: Comparison across Stable Diffusion (SD) [41], GLIGEN [28], and our ReGround. SD (2nd column) can generate an image aligned with the input prompt   (shown below each row), while it does not allow taking spatial constraints such as   bounding boxes and labels. GLIGEN (3rd column) enables spatial grounding using   gated self-attention, although it often disregards some descriptions in the input prompt   due to a bias towards bounding box conditions. Such trends also occur when only activating gated self-attention for 0.2 fraction of the initial denoising steps (4th column).   Our ReGround (last column) resolves the issue of description omission while accurately reflecting the bounding box information.   2   P.Y. Lee and M. Sung", "abstract": ". When an image generation process is guided by both a text   prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated selfattention into the U-Net reveals that spatial grounding often outweighs   textual grounding due to the sequential flow from gated self-attention to   cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring   the network architecture, changing from sequential to parallel for gated   self-attention and cross-attention. This surprisingly simple yet effective   solution does not require any fine-tuning of the network but substantially reduces the trade-off between the two groundings. Our experiments   demonstrate significant improvements from the original GLIGEN to the   rewired version in the trade-off between textual grounding and spatial   grounding. The project webpage is at https://re-ground.github.io.   Keywords: Textual Grounding \u00b7 Spatial Grounding \u00b7 Network Rewiring   1", "content": "The emergence of diffusion models [17,44,45] has markedly propelled the field of   text-to-image (T2I) generation forward, allowing users to generate high-quality   images from text prompts. In a bid to further augment the creativity and controllability, recent efforts [2\u20135,8,11,25, 28,32,36, 54, 55,57] have focused on enabling these models to understand and interpret spatial instructions, such as layouts [4,8,28,32,36,54,55], segmentation masks [2\u20135,11,25] and sketches [51,57].   Among them, bounding boxes are extensively employed in downstream image generation tasks [4, 8, 28, 32, 36, 55]. GLIGEN [28] is a pioneering work in   terms of enhancing existing T2I models with the capability to incorporate additional spatial cues in the form of bounding boxes. Its core component, gated   self-attention, is a simple attention module [50] that is plugged into each UNet [42] layer of a pretrained T2I model such as Stable Diffusion [41], and is   trained to accurately position various entities in their designated areas. A notable advantage of GLIGEN is that the original parameters of the underlying   model remain unchanged, inheriting the generative capability of the T2I model   while introducing the novel functionality of spatial grounding using bounding   boxes. This capability has been leveraged by numerous studies to facilitate highquality, layout-guided image generation [9,12,31,54].   However, our analysis reveals that GLIGEN\u2019s integration of the gated selfattention into an existing T2I model is not optimal for blending new spatial   guidance from bounding boxes with the original textual guidance. It often leads   to the omission of specific details from the text prompts. For instance, in the first   row and third column of Fig. 1, GLIGEN fails to reflect the description \u201clow poly   illustration\u201d from the input text prompt. Also in the second row, a crucial detail   in the text prompt, \u201cdraped with a colorful blanket\u201d, is neglected in the output   image. We refer to this issue as description omission. Such outcomes imply   ReGround: Improving Textual and Spatial Grounding at No Cost   3   that the current architectural design of GLIGEN does not effectively harmonize   the new spatial guidance and the text conditioning in the given T2I model.   Considering the widespread applications of GLIGEN in various layout-based   generation tasks [9,12,31,36,53,54,59], these limitations represent a significant   bottleneck.   To address the observed neglect of textual grounding in GLIGEN, we first   analyze the root causes. Our investigation reveals that the issue arises from the   sequential arrangement of the spatial grounding and textual grounding modules.   Specifically, the output of the gated self-attention is directed to a cross-attention   module in each layer of the U-Net architecture (Fig. 2-(b)).   Building on this insight, we propose a straightforward yet impactful solution: network rewiring. This approach alters the relationship between the two   grounding modules from sequential to parallel (Fig. 2-(c)). Remarkably, this network modification significantly reduces the grounding trade-off between textual   and spatial groundings without necessitating any adjustments to the network   parameters. Importantly, this rewiring does not require additional network training, extra parameters, or changes in computational load and   time. Simply reconfiguring the attention modules of the pretrained GLIGEN,   originally trained with the sequential architecture, during inference dramatically   enhances performance.   In our experiments on MS-COCO [30] and our newly introduced NSR-1KGPT datasets, we demonstrate that rewiring the pretrained GLIGEN substantially reduces the trade-off between textual and spatial groundings. This is   evidenced by the evaluation of text prompt alignment (measured using CLIP   score [38], PickScore [26] and user study) and bounding box alignment (assessed   by YOLO score [52]). Furthermore, we show that our rewiring also leads to   better outcomes in other frameworks using GLIGEN as a backbone, including   BoxDiff [54].   2   Related Work   2.1   Zero-Shot Guidance in Diffusion Models   The progress in diffusion models [17,44,45] has significantly elevated the capabilities of text-to-image (T2I) generation, resulting in foundation models [6,37,   39\u201341] that exhibit remarkable generative performance. Leveraging the robust   performance of these models, recent studies [3\u20135, 8, 11, 25, 28, 32, 36, 54, 55, 57]   have introduced efficient guidance techniques designed to further improve the   image generation process. Notably, numerous works [8,28,32,36,43,54,55] focus   on the internal architecture (Fig. 2-(a)) of the denoising U-Net of Latent Diffusion Models (LDMs) [41], where self-attention and cross-attention modules are   intertwined to facilitate inter-pixel communication and text conditioning. The   self-attention of U-Net can be utilized to improve image quality [19] or facilitate   image translation [49] and image editing tasks [7]. Since text conditions are integrated via cross-attention, the intermediate attention maps have been leveraged   4   P.Y. Lee and M. Sung   to improve text faithfulness [13] or enable spatial manipulation of the generation   process [35]. Recently, FreeU [43] analyzed the contributions of the backbone and   residuals of the U-Net and proposed a free-lunch strategy to enhance image quality: reweighting the backbone and residual features maps. In contrast to previous   works that only deal with self- and cross-attention in standard LDMs, we introduce a method to enhance GLIGEN [28] by reconnecting its gated self-attention   with the other attention modules, thereby achieving performance improvement   in zero-shot without any tuning of the network parameters.   2.2   Layout-Guided Image Generation   The use of layouts, particularly in the form of bounding boxes, has become a   popular intermediary to bridge the gap between textual inputs and the images   generated [14, 18, 21, 27, 29, 46, 47, 56, 58]. Layout2Im [58] samples object latent   codes from a normal distribution, eliminating the need to predict instance masks   as done in prior works [18,21]. LostGAN [46] controls the style of each object by   devising an extension of the feature normalization layer used in StyleGAN [22\u2013   24], while OC-GAN [47] incorporates the spatial relationships between objects   using a scene-graph representation. LAMA [29] introduces a mask adaptation   module that mitigates the semantic ambiguity arising from overlaps in the input   layout. While these developments have greatly improved user control over image   generation, their applicability is confined to the categories found in the training   data, such as those of the MS-COCO [30] dataset.   In contrast, recent studies [5, 8, 10, 28, 36, 54, 55, 60] have extended layoutguided image generation towards open-vocabulary, building on the advancements   of foundational text-to-image (T2I) models [41]. Training-free approaches [3,5,8,   36,54] aim to improve the spatial grounding of T2I models through straightforward guidance mechanisms. GLIGEN [28], on the other hand, introduces gated   self-attention, which is injected into the U-Net architecture of the Latent Diffusion Model [41], and is trained to equip the underlying model with spatial grounding abilities. Given the simple architecture of GLIGEN and its robust grounding   accuracy with the input bounding boxes, numerous studies [36,53,54,59] build   upon its framework and propose further refinements to increase performance. In   this work, we identify and address a significant performance bottleneck in GLIGEN related to description omission and propose a simple yet effective solution.   3   Background \u2014 Latent Diffusion Models [41]   Rombach et al. [41] proposed Latent Diffusion Model (LDM), a text-to-image   (T2I) diffusion model with a U-Net as the noise prediction network. It is trained   to generate an image from an input text prompt by predicting the noise \u03f5(xt, t, c)   conditioned both on the timestep t and the text embedding c. Each layer of   LDM\u2019s U-Net consists of three core components: a convolutional residual block,   followed by a self-attention (SA), and a cross-attention (CA) module (Fig. 2(a)). In each l-th layer of the U-Net, its residual block first extracts intermediate visual features F = (f1, ..., fNl)T from the output of the previous layer.   ReGround: Improving Textual and Spatial Grounding at No Cost   5   (c) ReGround   (Ours)   (a) LDM    (Rombach et al.)   x!   \ud835\udf16!   (b) GLIGEN   (Li et al.)   F   F   Self   Attn   Cross   Attn   F   Gated   Self    Attn   Cross   Attn   Self   Attn   Text    embeddings   Self   Attn   Gated   Self    Attn   Cross   Attn   Text    embeddings   Grounding    tokens   Grounding    tokens   Text    embeddings   Parallel   Sequential   Fig. 2: Comparison between the U-Net architectures of (a) Latent Diffusion Model   (LDM) [41], (b) GLIGEN [28] and (c) our ReGround. From LDM, GLIGEN enables   spatial grounding by injecting Gated Self-Attention before cross-attention, forming a   sequential flow of them. Based on GLIGEN, our ReGround changes the relationship   of the two attention modules to become parallel, resulting in noticeable improvement   in textual grounding while preserving the spatial grounding capability. (The residual   block before self-attention is omitted.)   The self-attention module then facilitates interaction between the features in F.   Subsequently, the cross-attention module enables the interaction between each   visual feature fi and the text embedding c. Throughout this process, the output   feature of the previous module is also forwarded through a residual connection,   as illustrated in lines 4-5 and 7 of Alg. 1 and also in Fig. 2-(a).   4   GLIGEN [28] and Description Omission   In this section, we review GLIGEN [28] and its key idea of employing gated selfattention for spatial grounding. Then, we present our key observations on the   description omission issue that occurs due to the addition of gated self-attention.   4.1   Gated Self-Attention   Li et al. [28] propose a plug-in spatial grounding module, named gated selfattention, which adopts the gated attention mechanism [1] to equip a pretrained   T2I diffusion model [41] with spatial grounding capabilities (Fig. 2-(b)). Given   a set of bounding boxes and text labels for each of them, let bi be the xycoordinates of the i-th bounding box\u2019s top-left and bottom-right corners, and   pi be the corresponding text label. Then, the i-th grounding token is defined as   gi := G (T (pi) , F (bi)), where T (\u00b7) is a pretrained text encoder [20,38], F(\u00b7) is   the Fourier embedding [33,48] and G(\u00b7, \u00b7) is a shallow MLP network that concatenates the two given embeddings, respectively. Given a set of grounding tokens   {gi}, gated self-attention learns the self-attention among the unified feature set   (f1, ..., fNl, g1, ..., gM) where {fi} is the set of intermediate visual features in the   l-th layer of U-Net, and M is the number of bounding boxes.   As shown in Fig. 2-(b), gated self-attention receives the output of the selfattention along with the residual features as its input and forwards the output   features to the cross-attention module. By incorporating gated self-attention into   6   P.Y. Lee and M. Sung   each layer of the U-Net, the model enables the placement of the entity specified   in the text label pi at the location indicated by the bounding box bi. Note that   the integration of gated self-attention does not require training the network from   scratch or fine-tuning it, but can be accomplished simply by training the gated   self-attention parameters while keeping all other parameters in the backbone   model frozen.   Alg. 1 shows the pseudocode of the U-Net forward-pass including the plug-in   of gated self-attention in line 6. Note that \u03b2t is set to 1 for GLIGEN. If \u03b2t = 0,   the algorithm is identical to that of LDM [41].   Algorithm 1: Noise Prediction U-Net with Gated Self-Attention.   Parameters : \u03b2t;   // Weight for GSA.   Inputs: xt, c, {gi}i=0\u00b7\u00b7\u00b7N\u22121;   // Noisy data at timestep t, text condition,   and grounding tokens   Outputs: \u03f5t;   // Noise at timestep t \u22121.   1 Function U-Net(xt, c, {gi}):   2   F \u2190xt   3   for i = 0, . . . , L \u22121 do   4   FRS \u2190Conv(F) + F;   // Residual block.   5   FSA \u2190SA(FRS) + FRS;   // Self-Attention module.   6   FGSA \u2190\u03b2t \u00b7 GSA(FSA, {gi}) + FSA;   // Gated Self-Attention module.   7   F \u2190CA(FGSA, c) + FGSA;   // Cross-Attention module.   8   \u03f5t \u2190F;   9   return \u03f5t;   4.2   Description Omission   Despite its high accuracy in spatial grounding, GLIGEN [28] frequently struggles   to capture essential attributes specified in the input text prompt. As illustrated   in Fig. 3, the leftmost image shows \u201ca person\u201d and \u201ca skateboard\u201d accurately   placed in their designated regions. However, a critical detail from the input text   prompt, \u201cblack and white photography\u201d, is absent in the output image. This discrepancy often emerges when the input comprises distinct but equally important   descriptions regarding the image, presented through text prompts and bounding   boxes. Such omissions not only fail to convey the stylistic intent of the image but   also tend to overlook significant objects mentioned within the text prompt. Additional examples of this problem are showcased in Fig. 1, where the second row   demonstrates the absence of a \u201cblanket\u201d in the generated image, a key element   from the text prompt. This limitation significantly hampers GLIGEN\u2019s fidelity   to user-provided text prompts, a challenge we term as description omission.   ReGround: Improving Textual and Spatial Grounding at No Cost   7   \u201cA black and white photograph of a person wearing DC shoes on a skateboard in the sun\u201d   \ud835\udefe= 1.0   \ud835\udefe= 0.4   \ud835\udefe= 0.2   \ud835\udefe= 0.1   \ud835\udefe= 0.0   \ud835\udefe= 1.0   (a) GLIGEN   (b) ReGround   Fig. 3: (a) Images generated by GLIGEN [28] with varying activation duration of gated   self-attention \u03b3 in scheduled sampling (Sec. 5.1). The red words in the text prompt   denote the words used as labels of the input bounding boxes. Note that for GLIGEN   to reflect the underlined description in the text prompt in the final image, \u03b3 must be   decreased to 0.1, which compromises spatial grounding accuracy. (b) In contrast, our   ReGround reflects the underlined phrase even when \u03b3 = 1.0, therefore achieving high   accuracy in both textual and spatial grounding.   5   ReGround: Rewiring Attention Modules   Gated self-attention and cross-attention each play a crucial role in enabling spatial and textual groundings, by taking bounding boxes and text prompts as   inputs, respectively. To tackle the issue of description omission, we first examine   the impact of attention modules on the groundings they do not address: the effect of gated self-attention on textual grounding (Sec. 5.1), and the influence of   cross-attention on spatial grounding (Sec. 5.2). Building on this analysis, we propose an approach for network reconfiguration, modifying the connections among   self-attention, gated self-attention, and cross-attention modules (Sec. 5.3).   5.1   Impact of Gated Self-Attention on Textual Grounding   As the issue of description omission arises due to the newly added gated selfattention in GLIGEN [28], we first attempt to mitigate the impact of gated   self-attention by using scheduled sampling [28], activating gated self-attention   only in a few initial steps of the denoising process. This approach is inspired by   the observation that the coarse structure of the final image is established within   the first few denoising steps. The scheduling is applied by setting the weight of   gated self-attention \u03b2t (line 6 of Alg. 1) as     \\   b   e   ta  _ t  =   \\   be g i n  {cases} 1 & (t \\leq \\gamma \\cdot T) \\\\ 0 & (t > \\gamma \\cdot T), \\end {cases}    (1)   where \u03b3 \u2208[0, 1] represents the fraction of the initial denoising steps to activate   gated self-attention.   Fig. 3-(a) shows an example of generated images while incrementally adjusting \u03b3 from 1.0 to 0.0. As \u03b3 is reduced from 1.0 to 0.0, the details specified in   the text prompt, \u201ca black and white photograph\u201d, begin to be reflected starting   8   P.Y. Lee and M. Sung   at \u03b3 = 0.1, demonstrating that longer activation of gated self-attention may interfere with the alignment of the output image with the text prompt. However,   as gated self-attention is activated for shorter durations, the spatial grounding   diminishes, as shown in the objects\u2019 reduced alignment with the input bounding   boxes. This phenomenon illustrates the inherent trade-off between spatial and   textual grounding, which cannot be resolved by controlling the duration of gated   self-attention activation.   5.2   Impact of Cross-Attention on Spatial Grounding   We also investigate whether cross-attention has influence on spatial grounding.   For this, we conduct a toy experiment by removing cross-attention modules   in GLIGEN [28], allowing the output of the gated self-attention to be directly   passed to the next layer of the U-Net. This modification is equivalent to changing   line 7 of Alg. 1 to F \u2190FGSA.   The results are displayed in Fig. 4. Note that, while the appearance of   the background and objects changes, the silhouettes of the cat (left) and the   individuals (right) remain precisely positioned within their respective bounding boxes without cross-attention. This observation indicates that while gated   self-attention that is performed before cross-attention may compromise textual   grounding, cross-attention that processes the output of gated self-attention does   not affect spatial grounding.   Layout   w/ CA   w/o CA   Layout   w/ CA   w/o CA   Fig. 4: Comparison of the output of GLIGEN [28] with and without cross-attention.   While the absence of cross-attention reduces realism and quality of the image, the   silhouette of objects remains grounded within the given bounding boxes, as shown in   the third column of each case.   5.3   Network Rewiring: From Sequential to Parallel   Building on the analyses above, we propose a simple yet effective modification to   the grounding mechanism, changing the relationship between gated self-attention   and cross-attention from sequential to parallel. This change eliminates the placement of gated self-attention before cross-attention, thus preventing the reduction of text grounding caused by gated self-attention. Moreover, in this parallel   arrangement, the preservation of spatial grounding is assured, as gated selfattention for spatial grounding does not require subsequent cross-attention.   ReGround: Improving Textual and Spatial Grounding at No Cost   9   Specifically, recall that in GLIGEN [28], the output of gated self-attention   is added to the residual from self-attention, which is then passed to the crossattention module as follows:     \\b e gin {spl it} F   _   {G   S   A} \\lef tarrow \\u   nderb   r a ce {\\tex t    {   GS   A   } (F_{S A}, \\{ g_   i \\})}_{\\textcolor {purple}{spatial\\;grounding}} + F_{SA};\\\\ F \\leftarrow \\underbrace {\\text {CA} (F_{GSA}, c)}_{\\textcolor {teal}{textual\\;grounding}} + F_{GSA}; \\label {eq:combined} \\end {split}    (2)   We propose to transform this sequence grounding pipeline into two parallel   processes as follows:     F \\lefta rrow    \\   un   d   erbrace  {\\text {   G   SA} (F_ {S   A   },       \\{ g_i \\})}_{\\te   x tco   lor    {purple}   {spatial\\;grounding}} + \\underbrace {\\text {CA} (F_{SA}, c)}_{\\textcolor {teal}{textual\\;grounding}} + \\underbrace {F_{SA}}_{residual};   (3)   Refer to Fig. 2 for the visualization of network architecture changes ((b) \u2192   (c)). This network rewiring is feasible because the input to gated self-attention   remains unchanged, while the input to cross-attention shifts to FSA, for which   it was originally designed in the context of Latent Diffusion Models [41].   It is important to note that the modification is effective even when applied   to the pretrained GLIGEN, which was trained with the sequential structure   of the attention modules. Therefore, our rewiring does not require any   additional training or fine-tuning, introduces no extra parameters,   and does not affect computation time or memory usage during the   generation process. The only requirement is the simple reconfiguration of the   attention modules at inference time.   6   Experiments   In this section, we show the effectiveness of our ReGround by evaluating the   spatial grounding on existing layout-caption datasets [12, 30] and the textual   grounding on text-image alignment metrics [26, 38]. We use the official GLIGEN [28] checkpoint which is trained based on Stable Diffusion v1.4 [41].   6.1   Datasets   MS-COCO. We use the validations sets of both MS-COCO-2014 and MSCOCO-2017 datasets [30]. Each dataset provides image-captions pairs and the   xy-coordinates of bounding boxes along with their corresponding object categories.   NSR-1K-GPT. We also use the NSR-1K benchmark [12] for evaluation. Based   on each subset of NSR-1K\u2014Counting and Spatial\u2014we develop a new benchmark, NSR-1K-GPT, augmenting each original caption in NSR-1K using GPT4 [34]. The instructions for augmentation are to (i) elaborate on the descriptions   of each mentioned entity and (ii) provide additional details about the background of the image. More details on the evaluation datasets are provided in the   Supplementary (Sec. S1).   10   P.Y. Lee and M. Sung   Layout   SD   GLIGEN\u03b3=1.0   GLIGEN\u03b3=0.2   ReGround   \u201cA woman in a white shirt standing in front of a fence smiling.\u201d   \u201cOne spotted cow in the photo, [...] under a clear blue sky.\u201d   \u201cA foreign airline status board with a clock.\u201d   \u201cA palm tree shower curtain is shown in a small bathroom.\u201d   \u201cA view of a living room with large windows.\u201d   tie   person   cow   clock   toilet   tv   couch   dining   table   vase   remote   vase   book   Fig. 5: Qualitative comparisons. Stable Diffusion (SD, 2nd column) generates images   that align with the given text descriptions, including the underlined phrase in each   row, but cannot take bounding boxes as input. GLIGEN (3rd column) creates images   that match the input layouts but suffers from description omission, failing to reflect   the underlined descriptions. Scheduled sampling strategy (4th column) can partially   address this issue (for instance, in the 5th row, where \u201cwindow\u201d appears in the room),   but it results in a noticeable decline in spatial accuracy (as seen in the 1st row, where   the tie is not generated). In contrast, our method (last column) accurately incorporates   the underlined text descriptions while maintaining precise spatial representation.   ReGround: Improving Textual and Spatial Grounding at No Cost   11   (a) MS-COCO-2014   (b) MS-COCO-2017   (c) NSR-1K-GPT-Counting   Fig. 6: Comparisons on MS-COCO [30] and NSR-1K-GPT. Each plot shows the relationship between textual grounding (i.e. CLIP score [15]) and spatial grounding (i.e.   YOLO score [52]) accuracy of GLIGEN [28] and our method. Note that the plot of   our ReGround is positioned in the top-right quadrant relative to GLIGEN, signifying   that it alleviates the inherent trade-off between textual and spatial grounding.   6.2   Evaluation Metrics   \u2013 YOLO score: Spatial grounding accuracy is assessed using YOLO score [52].   We employ YOLOv7 [52] to detect objects in each generated image and compute the average precision (AP) based on the ground truth bounding box   annotations from MS-COCO [30].   \u2013 CLIP score: Textual grounding accuracy is assessed using CLIP score [15].   \u2013 FID: Image quality and diversity are evaluated using FID [16].   \u2013 User Study and PickScore: We conduct a user study to assess human   p", "conf": "ECCV", "year": "2024", "index": 462}, {"title": "3DJCG: A Unified Framework for Joint Dense Captioning and Visual   Grounding on 3D Point Clouds   Daigang Cai1, Lichen Zhao1, Jing Zhang\u20201, Lu Sheng1, Dong Xu2   1College of Software, Beihang University, China,   2The University of Sydney, Australia   {caidaigang, zlc1114, zhang jing, lsheng}@buaa.edu.cn, dong.xu@sydney.edu.au", "abstract": "Observing that the 3D captioning task and the 3D   grounding task contain both shared and complementary information in nature, in this work, we propose a unified   framework to jointly solve these two distinct but closely related tasks in a synergistic fashion, which consists of both   shared task-agnostic modules and lightweight task-specific   modules. On one hand, the shared task-agnostic modules   aim to learn precise locations of objects, fine-grained attribute features to characterize different objects, and complex relations between objects, which benefit both captioning and visual grounding. On the other hand, by casting   each of the two tasks as the proxy task of another one,   the lightweight task-specific modules solve the captioning   task and the grounding task respectively.   Extensive experiments and ablation study on three 3D vision and language datasets demonstrate that our joint training framework achieves significant performance gains for each individual task and finally improves the state-of-the-art performance for both captioning and grounding tasks.   1.", "content": "There is increasing research interest in the intersection   field between 3D visual understanding and natural language   processing, such as 3D dense captioning [9] and 3D visual   grounding [1, 7, 21, 50]. These two tasks push the advance   of the intersection field along different directions (i.e., from   vision to language versus from language to vision), and encouraging progress has been achieved by separately solving   each task. It still remains an open issue on whether it is possible to develop a unified framework to jointly solve the two   closely related tasks in a synergistic fashion.   We observe that the two 3D vision-language tasks contain both shared and complementary information in nature,   and it is possible to enhance the performance of both tasks   if we treat one task as a proxy task of the other. On one   \u2020 Corresponding author: Jing Zhang.   hand, each of the two tasks can be decomposed into several   sub-tasks, and some of these sub-tasks share the common   objectives and network structures. For example, as shown   in the previous vision-language works [1,7,9,21,44,47,50]   on RGB-D scans, both 3D dense captioning and 3D visual grounding require: 1) a 3D object detector to detect   the salient object proposals in a 3D scene, 2) a relation   modeling module to model complex 3D relations among   these detected objects, and 3) a multi-modal learning module to learn fused information from both visual features and   textual features to generate sentences or produce bounding   boxes based on each input sentence. On the other hand,   the opposite procedures are also used to separately solve   the two problems, namely, the captioning task is to generate a meaningful textual description from the detected boxes   (i.e., from vision to language), while the grounding task is   to locate the desired box by understanding a given textual   description (i.e., from language to vision).   Moreover, the 3D point clouds generated from RGB-D   scans often contain rich and complex relations among different objects, while the corresponding RGB data provides   more fine-grained attribute information, such as color, texture, and materials. Thus, the RGB-D scans intrinsically   contain rich and abundant attribute and relation information   for enhancing both 3D captioning and 3D grounding tasks.   However, we empirically observe that the 3D dense captioning task is more object-oriented, which tends to learn more   attribute information of the target objects (i.e., the objects   of interest) in a scene and only the primary relationship between the target object and its surrounding objects. In contrast, the 3D visual grounding task is more relation-oriented,   which focuses more on the relations between objects and   distinguishes different objects (especially the objects from   the same class) based on their relations. Thus, it is desirable to develop a joint framework to unify both 3D dense   captioning and 3D visual grounding tasks and take advantage of each other for improving the performance of both   tasks.   To this end, in this work, we propose a joint framework by unifying the distinct but closely related 3D vision16464   language tasks of 3D dense captioning and 3D visual   grounding. Specifically, the proposed framework consists   of three main modules: (1) a 3D object detector, (2) an attribute and relation-aware feature enhancement module, and   (3) a task-specific grounding or captioning head. Specifically, the 3D object detector and the feature enhancement   module are task-agnostic, which are designed for collaboratively supporting both captioning and grounding tasks.   The two modules output the object proposals as the initial   localization results of the potential objects in a scene, as   well as the improved features within the proposals by integrating both attribute information from each object proposal   and the complex relations between multiple proposals. With   the strong task-agnostic modules, the task-specific captioning head and grounding head are designed as lightweight   networks for dealing with each task, which consist of a   lightweight transformer-based module together with simple   preprocessing modules (i.e., the Query/Key/Value generation modules) and lightweight postprocessing modules (i.e.,   the word prediction or bounding box selection module). In   this way, the 3D captioning and 3D visual grounding tasks   can be cast as the proxy task of each other. In other words,   the more object-oriented captioning task can provide more   attribute information to potentially improve the grounding   performance, while the more relation-oriented grounding   task can help improve the captioning results by enhancing   the captioning task with more relation information. Moreover, our joint framework also inspires the insights of the   design of each individual captioning network and grounding network.   The contribution of this work is two-fold: (1) By analyzing both 3D dense captioning and 3D visual grounding tasks, we propose a unified framework to jointly solve   the two distinct but closely related tasks by using our   simple and strong network structure, which consists of a   task-agnostic module with a 3D object detector and an   attribute and relation-aware feature enhancement module,   and two lightweight task-specific modules (i.e., a captioning head and a grounding head).   (2) Extensive experiments conducted on three benchmark datasets ScanRefer [7], Scan2Cap [9], and Nr3D dataset [1] demonstrate   our joint framework achieves the state-of-the-art results for   both 3D dense captioning and 3D visual grounding tasks.   2. Related Work   2D Vision and Language tasks. Deep learning technologies have been extensively studied in various 2D vision and   language tasks, such as visual grounding [15, 26, 35, 45].   image captioning & dense captioning [2,11,17,18,42], visual question answering [2, 4, 43] and text-to-image generation [25]. These impactful research problems advance   the intersection research field between computer vision and   natural language processing. With the rapid development of   deep learning, researchers introduced several collaborative   methods (e.g., speaker-listener models [3,46]) to solve various 2D vision and language tasks jointly. However, these   models focus on 2D image-based tasks, while our method   focuses on RGB-D-based tasks, where different types of   data to be handled in our work require different network   design strategies. Specifically, we propose a carefully designed task-agnostic feature enhancement module and the   lightweight task-specific captioning and grounding heads,   which all build upon the transformer architecture. Recently,   several joint frameworks [8,23,24,27,41,48] focus on learning more generalizable image-text representations through   a cumbersome model (e.g., VilBERT [23]) by using abundant and diverse 2D vision and language datasets. By contrast, based on in-depth analysis of the intrinsic properties of   RGB-D scans and the characteristics of both 3D captioning   and grounding tasks, our carefully designed joint learning   framework with lightweight modules can effectively solve   both tasks in a synergistic fashion without relying on a huge   amount of paired training data.   3D Dense Captioning and Visual Grounding.   Deep   learning in 3D data has attracted a great deal of interest [10, 13, 20, 22, 32\u201334, 39, 40, 49, 51]. Recently, some   dense captioning and visual grounding tasks tailored to 3D   data are proposed. For example, some researches [9] proposed the 3D dense captioning methods and achieved impressive results by explicitly modeling the relation between   different objects [9]. However, the dense captioning task is   more object-oriented, which often focuses on the precise attribute descriptions based on the object appearance and thus   the complex 3D geometrical relations among different objects might be ignored (even though they are intrinsically   contained in the 3D data). As a result, the generated captions may be monotony.   Except for 3D dense captioning, visual grounding on 3D   point clouds [1, 7, 14, 16, 44, 47, 50] has also attracted increasing research interest. Chen et al. [7] introduced the   ScanRefer dataset for localizing objects by using natural   language descriptions. Most recent 3D visual grounding   methods [7, 16, 47] are composed of two stages.   In the   first stage, a 3D object detector or a panoptic segmentation   model is applied to generate the target object proposals from   the input scenes. In the second stage, a referring module is   used to match the most relevant regions from the selected   object proposals and the query sentences. These methods   mainly focus on how to model the complex relations based   on the object detection results, and pay less attention to the   appearance features that characterize different objects, especially the objects within the same class. In other words,   the current grounding methods are more relation-oriented.   Our joint framework takes advantage of the overlooked   attribute information in the grounding task through the help   of the more object-oriented captioning task, and employs   16465   the relatively less explored relation information in the captioning task to increase the variety of generated sentences   with the help of the more relation-oriented grounding task.   3. Methodology   In this section, we describe the technical details of our   framework. As shown in Fig. 1(a), our framework consists of three modules: 1) the object detection module, 2)   the attribute and relation-aware feature enhancement module, and 3) the task-specific captioning head and grounding   head. The object detection module and feature enhancement module are task-agnostic and shared by both tasks.   The captioning and grounding heads are task-specific with   the lightweight transformer-based network structures for   the captioning and grounding tasks, respectively. Specifically, the point clouds are encoded by the VoteNet [31] object detection module with an improved bounding box modeling method to more precisely locate the salient objects   and produce the initial object proposals. Then the proposal   features are enhanced through a task-agnostic attribute and   relation-aware feature enhancement module to generate the   enhanced object proposals. The enhanced object proposals   are then fed into the captioning head and grounding heading   for the dense captioning task and the visual grounding task,   respectively, and generate the final result for each task.   3.1. Detection Module   The input of the detection module is the point cloud   P \u2208RN\u00d7(3+K), which represents the whole 3D scene by   N 3D coordinates together with K-dimensional auxiliary   features. Here, we adopt the same 132-dimensional auxiliary features as in   [7, 9], which include the pretrained   128-dimensional multi-view appearance features [7], 3dimensional normals, and 1-dimensional height of each   point above the ground.   We use VoteNet [31] as our detection module. Since   the success of both captioning and grounding tasks relies   on precise localization of initial object proposals together   with discriminative features, we borrow the idea from the   anchor-free FCOS method [36] to generate the initial object proposals by predicting the distance between the voting   point and each side of the object proposal.   3.2. Attribute and Relation-aware Feature Enhancement Module   The initial object proposal features produced by the detection module are discriminative with respect to different   object classes, thanks to the detection-related loss. However, they are unaware of the fine-grained object attributes   (e.g., object positions, colors, and materials), especially for   the within-class objects, and the complex relations among   different objects, which are the key to the success of both   3D captioning and 3D grounding tasks. Hence, we further   propose an attribute and relation-aware feature enhancement module to strengthen the features for each proposal   and better model the relations between proposals. Motivated by the Transformer encoder structure [37], we model   the proposal feature enhancement module as two multi-head   self-attention layers with additional attribute encoding module and relation encoding module, where the attribute or relation encoding module is composed of several fully connected layers.   The attribute encoding module. To aggregate the attribute features and the initial object features, we encode   the auxiliary bounding box attribute related features (i.e., a   155-dimensional feature via a concatenation operation on   the 27-dimensional box center and corner coordinates, and   the 128-dimensional multi-view RGB features that potentially contain the attribute information such as colors and   materials) into a 128-dimensional attribute embedding by   using a fully connected layer. The attribute embedding has   the same dimension as the initial object proposal features. It   can then be added to the initial proposal features to enhance   the initial object features with more attribute information.   The relation encoding module. Motivated by [50], we   also encode the pairwise distances between any two object   proposals to capture the complex object relations. Different   from [50], we encode not only the (inverse) relative Euclidean distances (i.e., Dist \u2208RM\u00d7M\u00d71) but also three   pairwise distances between any two centers of the initial   object proposals along x, y, z direction (i.e., Dx, Dy, Dz   \u2208RM\u00d7M\u00d71) to better capture object relations along different directions, where M is the number of initial object   proposals. All four spatial proximity matrices (Dx, Dy,   Dz, and Dist) are then aggregated along the channel dimension and fed into fully connected layers to produce the   relation embeddings with the channel dimension H matches   the number of attention heads (i.e., H = 4 in our implementation) in the multi-head attention module. Each relation   embedding (with the size of M \u00d7 M \u00d7 1) is then added   with the similarity matrix (i.e., the so-called attention map)   generated from each head of the multi-head self-attention   module.   Note the task-agnostic 3D object detector and the feature   enhancement module can produce more accurate localization results and improved object features for both captioning   and grounding tasks, and thus we can use more lightweight   task-specific captioning head and grounding head in our   framework which are simpler than the state-of-the-art methods [9, 50]. For both task-specific heads, we adopt similar   lightweight 1-layer multi-head cross-attention-based network structures together with simple preprocessing modules (i.e., Query/Key/Value generation as shown in Fig. 2)   and postprocessing modules (i.e., word prediction or BBox   selection).   16466   (b) Attribute & Relation Aware    Feature Enhancement   (c) Captioning Head   (d) Grounding Head   This is a                   ...   FC & Word    Prediction   Enhanced Object Proposals   (and GT BBox when training)   Softmax   Softmax   FC   FC   FC   FC   FC   FC   FC   FC   FC   Q   K   V   Softmax   Key & Value    Generation   Output Word   Keyboard   Hidden Feature   Query    Generation   Q   K   V   FC   FC   FC   Softmax   FC   FC   FC   Softmax   FC   FC   FC   Softmax   \u00d72   \u00d71   Textual Input   A black chair is    sitting at the desk.   FC & BBox    Selection   Referred Bounding Box   Add & Norm   Softmax   Softmax   FC   FC   FC   FC   FC   FC   FC   FC   FC   FC   Q   K   V   Softmax   Concatenation   Key & Value    Generation   Enhanced    Object    Proposals   \u00d71   Enhanced Object Proposals   Initial Object Proposals   Add & Norm   Concatenation   FC   Add & Norm   FC   Concatenation   Spatial    Distance   Matrices   Auxiliary   Features   FC   FC   Attribute    Embeddings   3D Object Detector   Input 3D Point Clouds   Initial Object  Proposals   (a) Overview of our framework   Attribute & Relation    Aware Feature    Enhancement   Enhanced Object  Proposals   Textual    Input   Captioning    Head   Grounding    Head   Referred BBox   Caption   Target GT BBox   (when training)   Spatial    Distance    Matrices &   Auxiliary    Features    Calculation   Figure 1. (a) The overview of our framework. (b) The attribute and relation aware feature enhancement module. (c) The captioning head   within our framework (d) The grounding head within our framework. \u201cFC\u201d means the fully connected layer.   Text    Class   Key and    Value   k NN   Concatenation & FC   Query   Key and Value   GRU   Glove   Proposal    Selection & FC   Glove   Enhanced    Object    Proposals   Textual    Input   Target GT   BBox   (when Training)   Previous    Word   Previous    Hidden    Feature   Target GT   BBox   (when Training)   Enhanced    Object    Proposals   (c) Key & Value Generation   (Grounding)   (b) Key & Value Generation   (Captioning)   (a) Query Generation   (Captioning)   Proposal    Selection & FC   Tatget  Object Proposal   Tatget  Object Proposal   Figure 2. The Query, Key & Value generation processes for both   captioning head and grounding head. For the captioning head, we   firstly choose the object of interest to produce the target object proposal. We concatenate the target proposal feature, the tokenized   word feature from the previous word and the hidden feature recurrently output by the multi-head cross-attention module, and use   fully connected layers to generate the Query. We select K nearest   neighbors of the target object proposal as the Key and Value. For   the grounding head, the textual input is firstly tokenized and fed   into a GRU cell to produce the the Key and Value of the multihead cross-attention module. The Query for the grounding task is   the enhanced object proposal features (see Fig. 1(d)).   3.3. Captioning Head   The 3D dense captioning task is to generate descriptions   for each detected bounding box from the input point cloud,   which is more object-oriented. Thus, the objectness (for   accurately locating each object), the attribute information   (for reasonably describing the attributes of objects), and the   primary context (for further describing the key relations between each object with other objects) of all the objects in   a scene are of great importance. Since the object detector and the feature enhancement module can provide rich   object class information, attribute features, and global context features, we simply design our captioning head with   a 1-layer multi-head cross-attention network structure for   effective message passing between the enhanced features   from the target object proposal and all other initial object   proposals, which will focus more on the primary context   features.   For generating the query (Q) input of the multi-head   cross-attention module, we firstly select the target object   proposal and then encode the corresponding object features   with a fully connected layer. During the training stage, we   select the object proposal with the highest IoU score with   the ground-truth bounding box as the query object. In the   testing stage, we use all object proposals in the scene (after   the Non-Maximum Suppression (NMS) process) in a oneby-one fashion as the query object. For the target object   proposal, we follow most of the captioning methods [9] to   use a recurrent network structure to progressively generate   each word of the caption. Then, we recurrently aggregate   the hidden feature output by the multi-head cross-attention   module and the tokenized word feature of the previous word   (which is the ground-truth word in the training stage, and   the newly predicted word in the testing stage) with the current query object features. The fused features form the final   generated query input.   In the recursive query generation process, to alleviate   the exposure bias [6] in the sequence generation task be16467   tween the training stage (which uses the ground-truth word)   and the testing stage (which uses the previously predicted   word), we randomly use the autoregressive strategy during training. In details, we randomly replace 10% of the   ground-truth word tokens with the predicted word tokens as   the input word feature during the training process.   In the key (K) and value (V) generation module, we use   the k-NN strategy to select the top k object proposals that   are located closest to the target proposal based on their center distance in the 3D coordinate space, which filters out   the less related objects in the scene. The selected object   proposals are used as the key and value for the multi-head   cross-attention module. In our experiment, k is empirically   set as 20. This strategy is specially designed for the captioning task, because it mainly cares about the most obvious (or primary) relations between the target object and its   surrounding objects and the rest of the relation information   might be less important to the captioning task.   Finally, the multi-head cross-attention module is followed by a fully connected layer and a simple word prediction module to predict each word of the caption in a oneby-one fashion.   3.4. Grounding Head   For the 3D visual grounding task, the inputs include the   3D point clouds of a scene and the text-form language descriptions of one of the objects in the scene, and the task   is to locate the object of interest based on the language description. Since the task-agnostic 3D object detector and   the feature enhancement module already capture the object attributes and the complex relations among objects in a   scene, the grounding head mainly focuses on matching between the given language descriptions and the detected object proposals. The grounding head in our method is more   lightweight by simply using a 1-layer multi-head crossattention module instead of multiple stacked cross-attention   modules as used in [50] and [14].   The key (K) and value (V) inputs are generated based   on the input language descriptions. Specifically, we use the   similar language encoder as in ScanRefer [7]. The input   language is firstly encoded by using a pretrained Glove [30]   module, and then input to a GRU cell. The output word   feature of the GRU cell forms the key (K) and value (V) inputs. Moreover, a global language feature is also generated   from the GRU cell to predict the subject category of each   sentence. The object proposals are used as the query (Q)   input. By using the multi-head cross-attention mechanism   between the language descriptions (K & V) and the object   proposals (Q), the relationship between the sentence and the   detected proposals is well captured.   To fully explore the contextual relations among the given   textual description, we follow [50] to use two data augmentation strategies for both modalities (e.g., randomly erase   some words or change the order of the input text for the text   input, and randomly copy some object proposals from other   scene as the negative samples for enhancing object proposals), please refer to [50] for more details about the two data   augmentation strategies.   Finally, a grounding classifier is used to generate the   confidence score of each object proposal, and the proposal   with the highest prediction score is considered as the final   grounding result.   3.5. Training details   The loss function of our framework is a combination of   the detection loss Ldetection, the grounding loss Lgrounding and   the captioning loss Lcaptioning.   The object detection loss is similar to that used in Qi et   al. [31] for the ScanNet dataset [12], where Ldetection =   10Lvote-reg + Lobjn-cls + Lsem-cls + 200Lboundary-reg, except   that we replace the bounding box classification loss Lbox-cls   and the regression loss Lbox-reg in [7,31] with the boundary   regression loss Lboundary-reg [36]. For the visual grounding   task, we apply the similar loss function as used in ScanRefer [7], which is a combination of the localization loss Lloc   for visual grounding and an auxiliary language-to-object   classification loss Lcls to enhance the subject classification   of the input sentence, and Lgrounding = Lloc + Lcls. For the   dense captioning task, we input the ground-truth words (or   the predicted words with a probability of 10%) sequentially   and Lcaptioning is the average cross-entropy loss over all generated words. The final loss is a linear combination of these   loss terms, i.e., L = Ldetection +0.3Lgrounding +0.2Lcaptioning,   where the trade-off parameters are empirically set for balancing different loss terms.   4. Experiments   4.1. Datasets and implementation details   Visual Grounding Dataset: We use the ScanRefer [7]   dataset to evaluate our method for the visual grounding   task. The ScanRefer dataset contains 51, 583 textual descriptions about 11, 046 objects from 800 scenes. The overall accuracy and the accuracies on both\u201cunique\u201d and \u201cmultiple\u201d subsets are reported. We label each grounding data   as \u201cunique\u201d if it only contains a single object from its class   in the scene, otherwise it will be labeled as \u201cmultiple\u201d. For   this dataset, we use Acc@0.25IoU and Acc@0.5IoU as our   evaluation metrics. We also compare our method with the   baseline methods on both the validation set and the online   test set available at the ScanRefer\u2019s benchmark website1.   Visual Captioning Datasets: Scan2Cap [9] is a dense   captioning dataset for 3D scenes. The descriptions that are   longer than 30 tokens in the ScanRefer dataset are truncated   and two special tokens [SOS] and [EOS] are added to   1http://kaldir.vc.in.tum.de/scanrefer_benchmark   16468   Table 1. Comparison of the visual grounding results from different methods on the ScanRefer [7] dataset. We report the percentage of the   correctly predicted bounding boxes whose IoU scores with the ground-truth boxes are larger than 0.25 and 0.5, respectively. The results on   both \u201cunique\u201d and \u201cmultiple\u201d subsets are also reported. [*]: Note the InstanceRefer [47] method filters the predicted 3D proposals based   on the object class prediction results such that this method only selects the target object proposal from the proposals in the same class,   which simplifies the 3D visual grounding problem. This strategy is not adopted in our work.   Unique   Multiple   Overall   Detector   Data   Acc@0.25   Acc@0.5   Acc@0.25   Acc@0.5   Acc@0.25   Acc@0.5   Validation set   ScanRefer [7]   VoteNet   3D Only   67.64   46.19   32.06   21.26   38.97   26.10   InstanceRefer [47]*   PointGroup   3D Only   77.13   66.40   28.83   22.92   38.20   31.35   Non-SAT [44]   VoteNet   3D Only   68.48   47.38   31.81   21.34   38.92   26.40   3DVG-Transformer [50]   VoteNet   3D Only   77.16   58.47   38.38   28.70   45.90   34.47   Ours   VoteNet   3D Only   78.75   61.30   40.13   30.08   47.62   36.14   ScanRefer [7]   VoteNet   2D + 3D   76.33   53.51   32.73   21.11   41.19   27.40   TGNN [16]   3D-UNet   2D + 3D   68.61   56.80   29.84   23.18   37.37   29.70   SAT [44]   VoteNet   2D + 3D   73.21   50.83   37.64   25.16   44.54   30.14   InstanceRefer [47]*   PointGroup   2D + 3D   75.72   64.66   29.41   22.99   38.40   31.08   3DVG-Transformer [50]   VoteNet   2D + 3D   81.93   60.64   39.30   28.42   47.57   34.67   Ours   VoteNet   2D + 3D   83.47   64.34   41.39   30.82   49.56   37.33   Online Benchmark   ScanRefer [7]   VoteNet   2D + 3D   68.59   43.53   34.88   20.97   42.44   26.03   TGNN [16]   3D-UNet   2D + 3D   68.34   58.94   33.12   25.26   41.02   32.81   InstanceRefer [47]*   PointGroup   2D + 3D   77.82   66.69   34.57   26.88   44.27   35.80   3DVG-Transformer [50]   VoteNet   2D + 3D   75.76   55.15   42.24   29.33   49.76   35.12   Ours   VoteNet   2D + 3D   76.75   60.59   43.89   31.17   51.26   37.76   Table 2. Comparison of the 3D dense captioning results from different methods on the Scan2Cap [9] validation set. We average the scores   from the conventional captioning metrics based on the predicted bounding boxes whose IoU scores with the ground-truth boxes are larger   than 0.25 and 0.5, respectively.   Detector   Data   C@0.25   B-4@0.25   M@0.25   R@0.25   C@0.5   B-4@0.5   M@0.5   R@0.5   Scan2Cap [9]   VoteNet   3D Only   53.73   34.25   26.14   54.95   35.20   22.36   21.44   43.57   Ours   VoteNet   3D Only   60.86   39.67   27.45   59.02   47.68   31.53   24.28   51.08   VoteNetRetr [31]   VoteNet   2D + 3D   15.12   18.09   19.93   38.99   10.18   13.38   17.14   33.22   Scan2Cap [9]   VoteNet   2D + 3D   56.82   34.18   26.29   55.27   39.08   23.32   21.97   44.48   Ours   VoteNet   2D + 3D   64.70   40.17   27.66   59.23   49.48   31.03   24.22   50.80   indicate the start and end of the description, and thus the   textual descriptions for ScanRefer and Scan2Cap datasets   are different.   As a sub-dataset of ReferIt3D [1], Nr3D is also built   based on ScanNet with additional textual descriptions, and   it contains 41, 503 samples collected by ReferItGame. We   use the same metric as used for performance evaluation on   the Scan2Cap dataset.   Specifically, the metric for performance evaluation on   these two 3D captioning datasets combines the standard   image captioning metrics under different IoU scores between the predicted bounding boxes and the target bounding boxes. The combined metric is defined as m@kIoU =   1   P   PP   i=0 miui , where ui \u2208{0, 1} is set to 1 if the detection IoU score for the i-th bounding box is greater than k,   and 0 otherwise. We use mi to represent the captioning   metrics such as CiDEr [38], BLEU [28], METEOR [5] and   ROUGE-L [19], which are respectively abbreviated as C,   B-4, M and R in the following tables. P is the number of   ground-truth or detected object bounding boxes.   Implementation Details. We follow [50] to use 8 sentences for each scene from both datasets when training our   framework. Our experiment is carried out on the machine   with a single NVIDIA 11GB 2080Ti GPU and it tasks 200   epochs to train our framework on both ScanRefer [7] and   Scan2Cap [9] datasets with a batch size of 10 in each iteration (i.e., there are 80 sentences from 10 point clouds).   We apply the cosine learning rate decay strategy with the   AdamW optimizer and a weight decay factor of 1e-5 to train   our method. We empirically set the initial learning rate as   2e-3 for the detector, and 5e-4 for other modules of our   framework (i.e., the feature enhancement module and two   task-specific heads). In addition, the captioning task with   the cross-entropy loss is prone to overfitting, so we only   add the captioning loss during the last 50 epochs.   16469   4.2. Comparison with the state-of-the-art methods   Following the works ScanRefer [7] and Scan2Cap [9],   we report the results under both \u201c3D Only\u201d and \u201c2D +   3D\u201d settings according to whether the auxiliary features are   used. Under the \u201c3D Only\u201d setting, we use \u201cxyz + RGB +   normals\u201d as the auxiliary features. Under the \u201c2D + 3D\u201d   setting, the auxiliary features contain \u201cxyz + multiviews +   normals\u201d, where \u201cmultiviews\u201d means multiview image features from a pretrained ENet [29], and \u201cnormals\u201d means the   normal vectors from point clouds.   In Table 1 and Table 2, we compare the dense captioning and visual grounding results of our framework   with several state-of-the-art methods on both ScanRefer [7]   and Scan2Cap [9] datasets.   Specifically, on the ScanRefer dataset, we compare our method with the 3D instance segmentation-based methods TGNN [16] and InstanceRefer [47] and the 3D detection-based methods including ScanRefer [7] and 3DVG-Transformer [50].   On   the Scan2Cap dataset, we compare our method with the   state-of-the-art 3D detection-based method Scan2Cap [9]   and VoteNetRetr [31].   From Table 1, we observe that our method outperforms   the baseline methods for the visual grounding task. Note   that we use a simpler network structure when compared   with the state-of-the-art method 3DVG-Transformer [50],   so the results validate that our joint learning framework   can benefit the grounding task with only a lightweight   grounding head. Specifically, in terms of Acc@0.25 and   Acc@0.5 metrics, our method achieves around 1.9% and   2.6% improvements in the \u201coverall\u201d case when compared   with 3DVG-Transformer [50] on the validation set under   the \u201c2D+3D\u201d setting. When compared with other detectionbased methods, our method achieves more improvement   on the \u201cUnique\u201d subset, possibly because the attribute information of the objects plays a more important role in   the \u201cUnique\u201d subset when there is no confusing objects   from the same category in the scene. The results also verify that the object-oriented captioning task enhances the   grounding performance by providing more attribute information. Note that the baseline methods InstanceRefer [47]   and TGNN [16] use the extra instance segmentation masks   for generating 3D proposals, while the InstanceRefer [47]   method further filters the instances based on the semantic   prediction results, namely, it only retains the instances from   the same predicted class for generating the visual grounding   results. Possibly due to these two aspects, the InstanceRefer [47] method achieve good results in the \u201cUnique\u201d subset.   In contrast to [16,47], our work only relies on the detection   results, and it still outperforms both methods in both \u201cMultiple\u201d and \u201cOverall\u201d cases.   When compared with the baseline method \u201cScan2Cap\u201d,   from the results in Table 2, we observe that our joint learning framework using a simple feature enhancement module   and a lightweight captioning head achieves significant performance improvement for the captioning task. Under the   \u201c2D+3D\u201d setting, our method achieves remarkable performance improvement of 10.4%, 7.71% and 6.32% in terms   of C@0.5IoU, B-4@0.5IoU and R@0.5IoU, respectively.   For this task, the improvement comes from both network   structure design (e.g., the attribute and relation aware feature enhancement module, and the lightweight captioning   head) and the joint training strategy. The contribution of   each module will be discussed in the ablation study below.   4.3. Ablation Study   Effectiveness of the feature enhancement module and   the joint training strategy. To evaluate the effectiveness   of the proposed task-agnostic feature enhancement module as well as the joint training strategy, we conduct the   ablation study and report the corresponding results in Table 3. Without using the joint training strategy, the alternative method \u201cw/o Grounding Head\u201d (resp., \u201cw/o Captioning   Head\u201d) means we train the two separate networks consisting of two task-agnostic modules and the captioning head   (resp., grounding head) for the 3D dense captioning task   (resp., the visual grounding task). \u201cw/o Feature Enhancement\u201d means we remove the \u201cattribute & relation aware feature enhancement\u201d module in our joint learning framework.   For both dense captioning and the visual grounding tasks,   our complete 3DJCG method based on the default training data (i.e., from both Scan2Cap and ScanRefer datasets)   outperforms those alternative methods, which indicate both   strategies contribute to the final performance improvement   to certain degree.   Does performance improvement come from more training data? Our joint training framework uses both the captioning and grounding training data, in which the only difference is the textual descriptions (i.e., the descriptions used   for the grounding task are relatively longer or with more   complex relations, while the dense captions are shorter textual descriptions focusing more on the object class and the   corresponding attributes). Hence, we conduct the experiments to verify whether the performance improvement is   due to the utilization of more training data (i.e., more textual descriptions from both tasks).   In Table 3 (a) and (b), 3DJCG (\u201cCaptioning Data Only\u201d   (resp., 3DJCG (\u201cGrounding Data Only\u201d)) indicates that we   only use the 3D captioning dataset Scan2Cap [9] (resp., the   3D visual grounding dataset ScanRefer [7]) when training   our joint learning framework including both captioning and   grounding heads and the two task-agnostic modules. Note   both Scan2Cap and ScanRefer datasets can be readily used   as the training data for these two tasks. By default, we use   both datasets as the default training data when training our   joint learning framework.   The results show that our 3DJCG framework using \u201cCap16470   Table 3. Comparison of the visual grounding results under the \u201c2D+3D\u201d setting and the dense captioning results based on the correctly   predicted bounding boxes whose IoU scores with the ground-truth boxes are larger than 0.5. In the \u201cNetwork Modules\u201d column, for better   presentation, we label our detector, the feature enhancement module, the captioning head and the grounding head as \u201cDE\u201d, \u201cFE\u201d, \u201cCH\u201d   and \u201cGH\u201d, respectively.   (a) The 3D dense captioning results on the dataset Scan2Cap [9]   Training Dataset(s)   Network Modules   Dense Captioning Results   Scan2Cap   ScanRefer   DE   FE   CH   GH   B-4@0.5   C@0.5   R@0.5   M@0.5   3DJCG (w/o Grounding Head) / 3DJCG-C   \u2713   \u2713   \u2713   \u2713   26.24   45.04   46.69   23.27   3DJCG (w/o Feature Enhancement)   \u2713   \u2713   \u2713   \u2713   \u2713   29.08   47.67   49.58   23.78   3DJCG (Captioning Data Only)   \u2713   \u2713   \u2713   \u2713   \u2713   30.40   47.29   50.29   23.91   3DJCG (Default Training Data)   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   31.03   49.48   50.80   24.22   (b) The 3D visual grounding results on the dataset ScanRefer [7]   Training Dataset(s)   Network Modules   Visual Grounding Results   Scan2Cap   ScanRefer   DE   FE   CH   GH   Unique@0.5   Multiple@0.5   Overall@0.5   3DJCG (w/o Captioning Head) / 3DJCG-G   \u2713   \u2713   \u2713   \u2713   62.60   30.48   36.72   3DJCG (w/o Feature Enhancement)   \u2713   \u2713   \u2713   \u2713   \u2713   63.20   28.36   35.12   3DJCG (Grounding Data Only)   \u2713   \u2713   \u2713   \u2713   \u2713   64.50   30.29   36.93   3DJCG (Default Training Data)   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   64.34   30.82   37.33   Table 4. The dense captioning results of different methods and different training strategies on the Nr3D dataset from ReferIt3D [1].   B-4@0.5 C@0.5 R@0.5 M@0.5   Scan2Cap [9]   17.24   27.47   49.06   21.80   3DJCG-C (From Scratch)   20.45   33.03   51.73   23.05   3DJCG-C* (Finetune)   22.82   38.06   52.99   23.77   tioning Data Only\u201d (resp., \u201cGrounding Data Only\u201d) generally improves the performance for the captioning task   (resp., the grounding task) when compared to the alternative   method 3DJCG (\u201cw/o Grounding Head\u201d) (resp., 3DJCG   (\u201cw/o Captioning Head\u201d)), especially for the dense captioning task. The results validate that the performance gains   come from both strategies (i.e., our network design and   utilization of the additional training data). Moreover, the   improved results from our joint learning framework under   \u201cCaptioning Data Only\u201d and \u201cGrounding Data Only\u201d settings also verify that our joint framework can also inspire   the network design of each individual task.   Experiments on the Nr3D [1] dataset.   We also take   the dense captioning task on the Nr3D dataset as an example to evaluate our proposed framework when training   from scratch or using the fine-tuning strategy. \u201c3DJCG-C   (From Scratch)\u201d indicates that we train our 3DJCG-C network from scratch without using any pre-training strategies. \u201c3DJCG-C* (Finetune)\u201d indicates we fine-tune the   pretrained model based on the Nr3D dataset.   Note the   pretrained model is learnt based on both ScanRefer and   Scan2Cap datasets, and we also remove \u201cGrounding Head\u201d   before performing the finetune process. We also list the results of the baseline method Scan2Cap trained from scratch   based on the Nr3D dataset.   As shown in Table 4, our   method \u201c3DJCG-C (From Scratch)\u201d outperforms the baseline method \u201cScan2Cap [9]\u201d, which further verifies the effectiveness of our newly designed network structure. We   also observe that our \u201c3DJCG-C* (Finetune)\u201d method further improves \u201c3DJCG-C (From Scratch)\u201d, which demonstrates that the results of our framework could also be   boosted by using the fine-tuning strategy.   5. Conclusion and Future Work   Observing the shared and complementary properties of   two different but closely related tasks 3D dense captioning   and 3D visual grounding, we propose a unified framework   to jointly solve the two tasks in a synergistic manner. In   our framework, the task-agnostic modules are responsible   for the precise object localization, the enhancement of the   geometry and the fine-grained attribute features, and fully   exploration of the complex geometrical relations between   objects in a 3D scene, while the task-specific lightweight   captioning head and grounding head solve the two tasks, respectively. The experimental results validate the effectiveness of the proposed framework for both tasks. While the   joint framework improves the performance of both tasks,   the performance improvement for the visual grounding task   is not as significant as that for the dense captioning task. In   our future work, we will develop more advanced joint training framework to further improve the 3D visual grounding   performance.   Acknowledgement This work was supported by the National Key Research and Development Project of China   (No.   2018AAA0101900), and the National Natural Science Foundation of China (No.61906012, No.62006012,   No.62132001).   16471", "conf": "CVPR", "year": "2022", "index": 170}, {"title": "Mask Grounding for Referring Image Segmentation   Yong Xien Chng1,2   Henry Zheng1   Yizeng Han1   Xuchong Qiu2\u2020   Gao Huang1B   1Department of Automation, BNRist, Tsinghua University   2Bosch Corporate Research", "abstract": "Referring Image Segmentation (RIS) is a challenging   task that requires an algorithm to segment objects referred   by free-form language expressions.   Despite significant   progress in recent years, most state-of-the-art (SOTA) methods still suffer from considerable language-image modality   gap at the pixel and word level. These methods generally   1) rely on sentence-level language features for languageimage alignment and 2) lack explicit training supervision   for fine-grained visual grounding. Consequently, they exhibit weak object-level correspondence between visual and   language features. Without well-grounded features, prior   methods struggle to understand complex expressions that   require strong reasoning over relationships among multiple   objects, especially when dealing with rarely used or ambiguous clauses. To tackle this challenge, we introduce a   novel Mask Grounding auxiliary task that significantly improves visual grounding within language features, by explicitly teaching the model to learn fine-grained correspondence between masked textual tokens and their matching   visual objects. Mask Grounding can be directly used on   prior RIS methods and consistently bring improvements.   Furthermore, to holistically address the modality gap, we   also design a cross-modal alignment loss and an accompanying alignment module. These additions work synergistically with Mask Grounding. With all these techniques,   our comprehensive approach culminates in MagNet (Maskgrounded Network), an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO,   RefCOCO+ and G-Ref), demonstrating our method\u2019s effectiveness in addressing current limitations of RIS algorithms.   Our code and pre-trained weights will be released.   1.", "content": "Deep learning has greatly improved the performance of vision algorithms on many image segmentation tasks, such   as semantic segmentation [5, 48], instance segmentation   [2, 12, 24, 42] and panoptic segmentation [8, 36]. These   \u2020 Project lead.   B Corresponding author.   diving man   diving man   (b) Fine-grained visual grounding is required to understand   expressions used in uncommon or ambiguous contexts.   Correct   Wrong   diving man   Correct   third remote from the left   (a) Fine-grained visual grounding is required to reason over   complicated relationships among multiple objects.   third remote from the left   third remote from the left   Correct   Wrong   Wrong   Figure 1. Importance of Fine-grained Visual Grounding for RIS.   Most RIS algorithms lack well-grounded text features. As a result,   they struggle in difficult cases illustrated in (a) and (b). Red mask   are predictions of LAVT, one of the recent SOTA RIS methods.   Yellow dotted boxes are the ground truths.   tasks require grouping of image pixels under a fixed set of   pre-defined categories and mainly differ in the granularity   of grouping semantics required. In contrast to these unimodal segmentation tasks, Referring Image Segmentation   (RIS) [9, 28] is a challenging multi-modal task that requires   an algorithm to simultaneously understand fine-grained human language expression and make correct pixel-level correspondence to the referred object. Recently, it has gained   widespread research attention due to its potential to improve human-robot interaction [1], interactive image editing   [43, 52] and advanced driver-assistance systems [29].   The key challenge in RIS lies in how to reduce the   modality gap between language and image features [14, 64,   71]. To tackle this challenge, we need to have an effective   alignment between a given language expression and the corresponding image pixels for highlighting the referred target.   Ideally, with robust pixel-wise language-image alignment,   language and image features should have high feature similarity when referring to the same object and low feature simThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   26573   Alignment   Loss/Module   Multi-modal Decoder   Language Encoder   Image Encoder   sentence-level features   Weak Fine-grained   Visual Grounding   (b) Our proposed Mask Grounding   Language   Encoder   Image   Encoder   Masked   Token   Predictor   Mask   Encoder   Piles of\u00a0   <mask> behind   the grapes   implicit   supervision   Alignment   Auxiliary Task   Multi-modal Decoder   Language Encoder   Image Encoder   word-level features   Strong Fine-grained   Visual Grounding   Mask   Prediction   Loss   Mask   Grounding   Loss   explicit   supervision   (a) Comparison between our method and pervious methods   0.01   0.95   0.03   0.02   oranges   grapes   apples   bananas   Figure 2. (a) Current SOTA RIS methods mainly focus on designing and improving multi-modal alignment modules and/or alignment   losses. These methods generally 1) do not have explicit training supervision for fine-grained visual grounding and 2) use sentencelevel language features or image/pixel-level image features for alignment. As a result, their language features lack precise visual-textual   object correspondence. (b) Our proposed Mask Grounding remedies this problem by explicitly teaching our model to learn fine-grained   correspondence between masked word tokens and their matching visual objects through an auxiliary alignment task.   ilarity when referring to different objects. However, achieving such alignment is non-trivial because the language expression can be highly complex and diverse.   As depicted in Fig. 2, prevailing methods primarily focus on devising innovative losses [61, 71] or introducing   new network architectures/modules [14, 45, 63, 64, 72] to   bolster language-image alignment. Despite their advancements, two overarching limitations persist. First, these approaches tend to rely on sentence-level language features   for language-image alignment. Second, they often lack explicit training supervision for fine-grained visual grounding. These oversights result in their language features becoming noisy anchors for RIS prediction [63, 69], inhibiting the effective learning of fine-grained visual grounding.   Consequently, such models face challenges when interpreting referring expressions that require intricate reasoning   across complex inter-object relationships or contain clauses   used in rare or ambiguous contexts, as exemplified in Fig. 1.   To address this challenge, we introduce a novel Mask   Grounding auxiliary task to explicitly teach our model to   make fine-grained correspondence between masked textual   tokens and their matching visual objects. Specifically, during training, our model encounters randomly masked textual tokens and has to predict their identities. Instead of   relying solely on the surrounding textual context to predict   these tokens, our model integrates both visual and segmentation information. This integrated approach is pivotal for   the model to make accurate prediction, as it must discern   and establish the correct linkage between the masked tokens and their corresponding visual objects. Learning to do   so ensures that our model acquires a profound proficiency   in the highly-coveted fine-grained visual grounding. The   efficacy of Mask Grounding is empirically validated with   extensive ablation experiments. Moreover, we also show   that Mask Grounding is universal and can be directly used   on prior RIS methods to bring significant improvements.   In addition to Mask Grounding, we also design a crossmodal alignment loss and an alignment module to holistically bridge the modality gap. With all these enhancements, our resulting MagNet (Mask-grounded Network)   sets new records by significantly outperforming previous   SOTA methods across all key datasets (RefCOCO [67], RefCOCO+ [67] and G-Ref [49, 50]). Notably, our method   consistently outperforms these SOTA methods by large   margins of up to 2.48 points in overall IoU. Visual examination of MagNet\u2019s predictions reinforces our claim and   shows that our method works well in complex scenarios.   Our main contributions are summarized as follows:   1. We highlight the shortcomings in recent state-of-the-art   (SOTA) RIS algorithms, pinpointing the lack of finegrained visual grounding.   2. We introduce the Mask Grounding auxiliary task, a novel   method aimed at enhancing fine-grained visual grounding in existing RIS algorithms. Its effectiveness is validated through rigorous ablation studies.   3. Using Mask Grounding, together with our specially designed cross-modal alignment loss and an accompanying   alignment module, we present MagNet (Mask-grounded   Network), a new SOTA network for RIS.   26574   2. Related works   Architecture Design for RIS. Early works [28, 41, 44, 54]   follow a concatenate-then-convolve pipeline, where language and image features are fused by concatenation. Subsequent works [4, 41, 44, 65] improve upon this pipeline by   using RNN or dynamic networks [19\u201321, 23] to progressively refine the segmentation mask. Other works [16, 64]   investigate the position to perform language-image fusion   and conclude that early fusion performs the best.   Apart   from designing novel fusion mechanisms, some works   [31, 32, 68] exploit known linguistic structures or object relationships to enhance language-image fusion. Riding on the success of attention architecture [15, 22, 58],   current works mostly use unidirectional [54, 64, 66] or   bidirectional [30, 71] cross-attention modules to perform   language-image fusion. To improve model performance on   novel composition of learned concepts, a recent work [62]   uses meta learning [17]. Driven by the success of large language models [3, 57], newer works [10, 46, 74] explore formulating RIS as an auto-regressive vertex generation problem. Lately, VPD [72] attempts to exploit semantic information in diffusion models [27, 51, 55] for RIS, whereas   ReLA [45] and DMMI [29] generalize RIS to support an   arbitrary number of targets. Despite huge progress in RIS   architecture design, prior studies often expect languageimage alignment to be performed implicitly through mask   prediction. We enhance this by introducing an auxiliary task   for explicit language-image feature alignment.   Loss Design for RIS. Early works train RIS models with   simple binary cross entropy loss.   Inspired by the success of prior works [59, 73] in adopting contrastive loss   [6, 18, 25, 53] for semantic segmentation tasks, recent   works [14, 71] start to use contrastive loss in order to regularize the segmentation embedding space and achieve good   results. Contrary to prior works that use global-pooled language features for loss computation, we focus on learning   fine-grained object correspondence at the pixel-word level.   Masked Language Modeling. Masked language modeling   (MLM) is a powerful technique for natural language processing that trains a model to restore missing or corrupted   tokens in an input text. It was introduced by BERT [13] and   has become a popular technique for pre-training language   [11, 70] and visual language [35, 39, 56] models. Recently,   it has been shown to scale excellently [3, 33] and generalize   well to various downstream tasks [3, 57]. A work closely   related to ours is MaskedVLM [37], which is a multi-modal   adaptation of MLM that jointly performs masked vision   and language modeling. It does so by reconstructing the   masked signal of one modality with the help from the another modality. Mask Grounding differs from MaskedVLM   by using extra mask signals that directly match the missing   words to ensure clear and meaningful reconstructions, so   that fine-grained visual grounding can be effectively learnt.   3. Method   In this section, we first describe our architecture overview   (Sec. 3.1). Then, we explain our proposed Mask Grounding (Sec. 3.2) auxiliary task, cross-modal alignment module   (Sec. 3.3) and alignment loss (Sec. 3.4). Finally, we give the   overall loss function Sec. 3.5 for our model.   3.1. Architecture Overview   MagNet (Mask-grounded Network) adopts a unified approach that integrates three inter-linked modules to enhance   visual-textual object correspondence and segmentation accuracy.   Mask Grounding is the first of these integrated   modules, designed to improve fine-grained visual grounding in language features.   It accomplishes this by teaching the model to predict masked textual tokens, using a   combination of visual cues, linguistic context, and segmentation information. Building upon Mask Grounding\u2019s enriched language features, Cross-modal Alignment Module   (CAM) steps in to fine-tune the bi-directional interaction   between the refined language and image features. By incorporating global contextual information from multiple image scales, CAM ensures that the multi-modal features are   in sync, addressing the granularity discrepancies between   textual descriptions and visual information. Finally, Crossmodal Alignment Loss (CAL) cohesively weaves together   pixel-to-pixel and pixel-to-text alignments. By simultaneously considering these alignments, CAL ensures that segments created by the model are not only accurate in shape   but also correctly match their referring textual descriptions.   3.2. Mask Grounding   Inspired by prior works [13, 26, 60] that have shown the   effectiveness of using masked input modeling to learn good   feature representation, we propose a novel Mask Grounding   auxiliary task to improve the learning of fine-grained visual   grounding information in language features. As shown in   Fig. 3, given an input image, its corresponding referring   expression and segmentation mask, we randomly replace   some tokens in the tokenized referring expression with a   special learnable mask token and train our model to predict   the actual tokens being masked. By successfully predicting   the identities of masked tokens, our model will acquire the   ability to understand which parts of the text correspond to   which parts of the image, thus learning fine-grained visual   grounding in the process. Specifically, to perform this auxiliary task, we first encode the segmentation mask into a mask   embedding by first extracting the center coordinates of the   mask region and passing it through a 2-layer MLP. At the   same time, we use a linear layer to project the language embedding into the same dimension as the image embedding.   Then, we employ the proposed Masked Token Predictor to   jointly process all these concatenated embeddings with attention mechanism for masked token prediction. Finally, a   26575   Piles of apples   behind the   grapes   Mask Encoder   Tokenizer   extract\u00a0   center   coordinates   mask\u00a0   tokens   Concat   Masked Token Predictor   M   Linear   LayerNorm   Linear   GELU   CAM   1   2   4   Language Encoder   1   2   3   4   Image Encoder   3   Cross Entropy Loss   0.01   0.95   0.03   0.02   oranges   grapes   apples   bananas   CAM   CAM   CAM   Image Type Embed   Language Type Embed   Positional Embed   Mask Type Embed   Add   cross-modal   alignment   module   M Learnable Masked Tokens   Add & Norm   Multi-Head   Attention   Feed Forward   Linear   Add & Norm   Figure 3. Overview of Mask Grounding. This task enriches fine-grained visual grounding in language features by guiding the model to   learn detailed textual-visual associations. To perform this task, we first use an MLP-based Mask Encoder to encode center-coordinates   of segmentation masks. Then, we randomly mask textual tokens in language inputs before extracting their features. Finally, we pass the   encoded language, image and mask features to a Transformer-based Masked Token Predictor to perform masked token prediction.   cross-entropy loss Lgrounding is used to compare the final predicted distribution with the target distribution. The largescale BERT [13] vocabulary is adopted as our word class   list, as it is generally accepted to have open-vocabulary capability. Although additional forward pass through the language encoder is required to process the masked expression, overall computational cost only increase by 4.76% as   the language encoder is very small. We believe this slight   increase in computational cost is an acceptable trade-off to   improve visual grounding in language features.   A brief mathematical formulation for Mask Grounding   can be given as follows: Let T, I, M be the input to the   language encoder, image encoder and mask encoder,    \\ textbf {O} = \\text {Langu   age   E n coder}(\\text {Ma   sk}   ( \\ textbf {T})), \\   \\ \\   textbf {P} =  \\text { ImageEncoder}(\\textb f {I}), \\\\ \\textbf {C}=\\text {MaskEncoder}(\\textbf {M}), \\\\ \\mathcal {L}_{\\text {grounding}} = \\mathcal {L}_\\text {CE}(\\textbf {y}_{\\text {gt}}, \\text {Predictor}(\\text {Concat}([\\textbf {O}, \\textbf {P}, \\textbf {C}]),   (4)   where Predictor is a BERT [13]-like encoder, M is the center coordinates of ground truth masks, ygt is the label of the   masked token and LCE is the cross entropy loss. In our experiments, we use Swin-B [47] as our image encoder, and   BERT-base [13] as our language encoder, but our approach   is not specifically bound to these encoders.   Discussion. In Tab. 3(a), we demonstrate Mask Grounding\u2019s superiority over both the standard masked language   modeling (MLM) [3, 13, 33, 57] and masked-vision language modeling (MaskedVLM) [37], highlighting our approach\u2019s effectiveness.   Our advantages over these techniques include: 1) Modality Integration: Traditional MLM   is uni-modal and lacks correspondence between referring   expressions and their matching visual objects..   While   MaskedVLM is multi-modal, Mask Grounding surpasses   it by introducing an additional masking signal that aligns   with the masked words and their matching visual objects,   enabling a more coherent reconstruction. This approach exposes word-object correspondence and allows fine-grained   visual grounding to be learnt. 2) Task Nature: MLM and   MaskedVLM serve as general pre-training tasks and require   fine-tuning for specific downstream applications. In contrast, Mask Grounding is designed as a specialized auxiliary task for RIS, enhancing fine-grained visual grounding   within language features right from the training phase. Consequently, there is no need for additional fine-tuning. 3)   Prediction Context: While MLM and MaskedVLM predict   using textual or textual-visual contexts, Mask Grounding incorporates both with additional segmentation information.   By leveraging this additional information, our model can   outperform prior methods in complex scenarios where text   and visual elements are closely intertwined. For instance,   consider the scenario illustrated in Fig. 3. When the term   \u201capples\u201d in \u201cpiles of apples behind the grapes\u201d is masked,   a model lacking precise word-object correlation might falter in predicting the appropriate term. Several other words   might yield a semantically consistent sentence, but they   would not be accurate in the given visual context.   3.3. Cross-modal Alignment Module   To further improve the performance of our model, we   also make a meaningful improvement to the popular crossmodal alignment mechanism proposed by prior work [64].   As depicted in Fig. 4, our cross-modal alignment module   (CAM) improves language-image alignment by injecting   global contextual prior into image features before performing language-image fusion. CAM first uses pooling oper26576   Concat   tanh   MLP   Pool   MLP   X-MHA   Concat   tanh   MLP   Up   Figure 4. Cross-modal Alignment Module. This module enables   bidirectional language-image interaction and addresses granularity mismatches between language and image features, thereby   enhancing segmentation accuracy for RIS. X-MHA denotes bidirectional cross-modal multi-head attention. Pi and Pi+1 denote   input and output image features, whereas Oi and Oi+1 denote input and output language features. Up denotes upsampling.   ations with different window sizes to generate K feature   maps of different pyramid scales. Then, each of these feature maps passes through a 3-layer MLP to better extract   global information, before cross-attending with the opposite   modality. After that, all the output features are upsampled to   the original feature map via bilinear interpolation and concatenated along the channel dimension. A 2-layer MLP is   subsequently used to to reduce the channel dimension of   this concatenated feature back to the original one. To prevent the multi-modal signal from overwhelming the original   signal, a gate with Tanh nonlinearity is used to modulate the   final output. Finally, this post-gate feature is added back to   the input feature before being passed to the next stage of the   image or language encoder. We split language encoder into   4 stages with an equal number of layers and add CAM to   the end of every stage of image and language encoder.   Mathematically, CAM can be represented as follows:   Let Ti and Ii be the text/image input to each stage of the   language and image encoder. At each stage,    \\ t extbf {O}_i = \\tex t { LanguageStage}(   \\te   xt   b f  {T}_i),\\, \\text   bf    {P   }_i = \\t   ext { I mageStage} (\\   tex   tbf    {I}_i ) , \\\\ \\text   bf {P} _i^k  =    \\text    {ML   P}_k(\\ t ext {Pool}_k(   \\textb f {P }_i))   , \\\\ \\te   xtb   f {O } ^K _ {i,p2t}, \\textbf {   P}^k   _{i, t 2p }  = \\text {X-MHA}_k(\\textbf {O}_i, \\textbf {P}^k_n), \\\\ \\textbf {O}_{i,p2t} = \\text {Concat}([\\textbf {O}^i_{i,p2t},...,\\textbf {O}^N_{i,p2t}], \\\\ \\textbf {P}_{i,t2p} = \\text {Concat}([\\text {Up}(\\textbf {P}^1_{i,t2p},...,\\text {Up}(\\textbf {P}^N_{i,t2p})], \\\\ \\textbf {O}_{i+1} = \\textbf {O}_i + \\text {tanh}(\\text {MLP}(\\textbf {O}_{i,p2t})), \\\\ \\textbf {P}_{i+1} = \\textbf {P}_i + \\text {tanh}(\\text {MLP}(\\textbf {P}_{i,t2p})),   (11)   where Up denotes upsampling and X-MHA [40] denotes bidirectional cross-modal multi-head attention.   CAM enhances cross-modal alignment by enabling bidirectional language-image interaction, which stands in   contrast to the widely-used one-way language to image alignment module proposed by LAVT [64].   Moreover, CAM adopts a pyramid pooling technique to utilize   multi-scale average-pooled image features. This technique   adeptly resolves the granularity mismatch issue by capturing image features at multiple scales, allowing our network   to handle the varied levels of detail present in language descriptions. This is particularly beneficial for RIS, where the   model must accurately interpret and segment according to a   diverse range of descriptive queries.   3.4. Cross-modal Alignment Loss   On top of that, similar to previous works [61, 71], we also   use cross-modal alignment loss to explicitly align language   and image features. Our cross-modal alignment loss (CAL)   is holistic and consider both pixel-to-pixel (LP2P) and pixelto-text (LP2T) consistency.   Mathematically, CAL is computed as follows: Given   language feature T   \u2208   RM\u00d7D produced by the language encoder and final pixel decoder mask feature I \u2208   RCL\u00d7HL\u00d7WL with |P| positive pixel features, |N| negative   pixel features, let I+   i be the ith pixel feature in the positive   set P, I\u2212   j be the jth pixel feature in the background set N   and Tk be the kth language token, then    \\ma t hcal  {L_{   \\tex   t {C A L }   }}    = \\   m   a   thc   a l {   L}_{\\t   ext    {P2   P}} + \\ mat h   c   al    { L}_   { \\te   x t     {P2   T} }       \\   \\ \\   b egi   n {ali   gne   d } \\   mathca l  {L}   _   {\\t   e xt    { P2P }   } &=    -\\f r a c    {1   }{|   \\   m   ath   c al {P}|}    \\s   u m _i^{|\\ m ath c   a   l {   P }|}    \\fr ac {e^{\\textbf {I}^+_i \\cdot \\textbf {I}_\\text {avg}^+/\\tau _1}}{e^{\\textbf {I}^+_i \\cdot \\textbf {I}_\\text {avg}^+/\\tau _1} + \\sum _j^{|\\mathcal {N}|} e^{\\textbf {I}^+_i \\cdot \\textbf {I}^-_j / \\tau _1}} \\\\ &+ -\\frac {1}{|\\mathcal {N}|} \\sum _j^{|\\mathcal {N}|} \\frac {e^{\\textbf {I}^-_j \\cdot \\textbf {I}_\\text {avg}^- / \\tau _1}}{e^{\\textbf {I}^-_j \\cdot \\textbf {I}_\\text {avg}^-/\\tau _1} + \\sum _i^{|\\mathcal {P}|} e^{\\textbf {I}^-_j \\cdot \\textbf {I}^+_i/\\tau _1}}, \\end {aligned} \\\\ \\begin {aligned} \\mathcal {L}_{\\text {P2T}} &= -\\frac {1}{|\\mathcal {P}|} \\sum _i^{|\\mathcal {P}|} \\frac {e^{\\textbf {I}^+_i \\cdot \\textbf {T}_\\text {avg}/\\tau _2}}{e^{\\textbf {I}^+_i \\cdot \\textbf {T}_\\text {avg}/\\tau _2} + \\sum _j^{|\\mathcal {N}|} e^{\\textbf {I}^+_i \\cdot \\textbf {I}^-_j/\\tau _2}}, \\end {aligned}   (14)   where I+   avg =   1   |P|   P|P|   i   I+   i and I\u2212   avg =   1   |N |   P|N |   j   I\u2212   j are   the average pooled positive and negative pixel features,   Tavg = proj( 1   M   PM   m Tk) is the average pooled and linearly   projected word feature and \u03c41, \u03c42 are hyper-parameters that   affect the sharpness of the probability distribution. Note   that all language and image features are L2-normalized before any dot product computation, but not explicitly shown   in the equations above for brevity.   CAL differs from alignment losses used in prior works   [61, 71] by holistically integrating both pixel-to-pixel and   pixel-to-text alignments within a single cohesive system.   Precise pixel-to-pixel alignment ensures that segmentation   outputs have accurate shapes and boundaries, whereas precise pixel-to-text alignment enables our model to correctly   associate textual descriptions with their matching image regions. This dual alignment mechanism allows our model   to effectively parse and interpret the nuanced interplay between image details and language cues, leading to more accurate and contextually relevant segmentation outputs.   26577   Method   Backbone   RefCOCO (easy)   RefCOCO+ (medium)   G-Ref (hard)   val   test A   test B   val   test A   test B   val (U)   test (U)   val (G)   VLT [14]   Darknet-53   65.65   68.29   62.73   55.50   59.20   49.36   52.99   56.65   49.76   ReSTR [34]   ViT-B-16   67.22   69.30   64.45   55.78   60.44   48.27   54.48   CRIS [61]   ResNet-101   70.47   73.18   66.10   62.27   68.08   53.68   59.87   60.36   LAVT [64]   Swin-B   72.73   75.82   68.79   62.14   68.38   55.10   61.24   62.09   60.50   Single   VPD [72]   Swin-B   73.46   63.93   63.12   Dataset   CoupAlign [71]   Swin-B   74.70   77.76   70.58   62.92   68.34   56.69   62.84   62.22   PVD [10]   Swin-B   74.82   77.11   69.52   63.38   68.60   56.92   63.13   63.62   61.33   SADLR [65]   Swin-B   74.24   76.25   70.06   64.28   69.09   55.19   63.60   63.56   61.16   MCRES [62]   Swin-B   74.92   76.98   70.84   64.32   69.68   56.64   63.51   64.90   61.63   ReLA [45]   Swin-B   73.82   76.48   70.18   66.04   71.02   57.65   65.00   65.97   62.70   MagNet (Ours)   Swin-B   75.24   78.24   71.05   66.16   71.32   58.14   65.36   66.03   63.13   SEEM\u2020 [75]   Focal-T   65.7   Multiple / Extra   LISA-7B\u2020 [38]   SAM-H   74.1   76.5   71.1   62.4   67.4   56.5   66.4   68.5   Datasets   PolyFormer\u2020 [46]   Swin-B   74.82   76.64   71.06   67.64   72.89   59.33   67.76   69.05   MagNet\u2021 (Ours)   Swin-B   76.55   78.27   72.15   68.10   73.64   61.81   67.79   69.29   Table 1. Comparison with SOTA methods using the oIoU metric. Single dataset refers to strictly following the predefined train/test splits   of the original RefCOCO, RefCOCO+ and G-Ref datasets. Multiple datasets refers to combining the train splits from these 3 datasets with   test images removed to prevent data leakage. Extra datasets refers to using additional data beyond RefCOCO, RefCOCO+ and G-Ref. \u2020   indicates models that use extra datasets. \u2021 indicates that our model only uses multiple datasets. Bold indicates best.   3.5. Loss Function   Our loss function is a weighted combination of the following 4 different losses:     \\begin { g athered} \\m   athcal { L }g= \\lambda _{\\text {BCE}} \\mathcal {L}_{\\text {BCE}} + \\lambda _{\\text {Dice}} \\mathcal {L}_{\\text {Dice}} + \\\\ \\lambda _{\\text {CAL}} \\mathcal {L}_{\\text {CAL}} + \\lambda _{\\text {grounding}} \\mathcal {L}_{\\text {grounding}}, \\end {gathered} g   (15)   with \u03bbBCE = 2.0, \u03bbDice = 2.0, \u03bbCAL = 0.5, and \u03bbgrounding =   1.0 for all our experiments.   4. Experiments   In this section, we first describe the datasets and evaluation metrics (Sec. 4.1). Then, we compare our method with   SOTA RIS methods (Sec. 4.2). Finally, we show some visualization results (Sec. 4.3) and ablate our proposed method   (Sec. 4.4). Due to space limitation, exact implementation   details of our method are relegated to the Supplementary.   4.1. Datasets and Evaluation Metrics   We evaluate our proposed method on three standard benchmark datasets: RefCOCO [67], RefCOCO+ [67], and GRef [49, 50] using three commonly used metrics: overall intersection-over-union (oIoU), mean intersection-overunion (mIoU), and precision values at 0.5, 0.7, and 0.9   IoU threshold levels (P@X). More details regarding these   datasets and metrics can be found in the Supplementary.   4.2. Main Results   In Tab. 1, we evaluate MagNet against other SOTA methods on RefCOCO [67], RefCOCO+,[67] and G-Ref [49, 50]   datasets using the oIoU metric. Under the single dataset   setting, MagNet is the first method that consistently outperforms all previous methods on all evaluation subsets of   these datasets. Previous methods usually overfit to one of   these benchmarks and perform worse in others. Remarkably, on RefCOCO, MagNet outperforms the very recent   SOTA RIS method ReLA [45] by considerable margins of   1.42, 1.76, and 0.87 points on the validation, testA, and   testB subsets, respectively.   To have a more comprehensive evaluation of our method, we also assess MagNet using   other metrics and display the results on Tab. 2. As shown,   MagNet has much better mIoU and P@X performance than   all previous SOTA methods. In particular, our method surpasses previous SOTA methods by 0.32 points on the oIoU   metric, 0.91 points on the mIoU metric and 0.89, 1.25, 1.41   points on the precision metric at 0.5, 0.7 and 0.9 IoU threshold levels. Under the multiple / extra datasets setting, our   method also surpasses recent SOTA methods [38, 46] that   use large language models [57] or has much slower inference speed, by large margins of up to 2.48 points.   Method   RefCOCO val   oIoU   mIoU   P@0.5   P@0.7   P@0.9   LAVT [64]   72.73   74.46   84.46   75.28   34.30   ReLA [45]   73.82   75.61   85.82   77.71   34.99   CoupAlign [71]   74.70   75.49   86.40   77.59   32.40   MCRES [62]   74.92   86.23   77.25   35.61   SADLR [65]   74.24   76.52   86.90   78.76   37.36   MagNet (ours)   75.24   77.43   87.79   80.01   38.77   Table 2. Comparison with SOTA methods on RefCOCO val using   oIoU, mIoU and P@X (Precision at IoU threshold value X). Bold   indicates best and underline indicates second best.   26578   fourth remote from left   2nd remote from right   4th remote from left   LAVT   MagNet   (Ours)   diving man   officer on the right   player 4   left half of right sandwich   left half of the burrito   plane leading the pack   elephant with baby under it   phone reflection   LAVT   running man   (a) Cases involving complex visual object relationships   (b) Cases involving expressions used under uncommon context   (c) General cases where strong visual grounding in text features are important   MagNet   (Ours)   Figure 5. Visualization of MagNet\u2019s predictions. Compared to one of the state-of-the-art method, LAVT, our method performs much better   in various complex scenerios, suggesting its impressive capability to reason about various complex visual-object relationships.   4.3. Visualizations   In Fig. 5, we show some representative mask predictions of   MagNet and LAVT [64] on RefCOCO validation set. Here,   we only compare with LAVT because it is a SOTA method   that provides reproducible codes and pre-trained weights.   MagNet outperforms LAVT in scenes that involve complex   visual-object relationships, contain uncommon expressions   or require strong visual grounding information.   Impressively, MagNet demonstrates ability to grasp complex visual cues such as reflection, leading and running.   4.4. Ablation Studies   In this section, we investigate the effectiveness of all core   components of our model. For experimental efficiency, we   use a shorter training schedule of 10 epochs and smaller input images of 224 \u00d7 224, causing the results to be different   from Tab. 1. Other experimental settings are kept the same.   We reproduce the numbers for LAVT [64], ReLA [45] and   CRIS [61] using their official codes. All ablations are performed on validation splits of RefCOCO and RefCOCO+.   4.4.1   Ablating Different Aspects of Mask Grounding   Effect on RIS Performance.   In Tab. 3(a), we show   that both masked language modeling (MLM) and masked   vision-language modeling (MaskedVLM) fail to deliver   meaningful performance gains. In contrast, our proposed   Mask Grounding improves over MLM by encouraging our   model to learn fine-grained visual grounding in language   features through usage of additional visual and segmentation information. When added to LAVT, Mask Grounding   yields a significant performance gains of 1.44 points on RefCOCO and 1.28 points on RefCOCO+.   -0.331   -0.859   (a) Average cosine similarity between   positive language-image pairs on RefCOCO   LAVT   + MG   (b) Average cosine similarity between   negative language-image pairs on RefCOCO   -0.528   0.567   0.729   LAVT   + MG   +0.162   Figure 6. Mask Grounding Improves Language-Image Alignment.   Effect on Language-Image Alignment. Next, we check   if Mask Grounding can help to improve language-image   alignment in RIS models. Effective alignment is indicated   by high feature similarity for matching language-image   pairs and low similarity for non-matching pairs. To verify this property, we compare the average normalized cosine   similarity for all language-image pairs in the RefCOCO validation dataset before and after Mask Grounding is added to   LAVT. Since language and image features have different dimensions, we first train a linear layer with contrastive loss   to project language features to the same dimension as image   features before computing this metric. This method is similar to linear-probing widely used in self-supervised learning   [7, 25]. All other weights are frozen in the process. As illustrated in Fig. 6, Mask Grounding can indeed significantly   improve language-image alignment in existing RIS models.   Mask Encoder Design. In Tab. 3(b), we compare passing   two different types of mask input to the mask encoder in   Mask Grounding. As shown, using center coordinates of   masked region gives slightly better performance.   26579   Model   RefCOCO   RefCOCO+   Baseline   67.08   55.98   Baseline + MLM   67.31 (+0.23)   55.69 (-0.29)   Baseline + MaskedVLM   67.33 (+0.25)   56.13 (+0.15)   Baseline + MG   68.52 (+1.44)   57.26 (+1.28)   (a) Effectiveness of Mask Grounding over   masked language modeling (MLM) and   masked vision-language modeling   (MaskedVLM).   Type   RefCOCO   RefCOCO+   None   67.08   55.98   Average   68.19 (+1.11)   56.98 (+1.00)   Center   68.52 (+1.44)   57.26 (+1.28)   (b) Comparing different mask encoder input in   Mask Grounding. Center denotes center coordinates of masked region. Average denotes   average visual features within masked region.   MLP layers   RefCOCO   RefCOCO+   None   67.08   55.98   4   67.14 (+0.06)   57.19 (+1.21)   8   68.52 (+1.44)   57.26 (+1.28)   12   66.92 (-0.16)   55.95 (-0.03)   (c) Sensitivity of Mask Grounding\u2019s masked   token predictor to different MLP layers.   Model   RefCOCO   RefCOCO+   LAVT [64]   67.08   55.98   LAVT + MG   68.52 (+1.44)   57.26 (+1.28)   ReLA [45]   66.23   53.69   ReLA + MG   67.33 (+1.10)   55.21 (+1.52)   CRIS [61]   64.67   54.84   CRIS + MG   65.56 (+0.89)   56.23 (+1.39)   (d) Universality of Mask Grounding.   Pyramid scales   RefCOCO   RefCOCO+   None   67.08   55.98   {1}   67.18 (+0.10)   56.20 (+0.22)   {1,2}   67.44 (+0.36)   56.46 (+0.48)   {1,2,3}   67.79 (+0.71)   56.74 (+0.76)   {1,2,3,6}   68.06 (+0.98)   57.04 (+1.06)   (e) Effectiveness of language-image Crossmodal Alignment Module at different scales.   LP2P   LP2T   RefCOCO   RefCOCO+   \u2717   \u2717   67.08   55.98   \u2713   \u2717   67.63 (+0.55)   56.87 (+0.89)   \u2717   \u2713   68.27 (+1.19)   57.22 (+1.24)   \u2713   \u2713   68.44 (+1.36)   57.61 (+1.63)   (f) Effectiveness of language-image   Cross-modal Alignment Loss.   Table 3. Ablation Experiments. All experiments are run with a shorter training schedule of 10 epochs, causing the results here to be   different from the main results. Rows marked in gray indicate options that are used in the main results. MG denotes Mask Grounding.   Mask Token Predictor Design. In Tab. 3(c), we evaluate   the sensitivity of Mask Grounding\u2019s masked token predictor   to different MLP layers. We observe that a sufficiently deep   masked token predictor is important for good performance.   As shown, performance is the best when 8 layers are used.   When more layers are added, performance slightly drops,   as the model starts to overfit to the auxiliary task.   Universality of Mask Grounding. In Tab. 3(d), we show   that Mask Grounding is also compatible with other representative RIS methods. As shown, when Mask Grounding   is added, on RefCOCO and RefCOCO+, we can obtain a   performance gain of 0.89 and 1.29 points for CRIS and a   performance gain of of 1.10 and 1.52 points for ReLA.   4.4.2   Ablating Other Components of Our Method   Effectiveness of CAM. Cross-modal Alignment Module   (CAM) improves language-image alignment by injecting   global contextual prior into image features. As shown in   Tab. 3(e), when CAM is used, we can improve RefCOCO\u2019s   and RefCOCO+\u2019s oIoU by 0.98 and 1.06 points respectively. Additionally, Tab. 3(e) also shows that using more   pyramid scales is helpful in boosting performance.   Effectiveness of CAL. Cross-modal Alignment Loss   (CAL) provides additional pixel-to-pixel (LP2P) and pixelto-text (LP2T) alignment supervision to further reduce   language-image modality gap. As shown in Tab. 3(f), both   LP2P and LP2T alone can bring noticeable oIoU improvement on RefCOCO and RefCOCO+. When the both are   added together, we can surpass the baseline by 1.36 points   on RefCOCO and 1.63 points on RefCOCO+.   Compatibility of all MagNet components. As shown in   Fig. 7, all components of MagNet are highly compatible as   they progressively improves the LAVT\u2019s performance when   added incrementally. When all components are added, we   can improve LAVT by 3.15 points on RefCOCO+.   56   57   58   59   58.33   57.61   56.87   55.98   59.13   LAVT   +CAM   +MG   +0.89   +0.74 (+1.63)   +0.72 (+2.35)   +0.80 (+3.15)   RefCOCO+ oIoU   +   +   Figure 7. Compatibility of MagNet components.   5. Conclusion   In this paper, we present Mask Grounding, an novel method   designed to enhance RIS by teaching our model to predict   randomly masked textual tokens based on their surrounding textual, visual and segmentation information. This task   requires our model to learn fine-grained visual-textual object correspondence, thus learning visual grounding in the   process. When plugged into existing RIS algorithms, Mask   Grounding can improve their performance consistently. To   holistically address the modality gap, we also design a   cross-modal alignment loss and an accompanying alignment module. When all these techniques are used together,   our newly proposed MagNet achieves SOTA performance   in all RIS benchmarks. We believe that Mask Grounding   can also be used in other multi-modal dense prediction tasks   and will explore that in future work.   Acknowledgements. We thank Wan Ding and Kai Ding for   their kind support in this project. This work is supported   in part by the National Key R&D Program of China under   Grant 2021ZD0140407, the National Natural Science Foundation of China under Grants 62321005 and 62276150, and   the THU-Bosch JCML.   26580", "conf": "CVPR", "year": "2024", "index": 562}, {"title": "Proceedings of NAACL-HLT 2018, pages 408\u2013418   New Orleans, Louisiana, June 1 - 6, 2018. c\u20dd2018 Association for Computational Linguistics   Learning Visually Grounded Sentence Representations   Douwe Kiela   Facebook AI Research   dkiela@fb.com   Alexis Conneau   Facebook AI Research   aconneau@fb.com   Allan Jabri1   UC Berkeley   ajabri@berkeley.edu   Maximilian Nickel   Facebook AI Research   maxn@fb.com", "abstract": "We investigate grounded sentence representations, where we train a sentence encoder to   predict the image features of a given caption\u2014   i.e., we try to \u201cimagine\u201d how a sentence would   be depicted visually\u2014and use the resultant features as sentence representations. We examine   the quality of the learned representations on   a variety of standard sentence representation   quality benchmarks, showing improved performance for grounded models over non-grounded   ones. In addition, we thoroughly analyze the   extent to which grounding contributes to improved performance, and show that the system   also learns improved word embeddings.   1", "content": "Following the word embedding upheaval of the   past few years, one of NLP\u2019s next big challenges   has become the hunt for universal sentence representations: generic representations of sentence   meaning that can be \u201cplugged into\u201d any kind of system or pipeline. Examples include Paragraph2Vec   (Le and Mikolov, 2014), C-Phrase (Pham et al.,   2015), SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016a).   These representations   tend to be learned from large corpora in an unsupervised setting, much like word embeddings, and   e\ufb00ectively \u201ctransferred\u201d to the task at hand.   Purely text-based semantic models, which represent word meaning as a distribution over other   words (Harris, 1954; Turney and Pantel, 2010;   Clark, 2015), su\ufb00er from the grounding problem   (Harnad, 1990). It has been shown that grounding leads to improved performance on a variety   of word-level tasks (Baroni, 2016; Kiela, 2017).   Unsupervised sentence representation models are   often doubly exposed to the grounding problem, especially if they represent sentence mean1Work done while at Facebook AI Research.   ings as a distribution over other sentences, as in   SkipThought (Kiros et al., 2015).   Here, we examine whether grounding also leads   to improved sentence representations. In short, the   grounding problem is characterized by the lack of   an association between symbols and external information. We address this problem by aligning text   with paired visual data and hypothesize that sentence representations can be enriched with external   information\u2014i.e., grounded\u2014by forcing them to   capture visual semantics. We investigate the performance of these representations and the e\ufb00ect of   grounding on a variety of semantic benchmarks.   There has been much recent interest in generating actual images from text (Goodfellow et al.,   2014; van den Oord et al., 2016; Mansimov et al.,   2016). Our method takes a slightly di\ufb00erent approach: instead of predicting actual images, we   train a deep recurrent neural network to predict   the latent feature representation of images. That   is, we are speci\ufb01cally interested in the semantic   content of visual representations and how useful   that information is for learning sentence representations. One can think of this as trying to imagine, or form a \u201cmental picture\u201d, of a sentence\u2019s   meaning (Chrupa\u0142a et al., 2015). Much like a sentence\u2019s meaning in classical semantics is given by   its model-theoretic ground truth (Tarski, 1944), our   ground truth is provided by images.   Grounding is likely to be more useful for concrete words and sentences: a sentence such as   \u201cdemocracy is a political system\u201d does not yield   any coherent mental picture. In order to accommodate the fact that much of language is abstract, we   take sentence representations obtained using textonly data (which are better for representing abstract   meaning) and combine them with the grounded   representations that our system learns (which are   good for representing concrete meaning), leading   to multi-modal sentence representations.   408   In what follows, we introduce a system for   grounding sentence representations by learning to   predict visual content. Although it is not the primary aim of this work, it is important to \ufb01rst examine how well this system achieves what it is trained   to do, by evaluating on the COCO5K image and   caption retrieval task. We then analyze the performance of grounded representations on a variety   of sentence-level semantic transfer tasks, showing   that grounding increases performance over textonly representations. We then investigate an important open question in multi-modal semantics:   to what extent are improvements in semantic performance due to grounding, rather than to having   more data or data from a di\ufb00erent distribution? In   the remainder, we analyze the role that concreteness plays in representation quality and show that   our system learns grounded word embedding projections that outperform non-grounded ones. To   the best of our knowledge, this is the \ufb01rst work to   comprehensively study grounding for distributed   sentence representations on such a wide set of semantic benchmark tasks.   2   Related work   Sentence representations   Although there appears to be a consensus with regard to the methodology for learning word representations, this is   much more of an open problem for sentence representations.   Recent work has ranged from trying to learn to compose word embeddings (Le and   Mikolov, 2014; Pham et al., 2015; Wieting et al.,   2016; Arora et al., 2017), to neural architectures for   predicting the previous and next sentences (Kiros   et al., 2015) or learning representations via largescale supervised tasks (Conneau et al., 2017). In   particular, SkipThought (Kiros et al., 2015) led to   an increased interest in learning sentence representations. Hill et al. (2016a) compare a wide selection of unsupervised and supervised methods,   including a basic caption prediction system that   is similar to ours. That study \ufb01nds that \u201cdi\ufb00erent learning methods are preferable for di\ufb00erent   intended applications\u201d, i.e., that the matter of optimal universal sentence representations is as of yet   far from decided.   InferSent (Conneau et al., 2017) recently showed   that supervised sentence representations can be of   very high quality. Here, we learn grounded sentence representations in a supervised setting, combine them with standard unsupervised sentence   representations, and show how grounding can help   for a variety of sentence-level tasks.   Multi-modal semantics   Language grounding in   semantics has been motivated by evidence that   human meaning representations are grounded in   perceptual experience (Jones et al., 1991; Perfetti,   1998; Andrews et al., 2009; Riordan and Jones,   2011).   That is, despite ample evidence of humans representing meaning with respect to an external environment and sensorimotor experience   (Barsalou, 2008; Louwerse, 2008), standard semantic models rely solely on textual data. This   gives rise to an in\ufb01nite regress in text-only semantic representations, i.e., words are de\ufb01ned in terms   of other words, ad in\ufb01nitum.   The \ufb01eld of multi-modal semantics, which aims   to address this issue by enriching textual representations with information from other modalities,   has mostly been concerned with word representations (Bruni et al., 2014; Baroni, 2016; Kiela, 2017,   and", "conf": "NAACL", "year": "2018", "index": 46}, {"title": "Grounded Language-Image Pre-training   Liunian Harold Li\u22171\u2020, Pengchuan Zhang\u22172\u2660, Haotian Zhang\u22173\u2020, Jianwei Yang2, Chunyuan Li2, Yiwu Zhong4\u2020,   Lijuan Wang5, Lu Yuan5, Lei Zhang6, Jenq-Neng Hwang3, Kai-Wei Chang1, Jianfeng Gao2   1UCLA, 2Microsoft Research, 3University of Washington,   4University of Wisconsin-Madison, 5Microsoft Cloud and AI, 6International Digital Economy Academy", "abstract": "This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, languageaware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training.   The unification brings two benefits:   1) it allows GLIP   to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model;   2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the   learned representations semantic-rich. In our experiments,   we pre-train GLIP on 27M grounding data, including 3M   human-annotated and 24M web-crawled image-text pairs.   The learned representations demonstrate strong zero-shot   and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS   (without seeing any images in COCO during pre-training),   GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines.1   2) After fine-tuned on   COCO, GLIP achieves 60.8 AP on val and 61.5 AP on   test-dev, surpassing prior SoTA. 3) When transferred to 13   downstream object detection tasks, a 1-shot GLIP rivals   with a fully-supervised Dynamic Head. Code will be released at https://github.com/microsoft/GLIP.   1.", "content": "Visual recognition models are typically trained to predict   a fixed set of pre-determined object categories, which limits   their usability in real-world applications since additional labeled data are needed to generalize to new visual concepts   and domains. CLIP [40] shows that image-level visual representations can be learned effectively on large amounts of   raw image-text pairs. Because the paired texts contain a   boarder set of visual concepts than any pre-defined concept   \u2217The three authors contributed equally. \u2660Corresponding author.   \u2020Work done when interning at Microsoft Research.   1Supervised baselines on COCO object detection: Faster-RCNN w/   ResNet50 (40.2) or ResNet101 (42.0), and DyHead w/ Swin-Tiny (49.7).   pool, the pre-trained CLIP model is so semantically rich that   it can be easily transferred to downstream image classification and text-image retrieval tasks in zero-shot settings.   However, to gain fine-grained understanding of images, as   required by many tasks, such as object detection [31, 44],   segmentation [6,35], human pose estimation [49,56], scene   understanding [14, 25, 57], action recognition [17], visionlanguage understanding [7,27\u201330,36,48,50,63,65], objectlevel visual representations are highly desired.   In this paper, we show that phrase grounding, which is a   task of identifying the fine-grained correspondence between   phrases in a sentence and objects (or regions) in an image, is   an effective and scalable pre-training task to learn an objectlevel, language-aware, and semantic-rich visual representation, and propose Grounded Language-Image Pre-training   (GLIP). Our approach unifies the phrase grounding and object detection tasks in that object detection can be cast as   context-free phrase grounding while phrase grounding can   be viewed as a contextualized object detection task. We   highlight our key contributions as follows.   Unifying detection and grounding by reformulating   object detection as phrase grounding. The reformulation   changes the input of a detection model: it takes as input not   only an image but also a text prompt that describes all the   candidate categories in the detection task2. For example,   the text prompt for COCO object detection [32] is a text   string that consists of 80 phrases, i.e., the 80 COCO object   class names, joined by \u201c. \u201d, as shown in Figure 1 (Left).   Any object detection model can be converted to a grounding model by replacing the object classification logits in its   box classifier with the word-region alignment scores, i.e.,   dot product of the region (or box) visual features and the   token (or phrase) language features, as shown in Figure 1   (Right). The language features are computed using a language model, which gives the new detection (or grounding) model a dual-encoder structure. Different from CLIP   that fuses vision and language only at the last dot product   layer [40], we show that deep cross-modality fusion applied   2Different from typical phrase grounding tasks, phrases in the text   prompt for an object detection task may not be present in the image.   10965   ...   DyHead    Module   Fusion   BERT   Layer   A woman holds a blow dryer,   wearing protective goggles   A   wom   an   ...   prote   ctive   gogg   le   Pers   on   Bicyc   le   ...   Hair   #dry   er.   Person. Bicycle \u2026 Hairdryer.   Deep Fusion   Region Features   Word-Region    Alignment Score   Visual    Encoder   Text   Encoder   DyHead    Module   Fusion   BERT   Layer   ...   ...   O1   O2   O3   ON   Prompt   P1   P2   PM-1   PM   Alignment   Loss   Localization    Loss   Word   Features   \ud835\udc42\" # \ud835\udc43\" \ud835\udc42\" # \ud835\udc43%   \u2026   \ud835\udc42' # \ud835\udc43(   \ud835\udc42\" # \ud835\udc43()\"   \ud835\udc42' # \ud835\udc43\" \ud835\udc42' # \ud835\udc43%   \ud835\udc42\" # \ud835\udc43(   \u2026   \ud835\udc42% # \ud835\udc43\"   \ud835\udc42* # \ud835\udc43\"   \ud835\udc42% # \ud835\udc43(   \u2026   \ud835\udc42* # \ud835\udc43(   \u2026   \ud835\udc42+%,   \ud835\udc43,%+   \ud835\udc42+%,   ,   \ud835\udc43,%+   ,   \ud835\udc43\ud835\udc42\u2026   Figure 1. A unified framework for detection and grounding. Unlike a classical object detection model   which predicts a categorical class for each detected object, we reformulate detection as a grounding task by   aligning each region/box to phrases in a text prompt. GLIP jointly trains an image encoder and a language   encoder to predict the correct pairings of regions and words. We further add the cross-modality deep   fusion to early fuse information from two modalities and to learn a language-aware visual representation.   Two syringes and a small vial    of vaccine.   playa esmeralda in holguin,    cuba. the view from the top of    the beach. beautiful caribbean    sea turquoise   Two syringes   A small vial   vaccine   the view   playa esmeralda   beautiful caribbean sea    turquoise   Two syringes   Figure 2.   Grounding predictions from GLIP. GLIP   can   locate   rare   entities,   phrases with attributes, and   even abstract words.   by GLIP, as shown in Figure 1 (Middle), is crucial to learn   high-quality language-aware visual representations and to   achieve superior transfer learning performance. The unification of detection and grounding also allows us to pre-train   using both types of data and benefits both tasks. On the   detection side, the pool of visual concepts is significantly   enriched thanks to the grounding data. On the grounding   side, detection data introduce more bounding box annotations and help train a new SoTA phrase grounding model.   Scaling up visual concepts with massive image-text   data.   Given a good grounding model (teacher), we can   augment GLIP pre-training data by automatically generating grounding boxes for massive image-text-paired data, in   which noun phrases are detected by an NLP parser [2].   Thus, we can pre-train our (student) GLIP-Large model   (GLIP-L) on 27M grounding data, including 3M humanannotated fine-grained data and 24M web-crawled imagetext pairs. For the 24M image-text pairs, there are 78.1M   high-confidence (> 0.5) phrase-box pseudo annotations,   with 58.4M unique noun phrases. We showcase two real   examples of the generated boxes in Figure 2. The teacher   model can accurately localize some arguably hard concepts, such as syringes, vaccine, beautiful caribbean sea   turquoise, and even abstract words (the view).   Training   on such semantic-rich data delivers a semantic-rich student   model. In contrast, prior work on scaling detection data   simply cannot predict concepts out of the teacher models\u2019   pre-defined vocabulary [68]. In this study, we show that   this simple strategy of scaling up grounding data is empirically effective, bringing large improvements to LVIS   and 13 downstream detection tasks, especially on rare categories (Sections 4.2 and 5). When the pre-trained GLIPL model is fine-tuned on COCO, it achieves 60.8 AP on   COCO 2017val and 61.5 on test-dev, surpassing the current   public SoTA models [9, 58] that scale up object detection   data in various approaches.   Transfer learning with GLIP: one model for all. The   grounding reformulation and semantic-rich pre-training facilitate domain transfer. GLIP can be transferred to various   tasks with few or even no additional human annotations.   When the GLIP-L model is directly evaluated on the COCO   and LVIS datasets (without seeing any images in COCO   during pre-training), it achieves 49.8 and 26.9 AP on COCO   val2017 and LVIS val, respectively, surpassing many supervised baselines. When evaluated on 13 existing object detection datasets, spanning scenarios including fine-grained   species detection, drone-view detection, and ego-centric detection, the setting which we term \u201cObject Detection in the   Wild\u201d (ODinW) (Section 5.1), GLIP exhibits excellent data   efficiency. For example, a zero-shot GLIP-L outperforms a   10-shot supervised baseline (Dynamic Head) pre-trained on   Objects365 while a 1-shot GLIP-L rivals with a fully supervised Dynamic Head. Moreover, when task-specific annotations are available, instead of tuning the whole model, one   could tune only the task-specific prompt embedding, while   keeping the model parameters unchanged. Under such a   prompt tuning setting (Section 5.2), one GLIP model can   simultaneously perform well on all downstream tasks , reducing the fine-tuning and deployment cost.   2. Related Work   Standard object detection systems are trained to localize a fixed set of object classes predefined in crowd-labeled   datasets, such as COCO [32], OpenImages (OI) [25], Objects365 [45], and Visual Genome (VG) [23], which con10966   tains no more than 2,000 object classes.   Such humanannotated data are costly to scale up. GLIP presents an   affordable solution by reformulating object detection as a   phrase grounding (word-to-region matching) problem, and   thus enables the use of grounding and massive image-textpaired data. Though our current implementation is built   upon Dynamic Head (DyHead) [9], our unified formulation can be generalized to any object detection systems   [4,5,8,9,9,31,43,44,67].   Recently, there is a trend to develop vision-and-language   approaches to visual recognition problems, where vision   models are trained with free-form language supervision.   For example, CLIP [40] and ALIGN [18] perform crossmodal contrastive learning on hundreds or thousands of millions of image-text pairs and can directly perform openvocabulary image classification. By distilling the knowledge from the CLIP/ALIGN model into a two-stage detector, ViLD [12] is proposed to advance zero-shot object detection. Alternatively, MDETR [19] trains an end-to-end   model on existing multi-modal datasets which have explicit   alignment between phrases in text and objects in image. Our   GLIP inherits the semantic-rich and language-aware property of this line of research, achieves SoTA object detection   performance and significantly improves the transferability   to downstream detection tasks.   This paper focuses on domain transfer for object detection. The goal is to build one pre-trained model that seamlessly transfers to various tasks and domains, in a zero-shot   or few-shot manner. Our setting differs from zero-shot detection [1,12,41,42,61,66], where some categories are defined as unseen/rare and not present in the training set. We   expect GLIP to perform well on rare categories (Section   4.2) but we do not explicitly exclude any categories from   our training set, because grounding data are so semantically rich that we expect them to cover many rare categories.   This resembles the setting in open-vocabulary object detection [61], which expects raw image-text data to cover many   rare categories. Beyond performance on rare categories, we   also consider the transfer cost in real-world scenarios, i.e.,   how to achieve the best performance with the least amount   of data, training budget, and deployment cost (Section 5).   3. Grounded Language Image Pre-training   Conceptually, object detection and phrase grounding   bear a great similarity. They both seek to localize objects   and align them to semantic concepts. This synergy motivates us to cast the classical object detection task into a   grounding problem and propose a unified formulation (Sec   3.1). We further propose to add deep fusion between image   and text, making the detection model language-aware and   thus a strong grounding model (Sec 3.2). With the reformulation and deep fusion, we can pre-train GLIP on scalable   and semantic-rich grounding data (Sec 3.3).   3.1. Unified Formulation   Background: object detection. A typical detection model   feeds an input image into a visual encoder EncI, with CNN   [15, 51] or Transformer [34, 60, 62] as backbone, and extracts region/box features O, as shown in Figure 1 (Bottom). Each region/box feature is fed into two prediction   heads, i.e., a box classifier C and a box regressor R, which   are trained with the classification loss Lcls and the localization loss Lloc, respectively:   L = Lcls + Lloc.   (1)   In two-stage detectors, a separate region proposal network   (RPN) with RPN loss Lrpn is used to distinguish foreground   from background and refine anchors. Since Lrpn does not   use semantic information of object classes, we merge it into   the localization loss Lloc. In one-stage detectors, localization loss Lloc may also contain the centerness loss [52].   The box classifier C is typically a simple linear layer, and   the classification loss Lcls can be written as:   O=EncI(Img), Scls =OW T , Lcls =loss(Scls; T).   (2)   Here3, O \u2208RN\u00d7d are the object/region/box features of the   input image, W \u2208Rc\u00d7d is the weight matrix of the box   classifier C, Scls \u2208RN\u00d7c are the output classification logits, T \u2208{0, 1}N\u00d7c is the target matching between regions   and classes computed from the classical many-to-1 matching [8,31,43,44] or the bipartite Hungarian match [4,9,67].   loss(S; T) is typically a cross-entropy loss for two-stage   detectors and a focal loss [31] for one-stage detectors.   Object detection as phrase grounding. Instead of classifying each region/box into c classes, we reformulate detection as a grounding task, by grounding/aligning each region   to c phrases in a text prompt (see Figure 1). How to design   a text prompt for a detection task? Given object classes   [person, bicycle, car, ..., toothbrush], one simple way is   Prompt = \u201cDetect: person, bicycle, car, ... , toothbrush\u201d,   in which each class name is a candidate phrase to be   grounded. One could design better prompts, by providing   more expressive descriptions of these classes and/or by exploiting the preference of a pre-trained language model. For   example, when the pre-trained BERT model [10] is used to   initialize our language encoder EncL, the prompt \u201cperson.   bicycle. car. ... . toothbrush\u201d works better than the more   human-friendly prompt described above. We will discuss   the prompt design in Section 5.2.   In a grounding model, we compute the alignment scores   Sground between image regions and words in the prompt:   O=EncI(Img), P =EncL(Prompt), Sground =OP \u22a4, (3)   3N is the number of region/box features, d is the visual feature hidden   dimension, c is the number of object classes, and we ignore the bias in the   box classifier for simplicity.   10967   where P \u2208RM\u00d7d is the contextual word/token features   from the language encoder and plays a similar role to the   weight matrix W in (2), as shown in Figure 1 (Right).   The grounding model, consisting of both the image encoder   EncI and the language encoder EncL, is trained end-to-end   by minimizing the loss defined in (1) & (2), with a simple   replacement of the classification logits Scls in (2) with the   region-word aligment scores Sground in (3).   However, in (2), we now have the logits Sground   \u2208   RN\u00d7M and the target T \u2208{0, 1}N\u00d7c. The number of (sub)word tokens M is always larger than the number of phrases   c in the text prompt due to four reasons: 1) some phrases   contain multiple words, e.g., \u201ctraffic light\u201d; 2) some singleword phrases are splitted into multiple (sub)-word tokens,   e.g., \u201ctoothbrush\u201d to \u201ctooth#\u201d and \u201c#brush\u201d; 3) some are   the added tokens, such as \u201cDetect:\u201d, \u201c,\u201d, special tokens in   language models, and 4) a [NoObj] token is added at the   end of the tokenized sequence. When the loss is a (focal) binary sigmoid loss (the loss we use in Section 4 &   5), we expand the original target matrix T \u2208{0, 1}N\u00d7c to   T \u2032 \u2208{0, 1}N\u00d7M by making all sub-words positive match   if a phrase is a positive match and all added tokens negative match to all image features. With this change, the   loss(Sground; T \u2032) remains the same. During inference, we   average token probabilities as the phrase probability. 4   Equivalence between detection and grounding. With the   above reformulation, we can convert any detection model   into a grounding model, and the two views, i.e., detection   and grounding, are theoretically equivalent for both training and inference.   We also verify this empirically: the   SoTA DyHead detector [9] with Swin-Tiny backbone gives   the same performance on COCO val2017 before and after   our reformulation. Please refer to the appendix for discussions. With the reformulation, a pre-trained phrase grounding model can be directly applied to any object detection   task, thanks to the free-form input of the language encoder.   This makes it possible to transfer our GLIP model to arbitrary detection tasks in a zero-shot manner.   Related work. Our grounding formulation is inspired by   MDETR [19], and our grounding loss shares the same spirit   of MDETR\u2019s fine-grained contrastive loss. We go further   than MDETR by finding an effective approach to reformulate detection as grounding and a simple unified loss for   both detection and grounding tasks. Our grounding model   also resembles models for zero-shot detection [1,12,41,42,   66]. The seminal work of Bansal et al. [1] enables a detection model to conduct zero-shot detection, by using the pretrained Glove word embedding [38] as the phrase features   P \u2208Rc\u00d7d, if written in the form of (3). Recently, phrase   4When the loss is a multi-class cross entropy (CE) loss, following   MDETR [19], all box proposals with no positive match are matched to   the [NoObj] token. The loss(S, T \u2032) becomes a multi-label multi-class CE   loss, and we sum token probabilities as phrase probability during inference.   features extracted from pre-trained deep language models   are introduced in open-vocabulary detection [61]. GLIP differs from zero-shot detection in that GLIP provides a unified   view of detection and grounding, and enables two crucial   ingredients, i.e., language-aware deep fusion and scaling up   with image-text data, as to be described next.   3.2. Language-Aware Deep Fusion   In (3), the image and text are encoded by separate encoders and only fused at the end to calculate the alignment   scores. We call such models late-fusion models. In visionlanguage literature [7,19,27,28,30,36,48,50,65], deep fusion of visual and language features is necessary to learn a   performant phrase grounding model. We introduce deep fusion between the image and language encoders, which fuses   the image and text information in the last few encoding layers, as shown in Figure 1 (Middle). Concretely, when we   use DyHead [9] as the image encoder and BERT [10] as the   text encoder, the deep-fused encoder is:   Oi   t2i, P i   i2t = X-MHA(Oi, P i),   i \u2208{0, 1, .., L \u22121}   (4)   Oi+1 = DyHeadModule(Oi+Oi   t2i),   O = OL,   (5)   P i+1 = BERTLayer(P i+P i   i2t),   P = P L,   (6)   where L is the number of DyHeadModules in DyHead [9],   BERTLayer is newly-added BERT Layers on top of the pretrained BERT, O0 denote the visual features from the vision   backbone, and P 0 denote the token features from the language backbone (BERT). The cross-modality communication is achieved by the cross-modality multi-head attention   module (X-MHA) (4), followed by the single modality fusion and updated in (5) & (6). Without added context vectors (Oi   t2i for vision modality and P i   i2t for language modality), the model is reduced to a late-fusion model.   In the cross-modality multi-head attention module (XMHA) (4), each head computes the context vectors of one   modality by attending to the other modality:   O(q) =OW (q,I), P (q) =PW (q,L), Attn=O(q)(P (q))\u22a4/   \u221a   d,   P (v) = PW (v,L), Ot2i = SoftMax(Attn)P (v)W (out,I),   O(v) = OW (v,I), Pi2t = SoftMax(Attn\u22a4)O(v)W (out,L),   where {W (symbol,I), W (symbol,L) : symbol \u2208{q, v, out}}   are trainable parameters and play similar roles to those of   query, value, and output linear layers in Multi-Head SelfAttention [53], respectively.   The deep-fused encoder (4)-(6) brings two benefits. 1) It   improves the phrase grounding performance. 2) It makes   the learned visual features language-aware, and thus the   model\u2019s prediction is conditioned on the text prompt. This   is crucial to achieve the goal of having one model serve all   downstream detection tasks (shown in Section 5.2).   10968   3.3. Pre-training with Scalable Semantic-Rich Data   Considerable efforts have been devoted to collecting detection data that are rich in semantics and large in quantity.   However, human annotations have been proven costy and   limited [13, 25]. Prior work seeks to scale up in a selftraining fashion [68].   They use a teacher (a pre-trained   detector) to predict boxes from raw images and generate   pseudo detection labels to train a student model. But the   generated data are still limited in terms of the size of the   concept pool, as the teacher can only predict labels defined   in the concept pool, constructed on the existing datasets. In   contrast, our model can be trained on both detection and,   more importantly, grounding data. We show that grounding   data can provide rich semantics to facilitate localization and   can be scaled up in a self-training fashion.   First, the gold grounding data cover a much larger vocabulary of visual concepts than existing detection data.   The largest attempts at scaling up detection vocabulary still   cover no more than 2,000 categories [13,23]. With grounding data, we expand the vocabulary to cover virtually any   concepts that appear in the grounded captions. For example, Flickr30K [39] contains 44,518 unique phrases while   VG Caption [23] contains 110,689 unique phrases, orders   of magnitude larger than the vocabulary of detection data.   We provide an empirical study in Section 4.4 to show that   0.8M gold grounding data brings a larger improvement on   detecting rare categories than additional 2M detection data.   Further, instead of scaling up detection data, we show a   promising route to obtaining semantically rich data: scaling   up grounding data. We use a simple approach inspired by   self-training. We first pre-train a teacher GLIP with gold   (human-annotated) detection and grounding data. Then we   use this teacher model to predict boxes for web-collected   image-text data, with noun phrases detected by an NLP   parser [2]. Finally, a student model is trained with both   the gold data and the generated pseudo grounding data. As   shown in Figure 2, the teacher is capable of generating accurate boxes for semantically rich entities.   Why can the student model possibly outperform the   teacher model? While discussions remain active in the selftraining literature [68], in the context of visual grounding,   we posit that the teacher model is utilizing the language   context and language generalization ability to accurately   ground concepts that it may not inherently know. For example, in Figure 2, the teacher may not directly recognize   certain concepts such as vaccine and turquoise, if they are   not present in gold data. However, the rich language context such as syntactic structures can provide strong guidance   for the teacher model to perform an \u201ceducated guess\u201d. The   model can localize vaccine if it can localize a small vail; it   can localize turquoise if it can find caribbean sea. When we   train the student model, the \u201ceducated guess\u201d of the teacher   model becomes a \u201csupervised signal\u201d, enabling the student   model to learn the concept of vaccine and turquoise.   4. Transfer to Established Benchmarks   After pre-training, GLIP can be applied to grounding and   detection tasks with ease. We show strong direct domain   transfer performance on three established benchmarks: 1)   MS-COCO object detection (COCO) [32] containing 80   common object categories; 2) LVIS [13] covering over 1000   objects categories; 3) Flickr30K [39], for phrase grounding.   We train 5 variants of GLIP (Table 1) to ablate its three core   techniques: 1) unified grounding loss; 2) language-aware   deep fusion; 3) and pre-training with both types of data.   Implementation deails are in the appendix.   GLIP-T (A) is based on a SoTA detection model, Dynamic   Head [9], with our word-region alignment loss replacing the   classification loss. It is based on the Swin-Tiny backbone   and pre-trained on O365 (Objects365 [45]), which contains   0.66M images and 365 categories. As discussed in Section   3.1, the model can be viewed as a strong classical zero-shot   detection model [1], relying purely on the language encoder   to generalize to new concepts.   GLIP-T (B) is enhanced with language-aware deep fusion   but pre-trained only on O365.   GLIP-T (C) is pre-trained on 1) O365 and 2) GoldG, 0.8M   human-annotated gold grounding data curated by MDETR   [19], including Flickr30K, VG Caption [23], and GQA [16].   We have removed COCO images from the dataset. It is designed to verify the effectiveness of gold grounding data   GLIP-T is based on the Swin-Tiny backbone and pretrained on the following data: 1) O365, 2) GoldG as in   GLIP-T (C), and 3) Cap4M, 4M image-text pairs collected   from the web with boxes generated by GLIP-T (C). We also   experiment with existing image caption datasets: CC (Conceptual Captions with 3M data) [46] and SBU (with 1M   data) [37]. We find that CC+SBU GLIP-T performs slightly   better than Cap4M GLIP-T on COCO, but slightly worse on   the other datasets. For simplicity, we report both versions   on COCO but only the Cap4M model for the other tasks.   We present the full results in the appendix.   GLIP-L is based on Swin-Large and trained with:   1)   FourODs (2.66M data), 4 detection datasets including Objects365, OpenImages [22], Visual Genome (excluding   COCO images) [23], and ImageNetBoxes [24]; 2) GoldG   as in GLIP-T (C); and 3) CC12M+SBU, 24M image-text   data collected from the web with generated boxes.   4.1. Zero-Shot and Supervised Transfer on COCO   We conduct experiments on MS-COCO to evaluate models\u2019 transfer ability to common categories. We evaluate under two settings: 1) zero-shot domain transfer, and 2) supervised transfer, where we fine-tune the pre-trained models using the standard setting. For the fine-tuning setting, we   additionally test the performance of a GLIP-L model, where   10969   Model   Backbone   Deep Fusion   Pre-Train Data   Detection   Grounding   Caption   GLIP-T (A)   Swin-T   \u2717   Objects365   GLIP-T (B)   Swin-T   \u2713   Objects365   GLIP-T (C)   Swin-T   \u2713   Objects365   GoldG   GLIP-T   Swin-T   \u2713   Objects365   GoldG   Cap4M   GLIP-L   Swin-L   \u2713   FourODs   GoldG   Cap24M   Table 1. A detailed list of GLIP model variants.   Model   Backbone   Pre-Train Data   Zero-Shot   Fine-Tune   2017val   2017val / test-dev   Faster RCNN   RN50-FPN   40.2 / Faster RCNN   RN101-FPN   42.0 / DyHead-T [9]   Swin-T   49.7 / DyHead-L [9]   Swin-L   58.4 / 58.7   DyHead-L [9]   Swin-L   O365,ImageNet21K   60.3 / 60.6   SoftTeacher [58]   Swin-L   O365,SS-COCO   60.7 / 61.3   DyHead-T   Swin-T   O365   43.6   53.3 / GLIP-T (A)   Swin-T   O365   42.9   52.9 / GLIP-T (B)   Swin-T   O365   44.9   53.8 / GLIP-T (C)   Swin-T   O365,GoldG   46.7   55.1 / GLIP-T   Swin-T   O365,GoldG,Cap4M   46.3   54.9 / GLIP-T   Swin-T   O365,GoldG,CC3M,SBU   46.6   55.2 / GLIP-L   Swin-L   FourODs,GoldG,Cap24M   49.8   60.8 / 61.0   GLIP-L   Swin-L   FourODs,GoldG+,COCO   - / 61.5   Table 2. Zero-shot domain transfer and fine-tuning on COCO.   GLIP, without seeing any images from the COCO dataset, can   achieve comparable or superior performance than prior supervised   models (e.g. GLIP-T under Zero-Shot v.s. Faster RCNN under   Fine-Tune). When fully fine-tuned on COCO, GLIP-L surpasses   the SoTA performance.   we include the COCO images in the pre-training data (the   last row). Specifically, we add the full GoldG+ grounding   data and COCO train2017 to the pre-training data. Note that   part of COCO 2017val images are present in GoldG+ [19].   Thus we only report the test-dev performance of this model.   Please see more details in the appendix.   We introduce an additional baseline:   DyHead pretrained on Objects365. We find that COCO 80 categories   are fully covered in Objects365. Thus we can evaluate DyHead trained on Objects365 in a \u201czero-shot\u201d way: during   inference, instead of predicting from 365 classes, we restrict   the model to predict only from the COCO 80 classes. We   list standard COCO detection models for reference. We also   list two state-of-the-art models pre-trained with extra data.   Results are present in Table 2.   Overall, GLIP models achieve strong zero-shot and supervised performance.   Zero-shot GLIP models rival or surpass well-established   supervised models. The best GLIP-T achieves 46.7 AP,   surpassing Faster RCNN; GLIP-L achieves 49.8 AP, surpassing DyHead-T. Under the supervised setting, the best   GLIP-T brings 5.5 AP improvement upon the standard DyHead (55.2 v.s.   49.7).   With the Swin-Large backbone,   GLIP-L surpasses the current SoTA on COCO, reaching   60.8 on 2017val and 61.5 on test-dev, without some bells   and whistles in prior SoTA [58] such as model EMA, mixup, label smoothing, or soft-NMS.   Model   Backbone   MiniVal [19]   Val v1.0   APr   APc   APf   AP   APr   APc   APf   AP   MDETR [19]   RN101   20.9   24.9   24.3   24.2   MaskRCNN [19]   RN101   26.3   34.0   33.9   33.3   Supervised-RFS [13]   RN50   12.3   24.3   32.4   25.4   GLIP-T (A)   Swin-T   14.2   13.9   23.4   18.5   6.0   8.0   19.4   12.3   GLIP-T (B)   Swin-T   13.5   12.8   22.2   17.8   4.2   7.6   18.6   11.3   GLIP-T (C)   Swin-T   17.7   19.5   31.0   24.9   7.5   11.6   26.1   16.5   GLIP-T   Swin-T   20.8   21.4   31.0   26.0   10.1   12.5   25.5   17.2   GLIP-L   Swin-L   28.2   34.3   41.5   37.3   17.1   23.3   35.4   26.9   Table 3. Zero-shot domain transfer to LVIS. While using no LVIS   data, GLIP-T/L outperforms strong supervised baselines (shown   in gray). Grounding data (both gold and self-supervised) bring   large improvements on APr.   Row   Model   Data   Val   Test   R@1   R@5   R@10   R@1   R@5   R@10   1   MDETR-RN101   GoldG+   82.5   92.9   94.9   83.4   93.5   95.3   2   MDETR-ENB5   GoldG+   83.6   93.4   95.1   84.3   93.9   95.8   3   GLIP-T   GoldG   84.0   95.1   96.8   84.4   95.3   97.0   4   O365,GoldG   84.8   94.9   96.3   85.5   95.4   96.6   5   O365,GoldG,Cap4M   85.7   95.4   96.9   85.7   95.8   97.2   6   GLIP-L   FourODs,GoldG,Cap24M   86.7   96.4   97.9   87.1   96.9   98.1   Table 4.   Phrase grounding performance on Flickr30K entities.   GLIP-L outperforms previous SoTA by 2.8 points on test R@1.   4.2. Zero-Shot Transfer on LVIS   We evaluate the model\u2019s ability to recognize diverse and   rare objects on LVIS in a zero-shot setting. We report on   MiniVal containing 5,000 images introduced in MDETR as   well as the full validation set v1.0. Please see the evaluation   details in the appendix.   Results are present in Table 3. We list three supervised   models trained on the annotated data of LVIS. GLIP exhibits strong zero-shot performance on all the categories.   GLIP-T is on par with supervised MDETR while GLIPL outperforms Supervised-RFS by a large margin.   The benefit of using grounding data is evident. Gold   grounding data brings a 4.2-point improvement on MiniVal   APr (model C v.s. model B). Adding image-text data further improves performance by 3.1 points. We conclude that   the semantic richness of grounding data significantly helps   the model recognize rare objects.   4.3. Phrase Grounding on Flickr30K Entities   We evaluate the model\u2019s ability to ground entities in natural language on Flickr30K entities [39]. Flickr30K is included in the gold grounding data so we directly evaluate   the models after pre-training as in MDETR [19]. We use the   any-box-protocol specified in MDETR. Results are present   in Table 4. We evaluate three versions of GLIP with different pre-training data. We list the performance of MDETR,   the SoTA grounding model. MDETR is trained on GoldG+,   containing 1.3M data (GoldG is a subset of GoldG+ excluding COCO images).   GLIP-T with GoldG (Row 3) achieves similar perfor10970   Row   Pre-Training Data   COCO   LVIS MiniVal   2017val   APr   APc   APf   AP   1   VG w/o COCO   26.9   4.9   10.4   23.2   16.1   2   + GoldG   29.2   7.8   14.0   24.5   18.5   3   OpenImages   29.9   12.8   12.1   17.8   14.9   4   + GoldG   33.6   15.2   16.9   24.5   20.4   5   O365   44.9   13.5   12.8   22.2   17.8   6   +GoldG   46.7   17.7   19.5   31.0   24.9   7   O365,GoldG,Cap4M   46.3   20.8   21.4   31.0   26.0   8   FourODs   46.3   15.0   22.5   32.8   26.8   Table 5. Effect of different detection data.   mance to MDETR with GoldG+, presumably due to the introduction of Swin Transformer, DyHead module, and deep   fusion. More interestingly, the addition of detection data   helps grounding (Row 4 v.s. 3), showing again the synergy   between the two tasks and the effectiveness of our unified   loss. Image-text data also helps (Row 5 v.s. 4). Lastly, scaling up (GLIP-L) can achieve 87.1 Recall@1, outperforming   the previous SoTA by 2.8 points.   4.4. Analysis   In this section, we perform ablation study by pre-training   GLIP-T on different data sources (Table 5). We answer two   research questions. First, our approach assumes the use of a   detection dataset to bootstraps the model. One natural question is whether grounding data brings improvement when   paired with different detection data. We find that adding   grounding data brings consistent improvement with different detection data (Row 1-6).   Second, we have shown the effectiveness of grounding   data for both common and rare categories. One orthogonal direction is to scale up detection data by including more   images and categories (Section 3.3). We intend to provide   an empirical comparison between scaling up detection data   and grounding data. We present GLIP trained with 4 public   detection datasets (Row 8) as an extreme attempt at scaling   up detection data with human annotations. The model is   trained with 2.66M detection data in total, with an aligned   vocabulary of over 1,500 categories. However, it still trails   behind Row 6 on COCO and APr of LVIS, where Row 6   is trained with only 0.66M detection data and 0.8M gold   grounding data. Adding image-text data further widens the   gap on LVIS APr (20.8 versus 15.0). We conclude that   grounding data are indeed more semantic-rich and a promising alternative to scaling up detection data.   5. Object Detection in the Wild   To evaluate GLIP\u2019s transferability to diverse real-world   tasks, we curate an \u201cObject Detection in the Wild\u201d   (ODinW) setting.   We choose 13 public datasets on   Roboflow5, each requiring a different localization skill.   5https : / / public . roboflow . com / object detection   Figure 3. Data efficiency of models. X-axis is the amount of taskspecific data, from zero-shot to all data. Y-axis is the average AP   across 13 datasets. GLIP exhibits great data efficiency, while each   of our proposed approach contributes to the data efficiency.   Many of the datasets are designed with a specific application purpose to mimic real-world deployment scenarios.   For example, EgoHands requires locating hands of a person; Pothole concerns detecting holes on the road; ThermalDogsandPeople involves identifying dogs and persons in infrared images. Please refer to the appendix for details.   We demonstrate that GLIP facilitates transfer to such diverse tasks. (1) GLIP brings great data efficiency, reaching   the same performance with significantly less task-specific   data than baselines (Section 5.1). (2) GLIP enables new   domain transfer strategies: when adapting to a new task,   we can simply change the text prompt and keep the entire   grounding model unchanged. This greatly reduces deployment cost because it allows one centralized model to serve   various downstream tasks (Section 5.2).   5.1. Data Efficiency   We vary the amount of task-specific annotated data, from   zero-shot (no data provided), to X-shot (providing at least   X examples per category [20, 55, 59]), to using all data   in the training set. We fine-tune the models on the provided data and use the same hyper-parameters for all models. Each dataset comes with pre-specified category names.   As GLIP is language-aware, we find it beneficial to re-write   some pre-specified names with more descriptive language   (see Section 5.2 for a discussion). We compare with the   SoTA detector DyHead-T, pre-trained on Objects365. We   test with the standard COCO-trained DyHead-T and find it   giving similar performance. For simplicity, we report only   the former. We also experiment with the scaled cosine similarity approach [54] but find it slightly underperforming the   vanilla approach so we report only the latter. Please refer to   the appendix for full statistics, including three independent   runs for X-shot experiments.   Results are shown in Figure 3.   We find that unified   grounding reformulation, deep fusion, grounding data, and   10971   Prompt: \u2026 stingray \u2026   Prompt: \u2026 stingray, which is flat    and round\u2026   Figure 4. A manual prompt tuning example from the Aquarium   dataset in ODinW. Given an expressive prompt (\u201cflat and round\u201d),   zero-shot GLIP can detect the novel entity \u201cstingray\u201d better.   model scale-up all contribute to the improved data efficiency (from the bottom red line (Dyhead-T) up to the upper purple line (GLIP-L)). As a result, GLIP exhibits transformative data efficiency. A zero-shot GLIP-T outperforms   5-shot DyHead-T while a one-shot GLIP-L is competitive   with a fully supervised DyHead-T.   5.2. One Model for All Tasks   As neural models become larger, how to reduce deployment cost has drawn an growing research interest.   Recent work on language models [47], image classification   [64], and object detection [54] has explored adapting a pretrained model to a new domain but only changing the least   amount of parameters. Such a setting is often denoted as   linear probing [21], prompt tuning [64], or efficient task   adapters [11]. The goal is to have a single model serving   various tasks, and each task adds only a few task-specific   parameters or no parameters to the pre-trained model. This   reduces training and storage cost. In this section, we evaluate models against the metric of deployment efficiency.   Manual prompt tuning.   As GLIP performs languageaware localization, i.e., the output of GLIP is heavily conditioned on the language input, we propose an efficient   way for GLIP to do task transfer: for any novel categories, the user can use expressive descriptions in the text   prompt, adding attributes or language context, to inject domain knowledge and help GLIP transfer. For example, on   the left hand side of Figure 4, the model fails to localize   all occurrences of the novel entity \u201cstingray\u201d. However, by   adding the attributes to the prompt, i.e., \u201cflat and round\u201d, the   model successfully localizes all occurrences of stringrays.   With this simple prompt change, we improve the AP50 on   stingray from 4.6 to 9.7. This resembles the prompt design   technique in GPT-3 [3] and is practically appealing, as it requires no annotated data or model re-training. Please refer   to the appendix for more details.   Prompt tuning. We further consider the setting where we   have access to task-specific training data but wish to tune   the least amount of parameters for easy deployment. For   classical detection models, Wang et al. [54] report the effectiveness of \u201clinear probing\u201d (i.e., train only the box reGLIP-L   GLIP-T   GLIP-T (A)   DyHead-T   Full-Model     Tuning   Prompt     Tuning   Linear     Probing   Figure 5. Effectiveness of prompt tuning. Solid lines are fullmodel tuning performance; dashed lines are prompt/linear probing   performance. By only tuning the prompt embeddings, GLIP-T   and GLIP-L can achieve performance close to full-model tuning,   allowing for efficient deployment.   gression and classification head). GLIP can also be \u201clinear   probed\u201d, where we only fine-tune the box head and a projection layer between the region and prompt embeddings.   Because of the language-aware deep fusion, GLIP supports   a more powerful yet still efficient transfer strategy: prompt   tuning [26, 47]. For GLIP, as each detection task has only   one language prompt (e.g., the prompt for Pothole could   be \u201cDetect pothole.\u201d for all images), we first get prompt   embeddings P 0 from the language backbone, then discard   the language backbone and only fine-tune P 0 as the taskspecific input (Section 3.2).   We evaluate the models\u2019 performance under three settings (Figure 5): linear probing, prompt tuning (only applicable for GLIP), and full-model tuning. For DyHeadT, prompt tuning is not applicable as the traditional object   detection model cannot accept language input; the gap between linear probing and full-model tuning is large. GLIP-T   (A) has no language-aware deep fusion; thus prompt tuning   and linear tuning achieve similar performance and lag significantly behind full-model tuning. However, for GLIP-T   and GLIP-L, prompt tuning almost matches the full-tuning   results, without changing any of the grounding model parameters. Interestingly, as the model and data size grow   larger, the gap between full-model tuning and prompt tuning becomes smaller (GLIP-L v.s. GLIP-T), echoing the   findings in NLP literature [33].   6. Conclusion   GLIP unifies the object detection and phrase grounding tasks to learn an object-level, language-aware, and   semantic-rich visual representation.   After pre-training,   GLIP showed promising results on zero-shot and finetuning settings on well-established benchmarks and 13   downstream tasks. We leave a detailed study of how GLIP   scales with text-image data size to future work.   10972", "conf": "CVPR", "year": "2022", "index": 270}, {"title": "Multi-Task Domain Adaptation for Language   Grounding with 3D Objects   Penglei Sun1\u22c6, Yaoxian Song2\u22c6, Xinglin Pan1 , Peijie Dong1 ,   Xiaofei Yang1,3 , Qiang Wang\f4 , Zhixu Li5 ,   Tiefeng Li2 , and Xiaowen Chu1\f   1 The Hong Kong University of Science and Technology (Guangzhou), China   {psun012,xpan413,pdong212}@connect.hkust-gz.edu.cn, xwchu@ust.hk   2 Zhejiang University, China {songyaoxian,litiefeng}@zju.edu.cn   3 Guangzhou University, China xiaofeiyang@gzhu.edu.cn   4 Harbin Institute of Technology (Shenzhen), China qiang.wang@hit.edu.cn   5 Fudan University, China zhixuli@fudan.edu.cn", "abstract": ". The existing works on object-level language grounding with   3D objects mostly focus on improving performance by utilizing the offthe-shelf pre-trained models to capture features, such as viewpoint selection or geometric priors. However, they have failed to consider exploring the cross-modal representation of language-vision alignment in the   cross-domain field. To answer this problem, we propose a novel method   called Domain Adaptation for Language Grounding (DA4LG) with 3D   objects. Specifically, the proposed DA4LG consists of a visual adapter   module with multi-task learning to realize vision-language alignment by   comprehensive multimodal feature representation. Experimental results   demonstrate that DA4LG competitively performs across visual and nonvisual language descriptions, independent of the completeness of observation. DA4LG achieves state-of-the-art performance in the single-view   setting and multi-view setting with the accuracy of 83.8% and 86.8%   respectively in the language grounding benchmark SNARE. The simulation experiments show the well-practical and generalized performance of   DA4LG compared to the existing methods. Our project is available at   https://sites.google.com/view/da4lg.   Keywords: Visual Language Grounding \u00b7 Multimodal Learning \u00b7 Domain Adaptation   1", "content": "Visual language grounding aims to identify the region or object within visual   content described by natural language [7,21]. It serves as an essential bridge for   current embodied agents to connect symbolic concepts with the perceptible real   world, enabling the evolution of an agent\u2019s intelligence from perceptual to cognitive decision-making [5,16]. For example, an agent could make a cup of coffee   \u22c6Equal Contribution.   2   P. Sun et al.   Fig. 1: The comparison between existing works and our model. Existing works focus on   (a) multi-view perception and (b) external prior. (c) We approach language grounding   from domain adaptation.   following a series of primitive instructions including detailed descriptions of the   target object from a Large Language Model (LLM) planner like GPT-4 [30].   Within the process, visual language grounding plays a key role in linking each   step instruction with the physically observed object [3,36]. Therefore, visual language grounding with 3D objects is the indispensable means of enabling an agent   to interact with the real world. A limited scale of high-quality vision-language   paired data hinders the development of visual language grounding technology,   especially 3D vision language grounding. To address this problem, existing work   attempts [9,28,38,42] to use multi-view perception or external priors, which need   extra data cost and existing domain gap caused by pre-trained feature encoders   in fixed settings. In this paper, we give an exploration from a domain adaptation   perspective for the language grounding task inspired by domain adaptation in   parameter-efficient tuning of large language model [13,14,19,22,27,39,41].   As shown in Fig. 1, existing research in language grounding mainly focuses   on two lines including the multi-view perception-enhanced method (Fig. 1 (a))   and the external prior-injected method (Fig. 1 (b)). For the former, Thomason   et al. [42] and Mitra et al. [28] propose a view-based method to improve the   prediction accuracy. Thomason et al. [42] design an auxiliary task of viewpoint   angle estimation to enhance 3D object understanding. Mitra et al. [28] design   a multi-view transformer to fuse visual features and textual features in shared   space. For the latter, Corona et al. [9] propose a voxel-informed method by   using pre-trained 3D volumetric generative model LegoFormer based on the view   of 3D objects [45]. Song et al. [38] construct explicit scene-driven multimodal   knowledge graph ManipMob-MMKG to design a knowledge-enhanced method.   In all, current methods still depend heavily on viewpoints or external priors.   In the representation learning aspect, existing works usually adopt visionlanguage feature encoders using freezing parameter mode pre-trained on a source   domain, which cannot work well for 3D language grounding tasks in multimodal alignment due to domain gap. Based on these findings, we propose a   novel multimodal domain adaptation method Domain Adaptation for Language   Multi-Task Domain Adaptation for Language Grounding with 3D Objects   3   Grounding named DA4LG, to improve 3D object-level understanding and multimodal alignment, which does not require extra visual or textual data, as shown   in Fig. 1(c). Given the language similarity between the source domain (e.g., the   WebImageText domain, where CLIP is pre-trained [31]) and the target domain   (e.g., language grounding domain), coupled with the generalization capabilities of pre-trained language models [12,20,44], DA4LG concentrates on domain   adaptation within the vision feature. Specifically, we design the pseudo-siamese   visual encoder network [17] to realize domain adaptation, in which one visual   encoder subnetwork is used to learn domain-specific 3D visual representation,   named Domain-specific Encoder, while another one is freezing to encode visual representations associated with the source domain. For training the model,   we design two auxiliary tasks with the main language grounding task (LGR) to   learn cross-modal representation. The first task is to distinguish different objects   using vision and language contrastive learning, while the second involves regenerating the input text from multi-modal fused features. We evaluate DA4LG   via the language grounding dataset SNARE proposed by Thomason et al. [42],   which discriminates natural language descriptions with 3D ShapeNet [6] objects. DA4LG achieves state-of-the-art (SOTA) performance in both single-view   and multi-view settings. Additionally, through simulation experiments, DA4LG   demonstrates generalization and robustness compared to existing models. Compared to multi-view perception-enhanced methods such as Thomason et al. [42]   and Mitral et al. [28], our DA4LG is immune to the number of viewpoints or   the selection of viewpoints. Compared to external prior-injected methods such   as Coronal et al. [9] and Song et al. [38], our proposed method requires only   adapting the visual encoder as a cloned module with limited parameter training, without the need for external prior injection. This approach presents a clear   advantage in terms of reducing the model\u2019s parameter size while simultaneously   enhancing its reliability.   Our main contributions can be summarized as follows:   1. We introduce a novel domain adaptation method (DA4LG) using multi-task   learning to reduce visual domain gap in vision-language aligned representation for language grounding with 3D objects.   2. DA4LG demonstrates SOTA performance, achieving 83.8% accuracy in a   single-view setting and 86.8% accuracy in a multi-view setting on the language grounding benchmark SNARE [42].   3. We conduct the simulation 3D object grounding experiment including a   grounding environment and a test set extended from Lang-SHAPE [37] referred to as Simulation-SNARE. The results indicate that DA4LG has   an obvious advantage in the robustness and generalization of applications   compared to existing models.   2   Related Work   Language Grounding. Language grounding with 3D can be classified into   two categories: language grounding for navigation and language grounding for   4   P. Sun et al.   Fig. 2: The framework of DA4LG. DA4LG is comprised of Encoder Layer, Embedding Reweighting Layer and Embedding Fusion Layer. Encoder Layer contains three   encoders: Language Encoder (L. Encoder), Vision Encoder, and Domain-specific Encoder. The snowflake and fire denote the freezing and unfreezing respectively.   interaction. Language grounding for navigation focuses on training an agent   to follow a set of natural language instructions to navigate towards a target   object in an environment, as discussed by Chen et al [8]. This task is relevant to   scene understanding including object localization [1,23,33,46], visual language   navigation [18,29,34,35].   Recently, research has emerged in the language grounding for interaction with   3D objects. ShapeGlot [2] investigates how linguistic expressions capture detailed   differences in the shapes of common objects based on their 3D representations.   Akula et al. [4] propose the use of neural module networks that share weights and   exploit associations between similar textual contexts, such as \"dark cube on the   left\" and \"black cube on the left\". SNARE [42] presents a challenge by requiring   the identification of referent objects that are highly similar to distractors. This   task encompasses various perspectives of objects in 3D space from ShapeNet [6],   thereby increasing the complexity of the problem in language referencing.   Current research encompasses two main approaches: the multi-view perceptionenhanced method and the external prior-injected method. The former employs   a multi-view framework with 3D objects to enhance prediction accuracy such   as MAGiC [28], LAGOR [42], among others. The latter approach integrates exMulti-Task Domain Adaptation for Language Grounding with 3D Objects   5   ternal priors to augment task performance, including VLG [9], LOCKET [38],   among others.   Domain Adaptation. Recent research has placed particular emphasis on adaptors designed for domain adaptation in the natural language processing (NLP).   Hu et al. [22] propose Low-Rank Adaptation(LoRA), a novel approach involving   freezing pre-trained model weights and introducing trainable rank decomposition matrices as adapter into each layer of the Transformer architecture. Effective   adaptation strategies based on this are introduced to bridge the domain gap in   NLP tasks [13,14,19,22,27,39,41]. Inspired by the existing research, we incorporate domain adapter into multimodal tasks and explore its utility in adapting   language grounding with 3D objects, extending the application of it beyond   traditional NLP tasks.   3   Proposed Method   Domain Adaptation. Given a pre-trained network model Fs(\u00b7) on a source   task Ts (e.g., CLIP pre-trained model [31]) in the source domain Ds (e.g., the   WebImageText domain where CLIP is pre-trained on [31]), and a set of training   examples with associated labels in the target domain Dt for target task Tt (i.e.   the language grounding task). Our goal is to create an adaption strategy to   promote the performance of target predictive function Ft(\u00b7) in Dt for Tt through   leveraging the Fs(\u00b7) in Ds and Ts [10].   Task Definition. In the language grounding task with a 3D object, given a   language description l as input, our objective is to identify the best matching   object \u02c6o in a set of candidate objects O = {oi | i \u2208{1, 2}} [42]:     \\ sma ll    \\lab   el {eq:definition} \\scalebox {1.0}{$ \\begin {aligned} \\hat {o} = \\mathop {\\arg \\max }\\limits _{{o_i} \\in \\mathcal {O}} \\; p(o_i | l), \\end {aligned} $}    (1)   where p(oi|l) denotes the conditional probability of the target object given the   language description. We expect our output \u02c6o to closely align with the ground   truth o\u2217as much as possible.   3.1   Domain Adaptation for Language Grounding   Overall of Network Structure.   As shown in Fig. 2 (a), DA4LG comprises   three encoders: a Vision Encoder (Evision(\u00b7)), a Language Encoder (Elang(\u00b7)),   and a Domain-specific Encoder (Edomain(\u00b7)). Additionally, it includes an Embedding Reweighting Layer (RW(\u00b7)) and an Embedding Fusion Layer (MLP(\u00b7)).   Therefore Eq. 1 can be rewritten as:        \\ s calebox {1.0   } {   $  \\begin {alig   n e d } f^v_j &   = E_{ v ision}(v _   j ),  \\   \\  f ^d_   j & = E _{d   omai   n}(v_j), \\\\ f^l &= E_{lang}(l), \\\\ Score &= MLP(RW(f^d_j), f^v_j, f^l), \\\\ \\hat {o} &= \\mathop {\\arg \\max } \\limits _{ {o_i} \\in \\mathcal {O} } \\; Score_{o_i}, \\end {aligned} $}    (2)   6   P. Sun et al.   where the object o is projected into a set of images V = {v1, ..., vj} from discrete   viewpoints. Evision and Elang are based on the pre-trained encoder in the freezing   state. We compute the Scoreoi to determine the output label \u02c6o.   Domain-specific Encoder.   To reduce the domain gap between Ds and Dt,   we design a Domain Vision Transformer named Domain-specific Encoder to   encode the V, as shown in Fig. 2 (c). Domain-specific Encoder is pre-trained on   Ds (e.g., the WebImageText domain [31]). Compared to vanilla Vision Transformer [15], we incorporate low-rank matrices W A   \u2217, W B   \u2217as domain adapters in   the Wq, Wk, Wv of multi-head attention layer MHA. Different from the adapters   employed for parameter-efficient tuning in NLP, the domain adapters in DA4LG   are designed to capture the domain-specific representation. In Domain-specific   Encoder, all other parameters are freezing except the W A   \u2217, W B   \u2217while training.   vj is input into the MHA to build domain features f d   j ,     \\s m al l  \\   s c a l   e b ox    { 1 .0 } {$  \\ b   e g i n    { ali   g n ed }  Q  & =    W _ q    \\ cdo   t     v _ j + W^ {A } _q \\cdot W^{B}_q \\cdot v_j ,\\\\ K &= W_k \\cdot v_j + W^{A}_k \\cdot W^{B}_k \\cdot v_j ,\\\\ V &= W_v \\cdot v_j + W^{A}_v \\cdot W^{B}_v \\cdot v_j ,\\\\ f_j^d &= MHA(Q, K, V). \\end {aligned} $}    (3)   Embedding Reweighting Layer.   As shown in Fig. 2 (b), we use RW(\u00b7) to   adjust the f d   j and reduce the impact of those features that are irrelated to f l.   Specifically, given the f d   j corresponding to the j-th view vj and the description f l, we compute the cosine similarity with sim(f d   j , f l) and get the weighted   combination of domain features f d   j = f d   j \u00b7 softmax(sim(f d   j , f l)).   Embedding Fusion Layer.   To enhance multimodal alignment and construct   joint features, we employ the aggregate operation agg in Embedding Fusion   Layer to build the vision features f v = agg(f v   j ) and domain features f d =   agg(f d   j ), where agg is max-pooling. We concatenate the features f = [f l, f v, f d]   which is fed into a multi-layer perceptron MLP(\u00b7) to compute Score.   3.2   Multi-task Learning   As shown in Fig. 2 (a), the DA4LG framework incorporates three different tasks:   the Language Grounding (LGR) Task, the Vision-Language Contrastive (VLC)   Task, and the Vision Grounding Caption (VGC) Task in multi-task learning. The   LGR Task is designed as the primary task following the existing works [42]. The   VLC and VGC tasks serve as auxiliary tasks to optimize the training objectives   inspired by BLIP-2 [24].   Language Grounding Task (LGR Task).   The primary task is the LGR   task, which involves predicting the target. We feed Score to predict the ground   truth label o\u2217and apply the binary cross-entropy loss for optimization:     \\s m all \\scalebox { 1 .0 } {$ \\begi n  {aligned} \\mathcal {L}_{LGR} = -\\mathbb {E} [o^{*}log(Score) + (1 - o^{*})log(1 - Score)]. \\end {aligned} $}    (4)   Vision-Language Contrastive Task (VLC Task).   We propose the VLC   task to learn an embedding that distinguishes samples from two different distributions. At each step, we sample some positive or negative pairs (l, o) during   Multi-Task Domain Adaptation for Language Grounding with 3D Objects   7   Table 1: Performance of our model and existing work on the SNARE dataset. Absence   of parentheses signifies that the original paper does not report standard deviations.   Standard deviations over five seeds are shown in parentheses (the same as below).   Model   Method   Validation   Test   Visual   Blind   All   Visual   Blind   All   ViLBERT [26]   External Prior   89.5   76.6   83.1   80.2   73   76.6   CLIP [31]   External Prior   83.7   65.2   74.5   80.0   61.4   70.9   MATCH [42]   External Prior   89.2 (0.9) 75.2 (0.7)   82.2 (0.4) 83.9 (0.5) 68.7 (0.9) 76.5 (0.5)   LAGOR [42]   Multi-view Perception 89.8 (0.4) 75.3 (0.7)   82.6 (0.4) 84.3 (0.4) 69.4 (0.5) 77.0 (0.5)   VLG [9]   External Prior   91.2 (0.4) 78.4 (0.7)   84.9 (0.3)   86.0   71.7   79.0   BLIP2 [24]   External Prior   51.2   50.9   51.5   LOCKET [38]   External Prior   90.9   78.4   84.7   86.1   71.5   79.0   MAGiC [28]   Multi-view Perception 92.1 (0.4) 81.3 (0.9) 86.8 (0.5)   87.7   75.4   81.7   DA4LG(ours)   Domain Adaptation   91.8 (0.3) 81.8 (0.6) 86.8 (0.5)   88.5   75.0   81.9   training. Specifically, samples from matched pairs are termed positives, whereas   those from unmatched pairs are termed negatives. We use cosine similarity, denoted as s(f l, f o), to measure the alignment between the language features f l   and the object features f o, where f o = agg([f v   j , f d   j ]). We optimize this function   to correctly select a single positive description sample l with \u03a6 negative object   samples and calculate the contrastive loss for object description:     \\s   mal l  \\scal   ebo x { 1.0   }{$  \\ b egi n  {   ali gne d}  \\m   at   hcal {L}_{con}^{o\\to l } = -\\mathbb {E} [\\text {log}\\frac {s(f^l,f^{o_p})}{s(f^l,f^{o_p}) + \\sum _{\\varphi =1}^{\\Phi }s(f^l,f^{o_n^\\varphi })} ], \\end {aligned} $}    (5)   where f op, f o\u03c6   n are the features of the positive and negatives sample for 3D   objects. Similarly, we can obtain the contrastive loss for the description-object   pairs:     \\s   mal l  \\scal   ebox  {1 . 0}   {$ \\be g in  {a   lig ned }    \\m a th   cal {L}_{con}^{l\\to o } = -\\mathbb {E} [\\text {log}\\frac {s_j(f^{l_p}, f^o)}{s(f^{l_p}, f^o) + \\sum _{\\varphi =1}^{\\Phi }s(f^{l_n^\\varphi }, f^{o})} ]. \\end {aligned} $}    (6)   And the VLC loss is denoted as LV LC = Ll\u2192o   con + Lo\u2192l   con .   Vision Grounding Caption Task (VGC Task).   Given the f o and l, we   design the VGC task to generate textual descriptions based on the freezing GPT2 [32]. Here, l represents a sequence of tokens denoted as {ci, i = [1, ..., N]}. Our   training objective is to predict the caption tokens conditioned on the output   token in auto-regressive. The training loss for the VGC task is formulated as:     \\s m all       \\   sca   leb ox {1. 0} {$ \\beg in {aligned} \\mathcal {L}_{VGC} =-\\mathbb {E} [\\sum _{i=1}^{N} \\text {log} \\; p(c_i| f^o,c_1, ..., c_{i-1}) ]. \\end {aligned} $}    (7)   During the multi-task learning phrase, our target loss function is     \\sma l l \\s c al ebox {1.0}{$ \\begin {aligned} \\mathcal {L} = \\mathcal {L}_{LGR} + \\mathcal {L}_{VLC} + \\mathcal {L}_{VGC}. \\end {aligned} $}    (8)   4   Experiment Design   To evaluate our proposed method, we investigate four key research questions   (RQs). The RQ1 concerns the DA4LG superior to the baselines. The RQ2   8   P. Sun et al.   and RQ3 focus on how to adopt DA4LG efficiently. The RQ4 concerns the   generalization of DA4LG in the downstream simulation environment.   \u2022 RQ1: What are the advantages of our DA4LG compared to other methods   for the language grounding task?   \u2022 RQ2: Which training policy in Domain-specific Encoder can obtain the   better performance with efficient parameters?   \u2022 RQ3: How do different learning tasks in DA4LG affect the language   grounding performance?   \u2022 RQ4: Can our DA4LG perform more effectively deployed in a downstream   task by a simulation environment compared to other methods?   4.1   Baseline Models   We conduct a comparative analysis between DA4LG and various public baselines, as summarized in Table 1. Two primary approaches are utilized in current   research: the multi-view perception-enhanced method, exemplified by MAGiC [28]   and LAGOR [42], and the external prior-injected method, represented by ViLBERT [26], MATCH [42], VLG [9], CLIP [31], LOCKET [38], and BLIP2 [24].   We list these baselines below:   \u2022 LAGOR adopts a multi-task learning approach by predicting the canonical viewing angle for individual view images.   \u2022 MAGiC performs joint reasoning over candidate referent objects, considering each object from multiple possible perspectives.   \u2022 MATCH and ViLBERT uses CLIP-ViT and ViLBERT respectively to   encode the views of each object. An MLP is trained to assign scores based   on the encoded views and language description embedding   \u2022 CLIP uses the cosine distance in CLIP embedding between visual and   language features to pick the object in the lowest distance.   \u2022 BLIP-2 is the multimodal LLM-based method in zero-shot setting.   \u2022 VLG leverages implicit 3D prior information from predicted volumetric   voxel maps to improve language grounding performance by LegoFormer [45].   \u2022 LOCKET is a knowledge enhancement method that employs a graph   convolutional network to encode a multimodal knowledge graph.   4.2   Implementation Details   Training and Inference Details. We employ the Adam optimizer with weight   decay 5e\u22124. The batchsize is 64, training epoch is 60, and learning rate is 5e\u22123.   Experiments are implemented with CUDA 11.2 and PyTorch 1.7.1 and run on   one NVIDIA RTX4090. In DA4LG, we employ the vision and language encoders   from CLIP ViT-B/32 [31] as the Vision Encoder and Language Encoder, respectively. Domain-specific Encoder is initialized from the vision encoder in CLIP   ViT-B/32.   Multi-Task Domain Adaptation for Language Grounding with 3D Objects   9   Benchmark Datasets. We train and evaluate our proposed method on SNARE   dataset [42] which is split into training, validation, and test sets following existing works [42]. SNARE is a benchmark for choosing the correct object with   small differences in multiple views given a language description. Each data in the   set has the label of visual or blind. The visual label means a comprehensive understanding of the object, providing relevant visual cues to guide the grounding   process (e.g., \u201cclassic armchair with white seat\u201d). The blind labels predominantly   focus on the object\u2019s shape and specific distinguishing attributes, intentionally   omitting color and other visual characteristics (e.g., \u201coval back and vertical legs\u201d).   The training set consists of 207 categories, 6, 153 objects, and 39, 104", "conf": "ECCV", "year": "2024", "index": 498}, {"title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded   Boyuan Zheng 1 Boyu Gou 1 Jihyung Kil 1 Huan Sun 1 Yu Su 1   https://osu-nlp-group.github.io/SeeAct", "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability   boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web   agent that can follow natural language instructions to complete tasks on any given website. We   propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual   understanding and acting on the web. We evaluate   on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by   developing a tool that allows running web agents   on live websites. We show that GPT-4V presents   a great potential for web agents\u2014it can successfully complete 51.1% of the tasks on live websites if we manually ground its textual plans into   actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller   models (FLAN-T5 and BLIP-2) specifically finetuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns   out to be not effective for web agents, and the   best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with   oracle grounding, leaving ample room for further improvement. All code, data, and evaluation   tools are available at https://github.com/   OSU-NLP-Group/SeeAct.   1The Ohio State University, Columbus, OH. Correspondence to: Boyuan Zheng <zheng.2372@osu.edu>, Huan Sun   <sun.397@osu.edu>, Yu Su <su.809@osu.edu>.   Proceedings of the 41 st International Conference on Machine   Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by   the author(s).   Rent a truck near zip 08817 on December 10 at 11:30AM returned to the exact location and date.   LMM   Move the cursor over   the \"Find Your Truck\"   button located in the   central portion of the   webpage, just below the   input fields for rental   details, and perform a   click action.   Grounding   Element: <input id=19 button find your truck />   Operation: CLICK   SeeAct   Browser Event   Figure 1: SEEACT leverages an LMM like GPT-4V to visually perceive websites and generate plans in textual forms.   The textual plans are then grounded onto the HTML elements and operations to act on the website.   1.", "content": "Large multimodal models (LMMs; Li et al. (2023); Alayrac   et al. (2022); Liu et al. (2023b)), especially recent ones   such as GPT-4V(ision) (OpenAI, 2023) and Gemini (Anil   et al., 2023), have shown a remarkable capability on   standard vision-and-language understanding and reasoning   benchmarks (Kazemzadeh et al., 2014; Goyal et al., 2016;   Hendrycks et al., 2020; Saikh et al., 2022; Lu et al., 2022;   Zhong et al., 2023; Yue et al., 2023). While web content has   been a primary source of training data, a largely overlooked   part of the web is the websites themselves\u2014every website   is designed to be rendered visually for easy consumption   by human users. This poses a new challenge and a new   opportunity for LMMs. On the one hand, screenshots of   rendered websites, which could contain thousands of elements with rich relations, are more complex than the images   in most existing benchmarks, which are usually object- or   scene-centric. On the other hand, if LMMs can accurately   comprehend websites, it will open the door for numerous   applications on the web.   In this work, we aim to investigate the potential of LMMs   1   GPT-4V(ision) is a Generalist Web Agent, if Grounded   as generalist web agents (Deng et al., 2023). A generalist   web agent, as defined in MIND2WEB (Deng et al., 2023), is   expected to follow natural language instructions and complete tasks on any given real-world website (e.g., Figure 1).   The tasks can be fairly diverse and complex, with one task   possibly taking 10+ actions across multiple dynamically rendered webpages. Existing work (Deng et al., 2023; Liu et al.,   2023d) primarily uses large language models (LLMs) such   as GPT-4 (OpenAI, 2023) on the raw HTML input. However, HTML code is noisier than the rendered visuals and   has a lower information density. For example, the screenshot in Figure 1 contains 423 HTML elements that would   require 186,490 textual tokens with the GPT-2 Tokenizer,   while requiring only 1,445 visual tokens using GPT-4V\u2019s   visual tokenizer. Furthermore, HTML alone provides incomplete information and misses critical semantics from,   e.g., embedded images.   To this end, we propose SEEACT, a generalist web agent   that harnesses the power of LMMs for integrated visual   understanding and acting on the web. We will focus on   GPT-4V, the most advanced LMM publicly available to   date, and compare it with smaller LMMs such as BLIP2 (Li et al., 2023) and LLaVA-1.5 (Liu et al., 2023a;c). We   find that GPT-4V exhibits a strong capability in visually understanding rendered webpages and generate the right plans   in textual forms across a wide range of websites and tasks.   However, grounding (Chandu et al., 2021; Gu et al., 2023),   i.e., converting the textual plan into precise actions on the   website, remains a major challenge. It involves selecting   the right HTML element to interact with as well as the right   operation (e.g., Click, Type, or Select). We propose   multiple grounding methods, including superpositioning   bounding boxes and index labels onto the image, similar to   set-of-mark prompting (Yang et al., 2023a) that has been   shown effective on object- or scene-centric images. However, we find that on complex images with rich semantic   and spatial relationships like webpage screenshots, severe   hallucination is observed from GPT-4V. The most effective   grounding strategy leverages the known correspondence between HTML element and their visual rendering, a unique   property for websites compared to natural images.   We evaluate SEEACT on the MIND2WEB dataset (Deng   et al., 2023) and compare it with text-only large language   models (LLMs) like GPT-4 (OpenAI, 2023) as well as   smaller models (FLAN-T5 (Chung et al., 2022) and BLIP2 (Li et al., 2023) for supervised fine-tuning, LLaVA1.5 (Liu et al., 2023a;c) and CogAgent (Hong et al., 2023)   for in-context learning). In addition to the standard offline   evaluation setting on cached websites, we further establish   a new online evaluation setting by developing a tool that   allows for running web agents on live websites. The major   findings from our exploration are summarized below:   \u2022 SEEACT with GPT-4V is a strong generalist web agent,   if oracle grounding is provided. In online evaluation, it   can successfully complete 51.1% of tasks on different   websites, substantially outperforming existing methods   like GPT-4 (13.3%) or FLAN-T5 (8.9%). This strongly   demonstrates the potential of LMMs like GPT-4V for   web agents.   \u2022 However, grounding is still a major challenge. The best   grounding strategy still has a 20-32% gap with oracle   grounding. Among the various grounding strategies,   the best one organically leverages both HTML text and   visuals, substantially outperforming image annotation   strategies (Yang et al., 2023a) by up to 10%.   \u2022 In-context learning with large models (both LMMs   and LLMs) shows better generalization to unseen websites, while supervised fine-tuning still has an edge on   websites seen during training.   \u2022 There is a non-negligible discrepancy between online   and offline evaluation because there can often be multiple viable plans for completing the same task. Online   evaluation is more indicative of a model\u2019s true performance.   2. SeeAct   In this section, we first explain the problem formulation of   web agents and then introduce SEEACT, a generalist web   agent based on LMMs. Specifically, given a web-based task   (e.g., \u201cRent a truck with the lowest rate\u201d in the car rental   website), we examine two essential capabilities of LMMs as   a generalist web agent: (i) Action Generation to produce   an action description at each step (e.g., \u201cMove the cursor   over the \u2018Find Your Truck\u2019 button and perform a click\u201d)   towards completing the task, and (ii) Element Grounding   to identify an HTML element (e.g., \u201c[button] Find Your   Truck\u201d) at the current step on the webpage.   2.1. Formulation   Given a website S (e.g., a car rental website) and a task   T (e.g., \u201cRent a truck with the lowest rate\u201d), the web   agent should generate a sequence of executable actions   A = [a1, a2, ..., an] to complete the task. Specifically, at   time step t, the agent should generate an action at based on   the current environment observation st, the previous actions   {a1, a2, ..., at\u22121}, and the task T:   at = \u03c0(st, T, {a1, a2, ..., at\u22121})   The environment observation st comprises an HTML document ht and a screenshot image it. LLMs can only be   grounded on the HTML document, while LMMs can be   grounded on both the HTML document and the screenshot   image. The website status is updated accordingly after each   2   GPT-4V(ision) is a Generalist Web Agent, if Grounded   TEXT: Find Your Truck   TYPE: BUTTON   A: <text id=0>Moving Trucks & Accessories</text>   B: <div id=1>Where do you want to pick up your    truck?</div>   C: <div id=5>When do you want to pick up your    truck?</div>   \u2026 \u2026 \u2026    F: <input id=18>No value=true</input>   G: <input id=19 button \u201cfind your truck \u201d/>   H: None    Move   the   cursor   over   the   \"Find   Your   Truck\"   button   located in the central portion   of the webpage, just below the   input fields for rental details,   and perform a click action.   Action Description   Element Attributes   Image Annotation   CHOICE: G   A   B   C   D   E   F   G   Textual Choices   CHOICE: G   Figure 2: An example of the element grounding process for a single action during completing the given task with three   different methods. In this action step, the model needs to click the \"Find Your Truck\" button to perform a search. For   grounding with textual choices, some element candidates represented with HTML text are given, the model is required to   generate the choice index of the target element. For image annotation, bounding boxes and index labels are added to the   image. The model is required to generate the label on the bottom-left of the target element. For grounding with element   attributes, the model needs to predict the text and type of the target element.   action:   st+1 = S(at) = {ht+1, it+1}   For simplicity, in subsequent step-wise formulations, the   time step notation t is omitted.   An action a corresponds to a browser event provided by the   website environment. Therefore, we formulate an action   as a triplet of three necessary variables for a browser event   (e, o, v). e \u2208E identifies the target webpage element to   operate on, such as the \"Find Your Truck\" button in Figure 2. E represents the set of webpage elements within the   environment S. The operation o \u2208O is the action to be   performed on the target element, with O encompassing all   possible operations in S (e.g., Click, Type). The variable   v denotes the additional value needed for a certain operation   (e.g., the date 12/10/2023 for a Type operation).   However, agents based on LLMs or LMMs are typically   unable to directly generate the three variables (e, o, v) required for a browser event. Instead, they generate a textual   description of the intended action \u02dca, containing information   about these variables as (\u02dce, \u02dco, \u02dcv). This process is referred to   as Action Generation. To interact with the environment, a   further step is required to convert \u02dca into a, which we refer   to as Action Grounding.   2.2. Action Generation   We explicitly instruct the LMM to imitate humans browsing   a webpage and analyze the task, webpage, and previous   actions. It is asked to generate an action description \u02dca based   on its analysis and reasoning. We take the screenshot image   i as the visual context without utilizing the HTML document   h for action generation.   2.3. Action Grounding   Despite the capability of LMMs in identifying and describing the next action to complete the given task in natural language, it is still challenging to convert the action description   \u02dca into an executable action a within the environment. Deriving operation type o and value v from the action description   \u02dca can be solved through string parsing reasonably well. The   key challenge is to identify the target element e from the   generated \u02dce, which we refer to as Element Grounding.   To address this challenge, we explore three approaches using different types of information: Grounding via Element   Attributes, Grounding via Textual Choices, and Grounding   via Image Annotation, as depicted in Figure 2. The prompting details of action generation and grounding are included   in Appendix D.   Grounding via Element Attributes.   This approach involves prompting the model to generate as detailed attributes   of the target element as possible, thereby providing more information to precisely match with the target HTML element.   Specifically, we prompt the model to not only describe the   element e, but also specify the target element\u2019s type and   the textual content in \u02dce. For example, as illustrated in Figure 2, the model would generate element text as \"Find Your   Truck\" and identify its type as a \"BUTTON.\" Following this,   a heuristic search is performed across the DOM elements,   using the element text and type to locate matching elements.   3   GPT-4V(ision) is a Generalist Web Agent, if Grounded   In cases where a single match is found, it is automatically selected. Otherwise, when multiple matches arise, the model   is further prompted to select the final selection.   Grounding via Textual Choices. The above approach demands precise and sufficient attribute descriptions from the   LMM and accurate matching by the heuristic search, which   can be highly demanding. For instance, many elements may   have no textual content or have textual information in a   nearby element instead of itself.   Alternatively, we provide the model with textual representations of elements as choices to facilitate grounding, which   has already been proven effective in MindAct (Deng et al.,   2023). Specifically, MindAct utilizes a ranking model to   select top-k candidate elements (e1, e2, ..., ek) with a pretrained cross-encoder. Each candidate element is represented as a choice in a multi-choice question with its HTML   text, as illustrated in Figure 2. After generating the action   description \u02dca, the model is further asked a multi-choice   question to choose its intended element from the given multiple choices (including a \u2018none\u2019 option).   Grounding via Image Annotation.   Textual representations alone are sometimes insufficient to distinguish similar   or identical elements, as illustrated in Appendix H. Therefore, in this approach, we propose to overlay a bounding box   for each candidate element e selected by the ranker as well   as a label around the bounding box1 with a label assignment   method to avoid overlapping between markups. The model   is expected to generate the label corresponding to the target   element.   Oracle Action Grounding. Ideally, the action description   \u02dca must encompass all necessary details to precisely identify   each variable (e, o, v) of the action triplet. To assess the performance of action generation, an oracle grounding method,   which ensures the variables be identified as long as they are   mentioned in the action description, is desired. Here we   approximate the oracle grounding method by asking human   annotators to identify the model\u2019s intended actions.   3. Experiments   3.1. Dataset   We evaluate our methods on MIND2WEB (Deng et al.,   2023), a comprehensive dataset encompassing over 2,000   complex web tasks with annotated actions. This dataset   spans 137 websites across 31 low-level domains, categorized into 12 high-level domains. It supports three primary   operations: Click, Type, and Select, with Hover and   Press Enter operations integrated into Click to avoid   ambiguity.   1We use the Supervision library for image annotation: https:   //supervision.roboflow.com/   The dataset\u2019s test sets aim to measure the generalization of   web agents across different tasks, websites, and domains.   Specifically, the Cross-Task setting focuses on evaluating   agents on tasks that are new to the training data but within   included domains and websites. The Cross-Website setting   evaluates agents with tasks across 10 new websites for each   of the top-level domains in the training data. The CrossDomain setting assesses agent performance on tasks in two   top-level domains held out from the training data.   We align each HTML document in the dataset with its corresponding webpage screenshot image from the MIND2WEB   raw dump, which undergoes human verification to confirm   element visibility and correct rendering for action prediction.   This cleaned version of the dataset is called Multimodal   Mind2Web, with the statistics in Table 1.2   3.2. Methods   SeeAct. In grounding via image annotation and textual   choices, we first employ the DeBERTa-base cross-encoder   from MindAct (Deng et al., 2023) to rank the top 50 elements for better comparison with its text-only counterparts.   Then, we cluster elements into groups of 17 options for   inference. In grounding via element attributes, no candidate   element is provided. We experiment all three grounding   methods with GPT-4V API, and use the best-performing   grounding method for Gemini Pro Vision (Anil et al., 2023),   and LLaVA-1.5 (Liu et al., 2023a;c).   MindAct. To compare with SEEACT, we also implement   methods based on text-only LLMs and BLIP-2 (Li et al.,   2023) following the two-stage strategy of MindAct (Deng   et al., 2023). Firstly, we employ the ranker above to pick   the top 50 elements. Subsequently, the action generation   problem is formulated as a multi-choice question answering   problem, with the candidate elements as options, including   a \"None\" option if the target element is absent. During   inference, elements are clustered into groups of 5 elements,   with iterative refinement, until a single choice is made or all   options are discarded. We evaluate supervised fine-tuning   (SFT) methods using FLAN-T5 (Chung et al., 2022) and   BLIP-2-T5 and in-context learning (ICL) methods using   GPT-3.5 and GPT-4.   Pixel-Level Grounding. LMMs can generate target element   coordinates in the image via training on datasets augmented   with object coordinates, especially for open-sourced models (Hong et al., 2023; Cheng et al., 2024; You et al., 2023).   We choose CogAgent (Hong et al., 2023) as a representative   model for this experiment. Details of each method can be   found in Appendix A.   2The dataset is released at https://huggingface.co/   datasets/osunlp/Multimodal-Mind2Web.   4   GPT-4V(ision) is a Generalist Web Agent, if Grounded   Table 1: Statistics of the cleaned Multimodal Mind2Web dataset. The average # of visual tokens is based on OpenAI visual   token calculator.   Split   # Tasks   # Domains   # Websites   Avg #   Avg #   Avg # HTML   Actions   Visual Tokens   Elements   Tokens   Train   1,009   17   73   7.7   4,240   602   128,827   Cross-Domain   694   13   53   5.9   4,314   494   91,163   Cross-Task   177   17   64   7.6   4,172   607   123,274   Cross-Website   142   9   10   7.2   4,653   612   114,358   3.3. Offline Evaluation   We adopt the evaluation metrics utilized in MIND2WEB.   Element Accuracy (Ele. Acc) compares the predicted element with the ground-truth elements. Operation F1 (Op.   F1) calculates the token-level F1 score for the predicted   operation comprised of action and input value. Step Success Rate (Step SR) measures the success of each action   step. A step is successful only if the selected element and   the predicted operation are correct. We report macro averages across tasks for these step-wise metrics. Success   Rate (SR) measures the success of an entire task. A task is   regarded successful only if all steps have succeeded. This   metric is stringent without allowing the model any space   for exploration and error correction. Therefore, for offline   evaluation, we focus on the first three metrics. However, we   also conduct online evaluation on live websites for better   evaluation on the whole task success rate, as detailed below.   3.4. Online Evaluation   We develop a new online evaluation tool using Playwright3   to evaluate web agents on live websites (instead of cached   websites in offline evaluation). Our tool can efficiently   tunnel multimodal inputs from the browser to the agent and   convert the predicted action (e, o, v) into a browser event for   execution. To adhere to ethical standards, our experiments   are restricted to non-login tasks in compliance with user   agreements, and we closely monitor agent activities during   online evaluation to prevent any actions that have potentially   harmful impacts, like placing an order or modifying the user   profile.   4. Results and Analysis   4.1. Offline Evaluation Results   GPT-4V can be a Generalist Web Agent with Oracle   Action Grounding.   Given an effective action grounding method, GPT-4V has the potential to serve as a generalist web agent.   Specifically, as described in subsection 2.3, we provide GPT-4V with an oracle action grounding method (SEEACTOracle) through human annotation, the   model achieves a step success rate of 61.9%, 65.0%, and   3https://playwright.dev/   62.1% across three test splits, respectively. As shown in   Table 2, this method substantially outperforms other models under all metrics across three test splits. Specifically,   it achieves a 8.4% step success rate improvement over the   second-best method in the Cross Task setting. The performance advantage is more pronounced under the CrossWebsite and Cross-Domain settings, where it leads by 23.9%   and 23.2% step success rates, demonstrating its generality   compared with supervised fine-tuning. This observation is   further corroborated within the online evaluation (Table 4).   Element Grounding Method Comparison.   However,   there is a noticeable gap between oracle grounding and   all three proposed grounding methods, as shown in Table 3. This demonstrates that grounding, especially element   grounding, is a major bottleneck. Element grounding via   textual choice (SEEACTChoice) demonstrates the best performance under all metrics across all settings, comparable to   supervised fine-tuning and showing a substantial improvement over text-only LLMs.   Grounding via image annotation (SEEACTAnnotation) offers   an intuitive approach and shows promising results in recent   work that focuses on object- or scene-centric images (Yang   et al., 2023a). However, we find that on complex images   with rich semantic and spatial relationships like webpage   screenshots, severe hallucination is observed from GPT-4V.   Specifically, it often fails to correctly map its generated element description (which is often correct according to oracle   grounding) to the right bounding box and index label in the   image, leading to a low element accuracy. This limitation   primarily arises from GPT-4V\u2019s weakness in understanding   image details and relative spatial location, a topic that we   will further delve into in Appendix E. We leverage bottomleft number labels around bounding boxes as it is identified   as the optimal markups in the ablation study in Appendix B.   Grounding via element attributes (SEEACTAttribute) also   demonstrates inferior performance. This method\u2019s effectiveness is primarily limited by its heuristic-based element   localization strategy, which depends on textual and locality   characteristics. This becomes problematic as not all webpage elements contain text, and sometimes the relevant text   is associated with a nearby but distinct element.   LMMs vs. LLMs. The SEEACTChoice with GPT-4V demon5   GPT-4V(ision) is a Generalist Web Agent, if Grounded   Table 2: Performance of different models. All models under SEEACT utilize \u201cChoices\u201d for grounding. Methods with * mark   are conducted on a subset with 30 tasks for each task split.   Model   Cross-Task   Cross-Website   Cross-Domain   Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR   Supervised Fine-Tuning   FLAN-T5-XL   57.1   75.7   53.5   43.8   67.7   41.1   41.4   65.9   38.9   BLIP-2-T5-XL   50.1   77.0   47.0   39.4   69.3   37.0   41.2   69.3   38.9   In-Context Learning   GPT-3.5*   19.4   59.2   16.8   14.9   56.5   14.1   25.2   57.9   24.1   GPT-4*   40.8   63.1   32.3   30.2   61.0   27.0   35.4   61.9   29.7   COGAGENT   22.4   53.0   17.6   18.4   42.2   13.4   20.6   42.0   15.5   SEEACT   \u2013 LLAVA-1.5   9.7   65.6   8.1   9.1   60.8   7.5   10.9   63.9   8.5   \u2013 GEMINI PRO VISION   21.5   67.7   19.6   17.1   61.3   15.4   20.7   64.3   18.0   \u2013 GPT-4V   46.4   73.4   40.2   38.0   67.8   32.4   42.4   69.3   36.8   \u2013 GPT-4V-Oracle*   66.4   79.2   61.9   69.5   78.9   65.0   72.8   73.6   62.1   Table 3: Step success rate (%) of GPT-4V on a subset of 30   tasks for each task split with different grounding methods.   \u201cAttributes\u201d, \u201cChoices\u201d, \u201cAnnotation\u201d, and \u201cOracle\u201d refer to   element grounding via Element Attributes, Textual Choices,   Image Annotation, and Human Annotation, respectively, as   described in subsection 2.3.   Grounding   Cross-Task Cross-Website Cross-Domain   Attribute   16.1   12.1   19.0   Annotation   20.3   13.9   23.7   Choice   39.1   32.7   42.0   Oracle   61.9   65.0   62.1   strates a substantial performance advantage over the textonly GPT-4 under all three metrics across all three test splits.   Specifically, it outperforms GPT-4 in step success rate of   7.9%, 5.4%, and 7.1% on three settings, respectively. Interestingly, fine-tuned BLIP-2-T5 does not show a noticeable   gain over FLAN-T5, despite having additional visual input.   Several factors may contribute to this. First, the CLIP model   used as the image encoder may not be sufficiently adept at   image details, as explored by Shen et al. (2021). This limitation is particularly relevant for our web navigation task,   which demands a high level of image detail comprehension.   Second, BLIP-2-T5 utilizes an off-the-shelf CLIP model   that may not be optimal for webpage screenshots. Finally, although the screenshots in the test splits are error-free, some   of the examples in the training set might contain issues   such as rendering failures or inaccuracies when annotators   capture the screenshot.   SFT vs. ICL. We compare SFT and ICL methods to offer   insights for developing web agents in different scenarios.   ICL (with SEEACT) demonstrates consistent and robust performance across three test splits. ICL is particularly advantageous in scenarios lacking annotations or requiring strong   Table 4: Whole task success rate (%) under both offline   and online evaluation. Offline0 and Offline1 refer to no   tolerance for error at any step and allowing for error at one   step, respectively.   Offline0   Offline1   Online   FLAN-T5-XL   4.4   24.4   8.9   GPT-4   1.1   12.2   13.3   SEEACTChoice   3.3   12.2   37.8   SEEACTOracle   13.3   27.8   51.1   generalization capabilities for new domains and websites.   As grounding methods improve towards oracle grounding,   ICL is poised to show even stronger performance. On the   other hand, SFT methods show better generalization across   tasks on websites already seen during training. Considering the high cost of data annotation for web agents and   the billions of websites on the Internet, ICL offers a more   compelling solution for generalist web agents. However, if   one only needs to develop a strong web agent for a certain   website, SFT is still a competitive solution.   4.2. Online Evaluation Results   In online evaluation, we pair a web agent with a human   annotator, where the human was tasked to monitor agent   actions that may change real-world states and determine   whether each task was successfully completed. For comparative analysis, we include success rates from offline evaluation, denoted as Offline0 (allowing zero wrong action) and   Offline1 (allowing one wrong action). For a fair comparison   between offline and online evaluations, we only re-write   time-sensitive tasks to ensure they are still valid when the   evaluation is conducted. For instance, we update the dates   for flight-related tasks. Finally, we conduct the online evalu6   GPT-4V(ision) is a Generalist Web Agent, if Grounded   Easy   Medium   Hard   0   10   20   30   40   50   60   70   Success Rate   SeeAct-Oracle   SeeAct-Choices   GPT-4   FLAN-T5-XL   Figure 3: Whole task success rate across task difficulty   levels. We categorize tasks based on the number of actions   to complete, i.e., Easy: 1-4, Medium: 5-9, and Hard: 10-18,   with 37, 35, and 18 tasks in each group, respectively.   ation on the same subset of total 90 tasks from the three test   splits and report the mean performance across these tasks.   Table 4 shows that the whole task success rate in online   evaluation substantially exceeds that of offline evaluation   (Offline0). This finding suggests that the whole task success   rate is likely underestimated in the offline evaluation due to   the variability in actions and plans. In other words, there   may be multiple viable plans for a task, but the reference   plan in offline evaluation only captures one of them.   Across all three settings, SEEACTChoice outperforms both   GPT-4 and FLAN-T5-XL by a large margin of over 20%   whole task success rate. Using oracle grounding further improves the performance substantially, reaching a remarkable   whole task success rate of 51.1%. Although GPT-4 shows   much worse performance than FLAN-T5-XL in step success rate under offline evaluation (Table 2), it outperforms   FLAN-T5-XL by 4.4% whole task success rate in the online evaluation. These results further confirm the potential   of large models for generalist web agents compared with   fine-tuned small models.   4.3. Analysis   Online Success Rate by Task Difficulty. We investigate   the performance of web agents on tasks across different   difficulty levels. We estimate the task difficulty based on the   number of actions taken by annotators during action trace   annotation. As shown in Figure 3, the whole task success   rate is negatively correlated with the number of actions\u2014   it decreases as the number of actions increases across all   four methods. SEEACTOracle consistently outperforms other   methods across all difficulty levels. Interestingly, the gap between SEEACTOracle and SEEACTChoice enlarges on longerhorizon tasks. This is understandable because grounding   errors cascade to later steps; nonetheless, it further shows   the challenge of grounding for GPT-4V and the need for   better grounding methods.   Error Analysis in Grounding via Image Annotation.   Set-of-mark prompting (Yang et al., 2023a) uses a similar method as grounding via image annotation and has been   shown effective on object- or scene-centric images (Lin   et al., 2014; Plummer et al., 2015; Zhou et al., 2017). However, this grounding method is suboptimal on webpage   screenshot images that are complex and contain rich semantic and spatial relationships. To analyze the reasons   behind the failures, we randomly sample 100 action predictions with correct action generation but wrong grounding   results. We observes major types of errors as : (1) Making   up bounding box & label; (2) Failure to link bounding boxes   with the correct labels. Illustrative examples are included in   Appendix E.   Our analysis reveals that 54% of the errors can be attributed   to GPT-4V\u2019s tendency of visual illusion (Guan et al., 2023),   where the model misinterprets and fabricates content over   the image. Specifically, the target element described in action generation does not have a bounding box or a label on   the bottom-left, where the model is supposed to generate   \"NA\". However, the model falsely assumes the presence of   a bounding box and makes up a label as the answer. Another   46% of errors are caused by GPT-4V\u2019s limitation in recognizing the relative position within an image. Specifically,   the model is capable of identifying the target element within   the bounding box. However, it struggles to correctly link   the bounding box with its corresponding label.   4.4. Case Study   GPT-4V exhibits promising capabilities, ranging from speculative planning, webpage content reasoning, and error correction to surpassing the limitations of superficial textual   similarity matching inherent in fine-tuned, text-only models.   World Knowledge. GPT-4V demonstrates substantial advantages in tasks requiring certain knowledge over finetuned models at a smaller scale. As shown in Appendix I,   GPT-4V is able to identify the IATA code of the airport in   Los Cabos as SJD. In contrast, smaller models are typically   weaker at knowledge-intensive tasks and are also likely to   lose knowledge during the fine-tuning process due to catastrophic forgetting.   World Model (for Websites). GPT-4V exhibits the potential of a \"world model\" for websites. As shown in Appendix G, GPT-4V can predict the state transitions on a   website (e.g., what would happen if I clicked this button).   Based on its awareness of website state transitions, GPT-4V   7   GPT-4V(ision) is a Generalist Web Agent, if Grounded   can conduct speculative planning involving a sequence of   subsequent actions in the future to complete the given task.   Error Correction Awareness. GPT-4V also exhibits the   awareness of error correction in the previous actions. In the   example in Appendix J, it realizes that the mobile phone   number is invalid due to the wrong format and generates the   description of the action to correct this error. This highlights   the model\u2019s potential for adaptation in online settings, where   actions may not always follow pre-defined, ideal paths as in   offline evaluations. This capability paves the way for adding   robustness and reasonable dynamic planning.   5. Related Work   Web Agent. Many works have focused on improving web   agents relying on the HTML document (Deng et al., 2023;   Gur et al., 2023; 2022; 2023; Kim et al., 2023; Sridhar   et al., 2023).   However, a raw HTML document is often massive making it infeasible or cost-prohibitively to   feed into LLMs directly.   MindAct (Deng et al., 2023)   instead employs a small language model to rank each   HTML element and selectively consider top elements as   the context.   WebAgent (Gur et al., 2023) proposes an   enhanced planning strategy by summarizing the HTML   documents and decomposing the instruction into multiple   sub-instructions. Another stream considers visual information for web agents (Shaw et al., 2023; Furuta et al., 2023;   Hong et al., 2023). Pix2Act (Shaw et al., 2023) leverages   Pix2Struct (Lee et al., 2022) to parse screenshot images into   simplified HTML to complete GUI-based tasks (Shaw et al.,   2023; Liu et al., 2018; Shi et al., 2017; Mazumder & Riva,   2020; Yao et al., 2022). WebGUM (Furuta et al., 2023) and   CogAgent (Hong et al., 2023) pre-train an LMM with massive screenshot-HTML data to enhance its decision-making   on real-world web navigation like Mind2Web. While all   these prior works show promise, generalizing to various   web environments remains a challenge for existing models.   Thus, SEEACT explores recently released, more powerful   LMMs such as GPT-4V and Gemini, to demonstrate their   potential as generalist web agents with comprehensive online and offline evaluation and analysis. In a concurrent   work (Yan et al., 2023), GPT-4V exhibits strong performance on mobile UI understanding, which is less complex   than the desktop websites we study.   Large Multimodal Models. GPT-4V (OpenAI, 2023) and   Gemini (Anil et al., 2023) represent significant progress   in LMMs. Several studies (Akter et al., 2023; OpenAI,   2023; Yang et al., 2023c; Zhang et al., 2023; Yang et al.,   2023a; Yan et al., 2023) have highlighted their remarkable   multimodal capabilities, emphasizing the advanced and versatile integration of visual and language reasoning abilities.   Their performance on a series of benchmarks (Kazemzadeh   et al., 2014; Goyal et al., 2016; Hendrycks et al., 2020;   Saikh et al., 2022; Lu et al., 2022; Zhong et al., 2023;   Yue et al., 2023) also showcases remarkable capabilities   on vision-and-language understanding and reasoning. Although open-sourced models still exhibit a performance gap   with GPT-4V, they have the advantages of controllability   and ease of fine-tuning for various applications. For example, in CogAgent (Hong et al., 2023), LMMs are fine-tuned   on HTML and screenshot image pairs to enhance webpage   understanding ability and further enhanced with an image   encoder for high-resolution image details. Ferret (You et al.,   2023) is finetuned to allow visual referring and grounding.   Visual Grounding. Despite LMMs having achieved remarkable vision-language understanding capabilities, they still   face challenges in fine-grained visual grounding. Various visual prompting (Shtedritski et al., 2023; Yang et al., 2023b;c;   Yan et al., 2023) methods have been proposed to augment   GPT-4V\u2019s image detail grounding ability by overlaying   visual marks onto the image. SoM (Yang et al., 2023a) involves segmenting the image into semantically meaningful   regions and overlaying an array of visual marks like numbers, alphabets, masks, or bounding boxes. Fine-tuning   vision-language models with image-annotated data is effective. Kosmos-2 (Peng et al., 2023) represents bounding box   locations through textual location tokens. BuboGPT (Zhao   et al., 2023) extract entities and find corresponding masks   for objects in the image. Shikra (Chen et al., 2023) handles   image detail referring and grounding by applying spatial   coordinates as text tokens in inputs and outputs, respectively. Ferret (You et al., 2023) represents regions with both   discrete coordinates and continuous features along with a   spatial-aware visual sampler to handle diverse spatial characteristics across various shapes.   6. Conclusion   In this work, we developed SEEACT, a generalist web   agent that harnesses the power of large multimodal models   (LMMs) like GPT-4V to integrate visual understanding and   acting on the web. We showed that LMMs present a great   promise for generalist web agents, with a success rate of   50% on live websites given an oracle grounding method.   GPT-4V also exhibits impressive capabilities, such as error correction and speculative planning. However, finegrained visual grounding is still a major challenge. The   most effective grounding strategies we explored in this paper   still exhibit a 20-25% performance gap compared to oracle   grounding. Future work should better leverage the unique   properties of the Web, e.g., the known correspondence between HTML and visual elements, for improving grounding   and reducing hallucinations from LMMs. Furthermore, we   show a significant discrepancy between online and offline   evaluations, emphasizing the importance of online evaluation for an accurate assessment of a model\u2019s capabilities.   8   GPT-4V(ision) is a Generalist Web Agent, if Grounded   This discrepancy is largely due to the variability in potential   plans for completing the same task, pointing to the dynamic   nature of web interactions.   Impact Statement   Generalist web agents hold the potential to automate routine web tasks, enhance user experiences, and promote web   accessibility, safety concerns related to their real-world deployment are also critical. These concerns span privacy   issues, such as access to users\u2019 personal profiles, and sensitive operations, such as financial transactions or application   form submissions. During the online evaluation, we noticed   the possibility for these web agents to generate harmful   actions on the web, and we manually validated the safety   of all the actions before execution. It is critical for further   research to thoroughly assess and mitigate the safety risks   associated with web agents, ensuring they are safeguarded   against producing and executing harmful actions. The code   will also be released solely for research purposes, with the   goal of making the web more accessible via language technologies under an OPEN-RAIL License. We are strongly   against any potentially harmful use of the data or technology   by any party.   Acknowledgments   The authors would like to thank colleagues from the OSU   NLP group for their thoughtful comments. This research   was supported in part by ARL W911NF2220144 and Cisco.", "conf": "ICML", "year": "2024", "index": 305}, {"title": "Grounding Large Language Models in Interactive Environments   with Online Reinforcement Learning   Thomas Carta * 1 Cl\u00b4ement Romac * 1 2 Thomas Wolf 2 Sylvain Lamprier 3 Olivier Sigaud 4   Pierre-Yves Oudeyer 1", "abstract": "Recent works successfully leveraged Large Language Models\u2019 (LLM) abilities to capture abstract knowledge about world\u2019s physics to solve   decision-making problems. Yet, the alignment   between LLMs\u2019 knowledge and the environment   can be wrong and limit functional competence   due to lack of grounding. In this paper, we study   an approach (named GLAM) to achieve this alignment through functional grounding: we consider   an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to   study higher-level forms of functional grounding,   and a set of spatial and navigation tasks, we study   several scientific questions: 1) Can LLMs boost   sample efficiency for online learning of various   RL tasks? 2) How can it boost different forms of   generalization? 3) What is the impact of online   learning? We study these questions by functionally grounding several variants (size, architecture)   of FLAN-T5.   1.", "content": "The recent rise of Transformer-based Large Language Models (LLMs) trained on massive text datasets in Natural Language Processing has led to models exhibiting impressive capabilities (e.g. natural language generation, question answering, reasoning, translation...) (Devlin et al., 2019; Brown   et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Scao   *Equal contribution 1Inria (Flowers), University of Bordeaux,   France 2Hugging Face 3Univ Angers, LERIA, SFR MATHSTIC,   F-49000 Angers, France 4Sorbonne University, ISIR, Paris, France.   Correspondence to: Thomas Carta <thomas.carta@inria.fr>,   Cl\u00b4ement Romac <clement.romac@inria.fr>.   Proceedings of the 40 th International Conference on Machine   Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright   2023 by the author(s).   et al., 2022). Recently, LLMs were shown to capture aspects   of the physical rules in our world, e.g. about space (Patel   & Pavlick, 2022), colors (Abdou et al., 2021) or even affordances between bodies and objects (Ahn et al., 2022). This   form of prior knowledge was exploited to suggest plans of   action to solve goals in robotics (Huang et al., 2022b; Ahn   et al., 2022; Liang et al., 2022). However, LLMs are known   to suffer from a lack of grounding which prevents them from   properly dealing with the meaning of inter-related concepts   and their use for functional competence in interactive environments (Mahowald et al., 2023). Indeed, alignment between statistical structures in such LLMs and environments   can be very limited, or even sometimes entirely wrong. This   is partly due to 1) a training process (predicting next words)   that is not directly incentivized to solve problems in an environment, 2) lack of abilities to intervene in the environment   to identify causal structures; 3) lack in abilities to learn   based on data collected as a result of interacting with the   environment (Bender & Koller, 2020; Bisk et al., 2020).   In the literature, language grounding has referred to various related objectives (Thill et al., 2014). First, symbol   grounding can be formulated as the general problem of   connecting a symbol system (Harnad, 1990), internal to   an agent, to the environment, in such a way that internal   processing of these symbols can be used to to act appropriately in this environment. One dimension of this problem   is associating \u201delementary\u201d symbols, such as the names of   objects, with invariant structures in high-dimensional perceptual modalities such as vision (Cangelosi et al., 2010;   Wiriyathammabhum et al., 2016). Such a grounding, called   \u201ddirect grounding\u201d, has been extensively studied in the past   leading to various efficient methods (Alayrac et al., 2022;   Radford et al., 2021; Lu et al., 2023), included in the context of robotic bodies (Cangelosi & Stramandinoli, 2018).   Another dimension is how to ground higher-order symbolic   tokens, or abstract concepts, into elementary symbols, often   through approaches such as distributional semantics (Harris, 1981; Boleda, 2019). This has been called \u201dgrounding   transfer\u201d (Cangelosi & Stramandinoli, 2018). Beyond such   mere associations, a key question about grounding is how   internal processes that manipulate symbols can model, predict and control external physical and social processes: they   1   Grounding LLMs in Interactive Environments with Online RL   need to be aligned on and constrained by these external   dynamics and relational structures (at various levels of abstraction). This last notion of grounding, which we refer   here as \u201dfunctional grounding\u201d, is relative to a particular   environment which may be the human physical environment   but also more abstract interactive environments simulated in   computers (where abstract physics can differ from human   environments).   In this paper, we consider interactive textual worlds (C\u02c6ot\u00b4e   et al., 2018; Jansen, 2021), which are precisely designed to   focus on these higher-level forms of functional grounding.   In textual worlds, environments can encode rich forms of   physical structures inspired by the ones in the human world,   e.g. (Wang et al., 2022), yet agents act and perceive in these   environments only through the textual modality. In this context, this paper aims to make progress towards the following   largely open question: how could LLMs be used as agent   policies producing actions towards goals in interactive environments, perceiving the outcome of these actions, and   incrementally grounding and updating their knowledge with   the new observations they collect?   Building on recent works successfully using Reinforcement   Learning (RL) to finetune LLMs for natural language generation tasks (Stiennon et al., 2020; Ouyang et al., 2022;   Ramamurthy et al., 2022), we propose the first study about   functional grounding of LLMs through incremental online   RL. In particular, we aim at empirically answering the following open scientific questions:   \u2022 Q1. Sample efficiency How fast can an LLM adapt   and learn to solve various spatial and navigation problems   specified in natural language? How does the use of pretrained knowledge from LLM boosts sample efficiency?   \u2022 Q2. Generalization to new objects: Once functionally   grounded, how can an LLM generalize to various kinds of   changes about objects, yet staying in trained tasks?   \u2022 Q3. Generalization to new tasks: How can such an   interactively trained LLM perform zero-shot generalization   to new tasks? How does generalization depend on the kind   of new tasks?   \u2022 Q4. Impact of online interventions: What is the empirical impact of grounding using online RL with incremental   interactions in comparison with offline Behavioral Cloning   from a dataset of expert trajectories?   To answer these scientific questions in Section 4, we present   a functional grounding method for LLMs (see Figure 1   and Section 3), and transpose the BabyAI environment   (Chevalier-Boisvert et al., 2019) into a textual version. Additionally, we aim to help the RL community further develop   grounding techniques for LLMs in interactive environments   by releasing, in addition of the code of this paper1, a Python   library named Lamorel2 facilitating the use of LLMs at   scale for RL practitioners. While many tools already exist   for LLMs and NLP tasks, moving to an RL setting with   interactive environments requires adaptations (e.g. very frequent need of fast inference to compute action probabilities)   making previous tools not well suited for RL practitioners   (see Section 3.4).   2. Related work   Language-conditioned RL   We position our work in the   Language-conditioned RL setting, where an instructionfollowing agent learns a policy that executes actions in an   interactive environment in order to fulfill a language instruction (Luketina et al., 2019). While several works studied   this setting for various tasks in 2D or 3D environments   (Hermann et al., 2017; Misra et al., 2017; Bahdanau et al.,   2018; Colas et al., 2020; Chevalier-Boisvert et al., 2019),   we here focus on text-only interactions (i.e. performing   textual commands given textual observations) as in Shridhar   et al. (2020). However, our work studies how LLMs can not   only encode this instruction (Hill et al., 2020) but also be   directly used as agent policies choosing actions given the   observation.   Textual environments for RL   Many text-only environments have been used and developed (Jansen, 2021; Wang   et al., 2022). They usually implement high-level text commands along with very large action spaces and complex   dynamics between entities, often aiming to study functional   grounding of abstract policies. While these environments   offer interesting properties, we had to introduce a new one   given the purpose and constraints of our study. Dealing here   with computationally expensive LLMs, we chose to trade   complex action spaces for systematic experiments studying the questions of the introduction. Second, to perform   an in-depth analysis of our functional grounding method,   we focused on lower-level navigation skills in spatial environments (which lacks in most textual environments as the   agent can usually just change room and has direct access   to objects in a room). Moreover, several ablation studies   shown in Appendix B.5 required precise control over the   procedural generation (usually not offered by textual environments). For these reasons, we adapted the BabyAI   platform (Chevalier-Boisvert et al., 2019) into a procedural text-only version that enables decoupling exploration   challenges from perception challenges. Additionally, we   are still able to use BabyAI\u2019s visualization tools to analyze   trajectories (see Figure 1).   1https://github.com/flowersteam/   Grounding_LLMs_with_online_RL   2https://github.com/flowersteam/lamorel   2   Grounding LLMs in Interactive Environments with Online RL   Figure 1. The GLAM method: we use an LLM as agent policy in an interactive textual RL environment (BabyAI-Text) where the   LLM is trained to achieve language goals using online RL (PPO), enabling functional grounding. (a) BabyAI-Text provides a goal   description for the current episode as well as a description of the agent observation and a scalar reward for the current step. (b) At each   step, we gather the goal description and the observation in a prompt sent to our LLM. (c) For each possible action, we use the encoder to   generate a representation of the prompt and compute the conditional probability of tokens composing the action given the prompt. Once   the probability of each action is estimated, we compute a softmax function over these probabilities and sample an action according to this   distribution. That is, the LLM is our agent policy. (d) We use the reward returned by the environment to finetune the LLM using PPO. For   this, we estimate the value of the current observation by adding a value head on top of our LLM. Finally, we backpropagate the gradient   through the LLM (and its value head).   Foundation Models for decision making   Foundation   models trained on massive datasets were shown to exhibit   impressive abilities along with fast adaptation to a wide   range of downstream tasks in vision (Yuan et al., 2021),   language (Devlin et al., 2019; Brown et al., 2020) and crossmodalities (Ramesh et al., 2021; Jiang et al., 2022; Alayrac   et al., 2022). While such abilities have been leveraged   to provide reward to RL agents (Gupta et al., 2022; Fan   et al., 2022), a recent line of work started focusing on using   Foundation Models (and in particular LLMs) to guide agents   policy.   First, SayCan (Ahn et al., 2022), Code as Policies (Liang   et al., 2022) and Inner Monologue (Huang et al., 2022b)   used LLMs as high-level planners in robotics setups. Because their LLM is not directly used as agent policy for   low-level actions and is not grounded using its interactions   with the environment, Ahn et al. (2022) had to use an external affordance function to re-rank the actions proposed   by the LLM. Similarly, Yao et al. (2022) also featured a   closed-loop feedback between an LLM that is the planner   and an agent that is the actor but this time in a textual environment. Expanding on this, Dasgupta et al. (2022) added   a reporter observing the environment and reporting useful   information to the planner. While hinting at the usefulness   of prior knowledge contained in LLMs for embodied tasks,   these works are limited by the absence of grounding.   Second, several works proposed to first finetune LLMs on   expert trajectories before using them in the environment.   Using their ScienceWorld benchmark, Wang et al. (2022)   showed that LLMs finetuned using Behavioral Cloning performed worse than a much smaller and randomly initialized   Deep Q-Network trained using RL supporting the hypothesis that grounding in the environment through direct interactions is crucial. Finally, Reid et al. (2022) reused LLMs to   perform offline RL in non-linguistic environments leveraging the internal structures learned by LLMs but no longer   using words or symbols they were trained to manipulate   (Takagi (2022) investigated how these internal structures   can be relevant for unrelated tasks).   Finally, one may also pretrain a policy using Behavioral   Cloning or offline RL from expert trajectories before finetuning it with interactions with an environment. Related to   our work, the Online Decision Transformer (Zheng et al.,   2022) first uses offline RL to pretrain a transformer model   and eventually finetunes it with online RL. But compared   to our study, it did not use a general Language Modeling   pretraining objective and therefore did not study functional   grounding of language symbols.   Finetuning LLMs with RL   Recent works successfully   leveraged RL to finetune LLMs. RL was used in particular   to improve alignment between generated text and human   3   Grounding LLMs in Interactive Environments with Online RL   p", "conf": "ICML", "year": "2023", "index": 132}, {"title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation   Yapeng Tian1, Di Hu2,3\u2217, Chenliang Xu1\u2217   1University of Rochester, 2 Gaoling School of Arti\ufb01cial Intelligence, Renmin University of China   3 Beijing Key Laboratory of Big Data Management and Analysis Methods   {yapengtian,chenliang.xu}@rochester.edu, dihu@ruc.edu.cn", "abstract": "There are rich synchronized audio and visual events in   our daily life. Inside the events, audio scenes are associated   with the corresponding visual objects; meanwhile, sounding   objects can indicate and help to separate their individual   sounds in the audio track. Based on this observation, in this   paper, we propose a cyclic co-learning (CCoL) paradigm   that can jointly learn sounding object visual grounding and   audio-visual sound separation in a uni\ufb01ed framework. Concretely, we can leverage grounded object-sound relations   to improve the results of sound separation.   Meanwhile,   bene\ufb01ting from discriminative information from separated   sounds, we improve training example sampling for sounding object grounding, which builds a co-learning cycle for   the two tasks and makes them mutually bene\ufb01cial. Extensive experiments show that the proposed framework outperforms the compared recent approaches on both tasks, and   they can bene\ufb01t from each other with our cyclic co-learning.   The source code and pre-trained models are released in   https://github.com/YapengTian/CCOL-CVPR21.   1.", "content": "Seeing and hearing are two of the most important senses   for human perception. Even though the auditory and visual   information may be discrepant, the percept is uni\ufb01ed with   multisensory integration [5]. Such phenomenon is considered to be derived from the characteristics of speci\ufb01c neural cell, as the researchers in cognitive neuroscience found   the superior temporal sulcus in the temporal cortex of the   brain can simultaneously response to visual, auditory, and   tactile signal [18, 40]. Accordingly, we tend to perform as   unconsciously correlating different sounds and their visual   producers, even in a noisy environment. For example, for   a cocktail-party scenario contains multiple sounding and   silent instruments as shown in Fig. 1, we can effortlessly   \ufb01lter out the silent ones and identify different sounding ob\u2217Corresponding authors.   Figure 1. Our model can perform audio-visual joint perception to   simultaneously identify silent and sounding objects and separate   sounds for individual audible objects.   jects, and simultaneously separate the sound for each playing instrument, even faced with a static visual image.   For computational models, the multi-modal sound separation and sounding object alignment capacities re\ufb02ect in   audio-visual sound separation (AVSS) and sound source visual grounding (SSVG), respectively. AVSS aims to separate sounds for individual sound sources with help from   visual information, and SSVG tries to identify objects that   make sounds in visual scenes. These two tasks are primarily explored isolatedly in the literature. Such a disparity   to human perception motivates us to address them in a colearning manner, where we leverage the joint modeling of   the two tasks to discover objects that make sounds and separate their corresponding sounds without using annotations.   Although existing works on AVSS [8, 32, 11, 49, 13, 46,   9] and SSVG [27, 38, 43, 3, 33] are abundant, it is nontrivial to jointly learn the two tasks. Previous AVSS methods implicitly assume that all objects in video frames make   sounds. They learn to directly separate sounds guided by   encoded features from either entire video frames [8, 32, 49]   or detected objects [13] without parsing which are sounding   or not in unlabelled videos. At the same time, the SSVG   methods mostly focus on the simple scenario with single   sound source, barely exploring the realistic cocktail-party   environment [38, 3]. Therefore, these methods blindly use   information from silent objects to guide separation learn2745   ing, also blindly use information from sound separation to   identify sounding objects.   Toward addressing the drawbacks and enabling the colearning of both tasks, we introduce a new sounding objectaware sound separation strategy.   It targets to separate   sounds guided by only sounding objects, where the audiovisual scenario usually consists of multiple sounding and   silent objects. To address this challenging task, the SSVG   can jump in to help identify each sounding object from a   mixture of visual objects, whose objective is unlike previous approaches that make great efforts on improving the   localization precision of sound source in simple audiovisual scenario [38, 43, 3, 32]. Accordingly, it is challenging   to discriminatively discover isolated sounding objects inside scenarios via the predicted audible regions visualized   by heatmaps [38, 3, 20]. For example, two nearby sounding objects might be grouped together in a heatmap and we   have no good principle to extract individual objects from a   single located region.   To enable the co-learning, we propose to directly discover individual sounding objects in visual scenes from   visual object candidates. With the help of grounded objects, we learn sounding object-aware sound separation.   Clearly, a good grounding model can help to mitigate learning noise from silent objects and improve separation. However, causal relationship between the two tasks cannot ensure separation can further enhance grounding, because   they only loosely interacted during sounding object selection. To alleviate the problem, we use separation results   to help sample more reliable training examples for grounding. It makes the co-learning in a cycle and both grounding and separation performance will be improved, as illustrated in Fig. 2. Experimental results show that the two   tasks can be mutually bene\ufb01cial with the proposed cyclic   co-learning, which leads to noticeable performance and outperforms recent methods on sounding object visual grounding and audio-visual sound separation tasks.   The main contributions of this paper are as follows: (1)   We propose to perform sounding object-aware sound separation with the help of visual grounding task, which essentially analyzes the natural but previously ignored cocktailparty audiovisual scenario.   (2) We propose a cyclic colearning framework between AVSS and SSVG to make   these two tasks mutually bene\ufb01cial. (3) Extensive experiments and ablation study validate that our models can outperform recent approaches, and the tasks can bene\ufb01t from   each other with our cyclic co-learning.   2. Related Work   Sound Source Visual Grounding: Sound source visual   grounding aims to identify the visual objects associating   to speci\ufb01c sounds in the daily audiovisual scenario. This   task is closely related to the visual localization problem of   sound source, which targets to \ufb01nd pixels that are associated   with speci\ufb01c sounds. Early works in this \ufb01eld use canonical correlation analysis [27] or mutual information [17, 19]   to detect visual locations that make the sound in terms of   localization or segmentation. Recently, deep audio-visual   models are developed to locate sounding pixels based on   audio-visual embedding similarities [3, 32, 20, 22], crossmodal attention mechanisms [38, 43, 1], vision-to-sound   knowledge transfer [10], and sounding class activation mapping [33]. These learning fashions are capable of predicting audible regions by showing heat-map visualization of   single sound source in the simple scenario, but cannot explicitly detect multiple isolated sounding objects when they   are sounding at the same time. In the most recent work,   Hu et al. [21] propose to discriminatively identify sounding   and silent object in the cocktail-party environment, but relying on reliable visual knowledge of objects learned from   manually selected single source videos. Unlike the previous methods, we focus on \ufb01nding each individual sounding object from cocktail scenario of multiple sounding and   silent objects without any human annotations, and is cooperatively learned with audio-visual sound separation task.   Audio-Visual Sound Separation: Respecting for the longhistory research on sound source separation in signal processing, we only survey recent audio-visual sound source   separation methods [8, 32, 11, 49, 13, 46, 48, 37, 9] in   this section. These approaches separate visually indicated   sounds for different audio sources (e.g., speech in [8, 32],   music instruments in [49, 13, 48, 9], and universal sources   in [11, 37]) with a commonly used mix-and-separate strategy to build training examples [16, 24]. Concretely, they   generate the corresponding sounds w.r.t. the given visual   objects [49] or object motions in the video [48], where the   objects are assumed to be sounding during performing separation. Hence, if a visual object belonging to the training   audio source categories is silent, these models usually fail   to separate a all-zero sound for it. Due to the commonly   existing audio spectrogram overlapping phenomenon, these   methods will introduce artifacts during training and make   wrong predictions during inference. Unlike previous approaches, we propose to perform sounding object-aware audio separation to alleviate the problem. Moreover, sounding   object visual grounding is explored in a uni\ufb01ed framework   with the separation task.   Audio-Visual Video Understanding:   Since auditory   modality containing synchronized scenes as the visual   modality is widely available in videos, it attracts a lot   of interests in recent years. Besides sound source visual   grounding and separation, a range of audio-visual video   understanding tasks including audio-visual action recognition [14, 26, 29], audio-visual event localization [43, 31,   45], audio-visual video parsing [42], cross-modal generation [6, 7, 12, 51, 50, 47], and audio-visual video caption2746   Figure 2. Cyclic Co-learning of sounding object visual grounding and audio-visual sound separation, enabled by sounding object-aware   sound separation. Given detected objects from a video and the sound mixture, our model can recognize whether they are audible via   a grounding network (Audible?) and separate their sounds with an audio-visual sound separation network to help determine potential   sounding and silent sources (Which?).   ing [35, 41, 44] have been explored. Different from these   works, we introduce a cyclic co-learning framework for   both grounding and separation tasks and show that they can   be mutually bene\ufb01cial.   3. Method   3.1. Overview   Given an unlabeled video clip V with the synchronized   sound s(t)2, O = {O1, ..., ON} are N detected objects   in the video frames and the sound mixture is s(t) =   PN   n=1 sn(t). Here, sn(t) is the separated sound of the object On. When it is silent, sn(t) = 0. Our co-learning aims   to recognize each sounding object On and then separate its   sound sn(t) for the object.   The framework, as illustrated in Fig. 2, mainly contains two modules: sounding object visual grounding network and audio-visual sound separation network.   The   sounding object visual grounding network can discover isolated sounding objects from object candidates inside video   frames. We learn the grounding model from sampled positive and negative audio-visual pairs. To learn sound separation in the framework, we adopt a commonly used mixand-separate strategy [13, 16, 49] during training. Given   two training video and sound pairs {V (1), s(1)(t)} and   {V (2), s(2)(t)}, we obtain a mixed sound:   sm(t) = s(1)(t) + s(2)(t) =   N1   X   n=1   s(1)   n (t) +   N2   X   n=1   s(2)   n (t), (1)   and \ufb01nd object candidates O(1) = {O(1)   1 , ..., O(1)   N1} and   O(2) = {O(2)   1 , ..., O(2)   N2} from the two videos. The sounding object visual grounding network will recognize audible   objects from O(1) and O(2) and the audio-visual sound separation network will separate sounds for the grounded objects. Sounds are processed in a Time-Frequency space with   the short-time Fourier transform (STFT).   2s(t) is a time-discrete audio signal.   With the sounding object-aware sound separation, we   can co-learn the two tasks and improve audio-visual sound   separation with the help of sounding object visual grounding. However, the separation performance will highly rely   on the grounding model and the grounding task might not   bene\ufb01t from co-learning training due to the weak feedback   from separation. To simultaneously evolve the both models, we propose a cyclic co-learning strategy as illustrated   in Fig. 3, which has an additional backward process that   utilizes separation results to directly improve training sample mining for sounding object visual grounding. In this   manner, we can make the two tasks mutually bene\ufb01cial.   3.2. Sounding Object Visual Grounding   Videos contain various sounds and visual objects, and   not all objects are audible.   To \ufb01nd sounding objects in   videos V (1) and V (2) and further utilize grounding results   for separation, we formulate sounding object visual grounding as a binary matching problem.   Sounding Object Candidates: To better support the audiovisual matching problem, we choose to follow the widelyadopted image representation strategy of visual object proposal in image captioning [25, 2], which has been also employed in the recent work on audio-visual learning [13].   Concretely, the potential audible visual objects are \ufb01rst proposed from videos using an object detector.   In our implementation, we use the Faster R-CNN [36] object detector trained on Open Images dataset [30] from [13] to detect objects from video frames in V (1) and V (2) and obtain O(1) = {O(1)   1 , ..., O(1)   N1} and O(2) = {O(2)   1 , ..., O(2)   N2}.   Next, we learn to recognize sounding objects in O(1) and   O(2) associated with s(1)(t) and s(2)(t), respectively. For   simplicity, we use an object O and a sound s(t) as an example to illustrate our grounding network.   Audio Network: Raw waveform s(t) is transformed to an   audio spectrogram S with the STFT. An VGG [39]-like 2D   CNN network: VGGish followed by a global max pooling   (GMP) is used to extract an audio embedding fs from S.   Visual Network: The visual network extracts features from   2747   detected visual object O. We use the pre-trained ResNet18 [15] model before the last fully-connected layer to extract a visual feature map and perform a GMP to obtain a   visual feature vector fo for O.   Grounding Module: The audio-visual grounding module   takes audio feature fs and visual object feature fo as inputs   to predict whether the visual object O is one of the sounding   makers for s(t). We solve it using a two-class classi\ufb01cation   network. It \ufb01rst concatenates fs and fo and then uses a 3layer Multi-Layer Perceptron (MLP) with a Softmax function to output a probability score g(s(t), O) \u2208R2. Here, if   g(s(t), O)[0] >= 0.5, {s(t), O} is a positive pair and s(t)   and O are matched; otherwise, O is not a sound source.   Training and Inference:   To train the sounding object visual grounding network, we need to sample positive/matched and negative/mismatched audio and visual object pairs. It is straightforward to obtain negative pairs with   composing audio and objects from different videos. For example, s(1)(t) from V (1) and an randomly selected object   O(2)   r   from V (2) can serve as a negative pair. However, positive audio-visual pairs are hard to extract since not all objects are audible in videos. If an object from V (1) is not   audio source, the object and s(1)(t) will be a negative pair,   even though they are from the same video. To address the   problem, we cast the positive sample mining as a multiple   instance learning problem and sample the most con\ufb01dent   pair as a positive sample with a grounding loss as the measurement:   \u02c6n = argmin   n f(g(s(1)(t), O(1)   n ), ypos),   (2)   where f(\u00b7) is a cross-entropy function; ypos = [1, 0] is an   one-hot encoding for positive pairs; O(1)   \u02c6n   and s(1) will be   the positive audio-visual pair for training. With the sampled   negative and positive data, we can de\ufb01ne the loss function   to learn the sounding object visual grounding:   lgrds = f(g(s(1)(t), O(2)   r ), yneg)+f(g(s(1)(t), O(1)   \u02c6n ), ypos),   (3)   where yneg = [0, 1] is the negative label.   The visual   grounding network can be end-to-end optimized with sampled training pairs via lgrds.   During inference,   we can feed audio-visual pairs   {O(1)   i   , s(1)(t)}N1   i=1 and {O(2)   i   , s(2)(t)}N2   i=1 into the trained   model to \ufb01nd sounding objects insides the two videos. To   facilitate audio-visual sound separation, we need to detect   sounding objects from the sound mixture sm(t) rather than   s(1)(t) and s(2)(t), since the individual sounds are unavailable at a testing stage for separation task.   3.3. Sounding Object-Aware Separation   Given detected objects in O(1) and O(2), we separate   sounds for each object from the sound mixture sm(t) and   mute separated sounds of silent objects.   Sounding Object   Visual Grounding   Audio-Visual   Sound Separation   Forward: sounding object-aware separation   Backward: grounding example sampling   Figure 3. Cyclic co-learning. Facilitated by sounding object visual   grounding, our model can employ sounding object-aware sound   separation to improve separation. Meanwhile, separation results   can help to do effective training sample mining for grounding.   Using an audio-visual sound separation network, we   can predict sound spectrograms {S(1)   n }N1   n=1 and {S(2)   n }N2   n=1   for objects in O(1) and O(2), respectively.   According to waveform relationship in Eq. 1, we can approximate spectrogram magnitude relationship as:   S(1)   \u2248   PN1   n=1 S(1)   n   and   S(2) \u2248PN2   n=1 S(2)   n . To learn the separation network, we can optimize it with a L1 loss function:   lsep = ||S(1) \u2212   N1   X   n=1   S(1)   n ||1 + ||S(2) \u2212   N2   X   n=1   S(2)   n ||1.   (4)   However, not all objects are audible and spectrograms from   different objects contain overlapping content. Therefore,   even an object O(1)   n   is not sounding, it can also separate   non-zero sound spectrogram S(1)   n   from the spectrogram of   sound mixture Sm, which will introduce errors during training. To address the problem, we propose a sounding objectaware separation loss function:   l\u2217   sep = ||S(1) \u2212   N1   X   n=1   g\u2217(sm(t), O(1)   n )S(1)   n ||1   + ||S(2) \u2212   N2   X   n=1   g\u2217(sm(t), O(2)   n )S(2)   n ||1,   (5)   where g\u2217(\u00b7) is a binarized value of g(\u00b7)[0]. If an object O(1)   n   is not a sound source, g\u2217(sm(t), O(1)   n ) will be equal to zero.   Thus, the sounding object-aware separation can help to reduce training errors from silent objects in Eq. 4.   In addition, we introduce additional grounding loss   terms to guide the grounding model learning from the sound   mixture. Since we have no sounding object annotations, we   adopt a similar positive sample mining strategy as in Eq. 2   and de\ufb01ne a grounding loss as follows:   lgrdm =   2   X   k=1   min   n f(g(sm(t), O(k)   n ), ypos).   (6)   3.4. Co-learning in a Cycle   Combing grounding and separation loss terms, we can   learn the two tasks in a uni\ufb01ed way with a co-learning objective function: lcol = lgrds + l\u2217   sep + lgrdm.   2748   Although our sounding object visual grounding and   audio-visual sound separation models can be learned together, the two tasks loosely interact in l\u2217   sep. Clearly, a good   grounding network can help improve the separation task.   However, the grounding task might not be able to bene\ufb01t   from co-learning training since there is no strong feedback   from separation to guide learning the grounding model. To   further strengthen the interaction between the two tasks, we   propose a cyclic co-learning strategy, which can make them   bene\ufb01t from each other.   If an object O(k)   n   makes sound in video V (k), the separated spectrogram S(k)   n   should be close to S(k); otherwise,   the difference between S(k)   n   and S(k) should be larger than   a separated sound spectrogram from an sounding object   and S(k). We use L1 distance to measure dissimilarity of   spectrograms: d(k)   n   = ||S(k)   n   \u2212S(k)||1, where d(k)   n   will be   small for a sounding object O(k)   n . Based on the observation, we select the object O(k)   n   with the minimum d(k)   n   make   the dominant sound in V k to compose positive samples for   sounding object visual grounding. Let \u02c6n1 = argminn d(1)   n   and \u02c6n2 = argminn d(2)   n . We can re-formulate grounding   loss terms as:   l\u2217   grds = f(g(s(1)(t), O(2)   r ), yneg) + f(g(s(1)(t), O(1)   \u02c6n1 ), ypos),   l\u2217   grdm = f(g(sm(t), O(1)   \u02c6n1 ), ypos) + f(g(sm(t), O(2)   \u02c6n2 ), ypos).   In addition, if d(k)   n   is very large, the object O(k)   n   is   very likely not be audible, which can help us sample   potential negative examples for mixed sound grounding.   Speci\ufb01cally, we select the objects that are associated with   the largest d(k)   n , and d(k)   n   must be larger than a threshold \u01eb.   Let n\u2217   1   = argmaxn d(1)   n , s.t.   d(1)   n   > \u01eb and   n\u2217   2 = argmaxn d(2)   n , s.t. d(2)   n   > \u01eb. We can update l\u2217   grdm   with learning from potential negative samples: \u02c6l\u2217   grdm =   P2   k=1(f(g(sm(t), O(k)   \u02c6nk ), ypos)+f(g(sm(t), O(k)   n\u2217   k ), yneg)).   Finally, we can co-learn the two tasks in a cycle with optimizing the joint cyclic co-learning loss function: lccol =   l\u2217   grds + l\u2217   sep + \u02c6l\u2217   grdm. Inside cyclic co-learning as illustrated   in Fig. 3, we use visual grounding to improve sound separation and enhance visual grounding based on feedback from   sound separation. The learning strategy can make the tasks   help each other in a cycle and signi\ufb01cantly improve performance for both tasks.   4. Experiments   4.1. Experimental Setting   Dataset: In our experiments, 520 online available musical   solo videos from the widely-used MIT MUSIC dataset [49]   is used. The dataset includes 11 musical instrument categories: accordion, acoustic guitar, cello, clarinet, erhu,   ute, saxophone, trumpet, tuba, violin, and xylophone.   Methods   OTS [3]   DMC [20]   Grounding only   CoL   CCoL   Single Sound   58.7   65.3   72.0   67.0   84.5   Mixed Sound   51.8   52.6   61.4   58.2   75.9   Table 1. Sounding object visual grounding performance (%). Top2 results are highlighted.   The dataset is relatively clean and sounding instruments   are usually visible in videos.   We split it into training/validation/testing sets, which have 468/26/26 videos   from different categories, respectively. To train and test our   cyclic co-learning model, we randomly select three other   videos for each video to compose training and testing samples. Let\u2019s denote the four videos as A, B, C, D. We compose A, B together as V (1) and C, D together as V (2), while   sounds of V (1) and V (2) are only from A and C, respectively. Thus, objects from B and D in the composed samples   are inaudible. Finally, we have 18,720/260/260 composed   samples in our training/val/test sets for the two tasks.   Evaluation Metrics: For sounding object visual grounding, we feeding detected audible and silent objects in videos   into different grounding models and evaluate their binary   classi\ufb01cation accuracy. We use the mir eval library [34]   to measure sound separation performance in terms of two   standard metrics: Signal-to-Distortion Ration (SDR) and   Signal-to-Interference Ratio (SIR).   Implementation Details: We sub-sample audio signals at   11kHz, and each video sample is approximately 6 seconds.   The STFT is calculated using a Hann window size of 1022   and a hop length of 256 and each 1D audio waveform is   transformed to a 512 \u00d7 256 Time- Frequency spectrogram.   Then, it is re-sampled to T, F = 256.   The video frame   rate is set as 1fps and we randomly select 3 frames per 6s   video. Objects extracted from video frames are resized to   256\u00d7256 and then randomly cropped to 224\u00d7224 as inputs   to our network. \u01eb is set to 0.1. We use a soft sound spectrogram masking strategy as in [13, 49] to generate individual sounds from audio mixtures and adopt a audio-visual   sound separation network from [49]. More details about   the network can be found in our appendix. Our networks   are optimized by Adam [28]. Since the sounding object visual grounding and audio-visual sound separation tasks are   mutually related, we need to learn good initial models for   making them bene\ufb01t from cyclic co-learning. To this end,   we learn our CCoL model with three steps in a curriculum   learning [4] manner. Firstly, we train the sounding object   visual grounding network with lgrds. Secondly, we co-learn   the grounding network initialized with pre-trained weights   and the separation network optimized by lcol. Thirdly, we   use lccol to further \ufb01ne-tune the two models.   4.2. Sounding Object Visual Grounding   We compare our methods to two recent methods:   OTS [3] and DMC [20]. In addition, we make an ablation   2749   DMC   CCoL   Figure 4. Qualitative results of sounding object visual grounding for both audible and silent objects. We use two icons to denote whether   objects in video frames are audible or not and grounded sounding objects from DMC and CCoL are shown in green boxes. Our CCoL   model can effectively identify both sounding and silent objects, while the DMC fails. Note that 2-sound mixtures are used as inputs.   Erhu + Violin   Guitar (   )   SoP   CoSep   Ours   GT   Erhu (   )   Figure 5. Qualitative results of audio-visual sound separation for both audible and silent objects. Our CCoL model can well mitigate   learning noise from silent objects during training and generate more accurate sounds.   study to investigate the proposed models. The Grounding   only model is trained only with grounding losses: lgrds; the   co-learning (CoL) model jointly learn visual grounding and   sound separation using the lcol; and the cyclic co-learning   (CCoL) further strengthens the interaction between the two   tasks optimized via lccol. We evaluate sounding object visual grounding performance on both solo and mixed sounds.   Table 1 and Figure 4 show quantitative and qualitative sounding object visual grounding results,respectively.   Even our grounding only has already outperformed the OTS   and DMC, which can validate the effectiveness of the proposed positive sample mining approach. Then, we can see   that the CoL with jointly learning grounding and separation achieves worse performance than the Grounding only   model. It demonstrates that the weak interaction inside CoL   cannot let the grounding task bene\ufb01t from the separation   task. However, using separation results to help the grounding example sampling, our CCoL is signi\ufb01cantly superior   over both Grounding only and CoL models. The results can   demonstrate the sounding object visual grounding can bene\ufb01t from separation with our cyclic learning.   Methods   RPCA [23]   SoP [49]   CoSep [13]   Random Obj   CoL   CCoL   Oracle   SDR   -0.48   3.42   2.04   4.20   6.50   7.27   7.71   SIR   3.13   4.98   6.21   6.90   11.81   12.77   11.42   Table 2. Audio-visual sound separation performance. Top-2 results are highlighted.   4.3. Audio-Visual Sound Separation   To demonstrate the effectiveness of our CCoL framework on audio-visual sound separation, we compare it to   a classical factorization-based method: RPCA [23] and two   recent state-of-the-art methods: SoP [49], CoSep [13], and   baselines: Random Obj and CoL in Tab. 2. The SoM [48]   and Music Gesture [9] address music sound separation by   incorporating dynamic visual motions, and show promising   results. However, as the SoP and CoSep, they also did not   consider the silent object problem. Meanwhile, since there   are no source code for the two appraoches, we will not include them into comparison. Note that SoP and CoSep are   trained using source code provided by the authors and the   same training data (including audio preprocessing) as ours.   Moreover, we show separation results of an Oracle, which   feeds ground truth grounding labels of mixed sounds to train   the audio-visual separation network.   2750   mixture   Ours   SoP   sounding object visual grounding   audio-visual sound separation   Figure 6. Real-world sounding object visual grounding and audio-visual sound separation. The guitar and violin are playing and the \ufb02ute is   visible but not audible. Our model can identify sounding objects: guitar and violin and silent object: \ufb02ute. Moreover, it can simultaneously   separate individual sounds for each instrument, while the SoP using the same noisy training data as ours fails to associate objects with the   corresponding sounds, thus obtains poor separation results.   We can see that our CoL outperforms the compared SoP,   CoSep, and Random Obj, and CCoL is better than CoL.   The results demonstrate that sounding object visual grounding in the co-learning can help to mitigate training errors   from silent video objects in separation, and separation performance can be further improved with the help of enhanced   grounding model by cyclic co-learning. Compared to the   Oracle model, it is reasonable to see that CCoL has slightly   lower SDR. A surprising observation is that CoL and CCoL   achieve better results in terms of SIR. One possible reason is   that our separation networks can explore various visual objects as inputs during joint grounding and separation learning, which might make the models more robust on SIR.   Moreover, we illustrate quantitatively results for testing   videos with both audible and silent objects in Tab. 3. Both   SoP and CoSep are blind to whether a visual object makes   sound and they will generate non-zero audio waveform for   silent objects, and the Random Obj is limited in identifying object silent and sounding objects, thus they will have   even lower SDRs and SIRs. However, both CoL and CCoL   are capable of recognizing the audibility of objects and employ sounding object-aware separation, which helps the two   models achieve signi\ufb01cant better results. The experimental results can well validate the superiority of the proposed   sounding object-aware separation mechanism.   We further show qualitative separation results for audible   and silent objects in Fig. 5. We can see that both SoP and   CoSep generate nonzero spectrograms for the silent Guitar   and our CCoL can separate much better Erhu sound. The   results can validate that the CCoL model is more robust to   learning noise from silent objects during training and can effectively perform sounding object-aware sound separation.   Moreover, we train our model by letting the video \u2019B\u2019   and \u2019D\u2019 be randomly chosen from \u201cwith audio\u201d and \u201csilent\u201d.   Methods   SoP [49]   CoSep [13]   Random Obj   CoL   CCoL   GT   SDR   -11.35   -15.11   -11.34   14.78   91.07   264.44   SIR   -10.40   -12.81   -9.09   15.68   82.82   260.35   Table 3. Audio-visual sound separation performance (with silent   objects). To help readers better appreciate the results, we include   SDR and SIR from ground truth sounds.   In this way, each training video contains one or two sounding objects and the corresponding mixed sounds will be   from up to four different sources.   The SDR/SIR scores   from SoP and CoL, and CCoL are 2.73/4.08, 5.79/11.43,   and 6.46/11.72, respectively. The results further validate   that the proposed cyclic co-learning framework can learn   from both solo and duet videos and is superior over naive   co-learning model.   From the sounding object visual grounding and audiovisual sound separation results, we can conclude that our   cyclic co-learning framework can make the two tasks bene\ufb01t from each other and signi\ufb01cantly improve both visual   grounding and sound separation performance.   4.4. Real-World Grounding and Separation   Besides synthetic data, our grounding and separation   models can also handle real-world videos, in which multiple audible and silent objects might exist in single video   frames. The example shown in Fig. 6 consists of three different instruments: guitar, \ufb02ute, and violin in the video, in   which guitar and violin are playing and making sounds. We   can see that our grounding model can successfully identify the sounding objects: guitar and violin and silent object: \ufb02ute. Meanwhile, our sounding object-aware separation model can separate individual sounds for each music   instrument from the sound mixture. However, the SoP using the same noisy training data as our method obtains poor   separation results because it is limited in associating objects   2751   mixture   Ours   SoP   audio-visual   sound separation   sounding object visual grounding   Figure 7. Real-world sounding object visual grounding and audiovisual sound separation for a challenging outdoor video with a 4sound mixture and \ufb01ve instruments. From left to right, the instruments are saxophone, accordion, trombone, clarinet and tuba.   Our grounding network can successfully \ufb01nd audible and silent   objects, and our separation network can separate the trombone music, meanwhile suppressing unrelated sounds from other playing   instruments. However, the separated sound by SoP contains much   stronger noisy sounds from the accordion and tuba. Note that the   trombone is an unseen category, and we have no 4-sound mixture   during training. The example can demonstrate the generalization   capacity of our method.   with corresponding sounds.   Another more challenging example is illustrated in   Fig. 7. There are \ufb01ve different instruments in the video, and   four of them are playing together. Although our grounding   and separation networks are not trained with 4-sound mixtures, our sounding object visual grounding network can accurately \ufb01nd audible and silent objects in the video, and our   audio-visual separation network can separate the trombone   sound and is more capable of suppressing unrelated sounds   from other three playing instruments than the SoP.   From the above results, we learn that our separation and   grounding models trained using the synthetic data can be   generalized to handle real-world challenging videos. In addition, the results further demonstrate that our models can   effectively identify audible and silent objects and automatically discover the association between objects and individual sounds without relying on human annotations.   5. Limitation and Discussion   Our sounding object visual grounding model \ufb01rst processes video frames and obtains object candidates by a   Faster R-CNN [36] object detector trained on a subset of   Open Images dataset [30]. As stated in Sec. 3.2, such a   strategy of using visual object proposal has been widely   employed in image captioning [25, 2] and recent work on   audio-visual learning [13]. Intuitively, similar to these previous works [25, 2, 13], the grounding model performance   also highly relies on the quality of the object detector. If a   sounding object is not detected by the detector, our model   will fail to ground it. Thus, an accurate and effective object detector is very important for the grounding model. At   present, one possible way to address this limitation could be   performing dense proposal of object candidates to prevent   the above problem to some extent.   6. Conclusion and Future Work   In this paper, we introduce a cyclic co-learning framework that can jointly learn sounding object visual grounding and audio-visual sound separation. With the help of   sounding object visual grounding, we propose to perform   sounding object-aware sound separation to improve audiovisual sound separation. To further facilitate sounding object visual grounding learning, we use the separation model   to help training sampling mining, which makes the learning   process of the two tasks in a cycle and can simultaneously   enhance both grounding and separation performance. Extensive experiments can validate that the two different problems are highly coherent, and they can bene\ufb01t from each   other with our cyclic co-learning, and the proposed model   can achieve noticeable performance on both sounding object visual grounding and audio-visual sound separation.   There is various audio and visual content with different   modality compositions in real-world videos. Besides the   silent object issue, there are other challenging cases that affect audio-visual learning, as discussed below.   There might be multiple instances of the same instrument in a video. Our separation model mainly uses encoded   visual semantic information to separate sounds; however,   the visual dynamics that can provide additional discriminative information to identify different instances for the same   instrument are not exploited. In the future, we will consider how to incorporate dynamic visual information to further strengthen our model\u2019s ability on separating multiple   sounds of the same types as in [48, 9].   Sound sources are not always visible in videos. For example, guitar sound might only be background music. In   this case, there are no corresponding visual objects that can   be used as conditions to separate the guitar sound for existing audio-visual sound separation methods. To address this   problem, we might need to \ufb01rst parse input sound mixtures   and video frames to recognize invisible sounds and then   use other reliable conditions (e.g., retrieved video frames   or other semantic information) to separate the sounds.   Acknowledgement: Y. Tian and C. Xu were supported   by NSF 1741472, 1813709, and 1909912.   D. Hu was   supported by Fundamental Research Funds for the Central   Universities, the Research Funds of Renmin University of   China (NO. 21XNLG17), the Beijing Outstanding Young   Scientist Program NO. BJJWZYJH012019100020098 and   the 2021 Tencent AI Lab Rhino-Bird Focused Research   Program (NO. JR202141). The article solely re\ufb02ects the   opinions and conclusions of its authors but not the funding   agents.   2752", "conf": "CVPR", "year": "2021", "index": 127}, {"title": "GROUNDHOG   : Grounding Large Language Models to Holistic Segmentation   Yichi Zhang1\u2020, Ziqiao Ma1\u2020, Xiaofeng Gao2, Suhaila Shakiah2, Qiaozi Gao2, Joyce Chai1   1University of Michigan, 2Amazon AGI   zhangyic@umich.edu   https://groundhog-mllm.github.io/   Describe the image briefly.   A man and a little girl are    sitting in a shopping cart.   \ud83d\udc64   Generate a short caption.   Two dogs are playing with    a stick in a field.   \ud83d\udc64   Describe the given    picture in very detail.    In this image, we can see a    boat on the water. There    are few people inside the    boat. There are also few    people on the rocks. In the    background, there are    trees, hills, and sky.   \ud83d\udc64   (a) Grounded Image Captioning (GIC).   Provide a distinct description for    that <PTR>.   Blue container with apples.   \ud83d\udc64   <PTR>   Could you please segment out \"laptop    barely onscreen not apple\" in the image?    Laptop barely onscreen not apple.   Feb 2.   \ud83d\udc64   What date is shown on the    calendar?   \ud83d\udc64   (d) Referential Dialogue (RD).   (b) Referential Expression Segmentation (RES).   (c) Grounded Visual Question Answering (GVQA).   Figure 1. We propose GROUNDHOG, a multimodal large language model that enhances its text output with pixel-level phrase grounding   across diverse semantic granularities. The figure demonstrates outputs from our model on the four task types we considered in this work.", "abstract": "Most multimodal large language models (MLLMs)   learn language-to-object grounding through causal language modeling where grounded objects are captured by   bounding boxes as sequences of location tokens.   This   paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis.   In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic   segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual   entity tokens for the MLLM backbone, which then con\u2020 Work done during internship at Amazon AGI.   nects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained   Grounding, by harvesting a collection of segmentationgrounded datasets with rich annotations. Our experimental   results show that GROUNDHOG achieves superior performance on various language grounding tasks without taskspecific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides   easy-to-understand diagnosis in failure cases.   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   14227   1.", "content": "Multimodal large language models (MLLMs) have received an increasing amount of attention to address tasks   that necessitate non-linguistic knowledge, e.g., perception   and reasoning about the visual world [39, 84]. For finegrained visual understanding, grounded MLLMs often learn   language-to-object grounding by causal language modeling,   where grounded objects are captured by bounding boxes as   sequences of location tokens. However, bounding boxes   are insufficient in indicating amorphous stuff [5], semantic parts of objects [23], finer-grained regions with irregular   shapes [26], or groups of instances at the same time. As a   result, a single bounding box can often include other irrelevant semantics in order to engulf the target entities, leading to ambiguity in detection. In addition, the generated   box coordinate lacks interpretability. When the model hallucinates, such as incorrectly predicting the association between objects and language, it is hard to diagnose whether   the problem is due to the model\u2019s failure to detect the object,   or its incorrect alignment of the object with language.   To address these issues, in this work, we introduce   GROUNDHOG, an MLLM developed by grounding Large   Language Models to holistic segmentation. Our goal of language grounding is to connect text spans that refer to or   can be deduced from visual information, termed as groundable phrases [50], to their corresponding regions of visual   entities (Figure 1). GROUNDHOG incorporates a masked   feature extractor that takes an input image and a set of   class-agnostic entity mask proposals, and converts each   mask\u2019s features into visual entity tokens for an MLLM   backbone. This MLLM then connects groundable phrases   to unified grounding masks by retrieving and merging the   entity masks.   Compared to previous grounded MLLMs,   GROUNDHOG unlocks unprecedented pixel-level visionlanguage alignment.   It naturally supports visual pointers as input, and can plug-in-and-play with any choice of   mask proposal networks, e.g., Segment Anything Model   (SAM) [35], domain-specific semantic segmentation models, or user-provided mask candidates. We introduce an enhanced Mask2Former [10] as our default mask proposal network, which detects regions at multiple granularities, e.g.,   instances (things and stuff), semantic parts, and visual text,   leading to a holistic coverage of visual semantics.   To train GROUNDHOG, we curated a Multi-Modal MultiGrained Grounding (M3G2) dataset consisting of 2.5M   text-image pairs for visually grounded instruction tuning,   consisting of 36 sub-problems derived and augmented from   27 existing datasets.   We present extensive experiments   on vision-language tasks that require grounding, including   grounded language generation with minimal object hallucination, language-guided segmentation, visual question answering with answer grounding, and referential dialog with   spatial pointer inputs (Figure 1). Our empirical results show   that GROUNDHOG, without task-specific fine-tuning, can   achieve superior or comparable performance with previous   models that either require fine-tuning or are specialized only   for that dataset. In addition, GROUNDHOG has supports   easy-to-understand diagnosis when grounding fails.   2. Our Method: GROUNDHOG   The language grounding task can be succinctly delineated   into two fundamental components: localization and recognition, as established in the literature [50, 68, 92]. Such   categorization not only aids in the identification of object presence (objectness) without reliance on specific object classes, but also sets the stage for models to be robust in open-vocabulary settings. Building upon this framework, we formulate the grounding process as an entity segment selection problem, which involves (1) proposing entity segmentation masks where the masks encapsulate regions with discernible semantic content, and (2) recognizing the retrieved entities through the understanding of both   visual and language context. Concurrently performing both   tasks is where MLLMs bring a distinct advantage. This   decoupled design of entity mask proposal and languageguided grounding brings several advantages. First, it allows independent improvement of the mask proposal model   and MLLM, where specialized data, training, and inference setups can be applied. Second, by decoupling language grounding, it becomes straightforward to determine   if a failure is due to the model\u2019s inability to propose the   entity segment, or its misalignment of the object with the   language, thus improving the interpretability of the whole   framework. Third, as shown later, when connecting the two   parts to work in tandem in a model-independent manner, the   MLLM can benefit from multiple different vision specialist   models in a plug-and-play fashion. In the remainder of this   section, we give details of our model design.   2.1. Building Entity Features from Masks   Our approach assumes the availability of a mask proposal   model, which is capable of generating a set of classagnostic entity masks from an image with high coverage.   In contrast to prior studies that relied on low-level features [8, 13, 45, 56], GROUNDHOG interprets the image as a   collection of entities. The primary challenge then becomes   the derivation of effective visual features to accurately represent these entities. To achieve a complete decoupling of   the MLLM from the mask proposal model responsible for   providing the masks, we propose to condition the entity features solely on the binary masks without using any embeddings from the mask proposal model. Specifically, the mask   corresponding to each entity is employed to extract patch   features from pretrained vision foundation models, such as   CLIP [62] and DINOv2 [54], through a convolutional mask   pooling layer [12]. Given that the feature map dimensions   14228   beach   Multi-Modal Language Model   I      see  <GRD> two  dogs </GRD> on  <GRD> the   Grounding Mask   LM Head   Next Token   Visual Entity Tokens   Class-Agnostic Masks   Merge   Input Image   Masked    Feature    Extractor   CLIP   MLP   MLP   Down   Sampling   DINO v2   Mask    Pooling   Mask    Pooling   Mask Retrieval Head   Mask Proposal    Model   Figure 2. The model architecture of GROUNDHOG model. Given a set of class-agnostic entity mask proposals, the masked feature extractor   first extracts the feature of each entity as the visual input of the multi-modal large language model (left). The output hidden states of the   grounding tokens are averaged and used to retrieve the entities to ground, which will be merged into a single grounding mask for the phrase.   Modules are colored by their trainability: parameter-free operators (grey), frozen (blue), trainable (orange), and partially trainable (mix).   SAM   Multi-Modal Language Model   What    is   that <PTR> ?   A black dog \u2026   Masked Feature    Extractor   Figure 3. GROUNDHOG can take arbitrary spatial prompts that can   be resolved by an interactive segmentation model, such as SAM.   The placeholder pointer token <PTR> will be replaced by the extracted entity features and fed as input to the model.   are usually smaller than those of the mask proposals, we resize the masks to match the size of the feature maps prior to   pooling. The pooled features are then fed into a Multi-Layer   Perceptron (MLP) network to align with the input embeddings of the MLLM. We empirically find the combination   of CLIP and DINOv2 features yields the best result, and   these features are added to obtain the final input visual entity tokens to the MLLM.   Spatial Prompts   Furthermore, for grounded MLLMs to   be more broadly applicable, they must be capable of interpreting multi-modal user inputs, including spatial prompts.   Thanks to the mask model agnostic design, GROUNDHOGcan seamlessly support such inputs. As demonstrated   in Figure 3, by applying an interactive segmentation model   such as Segment-Anything (SAM) [35], arbitrary spatial   prompts can be translated into binary masks and processed   by the same masked feature extractor we just introduced.   This extracted feature for the pointed entity will replace the   pointer token <PTR> placeholder in the textual input.   2.2. Language Grounding to Entity Segmentation   Existing box-grounded MLLMs typically append location   tokens after the groundable phrases [8, 9, 56, 85]. However,   this method is not readily interpretable. To alleviate this disconnect, we introduce a pair of grounding tokens <GRD>   and </GRD> to indicate the start and end of groundable   phrases, with the assumption that grounding these phrases   requires mapping to certain representations of visual entities irrespective of the visual modality. In Figure 2, a sentence can be represented as I see <GRD> two dogs   </GRD> on <GRD> the beach </GRD>, with two   distinct visual entities grounded. The representation of each   groundable phrase, termed as the grounding query, is obtained by adding <GRD> and </GRD>\u2019s output embedding   from the last transformer layer of the MLLM. The representation is then used to retrieve the entities that the phrase   should be grounded to. In particular, we concatenate the   grounding query with the last layer output of each visual   entity token, and use an MLP to predict a scalar score for   each entity. Finally, we merge all the mask proposals into   one single mask with pixel-wise maximization:   Mh,w = max   q   \u0010   Sq \u00b7 c   Mq,h,w   \u0011   where Sq is the normalized score of the q-th mask ranging   from 0 to 1, and c   Mq,h,w denotes the pixel probability at   position (h, w) for the q-th mask. Note that a phrase may   ground to multiple entities, thus multiple mask proposals   may get a high score simultaneously and be selected in conjunction. One of the primary benefits of this decoupled design is its transparency in the selection of entities. Users can   easily visualize both the mask proposals and their respective   14229   scores, providing a clear understanding of how a grounding   mask is predicted. This level of clarity and interpretability   is a significant advantage, offering users a tangible insight   into the model\u2019s grounding process.   2.3. Towards Holistic Entity Mask Proposals   In order to support holistic language grounding to arbitrary   segmentations, the entity proposal should have two essential properties. First, the proposals should strike a delicate   balance in terms of semantic atomicity. While it is possible to merge multiple proposals later to form multi-entity   segmentations, the reverse, i.e., dividing a single proposal   into smaller segments, is not feasible. Therefore, instance   segmentation is generally preferred over semantic segmentation.   However, the segmentation should not be excessively fine-grained to the extent that it compromises basic   semantic integrity. Over-segmentation can lead to a loss of   the coherent concept of an entity, which is detrimental to   the grounding process. Second, the entity proposals should   have a high coverage of entities, encompassing a diverse   range of granularities. This includes not only tangible objects (things) and amorphous concepts (stuff) but also extends to sub-components of objects (parts of things) and   structured regions such as areas containing visual text. The   ability to propose entities across this spectrum of granularity is pivotal, as it directly determines the upper bound of   the grounding capability of MLLM.   We initiated our study with a Mask2Former model pretrained on the COCO panoptic segmentation dataset. However, preliminary experiments revealed its limitations in   semantic coverage and adaptability to open-world scenarios.   To enhance this, we developed Mask2Former+, an   upgraded version designed for multi-grained segmentation.   This upgrade involved creating a diverse dataset by merging annotations from various sources, including COCO [5],   LVIS [25], Entity-v2 [60], Pascal [16], PACO [63] (Figure 7); MHP-v2 [40] for human part parsing; and TextOCR [67] for text segmentation. We expanded the model\u2019s   capabilities by adding 50 expert queries each for semantic parts and visual text regions, alongside the original 200   entity queries. We assessed Mask2Former+\u2019s performance   on 1000 images from validation splits from 4 grounding   benchmarks, RefCOCO+ [86], PhraseCut [76], ReasonSeg [37], and TextVQA-X [66]. We use the Any-IoU [30]   metric for evaluation, i.e., for each ground truth mask, we   extract the most overlapped mask proposals and compute   the IoU, then take the average. As Table 1 demonstrates,   Mask2Former+ shows consistent improvements across all   domains, particularly in those significantly divergent from   COCO. This highlights its enhanced adaptability and precision in a broader range of segmentation challenges, providing a good mask proposal model for GROUNDHOG. We   refer to Appendix A for more details of the model and data.   Model   RefCOCO+   PhraseCut   ReasonSeg   TextVQA-X   Mask2Former   0.867   0.563   0.602   0.137   Mask2Former+   0.873   0.624   0.745   0.446   Table 1. The average Any-IoU of the proposals on each dataset.   The vanilla Mask2Former is trained on the COCO-Panoptic   dataset and our Mask2Former+ is trained on our combined dataset.   Mask2Former+ obtains a consistent improvement in all scenarios,   especially in non-COCO domains.   Task   Dataset   Gr. Ann.   Sem. Gran.   # Pairs   M   B   Po   S   Th   Pa   G   Tx   Train   GCAP   PNG   \u2713   \u2713   \u2713   \u2713   \u2713   132k   Flickr30K-Entity   \u2713   \u2713   \u2713   \u2713   \u2713   149k   RES   RefCOCO   \u2713   \u2713   \u2713   113k   RefCOCO+   \u2713   \u2713   \u2713   112k   RefCOCOg   \u2713   \u2713   \u2713   80k   RefCLEF   \u2713   \u2713   \u2713   105k   gRefCOCO   \u2713   \u2713   \u2713   194k   PhraseCut   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   85k   D-Cube   \u2713   \u2713   \u2713   \u2713   10k   ReasonSeg   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   1k   RIO   \u2713   \u2713   \u2713   \u2713   28k   SK-VG   \u2713   \u2713   23k   GVQA   VizWiz-G   \u2713   \u2713   \u2713   \u2713   \u2713   6k   TextVQA-X   \u2713   \u2713   \u2713   15k   GQA   \u2713   \u2713   \u2713   \u2713   \u2713   302k   VQS   \u2713   \u2713   20k   Shikra-BinaryQA   \u2713   \u2713   \u2713   \u2713   \u2713   4k   EntityCount   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   11k   FoodSeg-QA   \u2713   \u2713   \u2713   \u2713   7k   LVIS-QA   \u2713   \u2713   \u2713   \u2713   \u2713   95k   RD   RefCOCO-REG   \u2713   \u2713   \u2713   \u2713   17k   RefCOCO+-REG   \u2713   \u2713   \u2713   \u2713   17k   RefCOCOg-REG   \u2713   \u2713   \u2713   \u2713   22k   gRefCOCO-REG   \u2713   \u2713   \u2713   \u2713   20k   VG-SpotCap   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   247k   V7W   \u2713   \u2713   \u2713   23k   PointQA   \u2713   \u2713   64k   VCR   \u2713   \u2713   \u2713   156k   ShikraRD   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   2k   SVIT-RD   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   33k   Guesswhat   \u2713   \u2713   \u2713   \u2713   193k   VG-RefMatch   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   247k   HierText   \u2713   \u2713   \u2713   \u2713   6k   M3G2 (Total)   2.5M   Table 2. Summary of datasets included in M3G2. We show the   availability of Grounding Annotations (Box, Mask, and Pointer   inputs), the Semantic Granularity (Stuff, Things, Parts, Groups,   and Text), and the number of text-image pairs for training.   3. Our Dataset: M3G2   In this section, we introduce M3G2, a Multi-Modal MultiGrained Grounding dataset consisting of 2.5M text-image   pairs for visually grounded instruction tuning, consisting of   36 sub-problems derived and augmented from 27 existing   datasets. We re-organize and augment public datasets of   language grounding, visual question answering, referring   expression segmentation, and referring expression generation into various forms of visually grounded dialogue for   grounded instruction tuning, outlined briefly in Table 2. The   dataset is categorized into four main types: (1) Grounded   Image Captioning (GIC), (2) Referential Expression Segmentation (RES), (3) Grounded Visual Question Answer14230   ing (GVQA), and (4) Referential Dialog (RD). We provide   illustrated descriptions of our prompt design, accompanied   by examples of each task type as depicted in Figure 4. We   detail the task schema in the following sections and provide   the complete sets of templates in Appendix B.   3.1. Grounded Image Captioning (GIC)   The task of grounded image captioning requires the model   to produce a narrative for the visual scene, and accurately   identify and associate the groundable phrases with their respective binary segmentation masks. The objective of this   task is to empower the model to articulate the scene while   acknowledging various visual elements and their spatial interrelations. We incorporate the Panoptic Narrative Grounding (PNG) dataset [34] for dense and detailed scene descriptions, as well as the Flickr30K-Entity dataset [58] for concise descriptions of the salient contents in the image. We   create a collection of task prompt templates that instruct the   model to describe the image either in detail or briefly.   3.2. Referring Expression Segmentation (RES)   In contrast to previous tasks, the referring expression segmentation task requires that the model generates a segmentation mask based on a given referring expression. Besides   the RefCOCO series [43, 52, 86], we have further leveraged existing RES benchmarks [37, 65, 76, 78, 79] for this   purpose. As shown in Figure 4, we instruct the model to   localize the referred objects, with the expected output being a repetition of the referring expression whose grounding   mask is the target object.   3.3. Grounded Visual Question Answering (GVQA)   The task of grounded visual question answering requires   the model to comprehend a question (with optional pointers) and to produce an answer that is grounded to a binary   segmentation mask that justifies the answer. The goal of this   task is to enable natural QA-based interaction with users   and reasoning in the model with grounded explanations.   Specifically, we harvest and adapt a collection of public   VQA datasets with grounding annotations [6, 8, 19, 29], QA   benchmarks on visual text [64], and create templated QA   pairs from segmentation datasets [25, 60]. Our prompt templates instruct the model to respond either as open-ended   answers or by selecting from multiple choices, with the response anchored to a segmentation mask (Figure 4).   3.4. Referential Dialogue (RD)   The task of referential dialogue requires the model to conduct dialogue communication with users, especially when   conditioned on user-provided spatial prompts.   This includes existing RD datasets [8, 51, 88, 90, 94], multi-turn   augmentations from segmentation datasets [17, 36, 47] as   well as the referring expression generation (REG) task the   [USER] Provide a concise and informative caption for    the image, focusing on the primary subjects.   [ASSISTANT] <GRD> A burger </GRD> and    <GRD> a pickle </GRD> sit on <GRD> a plate </GRD>.   [USER] Is this vegetarian? Highlight the region in the    picture that supports your answer.   [ASSISTANT] <GRD> No </GRD>.   [USER] Please describe <PTR> in the image in a    way that it can be uniquely identified.   [ASSISTANT] <GRD> the sandwich lying at the    farthest side of my plate </GRD>.   [USER] Localize the sandwich in the plate at the far end.   [ASSISTANT] Here it is: <GRD> the sandwich in the plate    at the far end </GRD>.   Visual Input   Segm. Output   Instruction Tuning with Grounded Dialogue   Grounded Image Captioning   Grounded Visual Question Answering   Referential Dialogue   Referring Expression Segmentation   Figure 4. The M3G2 dataset for grounded visual instruction tuning. M3G2 is a diverse dataset of multiple granularities, unifying   4 different task types with visually grounded dialogue.   RefCOCO series [43, 52, 86]. The REG task differs from   the region captioning task in that it demands the description   to be a referring expression that distinctly identifies the targeted object. Effective REG calls for the model to engage in   dialogue interactions cooperatively, adhering to the Gricean   Maxims [24] which dictate that communication should be   as informative, truthful, relevant, and clear as necessary.   4. Experiment and Analysis   We adopt the LLaMA2-7B model [70] as our base LLM,   and initialized the weight from LLaVA-1.5 [44]. For the   vision encoders, we use the OpenAI CLIP@336 [62] model   and DINOv2-L/14-reg [15] pretrained checkpoints. Please   refer to Appendix C for more implementation details.   4.1. Generalist in Grounded Vision-Language Tasks   We first demonstrate GROUNDHOG\u2019s capabilities as a generalist model for three different types of grounded visionlanguage tasks. It\u2019s worth noting that, unlike previous work   that needs dataset-specific fine-tuning on each of the tasks,   GROUNDHOG can achieve comparable performance on all   the tasks directly after training on M3G2, i.e., all the reported results from our model are from a single set of   weights without any dataset-specific fine-tuning.   Language Grounding To Segmentation.   We start by   evaluating the model on language grounding tasks, which   takes text as input and generates segmentation masks as output. We assess GROUNDHOG on Referential Expression   Segmentation (RES) [32] and Caption Phrase Grounding   (CPG) tasks. While traditional RES benchmarks [32, 53]   focus on single-instance referents requiring primarily visual   understanding, we expanded our evaluation to include complex scenarios involving multi-instance or negative queries   14231   Single Instance   Multi-/No Instance   Reasoning   Model   RefCOCO   RefCOCO+   RefCOCOg   gRefCOCO   PhraseCut   ReasonSeg   RIO   val   test-A   test-B   val   test-A   test-B   val-u   test-u   val   test   val   test-c   test-u   Specialist   MDETR [30]   53.7   44.1   22.0   CRIS [75]   70.5   73.2   66.1   62.3   68.1   53.7   59.9   60.4   55.3   LAVT [82]   72.7   75.8   68.8   62.1   68.4   55.1   61.2   62.1   58.4   ReLA [43]   73.8   76.5   70.2   66.0   71.0   57.7   65.0   66.0   63.6   22.4   PolyFormer [46]   76.0   78.3   73.3   69.3   74.6   61.9   69.2   70.2   48.8   26.8   UNINEXT-H [80]   82.2   83.4   81.3   72.5   76.4   66.2   74.7   76.4   Generalist   LISA7B [37]   74.1   76.5   71.1   62.4   67.4   56.5   66.4   68.5   44.0   LISA7B (FT) [37]   74.9   79.1   72.3   65.1   70.8   58.1   67.9   70.6   52.9   7B   78.5   79.9   75.7   70.5   75.0   64.9   74.1   74.6   66.7   54.5   56.2   57.9   33.9   Table 3. Results on 7 Referring Expression Segmentation (RES) benchmarks with single instance queries [32, 53], multi-/null instance   queries [43, 76] and reasoning-based queries [37, 61]. We report cIoU for RefCOCO/+/g and mIoU for other benchmarks, respectively.   Flickr30K-E   Model   R@1val   R@1test   Shikra13B   77.4   78.4   Ferret13B   81.1   84.8   Shikra7B   75.8   76.5   Ferret7B   80.4   82.2   7B   79.2   79.8   Table 4.   Top-1 box recall results on   Flickr30K-Entity [58].   PNG   Model   AR   ARth   ARst   ARs   ARp   PiGLET   65.9   64.0   68.6   67.2   54.5   7B   66.8   65.0   69.4   70.4   57.7   Table 5. Phrase grounding results on PNG [21].   Model   TextVQA-X [mIoU]   SAB   29.0   7B   39.8   Table 6. Visual text QA results on the TextVQAX [66] validation set.   PointQATwice   V7W   Model   Acc   Acc   Shikra13B   70.3   85.3   GPT4RoI13B   84.8   Shikra7B   GPT4RoI7B   81.8   7B   72.4   85.5   Table 7. Results on PointQATwice [51]   and V7W [94] test sets.   [43, 76], and those necessitating common sense reasoning [37, 61]. The results in Table 3 show GROUNDHOG outperforming the generalist model LISA across all benchmarks and achieving significant improvements over specialist models in multi-instance, null, and reasoning-based RES   tasks. It also performs comparably on the competitive RefCOCO series. For CPG tasks, which involve grounding   all phrases in a caption and demand a deep understanding   of the context for coreference resolution, we first evaluated   GROUNDHOG on the Flickr30K-Entity dataset [58]. Since   this dataset only has box annotations, we convert the mask   predictions of our model to box and compute the top-1 box   recall following the merged-box protocol [30]. Despite not   specializing in predicting boxes, GROUNDHOG still outperforms Shikra 7B/13B [8] and is on par with Ferret-7B [85]   in a concurrent work (Table 4). Additionally, on the PNG   dataset [34] which tests phrase grounding in longer narratives, GROUNDHOG surpasses the previous state-of-the-art   model, PiGLET [22], in all metrics including average recall   of grounding masks and detailed scores for things, stuffs,   and singular and plural entities (Table 5).   Grounded Language Generation.   Our model excels in   generating language that accurately grounds to segmentation masks during user conversations.   Quantitatively,   we assess grounded captioning on the Flickr30K-Entity   dataset [58], employing standard text generation metrics   Model   Bleu-4   METEOR   CIDEr   SPICE   F1all   Shikra13B   73.9   Ferret13B   37.0   25.5   76.1   18.3   15.1   Ferret7B   35.1   24.6   74.8   18.0   15.0   7B   36.7   26.5   91.3   20.4   32.1   Table 8. Grounded Captioning on Flickr30K-Entity [58].   such as Bleu-4 [55], METEOR [4], CIDEr [72], and   SPICE [2] for language quality; and the F1all score for   grounding accuracy following You et al. [85]. As shown   in Table 8, GROUNDHOG significantly surpasses existing   box-based grounded MLLMs, even their 13B versions, in   both language quality and grounding accuracy. This improvement is hypothesized to stem from the diverse task   distribution in our M3G2 dataset.   For groundable question answering, we evaluate on the TextVQA-X benchmark [64]. GROUNDHOG outperforms the state-of-the-art   specialist model SAB [33] by a significant margin, as measured by the mean IoU of the predicted mask (Table 6).   Spatial Prompt Understanding.   For grounded MLLMs,   accurately interpreting multimodal instructions is essential,   particularly in interactive tasks.   We evaluated   its performance on two pointer-based QA benchmarks,   PointerQATwice [51] and V7W [94], which require the   model to answer questions guided by spatial prompts, such   as bounding boxes. The model is tasked to generate free14232   Input Prompt   M2F+ Best Match   SAM   \u201ca red    brick tower\u201d   \u201cspire in red    brick\u201d   Figure 5.   Region caption using the best match proposal from   Mask2Former+ versus from SAM. Mask2Former+ fails to propose   the exact mask of the spire, leading to a less precise caption.   \u201cKWIK E MART\u201d   Merge   0.352   0.002   0.713   1.000   Figure 6. Illustration of a partially correct grounding. The grounding phrase and the ground truth mask are shown on the left. The   top-4 mask proposals are presented, with highly-scored masks   (green) selected for the merged mask, and low-scored masks   (red) excluded. This illustrates the failure to recognize the word   \u201cKWIK\u201d by the MLLM, despite its successful proposal.   form textual answers in PointerQATwice, and selects from   multiple-choice options in V7W. GROUNDHOG demonstrates superior performance in these benchmarks, outperforming previous models as shown in Table 7. This highlights its effectiveness in spatial understanding and response   accuracy. To further demonstrate the effectiveness of using   SAM for the pointer-to-mask conversion, we show the bestmatched mask proposal from our Mask2Former+ model in   comparison to the mask from SAM in Figure 5.   While   the best match proposal from the Mask2Former+ model includes a broader area, the SAM-generated mask offers a   more precise representation of the specified region, potentially leading to a more accurate caption.   4.2. Trustworthiness and Transparency   Beyond its superior performance as a grounding generalist, we highlight two key improvements for creating a more   trustworthy and transparent agent.   Reduced Object Hallucination.   Thanks to the varied   task distribution and the inclusion of negative questionanswering samples in M3G2 dataset, GROUNDHOG significantly reduces object hallucination. We assessed this using   the POPE [42] benchmark, which includes binary questions   about object existence across three splits, each with a different object distribution (with an order of difficulty Random < Popular < Adversarial). Remarkably, GROUNDModel   Accuracy   Precision   Recall   F1 Score   Yes (%)   Random   mPLUG-Owl   53.30   51.71   99.53   68.06   96.23   LLaVA   54.43   52.32   99.80   68.65   95.37   MultiModal-GPT   50.03   50.02   100.00   66.68   99.97   MiniGPT-4   77.83   75.38   82.67   78.86   54.83   InstructBLIP   88.73   85.08   93.93   89.29   55.20   Shikra-13B   86.90   94.40   79.26   86.19   43.26   Ferret-13B   90.24   97.72   83.00   89.76   43.26   7B   91.03   85.80   96.40   90.79   45.88   Popular   mPLUG-Owl   50.63   50.32   99.27   66.79   98.63   LLaVA   52.43   51.25   99.80   67.72   97.37   MultiModal-GPT   50.00   50.00   100.00   66.67   100.00   MiniGPT-4   68.30   64.27   82.40   72.21   64.10   InstructBLIP   81.37   75.07   93.93   83.45   62.57   Shikra-13B   83.97   87.55   79.20   83.16   45.23   Ferret-13B   84.90   88.24   80.53   84.21   45.63   7B   90.13   85.93   93.81   89.70   45.80   Adversarial   mPLUG-Owl   50.67   50.34   99.33   66.82   98.67   LLaVA   50.77   50.39   99.87   66.98   99.10   MultiModal-GPT   50.00   50.00   100.00   66.67   100.00   MiniGPT-4   66.60   62.45   83.27   71.37   66.67   InstructBLIP   74.37   67.67   93.33   78.45   68.97   Shikra-13B   83.10   85.60   79.60   82.49   46.50   Ferret-13B   82.36   83.60   80.53   82.00   48.18   7B   86.33   85.93   86.63   86.28   49.60   Table 9. Object hallucination results on the POPE [42] benchmark.   HOG consistently outperforms other models in both accuracy and F1 score across all splits, particularly on the more   challenging ones.   It shows an absolute improvement of   5.2% in accuracy for Popular and 4.0% for Adversarial over   the previously best-performing model. This suggests that   our model\u2019s enhanced grounding capability plays a significant role in mitigating the object hallucination problem.   Explainability and Diagnosability.   Another important   highlight of GROUNDHOG is its enhancement of explainability through the decoupled design of entity proposal and   selection, as outlined earlier in section 2.2. This is exemplified in the case study illustrated in Figure 6, which illustrates the mask proposal scoring and selective merging   process of our model. We show the top-4 masks, where the   higher-score masks are labeled in green while the lowerscore masks are labeled in red. Users can easily interpret   that the failure is due to the incapability of MLLM to recognize the word \u201cKWIK\u201d, despite it being successfully localized and proposed as an entity candidate.   4.3. Ablation Studies   We performed ablation studies to validate our design decisions, training, and evaluating a subset of the M3G2 dataset   that includes RefCOCO+, Flickr30K, and TextVQA. These   cover a range of visual entities from various image   sources and granularities.   We start by comparing our   Mask2Former+ with the original Mask2Former for mask   proposal effectiveness. As indicated in Table 10, the orig14233   Setups   RefCOCO+   Flickr30K   TextVQA-X   Mask Proposal Models   Mask2Former   67.1   69.0   9.8   Mask2Former+   66.6   77.2   34.0   Entity Features   CLIP   59.8   75.0   32.0   DINOv2   62.3   76.3   28.4   CLIP+DINOv2   66.6   77.2   34.0   Grounding Query   <GRD> only   64.4   67.5   34.2   </GRD> only   64.4   77.2   33.5   Sum   66.6   77.2   34.0   Eval Input Resolution   224\u2013480   54.7   67.2   27.6   480\u2013640   65.5   76.7   27.6   800\u20131024   66.6   77.2   34.0   Table 10. Ablation study on model design choices and evaluation   setups. Models are trained on RefCOCO+, Flickr30K, TextVQAX and tested on corresponding validation sets.   inal Mask2Former performs slightly better on RefCOCO,   as it is developed specificity on COCO object categories.   However, Mask2Former+ significantly surpasses the original in domains with non-COCO entities. Our second set   of experiments examined the choice of visual entity features.   Although using either CLIP or DINOv2 features   alone shows advantages in specific datasets, their combination consistently yields the best results across all datasets.   To obtain a robust grounding query representation, we experimented with using the output embedding of the <GRD>   token, the </GRD> token, and their sum. We found that   the latter approach achieves the best overall results. Finally,   we demonstrate that our decoupling design of the mask proposal model and MLLM allows for training at a lower resolution (320px) to expedite grounding training, while scaling   up the resolution during evaluation enhances performance.   5. Related Work   Multimodal Large Language Models.   Building on the   recent advance of large language models (LLMs), there is   an increasing effort in adapting pretrained large language   models for multimodal tasks, such as understanding and   interpreting visual information [1, 71]. More recently, visual instruction tuning has gained much interest due to its   surprising performance with a modest amount of data and   computing resources. Various models have been developed,   noticeably MiniGPT4[93] , LLaVA [44, 45] and concurrent   models [13, 20, 38, 74, 83]. Despite their promising performances, MLLMs often produce objects that are not presented in the given images, a phenomenon referred to as the   object hallucination problem [14, 31, 42].   MLLM with Language Grounding   The ability to connect language to their corresponding visual elements in   the physical world, known as grounding [27], is crucial   in everyday human communication about our shared surroundings. Grounding datasets have been shown to benefit vision-language pre-training, both in terms of objectlevel recognition [41] and language learning [50].   Recent works unify text and grounding regions into token sequences [49, 73, 81] in casual language modeling. Based   on such paradigm, researchers have developed a family   of grounded MLLM, including GPT4ROI [89], Kosmos2 [56], Shikra [8], PVIT [7], BuboGPT [91], Qwen-VL [3],   and Ferret [85]. Despite their promising performance, these   models focus on object grounding to bounding boxes, which   cannot handle pixel-level grounding across various semantic granularities. Furthermore, it lacks the diagnosability   and explainability in failure cases. Our model fills this gap.   Language-Guided Semantic Localization   The field of   language-guided semantic localization has a long history   in the vision-language research community, requiring that   the model localize a given referring expression with bounding boxes or segmentation masks. This task has evolved   from early attempts to understand simple referring expressions within images, such as the well-known RefCOCO series [52, 86] and their generalized variant [43] that takes   no-target and multi-target into account. The integration of   advanced language reasoning from LLMs has enabled research to tackle even more nuanced reasoning tasks that   involve complex language contexts [37, 57, 87]. Notably,   LISA [37] formulates a reasoning segmentation task to   bring language-informed reasoning into semantic segmentation, and contributes a powerful baseline.   Our model   builds on these developments, but is designed to be more   universally applicable as a grounded MLLM.   6. Conclusion   In this study, we introduce GROUNDHOG, a novel framework designed to enable pixel-level explainable grounding   in large language models, leveraging holistic segmentation.   The system builds upon a pre-trained mask proposal network to provide pixel-level visual features for the large language models, allowing them to retrieve segmentation mask   proposals that can be used for grounding. We also present   M3G2, a dataset of 1.9M training text-image pairs with 36   sub-problems derived from 27 existing datasets for visually grounded instruction tuning, facilitating precise visionlanguage alignment at the pixel level. We show that after training on M3G2, GROUNDHOG achieves superior performance on various grounding tasks. Through extensive   case studies, we further show that GROUNDHOG unlocks   explainability and diagnosability, and demonstrates better   grounding towards occluded objects, groups of multiple instances, amorphous background regions, semantic parts of   objects, and objects with irregular shapes.   Acknowledgements   This work was supported by Amazon and NSF IIS-1949634. We thank the anonymous reviewers for their valuable comments and suggestions.   14234", "conf": "CVPR", "year": "2024", "index": 797}, {"title": "Zero-shot Referring Expression Comprehension via Structural Similarity   Between Images and Captions   Zeyu Han1, Fangrui Zhu1, Qianru Lao2, Huaizu Jiang1   1Northeastern University   2Harvard University   {han.zeyu,zhu.fang,h.jiang}@northeastern.edu, estherbear17@gmail.com", "abstract": "Zero-shot referring expression comprehension aims at   localizing bounding boxes in an image corresponding to   provided textual prompts, which requires: (i) a fine-grained   disentanglement of complex visual scene and textual context, and (ii) a capacity to understand relationships among   disentangled entities. Unfortunately, existing large visionlanguage alignment (VLA) models, e.g., CLIP, struggle with   both aspects so cannot be directly used for this task. To   mitigate this gap, we leverage large foundation models to   disentangle both images and texts into triplets in the format of (subject, predicate, object).   After that,   grounding is accomplished by calculating the structural   similarity matrix between visual and textual triplets with a   VLA model, and subsequently propagate it to an instancelevel similarity matrix. Furthermore, to equip VLA models with the ability of relationship understanding, we design a triplet-matching objective to fine-tune the VLA models on a collection of curated dataset containing abundant entity relationships.   Experiments demonstrate that   our visual grounding performance increase of up to 19.5%   over the SOTA zero-shot model on RefCOCO/+/g. On the   more challenging Who\u2019s Waldo dataset, our zero-shot approach achieves comparable accuracy to the fully supervised model. Code is available at https://github.   com/Show-han/Zeroshot_REC.   1.", "content": "Visual grounding is a fundamental task across computer   vision and natural language processing, where the goal is   to find the correspondences between image content and   textual descriptions.   It has broad applications in image   captioning [18, 53], visual question answering [43, 66],   vision-language navigation [11], etc. Collecting detailed   grounding annotations to train specialist models, however,   is cumbersome.   Therefore, zero-shot visual grounding   [30, 38, 54] is an attractive alternative.   Cat touching a paper roll   A man watches as a man shoots over a man   Figure 1. Illustration of how we disambiguate visual entities   based on their interactions with other entities. The same entity   or relationships in the image and caption are in the same color.   As a visual grounding task, the essence of referring   expression comprehension (REC) is the alignment of text   queries with corresponding image regions. To achieve this   goal, it is critical for a grounding model to understand the   relationships of entities[28], both within and cross different   modalities (visual vs. textual), when identifying referred entities within an image. As shown in the upper part of Fig. 1,   the touching relationship is the key to resolve the ambiguity of identifying the correct cat. Similarly, the lower   part of Fig. 1 illustrates a more complex situation where   multiple entities are engaged in various interactions. Both   scenarios highlight the importance of relationship understanding within both the image and caption, where entities   are not merely isolated elements but interact dynamically   with others in the scene. In a zero-shot learning context,   the task of understanding these relationships can be more   challenging, as the model lacks exposure to specific training instances that could aid in interpretation.   Recent advances in zero-shot REC [30, 54, 65] have   been largely driven by the integration of large-scale visionlanguage aligned (VLA) models such as CLIP [47] and   FLAVA [52], which serve as bridges connecting text and   image domain. These approaches, however, fall short in   relationship understanding. On the one hand, in the textual domain, existing approaches adopt hand-crafted lanThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   14364   guage parsers [54, 65] to decompose the input caption into   a set of phrases, which are fragile and do not generalize   well to long, complex captions in real-world applications,   as shown in the bottom of Fig. 1. On the other hand, the   visual relationship understanding capability of VLA models is inherently not good enough. Recent studies have revealed that VLA models behave like \u201cbags-of-words\u201d [67],   and demonstrated that they fail to perform beyond chance   level at simple tasks requiring compositional understanding [7, 8, 12, 57, 67]. Some effort have been dedicated   to mitigate this issue by generating hard negative prompts   through word replacement [8, 20, 67], caption augmentation [7, 8], and feature augmentation [22]. These rule-based   methods, however, are limited in producing diverse samples   and have potential bias of design pattern, consequently restricting their generalization capabilities.   In this paper, we focus on the REC task by explicitly modeling the entity relations within both images and   captions using the structural similarity between them to   solve the zero-shot visual grounding problem.   Specifically, we decompose the image and caption into two sets of   triplets in the form of (subject, predicate, object),   in which each triplet captures a pair of potential entities   with their interrelation. By considering the similarity of   subject, object, and predicate jointly, we can find   better matchings of object proposals and their referrings.   Compared with existing work [54], our approach is more   principled and eliminates the ad-hoc post-processing spatial relation resolver. More importantly, to improve the relationship understanding in the caption, we resort to ChatGPT and leverage its powerful in-context learning capability [56, 59] for the triplet decomposition to find all possible   relation triplets given a sentence. In contrast to the dependency parser in [54], our parsing works better when dealing   with long captions and do not restrict to spatial relationships (e.g., to the left of), which can fully capture   the rich compositional semantics present in actions or interactions, such as walking or talking to.   To address the limitation of a VLA model\u2019s visual relationship understanding, we harness a curated collection   of data sources rich in relational knowledge, which include human-object interaction datasets [3, 46] and image   scene graph dataset [27]. Similar to our grounding pipeline,   we isolate visual entities and construct triplets in both visual and textual sides and then implement a triplet-level   contrastive learning objective to fine-tune the VLA model.   Compared with the existing rule-based negative prompts   construction, this design has two unique advantages. First,   by decomposing a single image into multiple triplets, we   can obtain more training instances and improve the diversity of the training data than simply using the entire image for fine-tuning. Furthermore, the isolation of entities   removes the distraction of other content in the image, providing more useful supervision for the model fine-tuning.   We fine-tune a VLA model in a parameter-efficient manner [15] using LoRA [19], improving its visual relationship   understanding while preserving its powerful generic feature   representations learned from large-scale data. Our resulting   model is called VR-VLA (Visual Relationship VLA).   Experimental results show that on the standard RefCOCO/g/+ datasets [42, 63], we can surpass the SOTA zeroshot baseline [54] up to 19.5%, and an average of 9.7%. On   the challenging Who\u2019s Waldo dataset [6], whose captions   are much longer and depict much richer interactions of humans, our zero-shot method significantly outperforms [54],   achieving comparable accuracy to supervised methods. We   also conduct ablation studies validating the effectiveness of   our model design.   To summarize, our main contributions are three-folded.   (1) A novel zero-shot visual grounding model, where we   harness the powerful capabilities of foundation models   (e.g., ChatGPT and CLIP) to explicitly model the structural   similarity between entities. (2) A novel recipe to improve   a VLA model\u2019s visual relationship understanding by efficiently incorporating the supervision from a collection of   curated data sources. (3) We report SOTA zero-shot visual grounding results on the REC datasets and also show   promising results on the Who\u2019s Waldo dataset, where our   zero-shot approach achieves comparable accuracy to the   fully supervised method.   2. Related Work   Visual grounding.   Based on the focus of the grounding task, visual grounding diverges into two main categories.   The first emphasizes the noun properties of the   query text. Precise understanding of the noun\u2019s meaning   enables locating the corresponding grounding box in the   image.   Representative datasets contain MS-COCO [37],   Object365 [49] (fixed-category), Flickr30K Entities [45]   (open-vocabulary), etc. The second category emphasizes   comprehending the interrelations among entities to localize   the correct visual entity (potentially from among many similar ones) corresponding to the query text. Representative   datasets contain RefCOCO/+/g [42, 63], Who\u2019s Waldo [6],   etc. Our research of interest belongs to the second category,   where relations are strong grounding clues.   Based on the use of grounding data, visual grounding   can be classified into supervised and zero-shot categories.   The majority of research [23, 35, 73] focuses on supervised   visual grounding, where models are specifically designed   and trained with grounding data for this purpose.   Conversely, zero-shot visual grounding methods [30, 38, 54]   adapt pre-existing vision-language models for grounding   tasks.   Our work is situated within this zero-shot visual   grounding paradigm.   Visual relationship understanding. Images and texts are   14365   constructed using fundamental elements \u2014 objects in images and noun spans in texts \u2014 along with their interactions   and relationships. For a model to understand relationship,   it must not only detect individual entities but also establish   relational links between them. Research communities such   as human-object interaction [3, 10, 24, 25, 34, 40, 46, 69\u2013   71, 75] and visual scene graph [27, 33, 55, 60, 61, 68, 72,   76] emphasize the relational aspect for visual tasks.   Visual relationship understanding is also highly relevant to the compositional reasoning [8, 12, 17, 20, 22, 67]   ability for VLA models.   Although we anticipate that   VLA models, trained via contrastive learning on extensive   image-text pairs, would inherently develop a capacity for   compositional reasoning, the reality is somewhat different.   Most SOTA VLA models behave like \u201cbags-of-words\u201d [67].   They are capable of matching textual entities with corresponding visual elements, but falling short in interpreting   their relationships or attributes. To address this, many studies have implemented strategies such as introducing negative text prompts [7, 8, 67] in training batches \u2014 including   noun substitutions, attribute and verb modifications, caption augmentation \u2014 or integrating scene graphs [17] during training or inference. In our work, we show that finetune the VLA models on the visual relationship datasets can   alleviate this problem. Furthermore, the explicitly defined   structural representation also help strengthen the compositional reasoning for visual grounding tasks.   Language parsing. In the language side, structure prediction [1, 5, 44, 50] is well studied and aims for solving several problems including entity recognition [9, 32, 51], relation classification [14, 74], semantic role labeling [2, 16],   event extraction [21, 31], coreference resolution [29, 58],   etc. The acquired structural representation can be illustrated   as either a language parsing tree [54] or a set of labels that   indicate the respective roles of each word [50].   3. Proposed Approach   Our proposed grounding pipeline contains two stages. First,   we decouple image and text entities and construct triplets   in the format of (subject, predicate, object). Second, we calculate triplet-level similarity matrix and propagate it to the instance-level and then obtain the bounding   box with the highest similarity score. The primary focus in   our matching pipeline is to accurately model the relationship between entities, which is achieved by the the tripletlevel structural similarity, as shown in Fig. 2. We also provide a novel recipe to equip the VLA models with better   compositional understanding ability.   3.1. Constructing Triplets   Given a caption, denoted by C, and its associated image,   denoted by I, we postulate that both C and I comprise sets   Cat touching a paper roll   input   Text Encoder   Cat touching a paper roll   structural   similarity   +   +   subject    similarity   predicate    similarity   object   similarity   Image Encoder   grounded triplets   =   Figure 2.   Illustration of the triplet-level structural similarity.   Visual and textual triplets are encoded by image encoder   and text encoder, respectively. Then the structural similarity is   calculated as the sum of cosine similarities between subject,   predicate, and object.   of entities1, denoted by ET = {eT   i }M   i=1 for the text and EI =   {eI   i }N   i=1 for the image, where M and N represent the total   number of entities in C and I, respectively. Interrelation of   an entity pair is represented by rT (\u00b7) for the text and rI(\u00b7)   for the image. In this stage, our objective is to construct   entity-relation triplets for both modalities.   For text, we denote the triplets as:   TT = {tT   ij = (eT   i , rT (eT   i , eT   j ), eT   j ) | 1 \u2264k, l \u2264M}.   (1)   For image, we denote the triplets as:   TI = {tI   kl = (eI   k, rI(eI   k, eI   l ), eI   l ) | 1 \u2264i, j \u2264N}.   (2)   The cardinality of the above two sets are defined as M \u2032   for text and N \u2032 for image, respectively.   Textual triplets construction.   Large language models   have exhibited a powerful capacity for a range of downstream tasks.   Here, we leverage its powerful in-context   learning capability to parse a caption C into triplets TT .   Specifically, We design a prompt to instruct the ChatGPT   to parse the caption text C. Fig. 3 provides an overview on   how we design prompt for RefCOCO/+/g dataset, and further details are elaborated as follows. Note that the prompts   can vary depending on datasets to accommodate different   distributions of the data.   As shown in Fig. 3, the prompt can be divided into four   parts: (i) general instruction; (ii) supporting details; (iii)   in-context learning examples, and (iv) task instruction, followed by LLM completion, which yields the output of the   LLM in the specified format and content. In part (i), we define a clear and general instruction for specific task. Then,   we elaborate supporting details in part (ii), including the   expected output format, essential elements, and p", "conf": "CVPR", "year": "2024", "index": 669}, {"title": "CityRefer: Geography-aware 3D Visual Grounding   Dataset on City-scale Point Cloud Data   Taiki Miyanishi1,3\u21e4, Fumiya Kitamori2\u21e4,   Shuhei Kurita3,   Jungdae Lee2, Motoaki Kawanabe1,   Nakamasa Inoue2   1ATR,   2Tokyo Institute of Technology,   3RIKEN AIP", "abstract": "City-scale 3D point cloud is a promising way to express detailed and complicated   outdoor structures. It encompasses both the appearance and geometry features of   segmented city components, including cars, streets, and buildings, that can be utilized for attractive applications such as user-interactive navigation of autonomous   vehicles and drones. However, compared to the extensive text annotations available for images and indoor scenes, the scarcity of text annotations for outdoor   scenes poses a signi\ufb01cant challenge for achieving these applications. To tackle this   problem, we introduce the CityRefer dataset1 for city-level visual grounding. The   dataset consists of 35k natural language descriptions of 3D objects appearing in   SensatUrban [19] city scenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the quality and accuracy of the dataset, all descriptions and   labels in the CityRefer dataset are manually veri\ufb01ed. We also have developed a   baseline system that can learn encoded language descriptions, 3D object instances,   and geographical information about the city\u2019s landmarks to perform visual grounding on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset   is the largest city-level visual grounding dataset for localizing speci\ufb01c 3D objects.   1", "content": "Advancements in urban 3D scanning technologies, such as unmanned aerial vehicle photogrammetry   and mobile laser scanning, enable the creation of accurate and photorealistic large-scale 3D scene   datasets. Examples of such datasets include street-level datasets acquired by automobiles [6, 17,   37, 39, 42, 43] and city-level datasets acquired by aerial vehicles [19, 25, 36, 44, 52, 60]. However,   while city-level photorealistic 3D scans have become practical and applicable in various \ufb01elds like   autonomous driving and unmanned vehicle delivery, the technology to comprehend city scenes   through human-interactive linguistic representations is still in its early stages of development. The   ability to ground linguistic expressions to urban components is highly desired for interactive and   interpretable applications, such as language-guided autonomous driving and aerial drone navigation.   Achieving this requires the development of a 3D visual grounding dataset based on the city-scale   point clouds, which presents a signi\ufb01cant challenge.   3D Visual grounding is a 3D and language task that involves localizing objects in 3D scenes based   on textual referred expressions. Compared to its 2D visual grounding counterparts [23, 29, 31, 53],   3D visual grounding poses additional challenges. The expressions used in 3D visual grounding   often require more information to localize object instances due to the rich context of 3D scenes,   making the problem further complex. Recent studies have made remarkable progress in 3D visual   \u21e4equal contribution   1https://github.com/ATR-DBI/CityRefer   37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.   The white building with a gray roof,    between two other similar looking    buildings, on Acrefield Drive, just    before Woodvale Lodge.   320 Footpath: Aylestone Road   377 TrafcRoad: Aylestone Road   The red car in the middle    of the five cars that are    parked diagonally at the    end of Acrefield Drive.   The blue car across the street from a    red and white car on Belvoir Road    that is parked near a large tree and    in front of a yellow building.   CityRefer Dataset   CityRefer Dataset   5k+ labels for buildings and roads   35k+ natural language descriptions   for city-level 3D visual grounding   Figure 1: The CityRefer dataset for city-level 3D visual grounding.   grounding, focusing on determining the precise position of 3D objects given natural language   descriptions [7, 10, 18, 20, 21, 22, 35, 54, 56, 58]. However, many of these methods have been   evaluated on 3D indoor datasets, which typically consist of point clouds of room scenes and labels   for household objects [2, 9, 14]. Subsequently, Kolmet et al. [24] proposed a district-based visual   grounding dataset on KITTI-360 [26]. Nevertheless, the availability of 3D visual grounding datasets   is still limited, especially in the context of aerial city-level 3D point clouds. Therefore, we aim to   address this gap by creating a publicly available 3D visual grounding dataset based on aerial city-level   3D point clouds.   In this paper, we introduce the CityRefer dataset for city-level 3D visual grounding. Speci\ufb01cally, we   provide 35k natural language descriptions to localize 3D objects in the SensatUrban [19] environment   as well as 5k labels of objects such as buildings and roads. Three example descriptions of a sample   scene are shown in Figure 1. As seen from the \ufb01gure, city-level visual grounding is very challenging   because a system is required to \ufb01nd objects from a wide city area while understanding the description   of the target object and the relationships between relevant objects. Although we used crowdsourcing   to scale up the annotation, we needed to thoroughly \ufb01lter out incorrect annotations by hand to \ufb01nalize   the dataset and ensure the quality of the annotations. The main contributions of the CityRefer dataset   are summarized as follows.   1. We provide instance-wise segmentation masks for 5k objects including 1.8k landmark objects   with their name labels. Examples include Kem River and Baker Street, Aylestone Road, and   Belvoir Road. These labels were obtained from the spatial joint between SensatUrban and   OpenStreetMap using our semi-automatic system (Section 3.1).   2. We provide 35k natural language descriptions for city-level visual grounding. These descriptions   are manually attached using our interactive annotation system (Section 3.2).   3. We provide a baseline system that performs city-level 3D visual grounding. Because it is   nontrivial to adapt previous visual grounding methods for our city-level dataset, we propose   a simple but effective method that narrows the search area to \ufb01nd the target object by using   geographical information.   2   Table 1: Comparison of 3D visual grounding datasets. Ndesc : Number of natural language descriptions. \u00afLdesc : Average description length. Npoints : Number of 3D points.   Dataset   Human   annot.   Ndesc   \u00afLdesc   Area   Environment (Format)   Npoints   Indoor   REVERIE [32]   Yes   21,702   18.0   Rooms   Matterport3D (RGB) [8]   SUN-Spot [30]   Yes   7,987   14.1   Rooms   SUN RGB-D [40]   SUNRefer [27]   Yes   38,495   14.1   Rooms   SUN RGB-D [40]   Nr3D [2]   Yes   41,503   11.4   Rooms   ScanNet (3D Scan) [14]   242M   ScanRefer [9]   Yes   51,583   20.3   Rooms   ScanNet (3D Scan) [14]   242M   Outdoor   TouchDown [11]   Yes   25,575   29.7   Roadside   Google Street View (RGB)   KITTI360Pose [24]   No   43,381   7.6   Roadside   KITTI-360 (3D Scan) [26]   1,000M   CityRefer (Ours)   Yes   35,196   26.3   City center   SensatUrban (3D Scan) [19]   2,847M   2   Related Work   Visual Grounding Datasets for 3D Spaces. Over the years, there has been signi\ufb01cant research   interest in 3D visual grounding as summarized in Table 1. We \ufb01rst introduce two types of 3D visual   grounding datasets, each focusing on a different level of the scene: the indoor and outdoor roadside.   The 3D visual grounding dataset is created by annotating indoor or outdoor 3D datasets with linguistic   descriptions.   (i) Indoor scene level. With the increasing availability of indoor 3D datasets [3, 5, 8, 14, 46, 34,   40, 49], several visual grounding datasets have been proposed to address the demand for 3D scene   understanding. REVERIE [32] comprises 10,318 panorama images captured across 86 indoor scenes   and a total of 4,140 target objects. This dataset also provides 21,702 language instructions with rich   textual annotations for guiding virtual agents within indoor photorealistic scenes of Matterport3D [8].   The SUN-Spot [30] and SUNRefer [27] datasets focus on object localization in single-view RGB-D   images within indoor environments from the SUN RGB-D dataset [40]. Both datasets provide detailed   language annotations indicating the spatial extent and location of objects in the images including   object bounding boxes. Speci\ufb01cally, SUNRefer contains 38,495 language annotations for 7,699   objects in indoor RGB-D images. Nr3D [2] and ScanRefer [9] are standard 3D visual grounding   datasets that are built on top of ScanNet [14], a real-world 3D scene dataset with extensive semantic   annotations. However, these datasets mainly focus on the indoor visual grounding task.   (ii) Outdoor scene level. In recent years, a number of richly annotated outdoor 3D datasets have been   created by scanning cities using sensors installed in cars and drones [6, 17, 19, 25, 37, 42, 44, 50, 60].   While there have been efforts to annotate outdoor 3D datasets with language descriptions for 3D visual   grounding, the availability of such datasets is still limited compared to indoor ones. The TouchDown   dataset [11] is aimed at text-guided navigation and spatial reasoning using real-life visual observations.   It contains 9,326 examples of instructions and spatial descriptions in the visual navigation environment   drawn from Google Street View. KITTI360Pose [24] is a text-based position localization dataset in an   outdoor 3D environment from the KITTI360 dataset [50], which provides nine static scenes obtained   by LiDAR scans. It is notable that the linguistic descriptions of KITTI360Pose are automatically   generated by a sentence template with position description query pairs. While both TouchDown and   KITTI360 datasets are based on the vehicle perspective and hence limited to the semantics from   roadsides, our dataset is based on SensatUrban [19], which covers 3D semantics of the board city   areas that are generated from aerial images by drones.   Learning Visual Grounding of 3D Scenes. To facilitate a deeper understanding of 3D scenes   through language, there have been many efforts to connect languages and the 3D visual world,   including 3D dense captioning [12, 47, 55], 3D change detection [33], and 3D visual question   answering [1, 4, 15, 28, 59]. Speci\ufb01cally, 3D visual grounding that aims to locate an object in 3D space   in response to a natural language query is the fundamental task of the 3D and language \ufb01eld [2, 9, 48].   Several approaches in the \ufb01eld of visual grounding use pre-computed 3D object detection or instance   segmentation results, utilizing the point cloud features extracted from the corresponding 3D bounding   boxes and segments [20, 56]. However, the challenge of object recognition arises due to the low   resolution of 3D data resulting from the reconstruction process. To overcome this limitation, recent   3   133_TrafcRoad: Lens\ufb01eld Road   SensatUrban   141_Building: King\u2019s College Chapel   133_TrafcRoad: Lens\ufb01eld Road   3D point clouds   Geographical data   Landmark/road labels   CityRefer   141_Building: King\u2019s College Chapel   Figure 2: Stage 1 of dataset construction. We perform a spatial join between the 3D SensatUrban   environment and the 2D OpenStreetMap. The CityRefer dataset contains 1,850 landmark/road labels   and geographical data. Examples include the landmark name King\u2019s College Chapel and the road   name Lans\ufb01eld Road.   studies have proposed the integration of both 2D images and 3D data [51, 22]. By combining   the rich spatial information provided by 3D data with the detailed appearance cues derived from   2D images, these hybrid approaches aim to enhance robustness in the context of visual grounding.   Furthermore, there have been studies proposing methods that integrate 3D visual captioning and   grounding, where both models are learned simultaneously to achieve synergistic effects [7, 10]. These   current 3D visual grounding methods mainly rely on two widely used indoor 3D visual grounding   datasets [2, 9]. Several studies proposed visual grounding on remote sensing data but were limited   to 2D images [41, 57]. For these reasons, the performance of 3D visual grounding on outdoor 3D   datasets remains unexplored. One of the initial studies [24] attempted to identify regions within   the 3D point cloud based on textual queries on city-level 3D datasets but were limited to arti\ufb01cial   language descriptions and did not fully use the geographic information 3D map despite the sparsity   of city-level 3D point cloud data. In contrast, our method uses geographic information from 3D maps   to achieve accurate city-level 3D visual grounding.   3   Dataset Construction   The CityRefer dataset consists of 1) instance-wise segmentation masks each with a label and geographical information, and 2) natural language descriptions for visual grounding. The 3D environment   we use is the SensatUrban [19], which consists of photogrammetric point clouds of two UK cities   covering 6 km2 of the city landscape. Semantic segmentation masks are provided with the environment, and our study re\ufb01nes them to instance-level masks for visual grounding. The annotation   proceeds in two stages: semi-automatic generation of instance-wise segmentation masks (Stage 1)   and manual language annotation (Stage 2).   3.1   Stage 1: Semi-automatic Generation of Instance-wise Segmentation Masks   The goal of this stage is to generate segmentation masks for each 3D object as well as to attach labels   of names with geographical data (longitude, latitude, and elevation) to each object. To achieve this,   we perform a spatial join between the 3D environment and OpenStreetMap, from which we can   obtain names and locations. Figure 2 shows an example result of this stage. As shown, regions of   King\u2019s College Chapel and Lens\ufb01eld Road are visualized precisely. The procedures are described in   detail below.   Georeferencing. The georeferencing is performed in the following three steps. First, given a block2   of the 3D city, we create a top view image as shown in Figure 3a, where the image size is \ufb01xed to   2048 by 2048 pixels. Second, we manually choose ten points of interest, such as corner and landmark   points, that are clearly visible in OpenStreetMap. In this step, we also extract an image of the 2D   map as shown in Figure 3c. Note that the coordinates of the ten points are manually annotated on   2We use 34 blocks (scenes) of Cambridge and Birmingham from SensatUrban.   4   SensatUrban (3D scene)   OpenStreetMap (2D map)   Result of georeferencing   Figure 3: Georeferencing of 3D environment and 2D map.   both images. Finally, we compute the geometric transformation between the two images by using the   transformation function from the scikit-image library. We manually tune the hyper-parameters of   transformation by visually verifying the results of the georeferencing. Here, the ten points of interest   are also updated and tuned if needed. Figure 3b shows an example result in which the 3D scene and   the 2D map are precisely joined.   Generating instance-wise segmentation masks. Given semantic segmentation masks with respect   to 13 object categories, we re\ufb01ne them to instance-level masks. We divide the categories into three   groups as shown in Table 2. For Group 1, we directly use geographic information obtained from   OpenStreetMap to create \ufb01lters for each instance. For example, for a segment of Lans\ufb01eld Road   in OpenStreetMap, we create a \ufb01lter in the 3D scene based on the result of the georeferencing.   Table 2: Three groups of categories.   Grp.   Method   Categories   1   Filtering   Ground, HighVegetation,   Building, Bridge, Rail,   Traf\ufb01cRoad, Footpath, Water   2   Clustering   Wall, Parking,   StreetFurniture, Bike   3   Detection   Car   For Group 2, we apply the DBSCAN algorithm, a clustering method, to 3D points. It   is not dif\ufb01cult to obtain accurate boundaries   between instances by clustering because objects in this group are small and located separately. For Group 3, we used YOLOv7 to   detect cars. The detection results are not perfect but are suf\ufb01cient for creating the dataset.   Note that we ask annotators to only use correctly segmented instances in the next stage.   3.2   Stage 2: Manual Language Annotation   The goal of this stage is to collect natural language descriptions that describe the target object in a   3D scene for visual grounding. We prepare two interfaces: the language annotation interface and   the quality control interface. The former is used to collect descriptions, and the latter is used to   verify whether the collected descriptions are accurate. Below we present details of the interface   implementation, the language annotation task, and the quality control procedure.   Interface. Figure 4 shows the interface we used for language annotation and veri\ufb01cation. To   interactively show 3D scenes to annotators, we implement the interface with Potree [38], an opensource WebGL-based point cloud renderer that can process large-scale point cloud data. In the \ufb01gure,   the target object is highlighted in red. We provide interactive features such as zooming and panning   as well as the labels of each object. The annotators can view the regions of each object by clicking   the labels.   Language annotation. We ask annotators to describe the target object with the following instructions.   You will see geographic objects in different 3D outdoor scenes. Please describe the objects in   the 3D scene so that the objects can be uniquely identi\ufb01ed on the basis of your descriptions   and what you observed when submitting your responses. Some of the information below (a   combination is preferred) should be included in your description:   \u2022 The object\u2019s appearance, e.g., colors, shapes, materials.   \u2022 The object\u2019s location in the scene, e.g., the parking lot is in front of Birmingham Library.   5   Belvoir Road   379_TrafcRoad:   Targeted object   Other objects   ID and label   click   click   zoom up   zoom up   top view   top view   Descriptions: With this description, you can   identify a single geographical object from other   similar objects in this scene (candidate objects   are circled in red).   Check here if there is no object in   the 3D scene which corresponds to   the given description, or if multiple   segments correspond to the description.   Find an object corresponding to   the description in the 3D scene!   Thank you for your participation. Please read the instructions carefully.    Object ID   Number only. E.g., 10   (when 10_Building is    the corresponding object)    Black car in the parking lot near Nike Factory   Store, in the fifth lane from The Food Warehouse,   with no cars parked in front, behind, or to the left   or right. There is a tree diagonally next to the car.    This is a gray car parked in the farthest row   from the Food Warehouse in the parking space.   On its right there is a white car and on its left   there is a black car.   A white car parked between a gray and a black   car with a red car below it in the sixth section of   No corresponding   unique object    No corresponding   unique object    No corresponding   Captions:   E.g., A house with a blue roof and white walls along Bragg Road with   two red cars and one white car in front   See EXAMPLE above for other examples.   - With this caption, you can identify a single segment from other     similar objects in this scene.   - Use the landmark's name on the 3D map in the caption if necessary.   - More than ten words are required. Longer captions are preferred.   - Don't use object_id in the captions.   Object ID   268   Car   Check if the specified object is   (i) too tiny to write up captions,   (ii) differs from the specified   object type in the tag,   (iii) corresponds to multiple objects   (e.g., two cars are circled in red).   Don't skip writing even if you check here!   This is for quality-control.    361   Describe geographic objects in the 3D scene!   Thank you for your participation. Please read the instructions carefully.    Figure 4: Stage 2 of dataset construction. We ask annotators to describe the target object in the   3D scene so that the object can be uniquely identi\ufb01ed based on the description. With our interface,   annotators can see each object\u2019s region. Functions such as zooming and panning are also available.   \u2022 The spatial relation between this object and other objects, e.g., this building is the second one   from the left.   Imagine you and your friend live in a certain city, and you would like to ask your friend to \ufb01nd   a geographic object in that city. Since there may be many similar geographical objects, the   description of the object should be as unique as possible.   To ef\ufb01ciently collect data, we show at most three target objects to annotators in a 3D scene. We also   ask annotators to report segmentation errors and incorrect labels via another free-form text box and   checkboxes.   Quality control. To ensure the quality of the annotations, we ask another set of annotators to   manually perform visual grounding with the following instructions.   You will see 1-3 descriptions for different geographic objects. Please choose the geographical   object that best matches the description from the 3D scene (candidate objects are circled in red.)   Based on the results, we remove incorrect descriptions. We measured the accuracy of the annotation   with visual grounding again, and the correct response rate was 91.53%.   We used Amazon Mechanical Turk (MTurk) for annotation and quality control. There were 918 hours   of work with a total cost of $9,699 (the estimated hourly rate paid was $10.56). The total number of   participating workers was 282.   4   Dataset Statistics   This section provides basic statistics of the CityRefer dataset in comparison to ScanRefer [9] (an   indoor dataset) and KITTI360Pose [24] (a roadside dataset). We summarize the statistics in Table 3   and discuss them in detail below.   Target objects and descriptions. The CityRefer dataset consists of 35,196 descriptions, each of   which describes an object in the 3D scenes. The target objects fall under one of four categories: Car,   Building, Ground, and Parking. The distribution is shown in Figure 5 (left). Note that all of them are   unnamed objects; that means that landmark objects such as famous buildings and roads, which can be   identi\ufb01ed by their names on OpenStreetMap, are excluded from the target objects. The distribution of   description lengths is presented in Figure 6, along with those of ScanRefer and KITTI360Pose for   comparison. Here, KITTI360Pose exhibits a sharp length distribution of the descriptions because   they are automatically generated from a database with templates, e.g., the pose is south of a gray road.   In contrast, our dataset provides moderate-length descriptions to perform visual grounding. To the   best of our knowledge, the CityRefer dataset is the \ufb01rst large-scale dataset with manually annotated   descriptions of city-level 3D scenes.   6   Table 3: Comparison of datasets. Area: Type of scanned area. Manual: Whether annotation is   manual or not. Geo data: Availability of geographical data. Ndesc: Number of descriptions. Nobj:   Number of objects. Nlandmark: Number of landmark objects. V : Vocabulary size.   Dataset   Area   Manual   Geo data   Ndesc   Nobj   Nlandmark   V   ScanRefer [9]   Indoor   X   51,583   11,046   0   4,197   KITTI360Pose [24]   Roadside   43,381   6,800   0   41   CityRefer (Ours)   City center   X   X   35,196   5,866   1,850   6,683   Car   47.1%   Parking 3.6%   Ground   7.9%   Ground 1.7%   Building   41.4%   Footpath   21.2%   Rail 1.0%   Bridge 0.8%   Vegetation 0.3%   Water 0.8%   1,850   landmarks   35,196   descriptions   Building   35.4%   TrafcRoad   38.8%   Figure 5: Object category distribution.   CityRefer   ScanRefer   KITTIPose360   Number of words in a description   Density   0   0.5   0.4   0.3   0.2   0.1   Figure 6: Description lengths   (a) Building   (b) Car   (c) Ground   (d) Parking   Figure 7: Visualization of word distributions.   Landmark objects. There are 1,850 landmark objects with their names across seven categories:   Traf\ufb01cRoad, Building, Footpath, Ground, Rail, Bridge, Water, and Vegetation. The distribution is   shown in Figure 5 (right). Examples include Senate House Hill, Wellhead Lane, and Parkside Police   Station. They are used to refer to target objects, e.g., the gray rectangular building to the right of the   parking lot next to St. John\u2019s College Chapel.   Words. The vocabulary size of the CityRefer dataset is 6,683. Figure 7 shows the visualization   of word distributions. We can observe that words that specify object locations in the following   three categories are frequently used: 1) colors e.g., red, blue, gray, 2) relative positions e.g., right,   left, across, and 3) nearby objects tree, street, road. Compared with the template-based position   descriptions of KITTI360Pose, our free-form descriptions contain more natural descriptions.   5   Experiments   Finally, we conducted experiments on the CityRefer dataset, focusing on two tasks: instance segmentation and visual grounding. To ensure comprehensive evaluation, we divided the dataset into   three subsets: training, validation, and testing. The data split is summarized in Table 4, providing the   number of descriptions (Ndesc), objects (Nobj), and landmark objects (Nlmark).   5.1   City-level instance segmentation   This experiment focuses on evaluating the segmentation performance. The objective is to generate   segmentation masks with corresponding category labels for each object using 3D point clouds as   input.   7   Table 4: Dataset split for training, validation, and testing   Subset   Ndesc   Nobj   Nlmark   Train   23,586   3,931   1,106   Val   5,934   989   243   Test   5,676   946   501   Total   35,196   5,866   1,850   Table 5: Instance segmentation performance.   Target   AP   AP50   AP25   mRec mRec50   mRec25   Ground   19.9   39.8   52.9   28.0   47.2   60.6   Building   3.7   12.2   24.4   7.2   16.8   26.5   Parking   5.9   17.0   48.3   15.1   30.8   59.0   Car   35.3   55.0   69.4   42.2   58.9   70.9   Average   16.2   31.0   48.7   23.1   38.4   54.2   Method. We provide a PyTorch-based implementation of a baseline method using the SoftGroup++   model [45]. Due to the larger number of 3D points (2,847M points) compared to previous datasets for   3D instance segmentation, we randomly sample 2% of the 3D points and feed them into the model.   To provide transparency and reproducibility, we summarize the training details along with manually   tuned hyperparameters in the Appendix.   Evaluation metrics. We use average precision (AP) and mean recall (mRec) as our primary metrics.   In addition, we report the top-N AP and recall, with N set to 50 and 25. Speci\ufb01cally, we refer to   these metrics as AP50, AP25, mRec50, and mRec25.   Results. Table 5 shows the resulting performance for the four target categories. We see that cars are   relatively easier to segment compared to the segmentation of other objects. However, the overall   segmentation performance remains modest. This is because the CityRefer dataset involves many   similar instances placed near each other.   5.2   City-level visual grounding   In this experiment, we assess the visual grounding performance, which involves locating the target   object within the 3D point clouds based on a given natural language description. To evaluate the   performance of both instance segmentation and visual grounding separately, we assume that groundtruth instance segmentation masks are provided. We provide ten candidate answers, including the   correct answer, for each description.   Method. We modi\ufb01ed the InstanceRefer model [56] to enable city-level visual grounding. The   modi\ufb01ed model follows a four-step process to perform visual grounding. First, we extract object   features from each candidate instance by applying a four-layer sparse convolution network [16]   with an average pooling layer to input 3D points. Second, we use a one-layer bi-directional GRU   (BiGRU) [13] to extract language features from the given description. Third, object features and   language features are concatenated and fed into another BiGRU to obtain visual-language features   that represent the relationship between the description and the 3D objects. Finally, we compute   scores using a two-layer MLP. These scores indicate the likelihood of a candidate instance being the   correct grounding for the given description. We use the cross-entropy loss for training. For more   comprehensive information regarding the model architecture and training process, please refer to the   Appendix.   Evaluation metrics. We assess the accuracy of our predictions by comparing their intersection over   union (IoU) with the ground truth values. Speci\ufb01cally, we focus on positive predictions that exhibit a   higher IoU with the ground truth instances than a certain threshold k. We use the Acc@kIoU metric,   which is commonly used in the \ufb01eld of indoor 3D visual grounding research [9]. For our experiments,   we set the threshold value k for IoU to 0.25.   Results. Table 6 presents the visual grounding performance for the four target categories, comparing   the results of Random (random guess) and Baseline (the method described above). Baseline +   Land incorporates landmark features extracted from 3D points and the names of landmark objects,   in addition to the object and language features. We see that the baseline methods signi\ufb01cantly   perform better than the random guess but there is still a large gap between the system performance   and human performance (Acc. = 0.950). This demonstrates that city-level visual grounding is a   challenging task despite advances in learning technology. Developing large 3D-vision-language   models for city-level visual grounding would be a potential future research direction. Figure 8   illustrates qualitative examples of 3D visual grounding when incorporating landmark information.   8   Table 6: City-level 3D visual grounding performance.   Method   Building   Car   Ground   Parking   Overall   Random   0.103 \u00b1 0.008   0.103 \u00b1 0.006   0.091 \u00b1 0.017   0.094 \u00b1 0.010   0.101 \u00b1 0.005   Baseline   0.255 \u00b1 0.005   0.282 \u00b1 0.010   0.477 \u00b1 0.024   0.835 \u00b1 0.034   0.312 \u00b1 0.006   Baseline + Land   0.255 \u00b1 0.008   0.298 \u00b1 0.007   0.489 \u00b1 0.009   0.853 \u00b1 0.020   0.320 \u00b1 0.005   Humans   0.947   0.956   0.937   0.945   0.950   The building at the end of    Straffordshire Garden directly in ffront    of the drive from Staffordshire Street    with a red pickup truck and a ladder    lift in the drive in front of it.   A white van parked diagonally in the    corner of a parking lot, between    another white van and a bluish gray car,    in front of an L-shaped Brown building    with a gray roof off Ashcroft Grove   The large parking lot on Livingstone Road in front of a large    white building and has a large white van in the handicap    parking.   The large parking lot on Livingstone    Road in front of a large white building    and has a large white van in the    handicap parking.   The square green piece of ground left    of Emmanuel College Chapel and    below Hall building and right of Saint    Andrew's Street.   Building   Car   Ground   Parking   The building at the end of    Straffordshire Garden directly    in ffront of the drivefrom    Staffordshire Street with a red    pickup truck and a ladder lift in    the drive in front of it.   A white van parked diagonally in    the corner of a parking lot,    between another white van and    a bluish gray car, in front of an Lshaped Brown building with a    gray roof off Ashcroft Grove   The square green piece of    ground left of Emmanuel    College Chapel and below Hall    building and right of Saint    Andrew's Street.   The large parking lot on    Livingstone Road in front of a    large white building and has a    large white van in the handicap    parking.   Figure 8: Qualitative examples.   The examples demonstrate how the inclusion of landmark information aids in accurately identifying   the target objects. This observation further reinforces incorporating landmark information is a   promising way to improve the city-level visual grounding performance.   6   Conclusion   We have introduced the CityRefer dataset, a dataset for city-level visual grounding tasks. This   dataset offers a comprehensive 3D environment where instance-wise segmentation masks, along with   geographic data and labels, are provided. The creation of this environment involved performing a   spatial joint between the SensatUrban environment and the OpenStreetMap, resulting in a rich and   realistic urban setting. We also provided 35k natural language descriptions to locate objects as well   as baseline systems for instance segmentation and visual grounding.   Limitations and future work. In this work, we tackled visual grounding in the 3D environment of   real cities. However, there are still gaps between the real world and the 3D environment. In particular,   because all 3D scenes are static, the vocabulary of the CityRefer dataset is limited to words that specify   static objects. To achieve more comprehensive and real-world applicability, it would be necessary to   extend the dataset to include dynamic environments, where objects and scenes can change over time.   Furthermore, while our visual grounding system has demonstrated promising results, there is still   room for improvement. The main purpose of this work is to introduce a new dataset and indeed it   is nontrivial to apply visual grounding systems for our dataset; however, developing more accurate   systems would be worth pursuing in the future. We hope that the CityRefer dataset promotes further   research and development as well as discussion on geography-aware learning technologies.   Broader impacts. Although the dataset is constructed on the basis of a publicly available 2D map   (OpenStreetMap), visual grounding in general may result in privacy issues or racial and gender biases.   The natural language descriptions we collect are carefully checked so that they do not include private   information or offensive text.   Acknowledgements   This work was supported by JST PRESTO JPMJPR22P8 and JPMJPR20C2, and JSPS KAKENHI   22K12159.   9", "conf": "IJCAI", "year": "2023", "index": 597}, {"title": "CityRefer: Geography-aware 3D Visual Grounding   Dataset on City-scale Point Cloud Data   Taiki Miyanishi1,3\u21e4, Fumiya Kitamori2\u21e4,   Shuhei Kurita3,   Jungdae Lee2, Motoaki Kawanabe1,   Nakamasa Inoue2   1ATR,   2Tokyo Institute of Technology,   3RIKEN AIP", "abstract": "City-scale 3D point cloud is a promising way to express detailed and complicated   outdoor structures. It encompasses both the appearance and geometry features of   segmented city components, including cars, streets, and buildings, that can be utilized for attractive applications such as user-interactive navigation of autonomous   vehicles and drones. However, compared to the extensive text annotations available for images and indoor scenes, the scarcity of text annotations for outdoor   scenes poses a signi\ufb01cant challenge for achieving these applications. To tackle this   problem, we introduce the CityRefer dataset1 for city-level visual grounding. The   dataset consists of 35k natural language descriptions of 3D objects appearing in   SensatUrban [19] city scenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the quality and accuracy of the dataset, all descriptions and   labels in the CityRefer dataset are manually veri\ufb01ed. We also have developed a   baseline system that can learn encoded language descriptions, 3D object instances,   and geographical information about the city\u2019s landmarks to perform visual grounding on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset   is the largest city-level visual grounding dataset for localizing speci\ufb01c 3D objects.   1", "content": "Advancements in urban 3D scanning technologies, such as unmanned aerial vehicle photogrammetry   and mobile laser scanning, enable the creation of accurate and photorealistic large-scale 3D scene   datasets. Examples of such datasets include street-level datasets acquired by automobiles [6, 17,   37, 39, 42, 43] and city-level datasets acquired by aerial vehicles [19, 25, 36, 44, 52, 60]. However,   while city-level photorealistic 3D scans have become practical and applicable in various \ufb01elds like   autonomous driving and unmanned vehicle delivery, the technology to comprehend city scenes   through human-interactive linguistic representations is still in its early stages of development. The   ability to ground linguistic expressions to urban components is highly desired for interactive and   interpretable applications, such as language-guided autonomous driving and aerial drone navigation.   Achieving this requires the development of a 3D visual grounding dataset based on the city-scale   point clouds, which presents a signi\ufb01cant challenge.   3D Visual grounding is a 3D and language task that involves localizing objects in 3D scenes based   on textual referred expressions. Compared to its 2D visual grounding counterparts [23, 29, 31, 53],   3D visual grounding poses additional challenges. The expressions used in 3D visual grounding   often require more information to localize object instances due to the rich context of 3D scenes,   making the problem further complex. Recent studies have made remarkable progress in 3D visual   \u21e4equal contribution   1https://github.com/ATR-DBI/CityRefer   37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.   The white building with a gray roof,    between two other similar looking    buildings, on Acrefield Drive, just    before Woodvale Lodge.   320 Footpath: Aylestone Road   377 TrafcRoad: Aylestone Road   The red car in the middle    of the five cars that are    parked diagonally at the    end of Acrefield Drive.   The blue car across the street from a    red and white car on Belvoir Road    that is parked near a large tree and    in front of a yellow building.   CityRefer Dataset   CityRefer Dataset   5k+ labels for buildings and roads   35k+ natural language descriptions   for city-level 3D visual grounding   Figure 1: The CityRefer dataset for city-level 3D visual grounding.   grounding, focusing on determining the precise position of 3D objects given natural language   descriptions [7, 10, 18, 20, 21, 22, 35, 54, 56, 58]. However, many of these methods have been   evaluated on 3D indoor datasets, which typically consist of point clouds of room scenes and labels   for household objects [2, 9, 14]. Subsequently, Kolmet et al. [24] proposed a district-based visual   grounding dataset on KITTI-360 [26]. Nevertheless, the availability of 3D visual grounding datasets   is still limited, especially in the context of aerial city-level 3D point clouds. Therefore, we aim to   address this gap by creating a publicly available 3D visual grounding dataset based on aerial city-level   3D point clouds.   In this paper, we introduce the CityRefer dataset for city-level 3D visual grounding. Speci\ufb01cally, we   provide 35k natural language descriptions to localize 3D objects in the SensatUrban [19] environment   as well as 5k labels of objects such as buildings and roads. Three example descriptions of a sample   scene are shown in Figure 1. As seen from the \ufb01gure, city-level visual grounding is very challenging   because a system is required to \ufb01nd objects from a wide city area while understanding the description   of the target object and the relationships between relevant objects. Although we used crowdsourcing   to scale up the annotation, we needed to thoroughly \ufb01lter out incorrect annotations by hand to \ufb01nalize   the dataset and ensure the quality of the annotations. The main contributions of the CityRefer dataset   are summarized as follows.   1. We provide instance-wise segmentation masks for 5k objects including 1.8k landmark objects   with their name labels. Examples include Kem River and Baker Street, Aylestone Road, and   Belvoir Road. These labels were obtained from the spatial joint between SensatUrban and   OpenStreetMap using our semi-automatic system (Section 3.1).   2. We provide 35k natural language descriptions for city-level visual grounding. These descriptions   are manually attached using our interactive annotation system (Section 3.2).   3. We provide a baseline system that performs city-level 3D visual grounding. Because it is   nontrivial to adapt previous visual grounding methods for our city-level dataset, we propose   a simple but effective method that narrows the search area to \ufb01nd the target object by using   geographical information.   2   Table 1: Comparison of 3D visual grounding datasets. Ndesc : Number of natural language descriptions. \u00afLdesc : Average description length. Npoints : Number of 3D points.   Dataset   Human   annot.   Ndesc   \u00afLdesc   Area   Environment (Format)   Npoints   Indoor   REVERIE [32]   Yes   21,702   18.0   Rooms   Matterport3D (RGB) [8]   SUN-Spot [30]   Yes   7,987   14.1   Rooms   SUN RGB-D [40]   SUNRefer [27]   Yes   38,495   14.1   Rooms   SUN RGB-D [40]   Nr3D [2]   Yes   41,503   11.4   Rooms   ScanNet (3D Scan) [14]   242M   ScanRefer [9]   Yes   51,583   20.3   Rooms   ScanNet (3D Scan) [14]   242M   Outdoor   TouchDown [11]   Yes   25,575   29.7   Roadside   Google Street View (RGB)   KITTI360Pose [24]   No   43,381   7.6   Roadside   KITTI-360 (3D Scan) [26]   1,000M   CityRefer (Ours)   Yes   35,196   26.3   City center   SensatUrban (3D Scan) [19]   2,847M   2   Related Work   Visual Grounding Datasets for 3D Spaces. Over the years, there has been signi\ufb01cant research   interest in 3D visual grounding as summarized in Table 1. We \ufb01rst introduce two types of 3D visual   grounding datasets, each focusing on a different level of the scene: the indoor and outdoor roadside.   The 3D visual grounding dataset is created by annotating indoor or outdoor 3D datasets with linguistic   descriptions.   (i) Indoor scene level. With the increasing availability of indoor 3D datasets [3, 5, 8, 14, 46, 34,   40, 49], several visual grounding datasets have been proposed to address the demand for 3D scene   understanding. REVERIE [32] comprises 10,318 panorama images captured across 86 indoor scenes   and a total of 4,140 target objects. This dataset also provides 21,702 language instructions with rich   textual annotations for guiding virtual agents within indoor photorealistic scenes of Matterport3D [8].   The SUN-Spot [30] and SUNRefer [27] datasets focus on object localization in single-view RGB-D   images within indoor environments from the SUN RGB-D dataset [40]. Both datasets provide detailed   language annotations indicating the spatial extent and location of objects in the images including   object bounding boxes. Speci\ufb01cally, SUNRefer contains 38,495 language annotations for 7,699   objects in indoor RGB-D images. Nr3D [2] and ScanRefer [9] are standard 3D visual grounding   datasets that are built on top of ScanNet [14], a real-world 3D scene dataset with extensive semantic   annotations. However, these datasets mainly focus on the indoor visual grounding task.   (ii) Outdoor scene level. In recent years, a number of richly annotated outdoor 3D datasets have been   created by scanning cities using sensors installed in cars and drones [6, 17, 19, 25, 37, 42, 44, 50, 60].   While there have been efforts to annotate outdoor 3D datasets with language descriptions for 3D visual   grounding, the availability of such datasets is still limited compared to indoor ones. The TouchDown   dataset [11] is aimed at text-guided navigation and spatial reasoning using real-life visual observations.   It contains 9,326 examples of instructions and spatial descriptions in the visual navigation environment   drawn from Google Street View. KITTI360Pose [24] is a text-based position localization dataset in an   outdoor 3D environment from the KITTI360 dataset [50], which provides nine static scenes obtained   by LiDAR scans. It is notable that the linguistic descriptions of KITTI360Pose are automatically   generated by a sentence template with position description query pairs. While both TouchDown and   KITTI360 datasets are based on the vehicle perspective and hence limited to the semantics from   roadsides, our dataset is based on SensatUrban [19], which covers 3D semantics of the board city   areas that are generated from aerial images by drones.   Learning Visual Grounding of 3D Scenes. To facilitate a deeper understanding of 3D scenes   through language, there have been many efforts to connect languages and the 3D visual world,   including 3D dense captioning [12, 47, 55], 3D change detection [33], and 3D visual question   answering [1, 4, 15, 28, 59]. Speci\ufb01cally, 3D visual grounding that aims to locate an object in 3D space   in response to a natural language query is the fundamental task of the 3D and language \ufb01eld [2, 9, 48].   Several approaches in the \ufb01eld of visual grounding use pre-computed 3D object detection or instance   segmentation results, utilizing the point cloud features extracted from the corresponding 3D bounding   boxes and segments [20, 56]. However, the challenge of object recognition arises due to the low   resolution of 3D data resulting from the reconstruction process. To overcome this limitation, recent   3   133_TrafcRoad: Lens\ufb01eld Road   SensatUrban   141_Building: King\u2019s College Chapel   133_TrafcRoad: Lens\ufb01eld Road   3D point clouds   Geographical data   Landmark/road labels   CityRefer   141_Building: King\u2019s College Chapel   Figure 2: Stage 1 of dataset construction. We perform a spatial join between the 3D SensatUrban   environment and the 2D OpenStreetMap. The CityRefer dataset contains 1,850 landmark/road labels   and geographical data. Examples include the landmark name King\u2019s College Chapel and the road   name Lans\ufb01eld Road.   studies have proposed the integration of both 2D images and 3D data [51, 22]. By combining   the rich spatial information provided by 3D data with the detailed appearance cues derived from   2D images, these hybrid approaches aim to enhance robustness in the context of visual grounding.   Furthermore, there have been studies proposing methods that integrate 3D visual captioning and   grounding, where both models are learned simultaneously to achieve synergistic effects [7, 10]. These   current 3D visual grounding methods mainly rely on two widely used indoor 3D visual grounding   datasets [2, 9]. Several studies proposed visual grounding on remote sensing data but were limited   to 2D images [41, 57]. For these reasons, the performance of 3D visual grounding on outdoor 3D   datasets remains unexplored. One of the initial studies [24] attempted to identify regions within   the 3D point cloud based on textual queries on city-level 3D datasets but were limited to arti\ufb01cial   language descriptions and did not fully use the geographic information 3D map despite the sparsity   of city-level 3D point cloud data. In contrast, our method uses geographic information from 3D maps   to achieve accurate city-level 3D visual grounding.   3   Dataset Construction   The CityRefer dataset consists of 1) instance-wise segmentation masks each with a label and geographical information, and 2) natural language descriptions for visual grounding. The 3D environment   we use is the SensatUrban [19], which consists of photogrammetric point clouds of two UK cities   covering 6 km2 of the city landscape. Semantic segmentation masks are provided with the environment, and our study re\ufb01nes them to instance-level masks for visual grounding. The annotation   proceeds in two stages: semi-automatic generation of instance-wise segmentation masks (Stage 1)   and manual language annotation (Stage 2).   3.1   Stage 1: Semi-automatic Generation of Instance-wise Segmentation Masks   The goal of this stage is to generate segmentation masks for each 3D object as well as to attach labels   of names with geographical data (longitude, latitude, and elevation) to each object. To achieve this,   we perform a spatial join between the 3D environment and OpenStreetMap, from which we can   obtain names and locations. Figure 2 shows an example result of this stage. As shown, regions of   King\u2019s College Chapel and Lens\ufb01eld Road are visualized precisely. The procedures are described in   detail below.   Georeferencing. The georeferencing is performed in the following three steps. First, given a block2   of the 3D city, we create a top view image as shown in Figure 3a, where the image size is \ufb01xed to   2048 by 2048 pixels. Second, we manually choose ten points of interest, such as corner and landmark   points, that are clearly visible in OpenStreetMap. In this step, we also extract an image of the 2D   map as shown in Figure 3c. Note that the coordinates of the ten points are manually annotated on   2We use 34 blocks (scenes) of Cambridge and Birmingham from SensatUrban.   4   SensatUrban (3D scene)   OpenStreetMap (2D map)   Result of georeferencing   Figure 3: Georeferencing of 3D environment and 2D map.   both images. Finally, we compute the geometric transformation between the two images by using the   transformation function from the scikit-image library. We manually tune the hyper-parameters of   transformation by visually verifying the results of the georeferencing. Here, the ten points of interest   are also updated and tuned if needed. Figure 3b shows an example result in which the 3D scene and   the 2D map are precisely joined.   Generating instance-wise segmentation masks. Given semantic segmentation masks with respect   to 13 object categories, we re\ufb01ne them to instance-level masks. We divide the categories into three   groups as shown in Table 2. For Group 1, we directly use geographic information obtained from   OpenStreetMap to create \ufb01lters for each instance. For example, for a segment of Lans\ufb01eld Road   in OpenStreetMap, we create a \ufb01lter in the 3D scene based on the result of the georeferencing.   Table 2: Three groups of categories.   Grp.   Method   Categories   1   Filtering   Ground, HighVegetation,   Building, Bridge, Rail,   Traf\ufb01cRoad, Footpath, Water   2   Clustering   Wall, Parking,   StreetFurniture, Bike   3   Detection   Car   For Group 2, we apply the DBSCAN algorithm, a clustering method, to 3D points. It   is not dif\ufb01cult to obtain accurate boundaries   between instances by clustering because objects in this group are small and located separately. For Group 3, we used YOLOv7 to   detect cars. The detection results are not perfect but are suf\ufb01cient for creating the dataset.   Note that we ask annotators to only use correctly segmented instances in the next stage.   3.2   Stage 2: Manual Language Annotation   The goal of this stage is to collect natural language descriptions that describe the target object in a   3D scene for visual grounding. We prepare two interfaces: the language annotation interface and   the quality control interface. The former is used to collect descriptions, and the latter is used to   verify whether the collected descriptions are accurate. Below we present details of the interface   implementation, the language annotation task, and the quality control procedure.   Interface. Figure 4 shows the interface we used for language annotation and veri\ufb01cation. To   interactively show 3D scenes to annotators, we implement the interface with Potree [38], an opensource WebGL-based point cloud renderer that can process large-scale point cloud data. In the \ufb01gure,   the target object is highlighted in red. We provide interactive features such as zooming and panning   as well as the labels of each object. The annotators can view the regions of each object by clicking   the labels.   Language annotation. We ask annotators to describe the target object with the following instructions.   You will see geographic objects in different 3D outdoor scenes. Please describe the objects in   the 3D scene so that the objects can be uniquely identi\ufb01ed on the basis of your descriptions   and what you observed when submitting your responses. Some of the information below (a   combination is preferred) should be included in your description:   \u2022 The object\u2019s appearance, e.g., colors, shapes, materials.   \u2022 The object\u2019s location in the scene, e.g., the parking lot is in front of Birmingham Library.   5   Belvoir Road   379_TrafcRoad:   Targeted object   Other objects   ID and label   click   click   zoom up   zoom up   top view   top view   Descriptions: With this description, you can   identify a single geographical object from other   similar objects in this scene (candidate objects   are circled in red).   Check here if there is no object in   the 3D scene which corresponds to   the given description, or if multiple   segments correspond to the description.   Find an object corresponding to   the description in the 3D scene!   Thank you for your participation. Please read the instructions carefully.    Object ID   Number only. E.g., 10   (when 10_Building is    the corresponding object)    Black car in the parking lot near Nike Factory   Store, in the fifth lane from The Food Warehouse,   with no cars parked in front, behind, or to the left   or right. There is a tree diagonally next to the car.    This is a gray car parked in the farthest row   from the Food Warehouse in the parking space.   On its right there is a white car and on its left   there is a black car.   A white car parked between a gray and a black   car with a red car below it in the sixth section of   No corresponding   unique object    No corresponding   unique object    No corresponding   Captions:   E.g., A house with a blue roof and white walls along Bragg Road with   two red cars and one white car in front   See EXAMPLE above for other examples.   - With this caption, you can identify a single segment from other     similar objects in this scene.   - Use the landmark's name on the 3D map in the caption if necessary.   - More than ten words are required. Longer captions are preferred.   - Don't use object_id in the captions.   Object ID   268   Car   Check if the specified object is   (i) too tiny to write up captions,   (ii) differs from the specified   object type in the tag,   (iii) corresponds to multiple objects   (e.g., two cars are circled in red).   Don't skip writing even if you check here!   This is for quality-control.    361   Describe geographic objects in the 3D scene!   Thank you for your participation. Please read the instructions carefully.    Figure 4: Stage 2 of dataset construction. We ask annotators to describe the target object in the   3D scene so that the object can be uniquely identi\ufb01ed based on the description. With our interface,   annotators can see each object\u2019s region. Functions such as zooming and panning are also available.   \u2022 The spatial relation between this object and other objects, e.g., this building is the second one   from the left.   Imagine you and your friend live in a certain city, and you would like to ask your friend to \ufb01nd   a geographic object in that city. Since there may be many similar geographical objects, the   description of the object should be as unique as possible.   To ef\ufb01ciently collect data, we show at most three target objects to annotators in a 3D scene. We also   ask annotators to report segmentation errors and incorrect labels via another free-form text box and   checkboxes.   Quality control. To ensure the quality of the annotations, we ask another set of annotators to   manually perform visual grounding with the following instructions.   You will see 1-3 descriptions for different geographic objects. Please choose the geographical   object that best matches the description from the 3D scene (candidate objects are circled in red.)   Based on the results, we remove incorrect descriptions. We measured the accuracy of the annotation   with visual grounding again, and the correct response rate was 91.53%.   We used Amazon Mechanical Turk (MTurk) for annotation and quality control. There were 918 hours   of work with a total cost of $9,699 (the estimated hourly rate paid was $10.56). The total number of   participating workers was 282.   4   Dataset Statistics   This section provides basic statistics of the CityRefer dataset in comparison to ScanRefer [9] (an   indoor dataset) and KITTI360Pose [24] (a roadside dataset). We summarize the statistics in Table 3   and discuss them in detail below.   Target objects and descriptions. The CityRefer dataset consists of 35,196 descriptions, each of   which describes an object in the 3D scenes. The target objects fall under one of four categories: Car,   Building, Ground, and Parking. The distribution is shown in Figure 5 (left). Note that all of them are   unnamed objects; that means that landmark objects such as famous buildings and roads, which can be   identi\ufb01ed by their names on OpenStreetMap, are excluded from the target objects. The distribution of   description lengths is presented in Figure 6, along with those of ScanRefer and KITTI360Pose for   comparison. Here, KITTI360Pose exhibits a sharp length distribution of the descriptions because   they are automatically generated from a database with templates, e.g., the pose is south of a gray road.   In contrast, our dataset provides moderate-length descriptions to perform visual grounding. To the   best of our knowledge, the CityRefer dataset is the \ufb01rst large-scale dataset with manually annotated   descriptions of city-level 3D scenes.   6   Table 3: Comparison of datasets. Area: Type of scanned area. Manual: Whether annotation is   manual or not. Geo data: Availability of geographical data. Ndesc: Number of descriptions. Nobj:   Number of objects. Nlandmark: Number of landmark objects. V : Vocabulary size.   Dataset   Area   Manual   Geo data   Ndesc   Nobj   Nlandmark   V   ScanRefer [9]   Indoor   X   51,583   11,046   0   4,197   KITTI360Pose [24]   Roadside   43,381   6,800   0   41   CityRefer (Ours)   City center   X   X   35,196   5,866   1,850   6,683   Car   47.1%   Parking 3.6%   Ground   7.9%   Ground 1.7%   Building   41.4%   Footpath   21.2%   Rail 1.0%   Bridge 0.8%   Vegetation 0.3%   Water 0.8%   1,850   landmarks   35,196   descriptions   Building   35.4%   TrafcRoad   38.8%   Figure 5: Object category distribution.   CityRefer   ScanRefer   KITTIPose360   Number of words in a description   Density   0   0.5   0.4   0.3   0.2   0.1   Figure 6: Description lengths   (a) Building   (b) Car   (c) Ground   (d) Parking   Figure 7: Visualization of word distributions.   Landmark objects. There are 1,850 landmark objects with their names across seven categories:   Traf\ufb01cRoad, Building, Footpath, Ground, Rail, Bridge, Water, and Vegetation. The distribution is   shown in Figure 5 (right). Examples include Senate House Hill, Wellhead Lane, and Parkside Police   Station. They are used to refer to target objects, e.g., the gray rectangular building to the right of the   parking lot next to St. John\u2019s College Chapel.   Words. The vocabulary size of the CityRefer dataset is 6,683. Figure 7 shows the visualization   of word distributions. We can observe that words that specify object locations in the following   three categories are frequently used: 1) colors e.g., red, blue, gray, 2) relative positions e.g., right,   left, across, and 3) nearby objects tree, street, road. Compared with the template-based position   descriptions of KITTI360Pose, our free-form descriptions contain more natural descriptions.   5   Experiments   Finally, we conducted experiments on the CityRefer dataset, focusing on two tasks: instance segmentation and visual grounding. To ensure comprehensive evaluation, we divided the dataset into   three subsets: training, validation, and testing. The data split is summarized in Table 4, providing the   number of descriptions (Ndesc), objects (Nobj), and landmark objects (Nlmark).   5.1   City-level instance segmentation   This experiment focuses on evaluating the segmentation performance. The objective is to generate   segmentation masks with corresponding category labels for each object using 3D point clouds as   input.   7   Table 4: Dataset split for training, validation, and testing   Subset   Ndesc   Nobj   Nlmark   Train   23,586   3,931   1,106   Val   5,934   989   243   Test   5,676   946   501   Total   35,196   5,866   1,850   Table 5: Instance segmentation performance.   Target   AP   AP50   AP25   mRec mRec50   mRec25   Ground   19.9   39.8   52.9   28.0   47.2   60.6   Building   3.7   12.2   24.4   7.2   16.8   26.5   Parking   5.9   17.0   48.3   15.1   30.8   59.0   Car   35.3   55.0   69.4   42.2   58.9   70.9   Average   16.2   31.0   48.7   23.1   38.4   54.2   Method. We provide a PyTorch-based implementation of a baseline method using the SoftGroup++   model [45]. Due to the larger number of 3D points (2,847M points) compared to previous datasets for   3D instance segmentation, we randomly sample 2% of the 3D points and feed them into the model.   To provide transparency and reproducibility, we summarize the training details along with manually   tuned hyperparameters in the Appendix.   Evaluation metrics. We use average precision (AP) and mean recall (mRec) as our primary metrics.   In addition, we report the top-N AP and recall, with N set to 50 and 25. Speci\ufb01cally, we refer to   these metrics as AP50, AP25, mRec50, and mRec25.   Results. Table 5 shows the resulting performance for the four target categories. We see that cars are   relatively easier to segment compared to the segmentation of other objects. However, the overall   segmentation performance remains modest. This is because the CityRefer dataset involves many   similar instances placed near each other.   5.2   City-level visual grounding   In this experiment, we assess the visual grounding performance, which involves locating the target   object within the 3D point clouds based on a given natural language description. To evaluate the   performance of both instance segmentation and visual grounding separately, we assume that groundtruth instance segmentation masks are provided. We provide ten candidate answers, including the   correct answer, for each description.   Method. We modi\ufb01ed the InstanceRefer model [56] to enable city-level visual grounding. The   modi\ufb01ed model follows a four-step process to perform visual grounding. First, we extract object   features from each candidate instance by applying a four-layer sparse convolution network [16]   with an average pooling layer to input 3D points. Second, we use a one-layer bi-directional GRU   (BiGRU) [13] to extract language features from the given description. Third, object features and   language features are concatenated and fed into another BiGRU to obtain visual-language features   that represent the relationship between the description and the 3D objects. Finally, we compute   scores using a two-layer MLP. These scores indicate the likelihood of a candidate instance being the   correct grounding for the given description. We use the cross-entropy loss for training. For more   comprehensive information regarding the model architecture and training process, please refer to the   Appendix.   Evaluation metrics. We assess the accuracy of our predictions by comparing their intersection over   union (IoU) with the ground truth values. Speci\ufb01cally, we focus on positive predictions that exhibit a   higher IoU with the ground truth instances than a certain threshold k. We use the Acc@kIoU metric,   which is commonly used in the \ufb01eld of indoor 3D visual grounding research [9]. For our experiments,   we set the threshold value k for IoU to 0.25.   Results. Table 6 presents the visual grounding performance for the four target categories, comparing   the results of Random (random guess) and Baseline (the method described above). Baseline +   Land incorporates landmark features extracted from 3D points and the names of landmark objects,   in addition to the object and language features. We see that the baseline methods signi\ufb01cantly   perform better than the random guess but there is still a large gap between the system performance   and human performance (Acc. = 0.950). This demonstrates that city-level visual grounding is a   challenging task despite advances in learning technology. Developing large 3D-vision-language   models for city-level visual grounding would be a potential future research direction. Figure 8   illustrates qualitative examples of 3D visual grounding when incorporating landmark information.   8   Table 6: City-level 3D visual grounding performance.   Method   Building   Car   Ground   Parking   Overall   Random   0.103 \u00b1 0.008   0.103 \u00b1 0.006   0.091 \u00b1 0.017   0.094 \u00b1 0.010   0.101 \u00b1 0.005   Baseline   0.255 \u00b1 0.005   0.282 \u00b1 0.010   0.477 \u00b1 0.024   0.835 \u00b1 0.034   0.312 \u00b1 0.006   Baseline + Land   0.255 \u00b1 0.008   0.298 \u00b1 0.007   0.489 \u00b1 0.009   0.853 \u00b1 0.020   0.320 \u00b1 0.005   Humans   0.947   0.956   0.937   0.945   0.950   The building at the end of    Straffordshire Garden directly in ffront    of the drive from Staffordshire Street    with a red pickup truck and a ladder    lift in the drive in front of it.   A white van parked diagonally in the    corner of a parking lot, between    another white van and a bluish gray car,    in front of an L-shaped Brown building    with a gray roof off Ashcroft Grove   The large parking lot on Livingstone Road in front of a large    white building and has a large white van in the handicap    parking.   The large parking lot on Livingstone    Road in front of a large white building    and has a large white van in the    handicap parking.   The square green piece of ground left    of Emmanuel College Chapel and    below Hall building and right of Saint    Andrew's Street.   Building   Car   Ground   Parking   The building at the end of    Straffordshire Garden directly    in ffront of the drivefrom    Staffordshire Street with a red    pickup truck and a ladder lift in    the drive in front of it.   A white van parked diagonally in    the corner of a parking lot,    between another white van and    a bluish gray car, in front of an Lshaped Brown building with a    gray roof off Ashcroft Grove   The square green piece of    ground left of Emmanuel    College Chapel and below Hall    building and right of Saint    Andrew's Street.   The large parking lot on    Livingstone Road in front of a    large white building and has a    large white van in the handicap    parking.   Figure 8: Qualitative examples.   The examples demonstrate how the inclusion of landmark information aids in accurately identifying   the target objects. This observation further reinforces incorporating landmark information is a   promising way to improve the city-level visual grounding performance.   6   Conclusion   We have introduced the CityRefer dataset, a dataset for city-level visual grounding tasks. This   dataset offers a comprehensive 3D environment where instance-wise segmentation masks, along with   geographic data and labels, are provided. The creation of this environment involved performing a   spatial joint between the SensatUrban environment and the OpenStreetMap, resulting in a rich and   realistic urban setting. We also provided 35k natural language descriptions to locate objects as well   as baseline systems for instance segmentation and visual grounding.   Limitations and future work. In this work, we tackled visual grounding in the 3D environment of   real cities. However, there are still gaps between the real world and the 3D environment. In particular,   because all 3D scenes are static, the vocabulary of the CityRefer dataset is limited to words that specify   static objects. To achieve more comprehensive and real-world applicability, it would be necessary to   extend the dataset to include dynamic environments, where objects and scenes can change over time.   Furthermore, while our visual grounding system has demonstrated promising results, there is still   room for improvement. The main purpose of this work is to introduce a new dataset and indeed it   is nontrivial to apply visual grounding systems for our dataset; however, developing more accurate   systems would be worth pursuing in the future. We hope that the CityRefer dataset promotes further   research and development as well as discussion on geography-aware learning technologies.   Broader impacts. Although the dataset is constructed on the basis of a publicly available 2D map   (OpenStreetMap), visual grounding in general may result in privacy issues or racial and gender biases.   The natural language descriptions we collect are carefully checked so that they do not include private   information or offensive text.   Acknowledgements   This work was supported by JST PRESTO JPMJPR22P8 and JPMJPR20C2, and JSPS KAKENHI   22K12159.   9", "conf": "NIPS", "year": "2023", "index": 597}, {"title": "Grounding DINO: Marrying DINO with Grounded   Pre-Training for Open-Set Object Detection   Shilong Liu1,2\u22c6, Zhaoyang Zeng2, Tianhe Ren2, Feng Li2, 3, Hao Zhang2, 3,   Jie Yang2, 4, Qing Jiang2, 6 Chunyuan Li5, Jianwei Yang5,   Hang Su1, Jun Zhu1\u22c6\u22c6, Lei Zhang2\u22c6\u22c6.   1 Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys.,   Institute for AI, Tsinghua-Bosch Joint Center for ML, Tsinghua University   2 International Digital Economy Academy (IDEA)   3 The Hong Kong University of Science and Technology   4 The Chinese University of Hong Kong (Shenzhen)   5 Microsoft Research, Redmond   6 South China University of Technology   liusl20@mails.tsinghua.edu.cn, leizhang@idea.edu.cn   Standard Object Detection   COCO pre-defined categories   Zero-Shot Transfer to    Novel Categories   worldcup   Human-input novel categories   ear, lion, bench   The left lion   The bottom man with his head up   Referring Object Detection   (Referring Expression Comprehension)   Human-input reference sentences   bench   person   (c) Application: Image Editing   Collaborate with stable diffusion.    Prompt (modify background): All people    around the world cheer with a worldcup.   Prompt (modify detected objects): Dog   (b) Open-Set Object Detection   Object localization   Text understanding   (a) Closed-Set Object Detection   Fig. 1: (a) Closed-set object detection requires models to detect objects of pre-defined   categories. (b) We evaluate models on novel objects and standard Referring expression   comprehension (REC) benchmarks for model generalizations on novel objects with   attributes. (c) We present an image editing application by combining Grounding DINO   and Stable Diffusion [40]. Best viewed in colors.", "abstract": ". In this paper, we develop an open-set object detector, called   Grounding DINO, by marrying Transformer-based detector DINO with   grounded pre-training, which can detect arbitrary objects with human   inputs such as category names or referring expressions. The key solution of   open-set object detection is introducing language to a closed-set detector   for open-set concept generalization. To effectively fuse language and vision   modalities, we conceptually divide a closed-set detector into three phases   and propose a tight fusion solution, which includes a feature enhancer, a   language-guided query selection, and a cross-modality decoder for modalities fusion. We first pre-train Grounding DINO on large-scale datasets,   \u22c6This work was done when Shilong Liu, Feng Li, Hao Zhang, Jie Yang, and Qing   Jiang were interns at IDEA.   \u22c6\u22c6Corresponding authors.   2   S. Liu et al.   including object detection data, grounding data, and caption data, and   evaluate the model on both open-set object detection and referring object   detection benchmarks. Grounding DINO performs remarkably well on   all three settings, including benchmarks on COCO, LVIS, ODinW, and   RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO zeroshot1 detection benchmark. It sets a new record on the ODinW zero-shot   benchmark with a mean 26.1 AP. We release some checkpoints and inference codes at https://github.com/IDEA-Research/GroundingDINO.   Keywords: Object Detection \u00b7 Image Grounding \u00b7 Multi-modal learning   1", "content": "A key indicator of an Artificial General Intelligence (AGI) system\u2019s capability is   its proficiency in handling open-world scenarios. In this paper, we aim to develop   a strong system to detect arbitrary objects specified by human language inputs,   a task commonly referred to as open-set object detection2. The task has wide   applications for its great potential as a generic object detector. For example, we   can cooperate with generative models for image editing (as shown in Fig. 1 (b)).   In pursuit of this goal, we design the strong open-set object detector Grounding DINO by following the two principles: tight modality fusion based on   DINO [56] and large-scale grounded pre-train for concept generalization.   Tight modality fusion based on DINO. The key to open-set detection   is introducing language for unseen object generalization [1,6,24]. Most existing   open-set detectors are developed by extending closed-set detectors to open-set   scenarios with language information. As shown in Fig. 2, a closed-set detector   typically has three important modules, a backbone for feature extraction, a neck   for feature enhancement, and a head for region refinement (or box prediction).   A closed-set detector can be generalized to detect novel objects by learning   language-aware region embeddings so that each region can be classified into novel   categories in a language-aware semantic space. The key to achieving this goal is   using contrastive loss between region outputs and language features at the neck   and/or head outputs.   Backbone   (ResNet,Swin,\u2026)   Neck   (DyHead,Encoder,\u2026)   Head   (ROIHead, Decoder, \u2026)   Image    Features   Refined   Image    Features   Query    Init   Output   Regions   Closed-Set    Detector   Text Encoder   Open-Set    Detector   Feature    Fusion A   Contrastive    Loss A   Feature    Fusion B   Feature    Fusion C   Contrastive    Loss B   Text Features   Fig. 2: Extending closed-set detectors to open-set scenarios.   To help a model   align   cross-modality   information some work   tried to fuse features   before the final loss   stage. We summarize   the modulized design   1 In this paper, \u2018zero-shot\u2019 refers to scenarios where the training split of the test dataset   is not utilized in the training process.   2 We view the terms open-set object detection, open-world object detection, and openvocabulary object detection the same task in this paper. To avoid confusion, we always   use open-set object detection in our paper.   Grounding DINO   3   of object detectors in Fig. 2. Feature fusion can be performed in three phases:   neck (phase A), query initialization (phase B), and head (phase C). For example,   GLIP [24] performs early fusion in the neck module (phase A), and OV-DETR [54]   uses language-aware queries as head inputs (phase B). We argue that introducing   more feature fusion into the pipeline can facilitate better alignment between   different modality features, thereby achieving better performance.   Although conceptually simple, it is hard for previous work to perform feature   fusion in all three phases. The design of classical detectors like Faster RCNN   makes it hard to interact with language information in most blocks. Unlike   classical detectors, the Transformer-based detector method such as DINO has a   consistent structure with language blocks. The layer-by-layer design enables it to   interact with language information easily. Under this principle, we design three   feature fusion approaches in the neck, query initialization, and head phases. More   specifically, we design a feature enhancer by stacking self-attention, text-to-image   cross-attention, and image-to-text cross-attention as the neck module. We then   develop a language-guided query selection method to initialize queries for the   detection head. We also design a cross-modality decoder for the head phase with   image and text cross-attention layers to boost query representations.   Large-scale grounded pre-train for zero-shot transfer. Most existing   open-set models [13,20] rely on pre-trained CLIP models for concept generalization. Nevertheless, the efficacy of CLIP, specifically pre-trained on image-text   pairs, is limited for region-text pair detection tasks, as identified in the RegionCLIP study by RegionCLIP [60]. In contrast, GLIP [24] presents a different way   by reformulating object detection as a phrase grounding task and introducing contrastive training between object regions and language phrases on large-scale data.   It shows great flexibility for heterogeneous datasets and remarkable performance   on closed-set and open-set detection.   We have adopted and refined the grounded training methodology. GLIP\u2019s   approach involves concatenating all categories into a sentence in a random order.   However, the direct category names concatenation does not consider the potential   influence of unrelated categories on each other when extracting features. To   mitigate this issue and improve model performance during grounded training, we   introduce a technique that utilizes sub-sentence level text features. It removes the   attention between unrelated categories during word feature extractions. Further   elaboration on this technique can be found in Section 3.4.   We pre-train the Grounding DINO on a large-scale dataset and evaluate the   performance on mainstream object detection benchmarks like COCO [28]. While   some studies have examined open-set detection models under a \"partial label\"   framework\u2014training on a subset of data (e.g., base categories) and testing on   additional categories\u2014we advocate for a fully zero-shot approach to enhance   practical applicability. Moreover, we extend the model to another important   scenario Referring Expression Comprehension (REC) [29,33]3, where objects are   described with attributes.   3 We use the term Referring Expression Comprehension (REC) and Referring (Object)   Detection exchangeable in this paper.   4   S. Liu et al.   We conduct experiments on all three settings, including closed-set detection,   open-set detection, and referring object detection, as shown in Fig. 1, to comprehensively evaluate open-set detection performance. Grounding DINO outperforms   competitors by a large margin. For example, Grounding DINO reaches a 52.5 AP   on COCO minival without any COCO training data. It also establishes a new   state of the art on the ODinW [22] zero-shot benchmark with a 26.1 mean AP.   Model   Model Design   Text Prompt   Closed-Set Settings   Zero-Shot Transfer   Referring Detection   Base Detector   Fusion (Fig. 2)   CLIP   Represent. Level (Sec. 3.4)   COCO   COCO   LVIS   ODinW   RefCOCO/+/g   ViLD [13]   Mask R-CNN   \u2713   sentence   \u2713   partial label   partial label   RegionCLIP [60]   Faster RCNN   \u2713   sentence   \u2713   partial label   partial label   FindIt [20]   Faster RCNN   A   sentence   \u2713   partial label   fine-tune   MDETR [17]   DETR   A,C   word   fine-tune   zero-shot   fine-tune   DQ-DETR [44]   DETR   A,C   word   \u2713   zero-shot   fine-tune   GLIP [24]   DyHead   A   word   \u2713   zero-shot   zero-shot   zero-shot   GLIPv2 [57]   DyHead   A   word   \u2713   zero-shot   zero-shot   zero-shot   OV-DETR [54]   Deformable DETR   B   \u2713   sentence   \u2713   partial label   partial label   OWL-ViT [34]   \u2713   sentence   \u2713   partial label   partial label   zero-shot   DetCLIP [51]   ATSS   \u2713   sentence   zero-shot   zero-shot   OmDet [59]   Sparse R-CNN   C   \u2713   sentence   \u2713   zero-shot   Grounding DINO (Ours)   DINO   A,B,C   sub-sentence   \u2713   zero-shot   zero-shot   zero-shot   zero-shot   Table 1: A comparison of previous open-set object detectors. Our summarization is   based on the experiments in their paper, but not the ability to extend their models to   other tasks. It is worth noting that some related works may not (only) be designed for   the open-set object detection initially, like MDETR [17] and GLIPv2 [57], but we list   them here for a comprehensive comparison with existing work. We use the term \u201cpartial   label\u201d for the settings, where models are trained on partial data (e.g. base categories)   and evaluated on other cases. [55]   2   Related Work   Detection Transformers. Grounding DINO is built upon the DETR-like   model DINO [56], which is an end-to-end Transformer-based detector. DETR   was first proposed in [2] and then has been improved from many directions   [4,5,12,16,32,47,63] in the past few years. DAB-DETR [30] introduces anchor   boxes as DETR queries for more accurate box prediction. DN-DETR [23] proposes   a query-denoising approach to stabilizing the bipartite matching. DINO [56]   further develops several techniques including contrastive de-noising and sets a   new record on the COCO object detection benchmark. However, such detectors   mainly focus on closed-set detection and are difficult to generalize to novel classes   because of the limited pre-defined categories.   Open-Set Object Detection. Open-set object detection is trained using   existing bounding box annotations and aims at detecting arbitrary classes with   the help of language generalization. OV-DETR [55] uses image and text embedding   encoded by a CLIP model as queries to decode the category-specified boxes in   the DETR framework [2]. ViLD [13] distills knowledge from a CLIP teacher   model into a R-CNN-like detector so that the learned region embeddings contain   the semantics of language. GLIP [11] formulates object detection as a grounding   problem and leverages additional grounding data to help learn aligned semantics   Grounding DINO   5   A cat sets on a table .   cat . person . mouse .   Image    Backbone   Text    Backbone   Feature Enhancer   Language-guide    Query Selection   Cross-Modality Decoder   1. Model Overall   Input Text   Input Image   Model Outputs   Keys&   Values   Cross-Modality    Queries   Text    Features   Image    Features   Vanilla Text    Features   A Cross-Modality    Decoder Layer   Cross-Modality Query   Self-Attention   Image Cross-Attention   Text Cross-Attention   FFN   Updated    Cross-Modality    Query   Text Features   Image Features   3. A Decoder Layer   2. A Feature Enhancer Layer   Self-Attention   Image-to-text Cross-Attention   Text-to-image Cross-Attention   FFN   Deformable    Self-Attention   Image    Features   Text    Features   FFN   Q,K,V   Q   K,V   K,V   Q   Q,K,V   Q,K,V   Q   K,V   K,V   Q   Updated Image    Features   Updated Text    Features   Vanilla Image    Features   Text    Features   Contrastive loss   Localization loss   1   1   A cat sets on a table .   cat . person . mouse .   1   1   1   Fig. 3: The framework of Grounding DINO. We present the overall framework, a feature   enhancer layer, and a decoder layer in block 1, block 2, and block 3, respectively.   at phrase and region levels. It shows that such a formulation can even achieve   stronger performance on fully-supervised detection benchmarks. DetCLIP [51]   involves large-scale image captioning datasets and uses the generated pseudo   labels to expand the knowledge database. The generated pseudo labels effectively   help extend the generalization ability.   However, previous works only fuse multi-modal information in partial phases,   which may lead to sub-optimal language generalization ability. For example,   GLIP only considers fusion in the feature enhancement (phase A) and OV-DETR   only injects language information at the decoder inputs (phase B). Moreover, the   REC task is normally overlooked in evaluation, which is an important scenario   for open-set detection. We compare our model with other open-set methods in   Table 1.   3   Grounding DINO   Grounding DINO outputs multiple pairs of object boxes and noun phrases for a   given (Image, Text) pair. For example, as shown in Fig. 3, the model locates   a cat and a table from the input image and extracts word cat and table from   the input text as corresponding labels. Both object detection and REC tasks can   be aligned with the pipeline. Following GLIP [24], we concatenate all category   names as input texts for object detection tasks. REC requires a bounding box for   6   S. Liu et al.   each text input. We use the output object with the largest scores as the output   for the REC task.   Grounding DINO is a dual-encoder-single-decoder architecture. It contains   an image backbone for image feature extraction, a text backbone for text feature   extraction, a feature enhancer for image and text feature fusion (Sec. 3.1), a   language-guided query selection module for query initialization (Sec. 3.2), and a   cross-modality decoder for box refinement (Sec. 3.3).   For each (Image, Text) pair, we first extract vanilla image features and   vanilla text features using an image backbone and a text backbone, respectively.   The two vanilla features are fed into a feature enhancer module for cross-modality   feature fusion. After obtaining cross-modality text and image features, we use   a language-guided query selection module to select cross-modality queries from   image features. Like the object queries in most DETR-like models, these crossmodality queries will be fed into a cross-modality decoder to probe desired features   from the two modal features and update themselves. The output queries of the   last decoder layer will be used to predict object boxes and extract corresponding   phrases.   cat . baseball glove . A cat is sleeping on a table .   Encoder   Per-word features   Encoder   cat . baseball glove . A cat is sleeping on a table .   Per-word features   Encoder   Encoder   Encoder   Encoder   Per-sentence features   (a) Sentence level   (b) Word level   (c) Sub-sentence level   cat. baseball glove . A cat is sleeping on a table .   Fig. 4: Comparisons of text representations.   3.1   Feature Extraction and Enhancer   Given an (Image, Text) pair, we extract multi-scale image features with an   image backbone like Swin Transformer [31], and text features with a text backbone   like BERT [7]. Following previous DETR-like detectors [56,63], multi-scale features   are extracted from the outputs of different blocks. After extracting vanilla image   and text features, we fed them into a feature enhancer for cross-modality feature   fusion. The feature enhancer includes multiple feature enhancer layers. We   illustrate a feature enhancer layer in Fig. 3 block 2. We leverage the Deformable   self-attention to enhance image features and the vanilla self-attention for text   feature enhancers. Inspired by GLIP [24], we add an image-to-text and a textto-image cross-attention modules for feature fusion. These modules help align   features of different modalities.   3.2   Language-Guided Query Selection   Grounding DINO aims to detect objects from an image specified by an input   text. To effectively leverage the input text to guide object detection, we design a   Grounding DINO   7   language-guided query selection module to select features that are more relevant   to the input text as decoder queries.   Let\u2019s denote the image feature as XI \u2208RNI\u00d7d and the text features as   XT \u2208RNT \u00d7d. Here, NI represents the number of image tokens, NT indicates   the number of text tokens, and d corresponds to the feature dimension. In our   experiments, we specifically utilize a feature dimension of d = 256. Typically, in   our models, the value of NI exceeds 10, 000, while NT remains below 256. Our   objective is to extract Nq queries from the encoder\u2019s image features to be used   as inputs for the decoder. In alignment with the DINO method, we set Nq to be   900. The top Nq query indices for the image feature, denoted as INq, are selected   using the following expression:     \\ I mat _{N_q} = \\text   t t {Top}_{{N_q}}(\\texttt {Max}^{(-1)}(\\Xmat _{I}\\Xmat _{T}^{\\intercal })).    (1)   In this expression, TopNq represents the operation to pick the top Nq indices.   The function Max(\u22121) executes the max operation along the \u22121 dimension, and   the symbol \u22badenotes matrix transposition. We present the query selection   process in Algorithm 1 in PyTorch style. The language-guided query selection   module outputs Nq indices. We can extract features based on the selected indices   to initialize queries. Following DINO [56], we use mixed query selection to   initialize decoder queries. Each decoder query contains two parts: content part   and positional part [32], respectively. We formulate the positional part as dynamic   anchor boxes [30], which are initialized with encoder outputs. The other part,   the content queries, are set to be learnable during training.   3.3   Cross-Modality Decoder   We develop a cross-modality decoder to combine image and text modality features,   as shown in Fig. 3 block 3. Each cross-modality query is fed into a self-attention   layer, an image cross-attention layer to combine image features, a text crossattention layer to combine text features, and an FFN layer in each cross-modality   decoder layer. Each decoder layer has an extra text cross-attention layer compared   with the DINO decoder layer, as we need to inject text information into queries   for better modality alignment.   3.4   Sub-Sentence Level Text Feature   Two kinds of text prompts are explored in previous works, which we named as   sentence level representation and word level representation, as shown in Fig. 4.   Sentence level representation [34,51] encodes a whole sentence to one feature.   If some sentences in phrase grounding data have multiple phrases, it extracts   these phrases and discards other words. In this way, it removes the influence   between words while losing fine-grained information in sentences. Word level   representation [11,17] enables encoding multiple category names with one forward   but introduces unnecessary dependencies among categories, especially when the   input text is a concatenation of multiple category names in an arbitrary order.   8   S. Liu et al.   Model   Backbone   Pre-Training Data   Zero-Shot   Fine-Tuning   2017val   2017val/test-dev   Faster R-CNN   RN50-FPN   40.2 / Faster R-CNN   RN101-FPN   42.0 / DyHead-T [5]   Swin-T   49.7 / DyHead-L [5]   Swin-L   58.4 / 58.7   DyHead-L [5]   Swin-L   O365,ImageNet21K   60.3 / 60.6   SoftTeacher [49]   Swin-L   O365,SS-COCO   60.7 / 61.3   DINO(Swin-L) [56]   Swin-L   O365   62.5 / DyHead-T\u2020 [5]   Swin-T   O365   43.6   53.3 / GLIP-T (B) [24]   Swin-T   O365   44.9   53.8 / GLIP-T (C) [24]   Swin-T   O365,GoldG   46.7   55.1 / GLIP-L [24]   Swin-L   FourODs,GoldG,Cap24M   49.8   60.8 / 61.0   DINO(Swin-T)\u2020 [56]   Swin-T   O365   46.2   56.9 / Grounding DINO T (Ours)   Swin-T   O365   46.7   56.9 / Grounding DINO T (Ours)   Swin-T   O365,GoldG   48.1   57.1 / Grounding DINO T (Ours)   Swin-T   O365,GoldG,Cap4M   48.4   57.2 / Grounding DINO L (Ours)   Swin-L   O365,OI [18],GoldG   52.5   62.6 / 62.7 (63.0 / 63.0)*   Grounding DINO L (Ours)   Swin-L   O365,OI,GoldG,Cap4M,COCO,RefC   60.7\u2021   62.6 / Table 2: Zero-shot domain transfer and fine-tuning on COCO. * The results in brackets   are trained with 1.5\u00d7 image sizes, i.e., with a maximum image size of 2000. \u2020The models   map a subset of O365 categories to COCO for zero-shot evaluations. \u2021This is not a real   zero-shot performance as we add COCO data during model training. We list the result   here for a reference.   As shown in Fig. 4 (b), some unrelated words interact during attention. To avoid   unwanted word interactions, we introduce attention masks to block attentions   among unrelated category names, named \u201csub-sentence\u201d level representation. It   eliminates the influence between different category names while keeping per-word   features for fine-grained understanding.   3.5   Loss Function   Following previous DETR-like works [2,23,30,32,56,63], we use the L1 loss and   the GIOU [39] loss for bounding box regressions. We follow GLIP [24] and use   contrastive loss between predicted objects and language tokens for classification.   Specifically, we dot product each query with text features to predict logits for   each text token and then compute focal loss [27] for each logit. Box regression and   classification costs are first used for bipartite matching between predictions and   ground truths. We then calculate final losses between ground truths and matched   predictions with the same loss components. Following DETR-like models, we add   auxiliary loss after each decoder layer and after the encoder outputs.   4   Experiments   We conduct extensive experiments on three settings: a closed-set setting on the   COCO detection benchmark (Sec. C.2), an open-set setting on zero-shot COCO,   LVIS, and ODinW (Sec. 4.2), and a referring detection setting on RefCOCO/+/g   (Sec. 4.3). Ablations are then conducted to show the effectiveness of our model   design (Sec. 4.5). We also explore a way to transfer a well-trained DINO to the   Grounding DINO   9   open-set scenario by training a few plug-in modules in Sec. C.1. The test of our   model efficiency is presented in Sec. C.4.   4.1   Implementation Details   We trained two model variants, Grounding DINO T with Swin-T [31], and   Grounding DINO L with Swin-L [31] as an image backbone, respectively. We   leveraged BERT-base [7] from Hugging Face [48] as text backbones. As we focus   more on the model performance on novel classes, we list zero-shot transfer and   referring detection results in the main text.   By default, we use 900 queries in our model following DINO. We set the   maximum text token number as 256. Using BERT as our text encoder, we follow   BERT to tokenize texts with a BPE scheme [41]. We use six feature enhancer   layers in the feature enhancer module. The cross-modality decoder is composed   of six decoder layers as well. We leverage deformable attention [63] in image   cross-attention layers.   Both matching costs and final losses include classification losses (or contrastive   losses), box L1 losses, and GIOU [39] losses. Following DINO, we set the weight of   classification costs, box L1 costs, and GIOU costs as 2.0, 5.0, and 2.0, respectively,   during Hungarian matching. The corresponding loss weights are 1.0, 5.0, and 2.0   in the final loss calculation.   Our Swin Transformer Tiny models are trained on 16 Nvidia V100 GPUs   with a total batch size of 32. We extract three image feature scales, from 8\u00d7 to   32\u00d7. It is named \u201c4scale\u201d in DINO since we downsample the 32\u00d7 feature map to   64\u00d7 as an extra feature scale. For the model with Swin Transformer Large, we   extract four image feature scales from backbones, from 4\u00d7 to 32\u00d7. The model is   trained on 64 Nvidia A100 GPUs with a total batch size of 64.   4.2   Zero-Shot Transfer of Grounding DINO   In this setting, we pre-train models on large-scale datasets and directly evaluate   models on new datasets. We also list some fine-tuned results for a more thorough   comparison of our model with prior works.   COCO Benchmark We compare Grounding DINO with GLIP and DINO in   Table 2. We pre-train models on large-scale datasets and directly evaluate our   model on the COCO benchmark. As the O365 dataset [42] has (nearly4) covered   all categories in COCO, we evaluate an O365 pre-trined DINO on COCO as a   zero-shot baseline. The result shows that DINO performs better on the COCO   zero-shot transfer than DyHead. Grounding DINO outperforms all previous   models on the zero-shot transfer setting, with +0.5AP and +1.8AP compared   with DINO and GLIP under the same setting. Grounding data is still helpful for   Grounding DINO, introducing more than 1AP (48.1 vs. 46.7) on the zero-shot   4 It is not an exact mapping between O365 and COCO categories. We made some   approximations during evaluation.   10   S. Liu et al.   transfer setting. With stronger backbones and larger data, Grounding DINO   sets a new record of 52.5 AP on the COCO object detection benchmark without   seeing any COCO images during training. Grounding DINO obtains a 62.6 AP   on COCO minival, outperforming DINO\u2019s 62.5 AP. When enlarging the input   images by 1.5\u00d7, the benefits reduce. We suspect that the text branch enlarges the   gap between models with different input images. Even though the performance   plateaus with larger input size, Grounding DINO gets an impressive 63.0 AP   on COCO test-dev with fine-tuning on the COCO dataset(See the number in   brackets of Table 2).   LVIS Benchmark LVIS [14] is a dataset for long-tail objects. It contains more   than 1000 categories for evaluation. We use LVIS as a downstream task to test   the zero-shot abilities of our model. We use GLIP and DetCLIPv2 as baselines   for our models. The results are shown in Table 3.   Model   Backbone   Pre-Training Data   MiniVal [17]   AP   APr/APc/APf   Zero-Shot Setting   GLIP-T (C)   Swin-T   O365,GoldG   24.9   17.7/19.5/31.0   GLIP-T   Swin-T   O365,GoldG,Cap4M   26.0   20.8/21.4/31.0   DetCLIPv2   Swin-T   O365,GoldG,CC15M   40.4   36.0/41.7/40.0   Grounding DINO T   Swin-T   O365,GoldG   25.6   14.4/19.6/32.2   Grounding DINO T   Swin-T   O365,GoldG,Cap4M   27.4   18.1/23.3/32.7   Grounding DINO L   Swin-L   O365,OI,GoldG,Cap4M,   COCO,RefC   33.9   22.2/30.7/38.8   Fine-Tune Setting   MDETR   RN101   GoldG,RefC   24.2   20.9/24.9/24.3   Mask R-CNN   RN101   33.3   26.3/34.0/33.9   DetCLIPv2 [50]   Swin-T   O365,GoldG,CC15M   50.7   44.3/52.4/50.3   Grounding DINO T   Swin-T   O365,GoldG   52.1   35.4/51.3/55.7   Table 3: Model results on LVIS.   We found two interesting phenomena in the results. First, Grounding DINO   works better than common objects than GLIP, but worse on rare categories. We   reviewed DETR-like models on LVIS and noted these models often exhibit lower   rare category AP despite similar overall AP, like Table 2 of [8] and Table 6 of [17].   To our knowledge, no existing DETR-like models effectively address the rarity   challenge in LVIS without extra training data, which may be a characteristic   limitation of the architecture.   The other phenomenon is that Grounding DINO has larger gains with more   data than GLIP. For example, Grounding DINO introduces +1.8 AP gains with   the caption data Cap4M, whereas GLIP has only +1.1 AP. We believe that   Grounding DINO has better scalability compared with GLIP. A larger-scale   training will be left as our future work.   Although achieving better results than GLIP, we found that Grounding DINO   is inferior to DetCLIPv2, which is trained on a larger scale data. This performance   difference might be attributed to the disparity in data distribution between the   training dataset and the LVIS dataset.   Grounding DINO   11   To unveil the full potential of Grounding DINO, we fine-tuned it on the LVIS   dataset. Table 3 highlights the commendable capability of our model. Remarkably,   despite being pre-trained only on the O365 and GoldG datasets, Grounding   DINO outperforms DetCLIPv2-T by a margin of 1.5 AP. This result shows   that Grounding DINO might have learned a better object-level representation   which helps yield a better performance after fine-tuning (aligning with the target   dataset). In our future work, we will perform more studies, including varying the   semantic concept coverage of the training data and increasing the scale of the   training data, to further improve the zero-shot generalization performance.   Model   Language Input   Backbone   Model Size   Pre-Training Data   Test   APaverage APmedian   Zero-Shot Setting   MDETR [17]   \u2713   ENB5 [45]   169M   GoldG,RefC   10.7   3.0   OWL-ViT [34]   \u2713   ViT L/14(CLIP)   >1243M   O365, VG   18.8   9.8   GLIP-T [24]   \u2713   Swin-T   232M   O365,GoldG,Cap4M   19.6   5.1   OmDet [59]   \u2713   ConvNeXt-B   230M   COCO,O365,LVIS,PhraseCut   19.7   10.8   GLIPv2-T [58]   \u2713   Swin-T   232M   O365,GoldG,Cap4M   22.3   8.9   DetCLIP [51]   \u2713   Swin-L   267M   O365,GoldG,YFCC1M   24.9   18.3   Florence [53]   \u2713   CoSwinH   \u2248841M   FLD900M,O365,GoldG   25.8   14.3   Grounding DINO T(Ours)   \u2713   Swin-T   172M   O365,GoldG   20.0   9.5   Grounding DINO T(Ours)   \u2713   Swin-T   172M   O365,GoldG,Cap4M   22.3   11.9   Grounding DINO L(Ours)   \u2713   Swin-L   341M   O365,OI,GoldG,Cap4M,COCO,RefC   26.1   18.4   Few-Shot Setting   DyHead-T [5]   \u2717   Swin-T   \u2248100M   O365   37.5   36.7   GLIP-T [24]   \u2713   Swin-T   232M   O365,GoldG,Cap4M   38.9   33.7   DINO-Swin-T [56]   \u2717   Swin-T   49M   O365   41.2   41.1   OmDet [59]   \u2713   ConvNeXt-B   230M   COCO,O365,LVIS,PhraseCut   42.4   41.7   Grounding DINO T(Ours)   \u2713   Swin-T   172M   O365,GoldG   46.4   51.1   Full-Shot Setting   GLIP-T [24]   \u2713   Swin-T   232M   O365,GoldG,Cap4M   62.6   62.1   DyHead-T [5]   \u2717   Swin-T   \u2248100M   O365   63.2   64.9   DINO-Swin-T [56]   \u2717   Swin-T   49M   O365   66.7   68.5   OmDet [59]   \u2713   ConvNeXt-B   230M   COCO,O365,LVIS,PhraseCut   67.1   71.2   DINO-Swin-L [56]   \u2717   Swin-L   218M   O365   68.8   70.7   Grounding DINO T(Ours)   \u2713   Swin-T   172M   O365,GoldG   70.7   76.2   Table 4: Model results on the ODinW benchmark.   ODinW Benchmark ODinW (Object Detection in the Wild) [22] is a more   challenging benchmark to test model performance under real-world scenarios. It   collects more than 35 datasets for evaluation. We report three settings, zero-shot,   few-shot, and full-shot results in Table 4. Grounding DINO performs well on   this benchmark. With only O365 and GoldG for pre-train, Grounding DINO T   outperforms DINO on few-shot and full-shot settings. Impressively, Grounding   DINO with a Swin-T backbone outperforms DINO with Swin-L on the full-shot   setting.   Grounding DINO outperforms GLIP under the same backbone for the zeroshot setting. Grounding DINO and GLIPv2-T show similar APaverage. However,   a key distinction lies in the APmedian, where Grounding DINO significantly   outperforms GLIPv2-T (11.9 vs 8.9). This suggests that while GLIPv2 may exhibit   larger performance variance across different datasets, Grounding DINO maintains   a more consistent performance level. GLIPv2 incorporates advanced techniques   like masked text training and cross-instance contrastive learning, making it   12   S. Liu et al.   more complex than our Grounding DINO model. Moreover, our model is more   compact (172M parameters) compared to GLIPv2 (232M parameters). These   factors combined\u2014performance consistency, model complexity, and size\u2014should   address concerns about our model\u2019s capability in true open-set scenarios.   Grounding DINO L set a new record on ODinW zero-shot with a 26.1 AP, even   outperforming the giant Florence models [53]. The results show the generalization   and scalability of Grounding DINO.   4.3   Referring Object Detection Settings   We further explore our models\u2019 performances on the REC task. We leverage   GLIP [24] as our baseline. We evaluate the model performance on RefCOCO/+/g   directly.5 The results are shown in Table 5. Grounding DINO outperforms GLIP   under the same setting. Nevertheless, both GLIP and Grounding DINO perform   not well without REC data. More training data like caption data or larger models   help the final performance, but quite minor. After injecting RefCOCO/+/g data   into training, Grounding DINO obtains significant gains. The results reveal that   most nowadays open-set object detectors need to pay more attention for a more   fine-grained detection.   Method   Backbone   Pre-Training Data   Fine-tuning   RefCOCO   RefCOCO+   RefCOCOg   val   testA testB   val   testA testB   val   test   MAttNet [52]   R101   None   \u2713   76.65 81.14 69.99 65.33 71.62 56.02 66.58 67.27   VGTR [9]   R101   None   \u2713   79.20 82.32 73.78 63.91 70.09 56.51 65.73 67.23   TransVG [6]   R101   None   \u2713   81.02 82.72 78.35 64.82 70.70 56.94 68.67 67.73   VILLA_L\u2217[10]   R101   CC, SBU, COCO, VG   \u2713   82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71   RefTR [25]   R101   VG   \u2713   85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01   MDETR [17]   R101   GoldG,RefC   \u2713   86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89   DQ-DETR [44]   R101   GoldG,RefC   \u2713   88.63 91.04 83.51 81.66 86.15 73.21 82.76 83.44   GLIP-T(B)   Swin-T   O365,GoldG   49.96 54.69 43.06 49.01 53.44 43.42 65.58 66.08   GLIP-T   Swin-T   O365,GoldG,Cap4M   50.42 54.30 43.83 49.50 52.78 44.59 66.09 66.89   Grounding DINO T (Ours)   Swin-T   O365,GoldG   50.41 57.24 43.21 51.40 57.59 45.81 67.46 67.13   Grounding DINO T (Ours)   Swin-T   O365,GoldG,RefC   73.98 74.88 59.29 66.81 69.91 56.09 71.06 72.07   Grounding DINO T (Ours)   Swin-T   O365,GoldG,RefC   \u2713   89.19 91.86 85.99 81.09 87.40 74.71 84.15 84.94   Grounding DINO L (Ours)*   Swin-L   O365,OI,GoldG,Cap4M,COCO,RefC   \u2713   90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02   Table 5: Top-1 accuracy comparison on the referring expression comprehension task.   We mark the best results in bold. All models are trained with a ResNet-101 backbone.   We use the notations \u201cCC\u201d, \u201cSBU\u201d, \u201cVG\u201d, \u201cOI\u201d, \u201cO365\u201d, and \u201cYFCC\u201d for Conceptual   Captions [43], SBU Captions [35], Visual Genome [19], OpenImage [21], Objects365 [62],   YFCC100M [46] respectively. The term \u201cRefC\u201d is used for RefCOCO, RefCOCO+,   and RefCOCOg three datasets. * There might be a data leak since COCO includes   validation images in RefC. But the annotations of the two datasets are different.   4.4   Effects of RefC and COCO Data   We add the RefCOCO/+/g (we note it as \u201cRefC\u201d in tables) and COCO into   training in some settings. We explore the influence of these data in Table 6.   5 We used the official released code and checkpoints in https://github.com/   microsoft/GLIP.   Grounding DINO   13   The results show that RefC helps improve the COCO zero-shot and fine-tuning   performance but hurts the LVIS and ODinW results. With COCO introduced,   the COCO results are greatly improved. It shows that COCO brings marginal   improvements in LVIS and slight decreases on ODinW.   Model   Pre-Train   COCO minival   LVIS minival   ODinW   Zero-Shot   Fine-Tune   Zero-Shot   Zero-Shot   Grounding DINO T   O365,GoldG   48.1   57.1   25.6   20.0   Grounding DINO T   O365,GoldG,RefC   48.5   57.3   21.9   17.7   Grounding DINO T   O365,GoldG,RefC,COCO   56.1   57.5   22.3   17.4   Table 6: Impacts of RefC and COCO data for open-set settings. All models are trained   with a Swin Transformer Tiny backbone.   4.5   Ablations   We conduct ablation studies in this section. We propose a tight fusion grounding   model for open-set object detection and a sub-sentence level text prompt. To   verify the effectiveness of the model design, we remove some fusion blocks for   different variants. Results are shown in Table 7. All models are pre-trained on   O365 with a Swin-T backbone.   The results show that encoder fusion significantly improves model performance   on both COCO and LVIS datasets. The results from comparing model #1 with the   baseline model #0 validate this observation. Other techniques, such as languageguided query selection, text cross-attention, and sub-sentence text prompt, also   contribute positively to the LVIS performance, yielding significant gains of +3.0   AP, +1.8 AP, and +0.5 AP, respectively. Additionally, these methods enhance the   COCO zero-shot performance, further underscoring their effectiveness. However,   we observed that language-guided query selection and sub-sentence text prompt   had minimal impact on the COCO fine-tune performance. This outcome is   reasonable, given that these methods do not alter model parameters or add   computational burdens. Text cross-attention, while introducing fewer parameters   than encoder fusion, showed less performance improvement compared to encoder   fusion (+0.6 vs. +0.8). This finding suggests that fine-tuning performance is   predominantly influenced by the model\u2019s parameters, indicating that scaling   models is a promising direction for enhancing performance.   5   Conclusion   We have presented a Grounding DINO model in this paper. Grounding DINO   extends DINO to open-set object detection, enabling it to detect arbitrary objects   given texts as queries. We review open-set object detector designs and propose a   tight fusion approach to better fusing cross-modality information. We propose   a sub-sentence level representation to use detection data for text prompts in a   more reasonable way. The results show the effectiveness of our model design and   14   S. Liu et al.   #ID   Model   COCO minival   LVIS minival   Zero-Shot   Fine-Tune   Zero-Shot   0   Grounding DINO (Full Model)   46.7   56.9   16.1   1   w/o encoder fusion   45.8   56.1   13.1   2   static query selection   46.3   56.6   13.6   3   w/o text cross-attention   46.1   56.3   14.3   4   word-level text prompt   46.4   56.6   15.6   Table 7: Ablations for our model. All models are trained on the O365 dataset with a   Swin Transformer Tiny backbone.   fusion approach. Moreover, we extend open-set object detection to REC tasks   and perform evaluation accordingly. We show that existing open-set detectors do   not work well for REC data without fine-tuning. Hence we call extra attention   to REC zero-shot performance in future studies.   Limitations: Despite the great performance on open-set object detection settings,   Grounding DINO cannot be used for segmentation tasks like GLIPv2. Our training   data is less than the largest GLIP model, which may limit our final performance.   Moreover, we find that our model will produce false positive results in some cases,   which may need more techniques or data to reduce the hallucination.   Social Impacts: The use of deep learning models, such as this one, exposes   them to vulnerabilities through adversarial attacks. Additionally, the accuracy   and correctness of the model\u2019s outputs cannot be guaranteed. There is also the   risk that the open-set detection capabilities of the model could be exploited for   unlawful purposes.   Acknowledgement   We thank the authors of GLIP [24]: Liunian Harold Li, Pengchuan Zhang, and   Haotian Zhang for their helpful discussions and instructions. We also thank   Tiancheng Zhao, the author of OmDet [59], and Jianhua Han, the author of   DetCLIP [51], for their response to their model details. We thank He Cao of   The Hong Kong University of Science and Technology for his help on diffusion   models.", "conf": "ECCV", "year": "2024", "index": 241}, {"title": "SAT: 2D Semantics Assisted Training for 3D Visual Grounding   Zhengyuan Yang1   Songyang Zhang1   Liwei Wang 2   Jiebo Luo1   1University of Rochester   2The Chinese University of Hong Kong   {zyang39, szhang83, jluo}@cs.rochester.edu   lwwang@cse.cuhk.edu.hk", "abstract": "3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented   in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited   semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem   more challenging. In this study, we propose 2D Semantics   Assisted Training (SAT) that utilizes 2D image semantics in   the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The   main idea is to learn auxiliary alignments between rich,   clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes.   SAT takes 2D   object semantics, i.e., object label, image feature, and 2D   geometric feature, as the extra input in training but does   not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the   accuracy on the Nr3D dataset from 37.7% to 49.2%, which   signi\ufb01cantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach   outperforms the state of the art by large margins on multiple   3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.   1.", "content": "Visual grounding provides machines the ability to   ground a language description to the targeted visual region.   The task has received wide attention in both datasets [54,   31, 19] and methods [16, 46, 53, 50]. However, most previous visual grounding studies remain on images [54, 31, 19]   and videos [57, 38, 51], which contain 2D projections of   inherently 3D visual scenes. The recently proposed 3D visual grounding task [1, 4] aims to ground a natural language   description about a 3D scene to the region referred to by a   language query (in the form of a 3D bounding box). The   3D visual grounding task has various applications, including autonomous agents [40, 47], human-machine interac(a) w/o 2D semantics   (c) Ours: 2D semantics assisted training   \"It is the office chair, next    to the one in front of the    computer monitor.\"   \"It is the office chair, next    to the one in front of the    computer monitor.\"   3D object proposals   Query words   2D image semantics   3D object proposals   Query words   Training losses for joint    representation learning   :   (b) 2D semantics as extra inputs   \"It is the office chair, next    to the one in front of the    computer monitor.\"   Query words   3D+2D visual inputs   Only in training   Previous methods   ...   ...   The main 3D grounding loss   :   SAT\u2019s 2D auxiliary losses   :   Figure 1. 3D visual grounding aims to ground a language query   to a targeted 3D object region, as shown by the green 3D bounding box. (a) Previous 3D visual grounding studies are trained with   a sole 3D grounding loss that maximizes the similarity between   positive object-query pairs. However, the sole objective is less effective as point clouds are sparse and noisy. (b) 2D semantics contain rich and clean object representations and can be used as extra   visual inputs to assist 3D grounding. However, requiring extra 2D   inputs in inference limits potential application scenarios. (c) Our   proposed 2D Semantics Assisted Training (SAT) uses 2D semantics only in training and does not require extra inputs in inference.   The green and red boxes are the targeted and distracting objects.   tion in augmented/mixed reality [20, 22], intelligent vehicles [29, 12], and so on.   Visual grounding tries to learn a good joint representation between visual and text modalities, i.e., the 3D   point cloud and language query in 3D visual grounding.   As shown in Figure 1 (a), previous 3D grounding studies [1, 4, 17, 55] directly learn the joint representation with   a sole 3D grounding objective of maximizing the positive object-query pairs\u2019 similarity scores. Speci\ufb01cally, the   model \ufb01rst generates a set of 3D object proposals and then   fuses each proposal with the language query to predict a   similarity score. The framework is trained with a sole objective that maximizes the paired object-query scores and minimizes the unpaired ones\u2019 scores. However, direct joint representation learning is challenging and less effective since   3D point clouds are inherently sparse, noisy, and contain   limited semantic information. Given that the 2D object representation provides rich and clean semantics, we explore   using 2D image semantics to help 3D visual grounding.   1856   How to assist 3D tasks with 2D image semantics remains   an open problem. Previous studies on 3D object detection   and segmentation have proposed a series of methods that   take 2D semantics as extra visual inputs to assist 3D tasks.   Representative approaches include aligning the 2D object   detection results with 3D bounding boxes [34, 48, 25] and   concatenating the image visual feature with 3D points [24,   14, 44, 41, 32]. However, these methods require extra 2D   inputs in both training and inference. The necessity of extra   input 2D data during inference limits potential application   scenarios since 2D inputs might not exist in inference or require extra pre-processing, such as 2D-3D matching and 2D   detection. Instead of as extra visual inputs in both training   and inference (as shown in Figure 1 (b)), we explore using   2D semantics only in training to assist 3D visual grounding.   In this study, we propose 2D Semantics Assisted Training (SAT), which utilizes 2D image semantics (in the form   of object label, image feature, and 2D geometric feature)   to ease joint representation learning between the 3D scene   and language query. As shown in Figure 1 (c), in addition   to the main 3D visual grounding loss [1, 4] that maximizes   the score between the paired 3D object and language query,   SAT introduces auxiliary loss functions that align objects in   2D images with the corresponding ones in 3D point clouds   or language queries. The learned auxiliary alignments effectively distill the rich and clean 2D object representation   to assist 3D visual grounding.   Speci\ufb01cally, in SAT, we   study the training loss design for auxiliary alignments and   the encoding method for 2D semantics features. For the former, we propose an object correspondence loss based on the   triplet loss [18, 10, 45, 26] for 3D and 2D object alignment.   For the latter, we propose a transformer attention mask that   generates good 2D semantics features and prevents leaking   2D inputs to the output module.   We experiment with the SAT approach on a transformerbased model [42] we propose and name as 3D grounding   transformer. We benchmark SAT on the Nr3D [1], Sr3D [1],   and ScanRef [4] datasets. The extra 2D semantics, together   with SAT\u2019s specially designed way of using them, effectively help the model learn a better 3D object point cloud   representation and ease joint representation learning. With   the same network architecture and inference inputs, SAT   improves the grounding accuracy on Nr3D from the nonSAT baseline\u2019s 37.7% to 49.2%.   In summary, our main contributions are:   \u2022 We propose 2D Semantics Assisted Training (SAT)   that assists 3D visual grounding with 2D semantics.   To the best of our knowledge, SAT is the \ufb01rst method   that helps 3D tasks with 2D semantics in training but   does not require 2D inputs during inference.   \u2022 With the proposed object correspondence loss and the   2D semantics encoding method, SAT effectively utilizes 2D semantics to learn a better 3D object representation, which leads to signi\ufb01cant accuracy improvements on the Nr3D [1] (+10.4% in absolute accuracy),   Sr3D [1] (+9.9%), and ScanRef [4] (+5.6%) datasets.   2. Related Work   3D visual grounding. 3D visual grounding aims to ground   the language referred object in a 3D scene (in the form of   RGB-XYZ point clouds) to a 3D bounding box. Two recent works Referit3D [1] and ScanRef [4], independently   proposed datasets and baseline methods for the 3D visual   grounding task. Both works [1, 4] augment the 3D scans   in the ScanNet [7] dataset with the manually annotated language queries to construct the 3D visual grounding datasets.   Previous 3D grounding studies [1, 4, 17, 55, 11, 36, 58]   follow a two-stage framework. In the \ufb01rst stage, multiple   3D object proposals are generated either with ground truth   objects [1] or a 3D object detector [4, 33]. In the second   stage, 3D object proposal features are fused with the language query to predict each proposal\u2019s matching scores. A   softmax grounding loss is applied to maximize the score between the paired object proposal and language query.   We \ufb01nd that the sole objective of similarity score maximization is less effective because the point clouds for object   proposals are sparse and noisy. In this study, we explore using 2D image semantics to assist 3D visual grounding.   2D semantics in 3D tasks. Studies on 3D object detection and segmentation have explored using 2D image semantics to assist 3D tasks. There exist two representative   approaches, i.e., 1) projecting image object detection results   into 3D space to assist 3D box prediction [34, 48, 25] and 2)   concatenating the image feature with each point in the 3D   scene as the extra information for the 3D tasks [24, 14, 44,   41, 32]. ImVoteNet [32] fuses the image object detection   results with 3D points.   Previous studies use 2D image semantics as the extra inputs to 3D tasks and thus require the extra 2D information   in both training and inference. Despite the performance improvement, the extra 2D inputs potentially limit the application scenarios since the 2D information either does not   exist in inference or requires tedious pre-processing, such   as 2D-3D matching and 2D object detection. In this study,   we explore using 2D semantics only in training to assist 3D   visual grounding.   Image visual grounding. 3D visual grounding is related to   the image visual grounding task [19, 30, 54, 28]. There are   mainly two approaches in image visual grounding, namely   the one- and two-stage frameworks. The one-stage methods [50, 37, 49, 8] fuse the language query with each   pixel/patch in image and predict grounding boxes densely   at all spatial locations. The two-stage methods [54, 45, 53]   \ufb01rst generate object proposals based on the visual objectiveness. The methods then compare each proposal with the   language query to select the grounding prediction.   1857   We follow previous 3D grounding studies [1, 4, 17] and   experiment with our proposed SAT on a two-stage framework introduced in Section 3. We focus on using 2D semantics to assist 3D grounding in this study and leave the   exploration of alternative frameworks to future studies.   3. 3D Grounding Transformer   Before introducing our proposed 2D semantics assisted   training (SAT), we \ufb01rst overview the problem modeling of   the 3D visual grounding task, and a transformer-based network architecture that we experiment on, named the 3D   grounding transformer.   3.1. 3D visual grounding inputs   The input to the 3D visual grounding task is a 3D scene   S \u2208RN\u00d76 in the form of RGB-XYZ point clouds with N   points and a natural language query with K words. In the   training stage, SAT takes the extra input of 2D semantics   extracted from the original ScanNet videos [7] to ease joint   representation learning. We detail how SAT represents and   utilizes 2D semantics in following sections.   3.2. Embedding for all modalities   3D scene embedding. Following previous studies [1, 4,   17], we assume the access to M 3D object proposals (in the   form of point cloud object segments) in scene S. The proposals are either generated with ground truth objects as in   Referit3D [1] or by a detection network [33] as in ScanRef [4]. After getting the proposals, we normalize each   object\u2019s center and size [43], and encode the point cloud   segment of each proposal into a feature vector xpc   m with   PointNet++ [35, 1, 4, 43]. We obtain the d-dimensional   3D proposal embedding {O1, \u00b7 \u00b7 \u00b7 , OM} with two learned   linear transforms, where   Om = LN(W1xpc   m) + LN(W2xoffset   m   ).   xpc   m is the PointNet++\u2019s output feature. xoffset   m   is a 4D vector   with the normalization offset, i.e., the center offsets (x, y, z)   and the original size r for proposal m. W1, W2 are learned   projection matrices. LN(\u00b7) is layer normalization [3].   2D semantics embedding. For each 3D proposal m, we   project its point clouds onto L sampled frames in the original ScanNet videos [7] and get the corresponded 2D image   semantics (image region, 2D bounding box, object class).   In sampled frame l \u2208{1, \u00b7 \u00b7 \u00b7 , L}, we represent the 2D semantics for proposal m by its visual feature xROI   m,l (Region   of interest feature from a visual genome [23] pre-trained   Faster-RCNN detector [39]), semantic feature xcls   m,l (onehot class vector), and geometric feature xgeo   m,l (2D bounding   box coordinates and frame\u2019s camera pose). We obtain the   d-dimensional 2D semantics Im,l with linear transforms:   Im,l = LN(W3xROI   m,l + W4xcls   m,l) + LN(W5xgeo   m,l),   (1)   where W3, W4, W5 are learned projection matrices and   LN(\u00b7) is layer normalization. We note that a 3D proposal   Om corresponds to multiple 2D semantic feature vector   Im,l obtained from different frames l. We randomly choose   one of Im,l, l \u2208{1, \u00b7 \u00b7 \u00b7 , L} as the corresponding 2D semantics in each epoch of training in SAT. We refer to the   sampled d-dimensional 2D semantic vector as Im, which   corresponds to the 3D proposal Om.   Text embedding. Given a query with K words, we embed   the text input with a pre-trained BERT model [9] into a set   of d-dimensional word feature vectors {Q1, \u00b7 \u00b7 \u00b7 , QK}. We   \ufb01ne-tune the BERT text encoder during training.   3.3. Fusion and grounding module   After respectively embedding each modality into multiple d-dimension feature vectors, we apply a stack of transformer layers [42] to fuse the input modalities (query words,   3D objects proposals, and if training 2D semantics). We denote the transformer\u2019s output features at the language, 3D   proposal, and 2D semantics positions as F Q, F O, and F I.   An output grounding module that consists of two fully   connected layers projects fused features {F O   1 , \u00b7 \u00b7 \u00b7 , F O   M}   into a set of M grounding scores {SO   1 , \u00b7 \u00b7 \u00b7 , SO   M}, respectively. The object proposal m with the highest grounding   score is selected as the \ufb01nal grounding prediction.   4. 2D Semantics Assisted Training (SAT)   SAT learns auxiliary alignments between the 2D object   semantics and the objects in 3D scenes/language queries   to assist 3D visual grounding. Figure 2 overviews SAT in   training and inference with the 3D grounding transformer.   We study two technical problems in SAT. First, in Section 4.1, we propose the auxiliary training objectives that   align 2D semantics with the 3D scene and language query.   Second, in Section 4.2, we introduce the 2D semantics encoding method that generates the fused feature F I from 2D   inputs I. We use F I in computing the auxiliary losses.   4.1. Training objectives   In addition to the main training objective between the 3D   scene and language query, SAT introduces auxiliary training   objectives to align 2D semantics with the 3D scene and language query. We apply the \u201cvisual grounding loss\u201d between   the query and 3D/2D visual inputs. We propose an \u201cobject   correspondence loss\u201d between the 3D and 2D objects.   3D visual grounding loss. We \ufb01rst introduce the main visual grounding loss LO   V G between the 3D scene and language query [27, 50, 1, 4].   Visual grounding loss LV G   is a softmax loss over grounding scores SO   m for proposals   m \u2208{1, \u00b7 \u00b7 \u00b7 , M}. The proposal with the highest Intersection over Union (IoU) with the ground truth region is labeled 1 and all remaining ones have label 0 (the highest IoU   1858   (a) SAT Training    (b) Inference   3D object proposals   Fusion Module (Multi-modal Transformer)   uery words | Label    words   2D image semantics   3D grounding   0/1   Fusion Module (Multi-modal Transformer)   ...   ...   ...   ...   ...   ...   3D object proposals   ...   ...   3D object proposals   Query words   2D image semantics   object correspondence   1   1   1   1   1   1   0   0   1   Attention mask   1   1   0   1   1   0   0   0   0   Attention mask   2D image semantics   \"It is the office chair,    next to the one in    front of the monitor.\"   ...   Query words   ...   Attention mask   3D object proposals   Query words   1   1   1   1   1   0   1   0   0   0   1   1   0   \"It is the office chair,    next to the one in    front of the monitor.\"   Query words   Prevent from Attending   Attention mask   Prevent from Attending   3D grounding   2D grounding   (   ,   )   Figure 2. The proposed 2D semantics assisted training (SAT) for 3D visual grounding. (a) In training, SAT takes 2D semantics as extra   input and helps 3D visual grounding with the auxiliary objectives of 2D visual grounding LI   V G and object correspondence prediction Lcor.   (b) In inference, SAT does not require 2D inputs and is easy to use. SAT\u2019s attention mask prevents query words and 3D proposals from   attending on 2D image semantics I in training (top \ufb01ve rows of the mask), avoiding performance drop in inference when I is not available.   equals 1.0 when experimented with ground truth object proposals). LO   V G encourages the model to generate high scores   for positive proposals. In inference, the proposal with the   highest score SO is selected as the \ufb01nal prediction.   2D visual grounding loss.   We apply the 2D grounding loss LI   V G with the same form as LO   V G between the   2D semantics and language query.   A separate grounding head with two fully connected layers projects the   fused features {F I   1 , \u00b7 \u00b7 \u00b7 , F I   M} into the 2D grounding scores   {SI   1, \u00b7 \u00b7 \u00b7 , SI   M}. LI   V G is the softmax loss computed over SI.   Object correspondence loss. The proposed object correspondence loss learns the correspondence between the objects in 3D scenes and the ones in 2D images. We design the   object correspondence loss as a triplet loss [18, 10, 45, 26]:   Lcor =   M   X   m=1   n\u0002   \u03b1 \u2212s(F O   m, F I   m) + s(F O   m, F I   i )   \u0003   +   +   \u0002   \u03b1 \u2212s(F O   m, F I   m) + s(F O   j , F I   m)   \u0003   +   o   ,   where s(\u00b7) is the similarity function. We use the inner product over the L2 normalized feature F O and F I as s(\u00b7) in   our experiments. \u03b1 is the margin with a default value of   0.1. i, j are the index for the hard negatives where i =   argmaxi\u0338=ms(F O   m, F I   i ) and j = argmaxj\u0338=ms(F O   j , F I   m).   We compute the object correspondence among the 3D and   2D object proposals m within each sample (3D scene). We   do not construct negatives across different 3D scenes.   We optimize the model with the following loss function:   L = LO   V G +LI   V G +Lcor \u2217wcor +(LO   cls +LQ   cls)\u2217wcls, (2)   where wcor is the weight for the object correspondence loss   with a default value of 10. In addition to 3D/2D grounding loss LV G and object correspondence loss Lcor, we add   query and object classi\ufb01cation losses LQ   cls and LO   cls as in   Referit3D [1]. The query feature Q0 and proposal feature   O are projected with fully connected layers to predict the   object classes for the language query and 3D proposals. We   follow the classi\ufb01cation loss weight wcls of 0.5 [1]. Ablation studies on the losses are in the supplementary material.   4.2. 2D semantics encoding   SAT uses the fused 2D semantic feature F I to compute   2D visual grounding loss LI   V G and object correspondence   loss Lcor. In this subsection, we introduce how to encode   F I from 2D semantics I. We show that a simple yet effective approach to encode F I is by introducing proper attention masks in the multi-modal transformer. Speci\ufb01cally,   we adopt the same stack of transformer layers to jointly encode the three input modalities Q, O, and I. We design   the attention mask in Figure 2 (a) such that F Q and F O do   not directly attend to 2D inputs I (the top \ufb01ve rows of the   mask). In this way, the proposed mask prevents the model   from directly using 2D inputs I for grounding prediction   and thus avoids the performance drop in inference when I   is not available. Meanwhile, the proposed attention mask allows the model to reference both 2D semantics I and other   input features Q and O when generating F I (the bottom   three rows of the mask).   We \ufb01nd that both properties of the proposed mask, i.e.,   masking 2D inputs I from F Q and F O, and referencing Q   1859   and O when generating F I, are critical to SAT\u2019s success.   We discuss alternative methods as follow. 1) Methods that   do not mask I from F Q and F O will leak 2D inputs to F O   and partially rely on I to generate the grounding prediction   in training. Therefore, the grounding accuracy drops catastrophically in inference when no 2D inputs I are available.   2) Encoding F Q/F O and F I independently with Q/O and   I avoid the 2D input leakage. However, without referencing   the scene context Q and O, the 2D feature F I fails to generate relevant object representations that effectively help 3D   visual grounding. We show related ablations in Section 5.4.   5. Experiments   5.1. Datasets   Nr3D. The Natural Reference in 3D (Nr3D) dataset [1] augments the indoor 3D scene dataset ScanNet [7] with 41, 503   natural language queries annotated by Amazon Mechanical   Turk (AMT) workers. There exist 707 unique indoor scenes   with targets belong to one of the 76 object classes. There   are multiple but no more than six distractors (objects in the   same class as the target) in the scene for each target. The   dataset splits follow the of\ufb01cial ScanNet [7] splits.   Sr3D/Sr3D+.   The Spatial Reference in 3D (Sr3D)   dataset [1] contains 83, 572 queries automatically generated   based on a \u201ctarget\u201d-\u201cspatial relationship\u201d-\u201canchor object\u201d   template. The Sr3D+ dataset further enlarges Sr3D with the   samples that do not have multiple distractors in the scene   and ends up with 114, 532 queries.   ScanRef. The ScanRef dataset [4] augments the 800 3D   indoor scenes in the ScanNet [7] dataset with 51, 583 language queries. ScanRef follows the of\ufb01cial ScanNet [7]   splits and contains 36, 665, 9, 508, and 5, 410 samples in   train/val/test sets, respectively.   5.2. Experiment settings   Evaluation metric.   We follow the experiment settings   in Referit3D [1] and ScanRef [4] for experiments with   ground truth and detector-generated proposals, respectively.   Speci\ufb01cally, Referit3D [1] assumes the access to ground   truth objects as the 3D proposals and converts the grounding   task into a classi\ufb01cation problem. The models are evaluated   by the accuracy, i.e., whether the model correctly selects   the referred object among M proposals. We choose this   \u201cusing ground truth proposal\u201d setting as the default setting   and present the results on all experimented datasets (Nr3d,   Sr3d, and ScanRef).   Alternatively, ScanRef [4] adopts a 3D object detector [33] to generate object proposals.   On the ScanRef   dataset, we also evaluate models using Acc@kIoU, i.e., the   fraction of language queries whose predicted box overlaps   the ground truth with IoU> kIoU. We experiment with the   IoU threshold kIoU of 0.25 and 0.5. For clarity, we present   the experiments with ground truth proposals in the main paper and postpone the experiments of \u201cSAT with detectorgenerated proposals\u201d to the supplementary material.   Implementation details.   We set the dimension d in all   transformer layers as 768. We experiment with a text transformer with 3 layers and a fusion transformer with 4 layers [15, 52]. The text transformer is initialized from the \ufb01rst   three layers of BERTBASE [9], and the fusion transformer is   trained from scratch. We sample 1024 points for each 3D   proposal from its point cloud segment and encode the proposal with PointNet++ [35]. We follow the max sentence   length and proposal numbers in Referit3D [1] and ScanRef [4] when experimented on Nr3D/Sr3D and ScanRef,   respectively. The model is trained with the Adam [21] optimizer with a batch size of 16. We set an initial learning   rate of 10\u22124 and reduce the learning rate by a multiplicative   factor of 0.65 every 10 epochs for a total of 100 epochs.   Compared methods. We compare SAT with the state-ofthe-art methods [1, 4, 17, 55] and the non-SAT baseline.   \u201cNon-SAT\u201d adopts the same \u201c3D grounding transformer\u201d   architecture used in \u201cSAT.\u201d The only difference is that \u201cnonSAT\u201d does not include 2D semantics in training and thus   does not use the auxiliary losses LI   V G and Lcor. With the   same network architecture and experiment settings, \u201cnonSAT\u201d is a directly comparable baseline to \u201cSAT.\u201d The performance difference shows how much SAT could help the   3D visual grounding task.   5.3. 3D visual grounding results   Nr3D. Table 1 reports the grounding accuracy on the   Nr3D [1] dataset. Both \u201cnon-SAT\u201d and \u201cSAT\u201d use the 3D   grounding transformer introduced in Section 3. For SAT\u2019s   reported accuracy, we encode 2D semantics I from the visual feature xROI, object semantic feature xcls, and geometric feature xgeo following Eq. 1. We postpone the ablation studies on the types of 2D semantics to Section 5.4.   Different columns show the results with different training   data, i.e., using Nr3D\u2019s training set only, or jointly training   with Sr3D/Sr3D+\u2019s training set. We take \u201cSAT-Nr3D\u201d as   the default setting and refer to it as \u201cSAT.\u201d We refer to the   experiments with extra data as \u201cSAT w/ Sr3D/Sr3D+.\u201d   The top \ufb01ve rows of Table 1 show that our baseline \u201cnonSAT\u201d already achieves comparable performance to the state   of the art (non-SAT: 37.7%, InstanceRefer [55]: 38.8%).   By effectively utilizing 2D semantics in training, our proposed SAT improves the non-SAT baseline accuracy from   37.7% to 49.2%, with the identical model architecture and   inference inputs. SAT also outperforms the state-of-the-art   accuracy [55] of 38.8% a large margin of +10.4%. Jointly   using the Sr3D/Sr3D+ training data further improves the   grounding accuracy. As shown in the last row, \u201cSAT w/   Sr3D+\u201d improves \u201cSAT-Nr3D\u201d from 49.2% to 56.5%.   Analyses reveal that SAT learns a better 3D object repre1860   Table 1. The 3D grounding accuracy on Nr3D [1] with different training data (Nr3D training set only or with extra data from Sr3D/Sr3D+).   Method   Nr3D   w/ Sr3D   w/ Sr3D+   V + L [1]   26.6\u00b10.5%   Ref3DNet [1]   35.6\u00b10.7%   37.2\u00b10.3%   37.6\u00b10.4%   TGNN [17]   37.3\u00b10.3%   IntanceRefer [55]   38.8\u00b10.4%   non-SAT   37.7\u00b10.3%   43.9\u00b10.3%   45.9\u00b10.2%   SAT (Ours)   49.2\u00b10.3%   53.9\u00b10.2%   56.5\u00b10.1%   Table 2. The 3D grounding accuracy on Sr3D [1] with different training data (Sr3D training set only or with extra data from Nr3D).   Method   Sr3D   w/ Nr3D   V + L [1]   33.0\u00b10.4%   Ref3DNet [1]   40.8\u00b10.2%   41.5\u00b10.2%   TGNN [17]   45.0\u00b10.2%   IntanceRefer [55]   48.0\u00b10.3%   non-SAT   47.4\u00b10.2%   50.1\u00b10.1%   SAT (Ours)   57.9\u00b10.1%   60.7\u00b10.2%   Table 3. The accuracy on ScanRef [4] with different training data   (ScanRef training set only or with extra data from Nr3D/Sr3D+).   Method   ScanRef   w/ Nr3D   w/ Sr3D+   Ref3DNet [1]   46.9\u00b10.2%   47.5\u00b10.4%   47.0\u00b10.3%   non-SAT   48.2\u00b10.2%   50.2\u00b10.1%   51.7\u00b10.1%   SAT (Ours)   53.8\u00b10.1%   57.0\u00b10.3%   56.5\u00b10.2%   sentation with the assist of 2D semantics, which leads to the   11.5% improvement over the non-SAT baseline. The improvement brought by Sr3D/Sr3D+ mainly comes from better modeling the spatial relationships in queries. We present   these analyses in Section 6.   Sr3D. Table 2 shows the grounding accuracy on Sr3D [1].   We draw similar conclusions from Table 2 as in Table 1 that   1) SAT signi\ufb01cantly improves the grounding accuracy from   47.4% to 57.9%, 2) SAT outperforms the previous state of   the art [1, 17, 55] by large margins, and 3) extra training data   (Nr3D) further boosts the accuracy from 57.9% to 60.7%.   ScanRef. Table 3 reports the grounding accuracy on the   ScanRef dataset [4] with ground truth object proposals. We   observe a signi\ufb01cant improvement of \u201cSAT\u201d over the nonSAT baseline (SAT: 53.8%, non-SAT: 48.2%). Extra training data from Nr3D and Sr3D+ further improves the accuracy from 53.8% to 57.0% and 56.5%, respectively.   In addition to the ground-truth object proposals [1], we   experiment with the proposal setting in ScanRef [4] that   generates proposals with a 3D detector [33]. To apply SAT,   we \ufb01rst compute the ground truth 2D semantics of\ufb02ine. In   training, we match each predicted 3D proposal with a 2D   semantics object that has the largest IoU with the 3D proposal.   We then evaluate the models with the Acc@0.25   and Acc@0.50 metrics. SAT achieves the Acc@0.25 and   Acc@0.50 of 44.54% and 30.14%, outperforming the nonSAT baseline of 38.92% and 26.40% by a large margin. We   introduce the details of \u201cSAT with detector-generated proposals\u201d in the supplementary material.   5.4. Ablation studies   Multi-modal transformer masks. SAT\u2019s attention mask   in the multi-modal transformer has two properties, i.e., 1)   masking 2D semantics I from F Q and F O, and 2) referencing context Q and O when generating F I. We verify the   importance of both properties with the ablation studies in   Table 4. In training, we replace our proposed transformer\u2019s   Table 4. Ablation studies on 2D semantics embedding with different transformer attention masks. The gray mask color indicates   prevent from attending. SAT\u2019s attention mask is in Figure 2 (a).   Training mask A   Training mask B   1   1   1   1   1   1   1   1   1   1   0   1   0   0   1   1   1   0   Prevent from Attending   Method   Accuracy   non-SAT   37.7\u00b10.3%   SAT-mask A   33.9\u00b10.2%   SAT-mask B   43.9\u00b10.2%   SAT (Ours)   49.2\u00b10.3%   Table 5. Ablation studies on different types of 2D semantics inputs   as in Eq 1. We highlight the default \u201cSAT\u201d setting by underline.   +xgeo   +xcls   +xROI   Acc.   (a)   37.7\u00b10.3%   (b)   \u0013   39.4\u00b10.3%   (c)   \u0013   \u0013   48.1\u00b10.2%   (d)   \u0013   \u0013   46.5\u00b10.1%   (e)   \u0013   \u0013   43.2\u00b10.2%   (f)   \u0013   \u0013   \u0013   49.2\u00b10.3%   attention mask in Figure 2 (a) with the alternative masks   A/B in Table 4. In inference, the model removes the extra 2D semantics input and follows the standard inference   setting as in Figure 2 (b).   Mask A does not mask 2D semantics I from F Q and   F O. We observe that the model directly relies on the extra 2D inputs I for grounding prediction. Consequently, the   grounding accuracy drops catastrophically to 33.9% when   no 2D inputs are available in inference. Mask B encodes F I   with 2D semantics I alone. Without referencing the scene   context in Q and O, the 2D feature F I fails to provide a   relevant object representation and is thus less effective in   helping 3D visual grounding. Although outperforming the   non-SAT baseline accuracy of 37.7%, \u201cSAT-mask B\u201d performs much worse than the SAT with our proposed attention   mask (SAT-mask B: 43.9%, SAT: 49.2%).   Types of 2D context inputs. Table 5 shows the ablation   studies on the types of 2D semantics. The combination of   xROI, xcls, and xgeo are projected into a d-dimension 2D   semantics feature I following Eq. 1.   Compared with the non-SAT baseline accuracy of 37.7%   in row (a), SAT with any 2D semantics signi\ufb01cantly boosts   the grounding accuracy (rows (b-f)). Jointly using visual   feature xROI, semantic feature xcls, and geometric feature   xgeo achieves the best accuracy of 49.2%, as in row (f).   1861   6. How does SAT help?   In this section, we analyze how does SAT help 3D visual   grounding. We draw three major conclusions: 1) SAT learns   a better 3D object representation F O with the assist of 2D   semantics I, leading to consistent performance improvements over samples with different target classes, number of   distractors, query lengths/types, etc. (Section 6.1). 2) Training with the extra data in Sr3D/Sr3D+ mainly bene\ufb01ts the   queries with spatial relationship referring (Section 6.2). 3)   The performance gap between SAT and the methods that require extra 2D inputs in inference is small, indicating the effectiveness of SAT in utilizing 2D semantics (Section 6.3).   Finally, we present qualitative examples in Section 6.4.   6.1. Linear probing   How could SAT achieve the large improvement over the   non-SAT baseline and the state of the art? We conjecture   that SAT learns a better 3D representation F O for noisy object point clouds with the assist of 2D semantics. Consequently, we observe consistent 3D grounding accuracy improvements on samples with different target object classes,   number of distractors, query lengths/types, etc., as shown in   the performance breakdown in the supplementary material.   We use linear probing [56, 13, 6] to evaluate the quality of the learned 3D object representations F O in different models. Speci\ufb01cally, we keep the pre-trained grounding network \ufb01xed and train a linear classi\ufb01er that maps   each proposal feature F O   m into one of Nr3D\u2019s 607 object   classes.   Because no classi\ufb01cation annotation is seen during the grounding network training, we evaluate learned   representations F O by the object classi\ufb01cation accuracy.   Similar to the use of linear probing in representation learning [56, 13, 6], we consider a higher linear probing accuracy   the indicator of a better 3D object representation F O.   Table 6 shows the linear probing accuracy on Nr3D. SAT   improves the linear probing accuracy from 35.7% to 60.1%,   compared with the non-SAT baseline. The signi\ufb01cant improvement supports the conjecture that SAT learns a better   3D object representation with 2D semantics in training. We   observe similar improvements in the full \ufb01ne-tuning setting,   where all layers are updated for object classi\ufb01cation. It is   worth noting that SAT\u2019s effectiveness in generating better   3D object representations may hold the promise of bene\ufb01ting not only 3D vision-language tasks such as grounding [1, 4] and captioning [5], but also 3D semantic understanding tasks such as 3D scene graph prediction [2, 43].   6.2. Spatial relationship referring   Our second observation is that the extra data in   Sr3D/Sr3D+ helps the queries with spatial relationship referring. On Nr3D\u2019s subset with spatial queries (76.7% of   the samples), the extra Sr3D+ training data leads to an 8.4%   improvement on \u201cSAT-Nr3D\u201d from 48.4% to 56.8%. In   Table 6. Linear probing accuracy on Nr3D.   Method   Linear probing   Full \ufb01ne-tuning   non-SAT   35.7%   63.4%   SAT   60.1%   65.4%   SAT w/ Sr3D+   61.7%   67.6%   contrast, the improvement is only 3.9% on the remaining   samples (from 50.9% to 54.8%). Furthermore, we observe   larger improvements on subsets that contain the frequently   appeared spatial words in Sr3D/Sr3D+, e.g., \u201cclosest\u201d of   +11.5% and \u201cfarthest\u201d of +13.5%.   6.3. 2D semantics as extra inputs   In this subsection, we compare SAT with the methods   that require extra 2D inputs in both training and inference.   We design two methods that directly use 2D semantics as   extra inputs, namely the \u201c2D input aligned\u201d and \u201c2D input   unaligned.\u201d Both methods use the same network architecture as SAT and take extra 2D inputs in both training and inference. For \u201c2D input aligned,\u201d we concatenate 2D semantics Im with 3D proposal feature Om and use the extended   proposal feature as Om in both training and inference. The   input sequence length for \u201c2D input aligned\u201d is M + K.   We train \u201c2D input aligned\u201d with the main grounding loss   LO   V G and the classi\ufb01cation loss Lcls in Eq. 2. For \u201c2D input   unaligned,\u201d we input 2D semantics Im as extra input tokens   to the multi-modal transformer in both training and inference. The input sequence length for \u201c2D input unaligned\u201d   is 2M + K. We train \u201c2D input unaligned\u201d with the same   loss L in Eq. 2 as SAT.   Table 7 shows the experiment results on Nr3D with no   2D semantics (upper portion), with 2D inputs only in training (middle portion), and in both training and inference   (bottom portion). The \u201chard\u201d subset contains more than 2   distractors and remaining samples belong to \u201ceasy.\u201d We observe a marginal accuracy gap of 1% between \u201cSAT\u201d and   \u201c2D input aligned/unaligned\u201d (\u201coverall\u201d in row (e): 49.2%,   rows (h,i): 50.3%). The comparable performance indicates   SAT\u2019s effectiveness in utilizing 2D semantics to help 3D   visual grounding. Meanwhile, SAT does not require extra   2D inputs in inference as \u201c2D input aligned/unaligned,\u201d and   thus is easier to use.   6.4. Qualitative insights   The left four examples of Figure 3 show representative   failure cases of \u201cnon-SAT\u201d that can be corrected by \u201cSAT.\u201d   We group common cases into three scenarios. 1) Object:   SAT improves non-SAT by better recognizing the object   classes.   Non-SAT occasionally fails to ground the head   noun and generates the object prediction in a different class,   e.g., \u201cbed\u201d instead of the referred \u201cdesk\u201d in Figure 3 (a).   2) Relationship: We observe that SAT is better in modeling relationships in language queries, despite no speci\ufb01c   modules are proposed in SAT for relationship understanding. For example, in Figures 3 (b,c), SAT correctly understands the relationship \u201cattach to\u201d and \u201cover.\u201d We conjec1862   Table 7. The bene\ufb01t of using 2D semantics in 3D visual grounding. The upper/middle/bottom portion of the table shows the results that   do not use 2D semantics/use only in training/use in both training and inference (as extra inputs). The results with extra training data   (Sr3D/Sr3D+) are shown in gray. Our SAT (#(e)) shows comparable performance to oracles that require 2D inputs in inference (#(h,i)).   Extra   2D semantics   Overall   Easy   Hard   View-dep.   View-indep.   data   Train   Test   (a)   Ref3DNet [1]   \u0017   \u0017   35.6\u00b10.7%   43.6\u00b10.8%   27.9\u00b10.7%   32.5\u00b10.7%   37.1\u00b10.8%   (b)   TGNN [17]   \u0017   \u0017   37.3\u00b10.3%   44.2\u00b10.4%   30.6\u00b10.2%   35.8\u00b10.2%   38.0\u00b10.3%   (c)   IntanceRefer [55]   \u0017   \u0017   38.8\u00b10.4%   46.0\u00b10.5%   31.8\u00b10.4%   34.5\u00b10.6%   41.9\u00b10.4%   (d)   non-SAT   \u0017   \u0017   37.7\u00b10.3%   44.5\u00b10.5%   31.2\u00b10.2%   34.1\u00b10.3%   39.5\u00b10.4%   (e)   SAT (Ours)   \u0013   \u0017   49.2\u00b10.3%   56.3\u00b10.5%   42.4\u00b10.4%   46.9\u00b10.3%   50.4\u00b10.3%   (f)   SAT w/ Sr3D (Ours)   Sr3D   \u0013   \u0017   53.9\u00b10.2%   61.5\u00b10.1%   46.7\u00b10.3%   52.7\u00b10.7%   54.5\u00b10.3%   (g)   SAT w/ Sr3D+ (Ours)   Sr3D+   \u0013   \u0017   56.5\u00b10.1%   64.9\u00b10.2%   48.4\u00b10.1%   54.4\u00b10.3%   57.6\u00b10.1%   (h)   2D input aligned   \u0013   \u0013   50.0\u00b10.1%   62.0\u00b10.2%   38.5\u00b10.3%   44.7\u00b10.3%   52.6\u00b10.3%   (i)   2D input unaligned   \u0013   \u0013   50.3\u00b10.4%   58.5\u00b10.7%   42.4\u00b10.5%   48.1\u00b10.4%   51.3\u00b10.5%   (j)   2D input aligned   Sr3D+   \u0013   \u0013   59.7\u00b10.1%   71.0\u00b10.3%   48.8\u00b10.5%   52.9\u00b10.3%   63.1\u00b10.2%   (k)   2D input unaligned   Sr3D+   \u0013   \u0013   61.0\u00b10.3%   69.0\u00b10.6%   53.2\u00b10.3%   58.4\u00b10.3%   62.2\u00b10.5%   Query   Non-SAT   SAT (Ours)   GT   Rendered   scene   (d) The bed with    the white and    green bedding.   (a) The bigger    brown desk to the    left of the bed.   (b) The shelf that    is attached to the    desk.   (c) The set of kitchen    cabinets over the    kitchen sink.   (e) Facing the wall    select the table on    the right.   (f) The chair that    is facing the desk.   bed   shelf   kitchen cabinets   bed   table   chair   desk   shelf   kitchen cabinets   bed   table   chair   desk   shelf   kitchen cabinets   bed   coffee table   office chair   Figure 3. The failure cases of non-SAT that can be corrected by SAT (the left four examples), and SAT\u2019s representative failure cases (the   right two examples). The green/red/yellow colors indicate the correct/incorrect/ground truth boxes. The object class for each box is shown   in text next to the 3D box. We provide rendered scenes in \ufb01rst row to better visualize the scene layout. Best viewed zoomed in and in color.   ture that SAT learns a better object representation for both   foreground and background objects, which bene\ufb01ts the relationship modeling. 3) Color and shape: SAT also performs better in understanding color and shape-related language queries, e.g., \u201cwhite and green\u201d in Figure 3 (d).   The right two examples of Figure 3 show SAT\u2019s representative failure cases. Figure 3 (e) shows a failure case   that requires understanding \u201cfacing the wall.\u201d   Although   SAT improves both view-dependent and independent samples (c.f. Table 7 \u201cView-dep.\u201d column), view understanding   remains an unsolved problem. Figure 3 (f) shows a failure case caused by ambiguous queries. The model predicts   the \u201cchair facing the desk\u201d instead of the referred \u201cof\ufb01ce   chair facing the desk\u201d in the ground truth. We observe that   the model and human annotators occasionally confuse objects in similar categories, such as chair/of\ufb01ce-chair (Figure 3 (f)), table/coffee-table (Figure 3 (e)), etc.   7. Conclusion   We have presented 2D semantics assisted training (SAT)   for 3D visual grounding. SAT uses 2D semantics in training   to assist 3D visual grounding and eases joint representation   learning between the 3D scene and language query. With   identical network and inference inputs, SAT beats the nonSAT baseline by 11.5% in absolute accuracy. SAT leads   to the new state of the art on multiple datasets and outperforms previous works by large margins. Analyses show that   SAT effectively uses 2D semantics to learn a better 3D point   cloud object representation that helps 3D visual grounding.   Acknowledgment   This work is supported in part by NSF awards IIS-1704337 and   IIS-1813709, as well as our corporate sponsors.   1863", "conf": "ICCV", "year": "2021", "index": 123}, {"title": "Unveiling Parts Beyond Objects:   Towards Finer-Granularity Referring Expression Segmentation   Wenxuan Wang1,2,3*   Tongtian Yue1,2\u2217   Yisi Zhang4   Longteng Guo1   Xingjian He1   Xinlong Wang3   Jing Liu1,2\u2020   1 Institute of Automation, Chinese Academy of Sciences (CASIA)   2 School of Arti\ufb01cial Intelligence, University of Chinese Academy of Sciences (UCAS)   3 Beijing Academy of Arti\ufb01cial Intelligence (BAAI)   4 University of Science and Technology Beijing (USTB)   {wangwenxuan2023@ia.ac.cn, wangxinlong@baai.ac.cn, jliu@nlpr.ia.ac.cn}   Figure 1. Classic Referring Expression Segmentation (RES) only supports expressions that indicate a single target object, e.g., (a). Compared with classic RES, the proposed Multi-Granularity Referring Expression Segmentation (MRES) task supports expressions indicating the speci\ufb01c part-level regions of target objects, e.g., part-level expressions like (b)-(e) from our newly built RefCOCOm benchmark.", "abstract": "Referring expression segmentation (RES) aims at segmenting the foreground masks of the entities that match the   descriptive natural language expression. Previous datasets   and methods for classic RES task heavily rely on the prior   assumption that one expression must refer to object-level   targets. In this paper, we take a step further to \ufb01ner-grained   part-level RES task. To promote the object-level RES task   towards \ufb01ner-grained vision-language understanding, we   put forward a new multi-granularity referring expression   segmentation (MRES) task and construct an evaluation   benchmark called RefCOCOm by manual annotations. By   employing our automatic model-assisted data engine, we   build the largest visual grounding dataset namely MRES32M, which comprises over 32.2M high-quality masks and   captions on the provided 1M images. Besides, a simple yet   strong model named UniRES is designed to accomplish the   uni\ufb01ed object-level and part-level grounding task. Extensive experiments on our RefCOCOm for MRES and three   datasets (i.e., RefCOCO(+/g)) for classic RES task demon*Equal contribution.   \u2020Corresponding author.   strate the superiority of our method over previous state-ofthe-art methods. To foster future research into \ufb01ne-grained   visual grounding, our benchmark RefCOCOm, the MRES32M dataset and model UniRES will be publicly available   at https://github.com/Rubics-Xuan/MRES.   1.", "content": "As one of the most challenging tasks in vision-language understanding, referring expression segmentation (RES) aims   to locate speci\ufb01c regions at the pixel level based on a descriptive language expression. Compared to traditional visual segmentation tasks that focus on images or videos   alone, RES poses greater dif\ufb01culties and challenges due   to the necessity of strong comprehension across modalities, but it can simultaneously alleviate the problem of prede\ufb01ned categories in conventional object detection or segmentation. With the real-world scene often requiring diversity in target identi\ufb01cation, RES task holds vast potential for   applications, e.g., language-based human-object interaction   and interactive image editing.   Since the concept of RES task was initially proposed in   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   12998   Table 1.   Comparison among different referring expression   datasets, including ReferIt [18], RefCOCO(+/g) [27, 44], PhraseCut [41], and our proposed MRES-32M. Part-Level: expressions   that speci\ufb01es various parts of the target object in the given image.   ReferIt   RefCOCO(+/g) PhraseCut   MRES-32M   Image Source   CLEF [11]   COCO [24]   VG [21]   Object365 [34]   Object-Level   \u0013   \u0013   \u0013   \u0013   Part-Level   \u0015   \u0015   \u0015   \u0013   Expression Type   free   free   templated   free   [13], various multimodal frameworks such as [13, 19, 40,   42] towards precise RES have been proposed to deal with   the challenging feature extraction and alignment problems   between visual and linguistic modalities. However, current   works are limited to the scope of object-level grounding.   As shown in Table 1, the RefCOCO dataset [44] stands   as one of the most widely used grounding benchmarks by   far, basically containing only object-level visual and textual annotations. It does not take into account the part-level   grounding task, which is crucial for future multimodal models to act as intelligent agents to realize the \ufb01ne-grained   perception of real world. The existing grounding datasets   couldn\u2019t support the training and evaluation of such advanced capabilities. Thus, exploring how to transcend the   current object-level constraints and delve into \ufb01ner-grained   part-level grounding, is highly meaningful and worthy of   in-depth study, which is the primary focus of this work.   In fact, prior to this work, a few works along another research line have made signi\ufb01cant strides towards   more \ufb01ne-grained visual understanding. The demand for   \ufb01ner-grained visual understanding of objects arouses research community into constructing high-quality part-level   dataset. Speci\ufb01cally, recent years have witnessed the introduction of various datasets that provide \ufb01ne-grained partlevel masks and bounding boxes annotations for either objects of speci\ufb01c categories [10, 33, 39] or general categories   [4, 12, 26, 32]. However, these datasets essentially correspond to unimodal (i.e., visual) downstream tasks, lacking a   deep connection between \ufb01ne-grained part-level masks and   rich textual descriptions. To our knowledge, few of previous studies have established this connection. Consequently,   there appears to be a limited availability of \ufb01ne-grained,   large-scale vision-language datasets that facilitate part-level   cross-modality understanding, which is necessary in terms   of two aspects. Firstly, when describing an object, people   always naturally gravitate towards detailing part-level local features, underscoring the indispensable need for multimodal agents to understand the part granularity. Secondly,   a \ufb01ne-grained understanding at the part level can positively   promote the perception of object-level targets, especially   under extreme conditions such as occlusion or deformation,   which will consistently propel advancements in widely focused object-level tasks like classic visual grounding.   Therefore, in this work, we attempt to \ufb01ll this important blank space that has been neglected before and move   towards \ufb01ner-grained part-level RES task. Speci\ufb01cally, to   push towards \ufb01ner-grained vision-language understanding,   we propose a new multi-granularity referring expression   segmentation (MRES) task and an evaluation benchmark   named RefCOCOm by manually annotating the part-level   targets based on the previous benchmark RefCOCO with   only object-level labels.   We also construct the largestscale visual grounding dataset which is also the \ufb01rst dataset   to support part-level visual-textual annotations, and build   a simple baseline model to accomplish the uni\ufb01ed multigranularity (i.e., object-level and part-level) RES task.   Our main contributions can be summarized as follows:   \u2022 We propose a new MRES task (as shown in Fig.   1)   with corresponding benchmark RefCOCOm for evaluation, pushing classic RES task towards \ufb01ner-granularity   understanding of real-world scenes.   \u2022 We build a multi-grained visual grounding dataset namely   MRES-32M, which to the best of our knowledge is the   \ufb01rst grounding dataset that supports part-level visionlanguage annotations and also the largest-scale visual   grounding dataset.   \u2022 To effectively unify both the object-level and part-level   RES tasks, we propose a simple yet strong model namely   UniRES, which achieves the new state-of-the-art (SOTA)   performance on three object-level benchmarks for classic   RES tasks and our multi-granularity RefCOCOm benchmark for the proposed MRES task.   2. Related Work   Referring Expression Segmentation. As a challenging visual grounding task the concept of RES is \ufb01rst proposed   by [13]. Subsequent works such as [15, 16, 40, 43, 45]   predominantly follow a two-step pipeline of encoding the   linguistic and visual features separately and deriving the   fused multimodal features from unimodal representations   for mask prediction. In this way, the effectiveness of obtained multimodal representations essentially dominates the   model performance, which has been continually studied in   the following research [7, 22, 42, 48, 49]. Recently, there   are some works [28, 36, 46] focusing on zero-shot RES task   which has great application potential. In addition, a few recent works [14, 25] have been devoted to address the limitations of the existing benchmark datasets [18, 27, 41, 44]   for visual grounding task. However, it\u2019s worth noting that   previous works primarily focus on classic object-level RES   methods and datasets, paying few attention to the vital \ufb01negrained part-level grounding.   Part-level Visual Perception. The increasing interest in   the \ufb01ne-grained understanding of objects has driven the creation of part-level annotated datasets across both specialized and general domains. Pioneering research in the former   \ufb01eld has introduced datasets with part-level annotations, fo12999   cusing on objects of particular categories like human body   parts [10], animal body parts [33] and vehicle components   [39]. In contrast, more general datasets that offer part annotations for a variety of common objects include Pascal-Part   [4], PartNet [26], and PartImageNet [12]. Furthermore, recent work [6] introduces a \ufb01ner-grained dataset with additional instance-speci\ufb01c part annotations for panoptic segmentation task. PACO [32] extends the available objectlevel datasets by including the annotations of part granularity and attribute for \ufb01ner-grained segmentation and detection tasks. Moreover, VLPart [35] proposes a parsing   pipeline for part segmentation and a detector that is capable   of realizing the segmentation task of both open-vocabulary   objects and their part regions.   3. Multi-Granularity Grounding Benchmark   3.1. Multi-Granularity RES Task   The interaction capability of a multimodal agent directly depends on the visual granularity it can perceive and understand. However, in the current research landscape, multigranularity visual grounding remains under-explored. To   break the limitations wherein visual-language alignment is   constrained to the object level, we intend to take a step forward towards \ufb01ner-grained vision-language understanding   and propose the multi-granularity RES task, which requires   models\u2019 pro\ufb01ciency in uniformly grounding entities across   both part and object levels in response to various textual", "conf": "CVPR", "year": "2024", "index": 760}, {"title": "Localizing Moments in Long Video Via Multimodal Guidance   Wayner Barrios1,   Mattia Soldan2,   Alberto Mario Ceballos-Arroyo3,   Fabian Caba Heilbron4,   Bernard Ghanem2   1 Dartmouth   2King Abdullah University of Science and Technology (KAUST) 3Northeastern University   4Adobe Research", "abstract": "The recent", "content": "of the large-scale, long-form   MAD and Ego4D datasets has enabled researchers to investigate the performance of current state-of-the-art methods for video grounding in the long-form setup, with interesting findings: current grounding methods alone fail at   tackling this challenging task and setup due to their inability to process long video sequences. In this paper, we   propose a method for improving the performance of natural language grounding in long videos by identifying and   pruning out non-describable windows. We design a guided   grounding framework consisting of a Guidance Model and   a base grounding model. The Guidance Model emphasizes   describable windows, while the base grounding model analyzes short temporal windows to determine which segments   accurately match a given language query. We offer two designs for the Guidance Model: Query-Agnostic and QueryDependent, which balance efficiency and accuracy.   Experiments demonstrate that our proposed method outperforms state-of-the-art models by 4.1% in MAD and 4.52%   in Ego4D (NLQ), respectively.   Code, data and MAD\u2019s   audio features necessary to reproduce our experiments   are available at: https://github.com/waybarrios/guidancebased-video-grounding.   1. Introduction   The task of grounding natural language in long-form   videos within large-scale datasets, such as MAD [33] and   Ego4D [12], can be particularly challenging due to the possibility of encountering many video segments that do not   contain any interesting or relevant moments to search for.   Given the large search space, it is critical to develop approaches that can quickly scan the video and identify queryable moments via natural language.   For instance, Figure 1 illustrates various moments occurring in a long-form video, where certain moments could   have greater relevance than others based on the video\u2019s storytelling. The dialogue moment, which spans more than   Audio    Speech   Bark   Splash   Language   Query   \u201cA dog jumps in the pool to    rescue a little girl.\u201d   Video   Video   Language   Query   Audio   \u201ca dog jumps to rescue a little girl\u201d   Bark   Speech   Splash   Long-range Movie Sequence   \u2026   31:00 \u2013 34:31   52:15 \u2013 55:43   Non-describable   Describable   Describable   Non-Describable   31:00 - 34:31   52:15 - 55:43   Full Movie Sequence   Figure 1: Describable Window. We depict the difference between describable and non-describable windows. The former are   temporal windows with relevant visual and auditory events that   are likely to contain one or more noteworthy moments. The latter can be categorized as \u201cboring\u201d video segments (temporal windows) where little happens and no moment of interest happens for   grounding.   three minutes, would only be described as \u201ctwo people sitting at a table\u201d, whereas the action scene contains many   moments that can be described, such as \u201ca girl falling into   a pool\u201d, \u201ca dog opening a fence\u201d, and \u201ca dog jumps to   rescue a little girl\u201d. This demonstrates that certain slices   within a video can contain numerous notable moments that   users would be highly interested in searching for.   In consequence, we introduce the concept of Describable windows (as shown in Figure 1), which are video   slices shorter than the original long-form video and have   a high probability of containing remarkable and relevant   visual moments that can be queried through natural language. Conversely, video slices that do not contain such   visually-rich moments that cannot be queried through   natural language are considered non-describable windows.   This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   13667   Our work builds on the hypothesis that certain video   segments (non-describable windows) are difficult to narrate   effectively by natural language, which can lead to noisy   predictions when retrieving moments via natural language   queries, particularly for long videos. In fact, state-of-the-art   grounding models achieve remarkable performance when   analyzing short videos [31, 16, 26]; however, their capabilities seem to saturate when tested on longer videos [33]. For   example, VLG-Net [34] achieves only marginal gains compared to a simple yet effective zero-shot approach based on   CLIP [29], and some models outright fail to achieve reasonable performance1. The above statements highlight the need   to detect video slices corresponding to the \u201cimportant\u201d parts   of a long-form video, i.e., those that contain as many moments as possible. Our intuition is that when analyzing the   entire long-form video, grounding models might overlook   the most critical segments of the video, which represents a   gap in the current research and motivates us to develop more   sophisticated techniques that can capture certain intricacies   using language cues or audio cues on top of the visual information.   To address this issue, we propose a guided grounding   framework [1, 10] comprised of two core components: a   Guidance Model that specializes emphasizing describable   windows and a base grounding model that analyzes short   temporal windows to determine which temporal segments   match a given query accurately.   Observing multimodal cues is key for detecting describable windows. For instance, let us suppose we want to find   the moment when \u201cthe dog jumps to rescue the little girl\u201d   (Figure 1). Here, the water splash (sound) and dog jumping (visual) are hints suggesting that lots of visual activities are happening in the scene. This intuition motivates   the multimodal design of our Guidance Model. In practice,   our model jointly encodes video and audio over an extended   temporal window using a transformer encoder [35].   The proposed Guidance Model can also be used in two   different frameworks: Query Agnostic and Query Dependent.   The former pre-computes which parts of a video   have a low-probability of containing describable windows,   making it suitable for real-time applications or limited   computational resources.   In contrast, the latter provides   more precise results by identifying irrelevant parts of a   video based on a given text query, at the cost of being more   computationally expensive as the number of queries grows.   We also highlight the fact that the two-stage approach can   be adapted to any grounding method, making it a flexible   and versatile option for a wide range of applications.   1We trained Moment-DETR [17] from scratch on the MAD [33] dataset   and found that, even though it achieves good grounding performance in   short videos, it utterly fails in long videos. More details can be found in   the supplementary material.   In summary, our contributions are three-fold:   1. We propose a two-stage guided grounding framework:   a general approach for boosting the performance of   current state-of-the-art grounding models in the longform video setting.   2. We introduce a Guidance Model that is capable of detecting describable windows: a temporal window that   contains one or more noteworthy moments.   3. Through extensive experiments, we empirically show   the effectiveness of our two-stage approach. In particular, we validate the benefits of leveraging a Guidance   Model that specializes in finding describable windows.   We improve state-of-the-art performance on the MAD   dataset [33] by 4.1% and improve the performance of   an Ego4D baseline model [12, 44] in the NLQ task by   4.52%.   2. Related Work   Video Grounding Methods. Video grounding methods can   be divided into two families of approaches: (i) proposalbased [11, 1, 34, 45, 40] and (ii) proposal-free [17, 43, 26,   6, 32, 19]. Proposal-based methods rely on producing confidence scores or alignment scores for a previously generated   set of M candidate temporal moments {(\u03c4start, \u03c4end)}M   1 .   On the other hand, proposal-free methods aim at directly regressing the temporal interval boundaries for a given videoquery pair. For instance, Mun et al. [26] tackle this problem by using a temporal attention-based mechanism that exploits local and global information in bimodal interactions   between video segments and semantic phrases in the query   to regress the target interval. Li et al. [19] uses a pyramid   network architecture to leverage multi-scale temporal correlation maps of query-enhanced video features as the input   to a temporal-attentive regression module.   Regarding efficiency, proposal-free approaches have   faster inference time since they do not require exhaustively   matching the query to a large set of proposals. Conversely,   proposal-based methods often provide higher performance   at the cost of a much slower inference time. In fact, these   methods produce predictions for a large number of temporal   proposals and successively apply expensive Non-Maximum   Suppression (NMS) algorithms to improve the ranking.   Soldan et al. [33] showcased how current state-of-the-art   grounding methods fail to tackle the long-form grounding   setup where videos are up to several hours in duration.   Our focus is thus to design a pipeline able to boost these   methods through a two-stage cascade approach.   In the   experimental section, we will showcase how our flexible   design is able to boost both proposal-based and proposalfree methods to achieve a new state-of-the-art performance.   13668   Long-form Video Grounding.   In long-form video understanding, grounding approaches are limited by the inability to process the entire video at once.   Such limitation stems from the large computational budget for deep   learning-based video perception algorithms and the limited   computational resources currently available. Recently, [33]   proposed to process the video in short windows to enable   grounding in long-form videos.   Long videos are therefore sliced in overlapping temporal windows, and grounding methods are used to generate predictions within each   window. Finally, predictions across all windows are gathered and ranked according to the confidence scores.   While video slicing is a potential solution for long-form   video grounding, it has a significant limitation as grounding   methods may introduce a considerable number of false   positives in the predictions for any long-form videos.   We address this challenge by introducing a two-stage   approach that guides any grounding model in making accurate predictions and reducing the presence of false positives.   Multimodal Transformers.   The great flexibility of the   transformer architecture [36] has promoted its adoption in   several different fields: computer vision [9, 22], natural language processing [8, 3, 38], and audio processing [37, 27].   Concurrently, several works have exploited this architecture   to process multimodal data [25, 29, 18, 20, 39, 5, 15, 30].   The key to successfully applying transformers to multimodal data is to learn a projection to a shared embedding   space for each element of each modality (frames, spectrograms, language tokens). Our design for the Guidance   model, therefore, follows these recent advancements. In   particular, we aim to exploit the flexibility of transformers   to design a unified architecture able to gather relevant cues   from different modalities. We detail the design choices for   our Guidance model in the following section.   Temporal Proposals. Temporal proposals refer to the identification and localization of specific actions or events in an   untrimmed video by identifying temporal segments that are   likely to contain them. [46, 4] generate a single video snippet per each specific event followed by an action classifier,   which makes it dependent on the label class. The existence   of several video snippets that correspond to specific events   in a long-form video, where multiple events can occur simultaneously, significantly adds to the complexity of the   task. For a given time segment t, N video snippets would   be required, where N represents the number of concurrent   events.   Recently, [41, 13, 28] leverages mechanisms that capture semantic information of video clips, self-attention, as   well as local and global temporal relationships using visual information. Although these methods have yielded outstanding outcomes, the nature of grounding in long-form   \u201cMum hangs up the    laundry outside the    farmhouse.\u201d   Video   Encoder   Text    Encoder   Audio    Encoder   Grounding    Model \ud835\udcb1\u210a\ud835\udc49, \ud835\udc44   Guidance    Model \ud835\udca2\ud835\udc49, \ud835\udc44   Fusion    Operator \ud835\udc40   (\ud835\udc4a, p*)   (\u03c4s, \u03c4e, s)   (\u03c4s, \u03c4e, s*)   Figure 2: Guided Grounding pipeline.   Two-stage approach   comprised of a video-language grounding model and a guidance   model. Given a set of predictions {(\u03c4s, \u03c4e, s)}M   1 from a grounding model, and plausibility score p\u2217, we generate a refined set of   predictions {(\u03c4s, \u03c4e, s\u2217)}M   1 .   videos necessitates an examination of supplementary cues   from other modalities to identify as many events as possible   that can be referenced in a shorter video duration, i.e., in a   describable window.   3. Method   Our goal is to train a guidance model G that finds   describable windows that might contain a moment to be   grounded in a long video. By finding these windows, we   aim to guide an existing grounding model Vg in improving its predictions. Let Vg\u2019s temporal moments predictions   be defined as: {(\u03c4s, \u03c4e, s)}M   1 , where (\u03c4s, \u03c4e) corresponds to   start/end time and s is the noisy confidence score; given a   temporal window W sampled from a long video, and the fusion operator M, we seek a model that generates temporal   moment predictions such that:     \\ma th ca l  {M}\\ lef t (\\m   a thcal {V}_g, \\mathcal {G}, W\\right )\\rightarrow \\{(\\tau _{s},\\tau _{e}, s^*)\\}^M_1,    (1)   where s\u2217denotes a refined confidence score for the   temporal moment.   Grounding model Vg. The grounding model takes as input   a video observation V sampled from a temporal window W,   and a natural language query Q; it then predicts M temporal   moments such that:     \\ma th c al {V }_g \\lef   t  (V,Q\\right )\\rightarrow \\{(\\tau _{s},\\tau _{e}, s)\\}^M_1.    (2)   In this work, we leverage several pre-trained grounding   models such as [17, 33, 34, 44]. While these works achieve   remarkable performance on localizing existing moments   within short temporal window, they suffer severely from   false positive predictions on windows that do not contain   any moment. As we will show in our experiments, modulating the confidence scores of a grounding model can   13669   greatly improve the overall performance of the model.   Guidance model G. We seek to train a guidance model that   gives low scores to predictions made on empty windows   (w/o moments) and gives higher scores to predictions derived from windows containing one or more moments (describable windows). Similar to the grounding model, G ingests a video observation V sampled from W, and the target   query Q. For a given window, we obtain a plausibility confidence score p\u2217as follows:     \\m at h cal {G}\\left (V,Q\\right )\\rightarrow p^*.    (3)   Guided grounding output M. After obtaining a set of   predictions {(\u03c4s, \u03c4e, s)}M   1 , and a plausibility score p\u2217for   window W, the fusion operator M aims to generate a refined confidence score for each predicted moment (Figure   2). To do so, we simply multiply the plausibility score p\u2217   with each confidence score {s}M   1 . This generates the final   (and refined) set of predictions:     P=\\{( \\ta u _{s   },\\tau _{e}, s^*)\\}^M_1    (4)   Grounding in long-form videos.   We follow [33] and   generate predictions in long-form videos by sliding a   short window throughout time. Assuming we analyze K   windows, our model generates K\u00d7M predictions for a   natural language query Q. We sort the resulting predictions   by their confidence scores s\u2217.   3.1. Guidance Model Design and Training Details   The Guidance Model is depicted in Figure 3. Our goal is   to train the model G on a dataset D containing binary labels   (of window plausibility) for a set of temporal windows and   their corresponding natural language queries.   Window Representation.   We sample an observation V   from an input window W. Such observation includes diverse inputs (i.e., video, audio, and text). The use of each   modality is setup-dependent and will be thoroughly ablated in Section 4.3.   In practice, we employ frozen encoders to extract embeddings for each modality. Formally,   we denote video embeddings, audio embeddings, and textual embeddings as Ev \u2208RLvg\u00d7Dv, Ea \u2208RLag\u00d7Da, and   Et \u2208RLtg\u00d7Dt, respectively, where Dv, Da, and Dt represent the dimensionality of the features. Input features are   projected to a shared embedding space of size dg by an MLP   projection followed by a layer normalization module [14].   Dropout is used for regularization.   Our design allows our Guidance Model to be querydependent or query-agnostic. The inputs for each of these   variants are as follows:   Transformer Encoder   \u201cMum hangs up the    laundry outside the    farmhouse.\u201d   +   +   +   +   +   +   +   +   +   +   Text    Encoder   Video   Encoder   Audio    Encoder   MLP    Head   Modality specific    feature   Modality specific    Positional embedding   CLS   Guidance Score   Figure 3: Guidance model. Our guidance model can process different modalities (setup-dependent). Each modality representation   is provided with a modality-specific positional embedding before   being fed to the Transformer Encoder module. The output of the   CLS token is projected to a real value, used as a guidance score to   condition the grounding models.   For query-dependent, the model\u2019s inputs are given   by Ein=|Ecls, Ev, Ea, Et|   \u2208   R(L+1)\u00d7dg, where   L=Lvg +Lag +Ltg and Ecls is a learnable CLS token.   For query-agnostic, the model\u2019s inputs are given by   Ein=|Ecls, Ev, Ea| \u2208R(L+1)\u00d7dg, where L=Lvg +   Lag and Ecls is a learnable CLS token.   A modality-specific positional embedding is also added   to each input tensor; specifically, we adopt a sinusoidal   positional embedding [35] for the video modality, while we   find that learnable positional embeddings [21] work best   with the audio and text modalities.   Architecture.   We adopt a transformer encoder [17, 15]   which is a powerful yet flexible architecture for processing   sequential data.   The input Ein is fed to a stack of Lt   transformer encoder layers. Each transformer encoder layer   is identical in design to [35, 42, 17, 15], using a multi-head   self-attention layer and a feed-forward network (FFN).   Finally, the first element of the encoder output (Eout),   corresponding to the CLS token position, is fed to an MLP   for predicting the plausibility score for the given window.   Loss function and supervision definition. We optimize   a binary cross entropy (BCE) loss to train G. The queryagnostic models are trained on a dataset Dagnostic that contains only a set of windows and a binary label that indicates,   for each window, whether or not it contains at least one moment. The query-dependent variant is trained on a dataset   Ddependent that contains window and query pairs making   up the set of describable (positives) and negative windows.   13670   4. Experiments   Metrics.   The video grounding metric of choice is   Recall@K for IoU=\u03b8 (R@K-IoU=\u03b8). This widely adopted   metric measures the quality of the predictions\u2019 ranking and   temporal overlapping (IoU) with the annotations. In this   work we evaluate our models for K\u2208{1, 5, 10, 50, 100}   and \u03b8\u2208{0.1, 0.3, 0.5}.   Additionally, we introduce Mean   Recall@K (mR@K), where we average the R@K-IoU=\u03b8   performance across all IoUs thresholds.   This metric   allows for easier comparison and more compact tables.   We also include Mean Recallall or average Recall as:   mRall =   1   |K|   P mR@K, \u2200K.   Datasets. MAD [33] is a large-scale dataset for the videolanguage grounding task that comprises more than 384K   natural queries temporally grounded in 650 full-length   movies for a total of over 1.2K hours of video.   MAD   introduces a new long-form video grounding setup that   brings new challenges to the task at hand while allowing   for unprecedented bias-free performance investigation. In   our experimental section, unless otherwise specified, we   report performance on the official test set. Ego4D [12] is a   comprehensive benchmark consisting of egocentric videos   from 931 camera wearers worldwide in different scenarios.   The subtask we focus on is the Natural Language Query   (NLQ), which uses 13 question types to locate different   types of information and retrieve relevant moments from   the episodic memory of camera wearers. Videos vary in   length from 3.5 to 20 minutes.   Baselines. For MAD dataset, we select three video grounding methods for our experimental setup, namely: VLGNet [34], zero-shot CLIP [33], and Moment-DETR [17].   The first two are both proposal-based methods, a trained   method and a zero-shot one, while the third is a proposalfree approach.   On the other hand, for Ego4D, we use   Moment-DETR [17] and VSL-Net [44], a multimodal spanbased framework based on context-query attention. We will   show that all approaches benefit from the score refinements   provided by our Guidance Model.   4.1. Implementation details   Feature Extraction.   The visual and text embeddings   are extracted following the CLIP-based methodology   presented in [33]. Specifically, visual features are extracted   at 5 FPS and with embedding dimensionality Dv=512. On   the other hand, text features are comprised of N tokens,   with embedding dimensionality Dt=512.   Finally, audio   embeddings are computed using OpenL3 [7], an audio   embedding model inspired by L3-Net [2].   Specifically,   we use the OpenL3 checkpoint that was pre-trained on   videos containing environmental audiovisual data, we use   a spectrogram time-frequency representation with 128   bands, and we set the audio embedding dimensionality Da   to 512.   Furthermore, we extract the audio embeddings   using a stride size equal to 0.2 seconds, i.e., using an   extraction frame rate of 5 Hz, which matches the frame   rate of the visual features. On the other hand, in the Ego4D   experiment we follow [23] without using audio streams, as   not all videos contain audio throughout their entire duration.   Guidance Model.   We train our Guidance Model using   three modalities: (i) visual, (ii) audio, and (iii) text for   query-dependent setup.   And two modalities: (i) visual   and (ii) audio for query-agnostic setup. Our transformer   encoder comprises 6 layers (Lt) with a hidden size of   256 (dg). The Guidance Model is trained using a sliding   window approach where the window size (Lvg) is equal   to 64 frames (unless otherwise specified), spanning 12.8   seconds for the MAD dataset and 34.13 seconds for the   Ego4D dataset. The model is trained for 100 epochs using   the AdamW [24] optimizer with learning rate 10\u22124, weight   decay 10\u22124, and batch size 512.   Grounding Models. We instantiate Moment-DETR [17]   using a hidden dimension size of 256, 2 encoder layers, 2   decoder layers, window length (Lv) of 128 (equivalent to   25.6 seconds at 5 FPS in MAD), and 10 moment queries;   we optimize the model with AdamW [24], setting the learning rate to 10\u22124 and the batch size to 256. VLG-Net [34]   is trained following the implementation details presented   in [33], while CLIP is evaluated in the zero-shot setup that   was also proposed in [33]. For VSL-Net [44], we follow   the approach presented in the Ego4D [12] repository with   an egocentric video-language pretraining [23].   At inference time, we discard highly redundant proposals via nonmaximum suppression (NMS) with a threshold of 0.3 for   MAD and 0.5 for Ego4D. All experiments are conducted   on a Linux workstation with a single NVIDIA 32GB V100   GPU and an Intel Xeon CPU with 64 cores.   4.2. Results   Table 1 presents the performance of the baselines (rows   1-3) and the corresponding improvement achieved by the   grounding models with the guidance model (rows 4-6). The   employed guidance model utilizes query-dependent setup   and audiovisual features. Further investigation into various   model design choices can be found in Section 4.3.   As anticipated, proposal-based methods (rows 1-2) have   consistently higher performance with respect to the   proposal-free Moment-DETR method (row 3) for all metrics.   Moreover, our guidance method is able to boost   the performance of all baselines (rows 4-6). In particular,   the most significant boost is obtained by Moment-DETR,   for which the improvement ranges between 3\u221216\u00d7, while   for VLG-Net and zero-shot CLIP baselines, the improve13671   Model   IoU=0.1   IoU=0.3   IoU=0.5   R@1   R@5   R@10 R@50 R@100   R@1   R@5   R@10 R@50 R@100   R@1 R@5 R@10 R@50 R@100   Zero-shot CLIP [33]   6.57   15.05   20.26   37.92   47.73   3.13   9.85   14.13   28.71   36.98   1.39   5.44   8.38   18.80   24.99   VLG-Net [34]   3.50   11.74   18.32   38.41   49.65   2.63   9.49   15.20   33.68   43.95   1.61   6.23   10.18   25.33   34.18   Moment-DETR [17]   0.31   1.52   2.79   11.08   19.65   0.24   1.14   2.06   7.97   14.29   0.16   0.68   1.20   4.71   8.46   \u2020Zero-shot CLIP [33]   9.30 18.96 24.30   39.79   47.35   4.65 13.06   17.73   32.23   39.58   2.16   7.40   11.09   23.21   29.68   \u2020VLG-Net [34]   5.60   16.07   23.64   45.35   55.59   4.28 13.14 19.86 39.77   49.38   2.48 8.78 13.72 30.22   39.12   \u2020Moment-DETR [17]   5.07   16.30 24.79 50.06   61.79   3.82   12.60   19.43 40.52   50.35   2.39   7.90   12.06   24.87   30.81   Table 1: Benchmarking of grounding methods on the MAD dataset. In the initial rows (row 1, 2, and 3), we present the performance   results of the three baselines (Zero-shot CLIP, VLG-Net, and Moment-DETR) on the test split. The subsequent rows (row 4, 5, and 6)   demonstrate the performance enhancement achieved by the grounding models aided by the proposed guidance model (indicated by the (\u2020)   symbol), utilizing the query-dependent setup with audiovisual features.   Method   mR@1 mR@5 mR@10 mR@50 mR@100 mRall   Moment-DETR [17]   6.72   19.68   23.85   24.67   \u2212   18.73   VSL-Net [44, 23]   11.38   19.64   24.27   36.09   42.12   26.70   \u2020Moment-DETR [17] 7.28   22.14   24.75   26.05   \u2212   20.05   \u2020VSL-Net [44, 23]   11.90   24.06   33.31   54.59   54.90   31.22   Table 2: Benchmarking on the Ego4D dataset.   The initial   rows (row 1 and 2) display the performance results of MomentDETR [17] and VSL-Net [44] baselines, employing Egocentric   VLP features [23] on the Ego4D validation set. The subsequent   rows (row 3 and 4) demonstrate the performance enhancement   brought about by our Guidance Model, indicated by the symbol   \u2020. For a complete table of results, kindly consult the supplementary material.   ment ranges between 1\u22123\u00d7. Remarkably, the Guidance   Model can boost R@10-IoU=0.1 from 2.79% to 24.79%   for Moment-DETR, allowing us to achieve state-of-the-art   performance for the MAD dataset. These results indicate   that our approach bridges the gap between proposal-free   and proposal-based methods, allowing the former to tackle   the long-form video grounding task.   We evaluated the performance of the guidance model   on the Ego4D [12] dataset using a query-dependent setup.   Since Ego4D lacks audio in a substantial portion of its   videos for the NLQ task, we opted for a fair comparison using only visual and text features. Table 2 displays the results   achieved, employing VSL-Net and Moment-DETR as the   baselines. The results demonstrate that the Guidance Model   had a positive impact, leading to a 4.52% improvement in   the mRall metric for VSL-Net and a 1.32% improvement   for Moment-DETR in the same metric.   Takeaway.   Our Guidance Model is general and can be   combined with all grounding methods to bridge their performance from the short-form to the long-form setup. We   refer the reader to the supplementary material for additional   evaluations of the baselines on the short-form video grounding setup.   4.3. Ablation Study   In order to determine the optimal configuration of the   Guidance Model, we conducted ablation studies focusing   on three critical factors: (i) the selection of modalities (i.e.,   visual and/or audio), (ii) the performance of query-agnostic   versus query-dependent guidance, and (iii) determining the   optimal window size using the MAD dataset. Moreover,   we performed an extensive investigation on actionless   moments, which provides a significant differentiation of   our implementation from the temporal proposals method.   Moreover, we investigated the influence of the audio   streams in video grounding models. Lastly, we conclude   with a qualitative analysis of our proposed pipeline. Full   detailed results in supplementary material.   Guidance through multimodality fusion. In this section,   we investigate the contribution of having multiple modalities for the Guidance Model. In Table 3 we report the baselines\u2019 performance as-is (rows 1,5,9), as well as their performances when combined with our Guidance Model. The   Guidance Model in each row was trained with different input modalities for the MAD dataset: (i) audio and text, (ii)   video and text, (iii) audio, video, and text. We report Mean   Recall@K (mR@K) for compactness.   For each baseline, the best performance can be achieved   when all modalities are used for the Guidance Model,   yielding an improvement between 1\u221218\u00d7 across all metrics. Using audio alone without visual input leads to modest performance improvement for Zero-shot CLIP, VLGNet, and Moment-DETR. Additionally, visual clues can   also boost all baselines, with Moment-DETR benefiting the   most. Nonetheless, when combining all modalities, we can   achieve the best overall performance.   Notice how the use of the Guidance Model is able to   boost the performance of Moment-DETR, getting it to   achieve a performance close to the proposal-based method   even though the baseline performance is particularly poor.   This finding allows us to conclude that strong short-video   13672   Model   Modalities   mR@1 mR@5 mR@10 mR@50 mR@100   Audio Visual   Zeroshot   CLIP   \u2717   \u2717   3.7   10.0   13.9   27.3   35.0   \u2713   \u2717   4.0   11.1   15.6   28.1   35.3   \u2717   \u2713   4.8   12.0   16.1   29.2   36.0   \u2713   \u2713   5.2   12.6   16.7   29.7   36.4   VLGNet   \u2717   \u2717   2.5   8.6   13.4   31.0   40.8   \u2713   \u2717   2.7   9.2   14.1   32.1   41.7   \u2717   \u2713   3.5   11.1   16.7   35.0   44.1   \u2713   \u2713   3.9   12.1   17.8   36.0   45.2   MomentDETR   \u2717   \u2717   0.2   1.0   1.8   7.5   13.1   \u2713   \u2717   1.0   4.2   7.1   20.7   29.5   \u2717   \u2713   3.1   10.7   16.2   34.1   42.9   \u2713   \u2713   3.6   11.5   17.2   35.2   43.8   Table 3: Modality comparison for the Query Dependant Guidance Model. The table reports the boost in performance achieved   by the chosen baselines (rows 1,5,9) when combined with a Guidance Model having access to different input modalities. Mean Recall (mR@K) metric is computed over the validation set.   grounding methods can be bridged to long-form grounding   through a two-stage approach that leverages a Guidance   Model to reduce the search space for the temporal alignment of video and text.   Describable windows.   The first ablation suggests that   using both audio and video is a powerful representation.   However, the Guidance Model used so far is querydependent and leverages textual queries for identifying   irrelevant windows. Our method can be very efficient if we   can identify windows that are non-describable regardless   of the input query so that the Guidance Model can process   the video/audio streams only once. The question we want   to answer in the following ablation is \u201cCan we devise an   efficient first stage to identify non-describable windows   and remove them from the search space of grounding   methods?\u201d To investigate this research question, we devise   a query-agnostic Guidance Model that does not process any   textual query in input. In Table 4, we report the comparison   between the query-agnostic and query-dependent Guidance   Model, which uses both audio and visual cues as inputs.   For compactness, we report the Mean Recallall (mRall).   Using audio-visual cues alone leads to improvements for   Zero-shot CLIP, VLG-Net, and Moment-DETR. Conversely, when our Guidance Model takes the text query   as input, it can better discriminate between relevant and   irrelevant moments leading to consistent improvements   across all baselines. We conclude that the query-dependent   setup offers superior performance, at the cost of being   more computationally expensive as the number of queries   grows.   On the other hand, the query-agnostic setup is   computationally efficient as the video only has to be   processed once by the Guidance Model, making it suitable   for real-time or low-resource scenarios.   Model   Baseline   Query   Query   Agnostic   Dependent   Zero-shot CLIP [33]   18.0   18.5   (+0.5) 20.1   (+2.1)   VLG-Net [34]   19.3   20.7   (+1.4) 23.0   (+3.7)   Moment-DETR [17]   4.7   8.3   (+3.6) 22.3   (+17.6)   Table 4: Describable windows. We report Mean Recallall for   the baselines performances (second column), query agnostic guidance filter combined with baselines (third column), and query dependent guidance filter combined with baselines (fourth column).   Performance is measured on the MAD validation set.   ZeroShot CLIP   VLG-Net   Moment-DETR   10   12   14   16   18   20   22   24   mRall (%)   Window   Size 16   Window   Size 32   Window   Size 64   Window   Size 128   Window   Size 256   Figure 4: Temporal field of view. We report mRall for the combination of three baseline models (Zero-shot CLIP, VLG-Net, and   Moment-DETR) with different Guidance Models trained with different window sizes. Results are computed on the validation split.   Temporal field of view. The next question we want to answer is: \u201cWhat is the optimal window size for the Guidance   Model?\u201d.   In Figure 4, we report the performance trend   for all baselines when combined with a query-dependent   audio-visual Guidance Model, where we vary the window   size (temporal field of view) the Guidance Model can reason about. The trade-off in this experiment is between the   fine-grained guidance versus the temporal context available   to the guidance model. We test window sizes ranging from   16 to 256 time steps in the MAD dataset, with each time   step accounting for 0.2s when the video FPS is set to 5.   This yields models with a temporal field of view ranging   from 3.2 to 51.2 seconds. For this experiment, we report   the mRall metric.   Figure 4 showcases that,   although short windows   provide a more fine-grained guidance, they do not yield the   best performance. For all methods investigated, the performance consistently increases from windows size 16 to 64.   After such a window size, performance starts to decrease   for the Zero-Shot CLIP and Moment-DETR, while little   change is observed for VLG-Net. Following these findings,   we fix the window size for the guidance model to be 64   in all other experiments. Hence, we can conclude that to   improve the performance of the video grounding task, it is   necessary to have sufficient context information so as to   delimit all possible predictions correctly.   13673   Method   mR@1 mR@5 mR@10   Zero-shot CLIP [33]   3.09   8.10   11.30   VLG-Net [34]   1.53   5.77   9.33   Moment-DETR [17]   0.15   0.65   0.91   \u2020Zero-shot CLIP   4.31   10.38   14.01   \u2020VLG-Net   2.32   8.07   13.13   \u2020 Moment-DETR   2.86   8.44   13.51   Table 5: Describable windows beyond actions. We compare the   performance of our baseline models with the Guidance Model (\u2020)   and without guidance on actionless queries only. For this purpose,   we extract actionless queries from MAD [33] test set and evaluate   using mR@K for K\u2208{1, 5, 10}. The results showcase that our   guidance method is not solely learning action-based concepts.   Describable windows beyond actions.   Our motivation   is to follow a data-driven approach to learn about what   segment of the videos are worth describing. Looking at   the data in MAD [33] reveals that a considerable portion   of the queries (10% and 18%, respectively) lack verbs and   describe only the environment and its attributes (adjectives,   nouns).   Examples of such queries include \u201cNight time   at SOMEONE\u2019s building\u201d and \u201cLater, on a circular   staircase\u201d.   We recognize that interpreting this type of   query is a challenging task, and it sets our approach apart   from other computer vision methods, such as temporal   proposals [4, 46, 41, 13, 28] because these moments do not   contain any action element. Therefore, to validate the above   hypothesis, we report in Table 5 the boost in performance   that our Guidance Model brings for actionless queries.   The metrics show an improvement when our method is   applied to actionless queries, providing evidence for the   effectiveness of our approach in capturing descriptions of   actionless moments.   Enhancing Video Grounding by Using Audio. By incorporating audio features in Moment-DETR [17] using the   MAD Dataset [33], we achieve a substantial performance   boost. As shown in Table 6, the mRall (mean Recall over   all classes) improves from 5.08% to 7.70%. Furthermore,   when combined with the Guidance Model, the mRall further increases from 24.19% to 28.30%, demonstrating a   consistent impact across setups. These results show a remarkable performance improvement when audio is integrated into the video grounding model.   We emphasize that our main focus is not on adding   audio to the video grounding model but rather on the   design and experimentation of the Guidance model. For   a fair comparison, models in Tables 1 and 2 solely use   visual and language features, excluding audio. However,   we believe these results show encouraging results for the   future inclusion of audio features in mainstream grounding   models.   Method   Audio m@R1 m@R5 m@R10 m@R50 m@R100 mRall   Moment-DETR [17]   \u2717   0.24   1.11   2.02   7.92   14.13   5.08   Moment-DETR [17]   \u2713   0.70   2.15   5.11   11.23   19.08   7.70   \u2020Moment-DETR [17]   \u2717   3.76   12.27   18.78   38.48   47.65   24.19   \u2020Moment-DETR [17]   \u2713   5.18   15.70   22.04   44.26   54.32   28.30   Table 6: Including audio in Grounding Model. The results   demonstrate a performance boost when audio is integrated into   the video grounding model, thereby opening new possibilities for   future research to explore this modality further. We enhance reader   understanding by emphasizing methods that include the Guidance   Model with (\u2020) in their name.   Qualitative Results.   Figure 5 presents three qualitative   grounding results from the MAD [33] test set.   The figure showcases how our Guidance Model can improve the   ranking of moments predicted by a grounding model (i.e.,   VLG-Net [34]). We report three positive cases (a-c) and a   failure case (d). In Figure 5a, the Guidance Model is able   to improve the ranking of the baseline model\u2019s prediction   by 12 positions (from rank 16 to rank 4). Given the tight   tIoU between the predicted moment and the ground truth,   the Guidance Model is able to positively boost the performance. An even larger improvement is showcased in 5b,   where the Guidance Model pushes the baseline prediction   from rank 24 to 6. Figure 5c depicts a case when our Guidance Model helps to filter out a non-describable window by   sensibly reducing its ranking from 5 to 62. Here, the moment predicted by the baseline model (blue) is ranked high,   however, the predicted moment possesses the characteristics of a non-describable window. Therefore, our Guidance   Model penalizes such a moment by reducing its ranking,   making this false positive less relevant. Combined with our   experimental section, the visualization provides a clearer   understanding of the function of our guidance filter. Although the Guidance Model has an overall beneficial impact over all baselines, it can also impair performance in   some cases. Figure 5d is such an example, where our Guidance Model worsens the ranking of a positive prediction;   the reader should notice how this example depicts a static   scene containing a moment relevant to the audience, with   the Guidance Model penalizing it in a wrong way.   5. Limitations   Our method presents a notable improvement in video   grounding performance.   Nevertheless, it also presents a   significant limitation regarding to the substantial inference   time needed to match each query with every segment in a   provided video. In our pursuit of finding a middle ground   between computational efficiency and performance, we offer a more cost-effective option by introducing a queryagnostic framework. It is important to recognize that opting   13674   38:38   38:45   38:38   Rank 16th   Rank 4th   38:47   27:19   27:27   27:24   Rank 24th   Rank 6th   27:30   43:43   43:51   43:50   43:47   Rank 33rd   Rank 50th   (a) He peeks into the corridor, as a nurse walks by the reception desk.   (b) Someone averts his gaze from the screen and notices a man in the room.   (d) Cigarette hanging out of his mouth.   (c) A person standing over a person lying on a hospital bed.   38:06   38:13   Rank 5th   Rank 62nd   1:53:32   1:53:38   Ground Truth   Grounding Model\u2019s prediction   Guided Grounding Model\u2019s prediction    Figure 5: Qualitative Results. We compare ground truth annotation (red box) and the predicted temporal endpoints (arrows) with their   respective rankings. We highlight in blue the prediction from the baseline VLG-Net [34] and in green the prediction of the baseline when   combined with our guidance model. Notably, examples (a-c) depict a positive impact of the guidance model over the baseline one by either   improving the ranking of a prediction or worsening it. Example (d) instead depicts a failure case for our pipeline.   for this alternative comes with a drawback: a decrease in   performance across various metrics. This trade-off arises   because the approach shift from query-dependent framework to detecting potentially interesting moments without   considering a natural language query. Nonetheless, we believe this work will inspire the community in pursuing further effective and efficient video grounding on long-form   video datasets.   6. Social impact   Our models development heavily leveraged the MAD   dataset [33].   This dataset, primarily drawn from cinematic sources, underpins our training efforts but also introduces the biases inherent in movie portrayals. While providing benefits for training, this method also brings in biases come from historical stories, cultural portrayals, and   societal norms seen in movies.   These biases could affect the model\u2019s comprehension of real-world concepts. To   address this, we conducted experiments using the Ego4D   dataset [12]. This dataset takes a different approach by collecting videos worldwide in partnership with local organizations. Its goal is to capture a variety of everyday experiences   across different cultures and areas, in order to offset any biases that may arise from how these experiences are depicted   in movies. Impressively, our models demonstrated robust   performance across both MAD and Ego4D datasets, showcasing their versatility and suggesting their potential for   knowledge transfer. Moreover, the incorporation of Ego4D   experiments contribute to the broader discourse on fair AI   by promoting testing video-based approaches on such diverse datasets.   7. Conclusion   We presented a novel approach for language grounding   in long videos. Our guidance framework is flexible, and our   experimental section provides ample demonstration that it   can boost performance for a variety of baselines, including   proposal-based models like VLG-Net and zero-shot CLIP,   as well as VSL-Net, and the proposal-free Moment-DETR.   Our proposed Guidance Model can operate over different   modalities and is very efficient in its query-agnostic   version. However, we found that using the queries as part   of the guidance mechanism allows for better performance.   Future work includes exploring the use of smaller, more   specialized Guidance Models to optimize inference time   by prioritizing recall or precision based on application   needs, effectively reducing inference time while maintaining desired performance levels. This could lead to a   customized processing pipeline for various applications,   achieving a balance between computational efficiency and   accuracy. We also believe the community will further pursue two-stage approaches for the task at hand by designing   more sophisticated and powerful guidance models that can   provide even better assistance to paired grounding methods.   Acknowledgments.   W. Barrios was supported by Dartmouth Graduate Fellowship through Guarini Office and   Computer Science Department at Dartmouth College.   M. Soldan was supported by the King Abdullah University of Science and Technology (KAUST) Office of   Sponsored Research through the Visual Computing Center (VCC) funding, as well as, the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence   (SDAIA-KAUST AI).   A. Ceballos Arroyo acknowledges financial support for   this work by the Fulbright U.S. Student Program, which is   sponsored by the U.S. Department of State and Fulbright   Colombia. In addition, he was partially supported by Northeastern University through a Graduate Fellowship.   13675", "conf": "ICCV", "year": "2023", "index": 302}, {"title": "Visual Relation Grounding in Videos   Junbin Xiao1, Xindi Shang1, Xun Yang1, Sheng Tang2, and Tat-Seng Chua1   1 Department of Computer Science, National University of Singapore, Singapore   {junbin,shangxin,chuats}@comp.nus.edu.sg, xunyang@nus.edu.sg   2 Institute of Computing Technology, Chinese Academy of Sciences, China   ts@ict.ac.cn", "abstract": ". In this paper, we explore a novel task named visual Relation Grounding in Videos (vRGV). The task aims at spatio-temporally   localizing the given relations in the form of subject-predicate-object in   the videos, so as to provide supportive visual facts for other high-level   video-language tasks (e.g., video-language grounding and video question   answering). The challenges in this task include but not limited to: (1)   both the subject and object are required to be spatio-temporally localized   to ground a query relation; (2) the temporal dynamic nature of visual   relations in videos is di\ufb03cult to capture; and (3) the grounding should   be achieved without any direct supervision in space and time. To ground   the relations, we tackle the challenges by collaboratively optimizing two   sequences of regions over a constructed hierarchical spatio-temporal region graph through relation attending and reconstruction, in which we   further propose a message passing mechanism by spatial attention shifting between visual entities. Experimental results demonstrate that our   model can not only outperform baseline approaches signi\ufb01cantly, but also   produces visually meaningful facts to support visual grounding. (Code   is available at https://github.com/doc-doc/vRGV).   1", "content": "Visual grounding aims to establish precise correspondence between textual query   and visual contents by localizing in the images or videos the relevant visual facts   depicted by the given language. It was originally tackled in language-based visual   fragment-retrieval [9,12,13], and has recently attracted widespread attention as a   task onto itself. While lots of the existing e\ufb00orts are made on referring expression   grounding in static images [8,19,22,23,28,41,42,44], recent research attempts to   study visual grounding in videos by \ufb01nding the objects either in individual frames   [10,32,47] or in video clips spatio-temporally [1,3,46]. Nonetheless, all these works   focus on grounding in videos the objects depicted by natural language sentences.   Although the models have shown success on the corresponding datasets, they   lack transparency to tell which parts of the sentence help to disambiguate the   object from the others, and thus hard to explain whether they truly understand   the contents or just vaguely learn from data statistics. Furthermore, the models   fail to e\ufb00ectively reason about the visual details (e.g., relational semantics), and   thus would generalize poorly on unseen scenarios.   2   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   \u2026   \u2026   person-jump_above-bicycle   Fig. 1: Illustration of the vRGV task. For the query relation person-jump abovebicycle and an untrimmed video containing the relation, the objective is to \ufb01nd   a video segment along with two trajectories corresponding to the subject (red   box) and object (green box) that match the query relation.   To achieve understandable visual analytics, many works have showed the   importance of comprehending the interactions and relationships between objects   [8,16,31]. To this end, we explore explicit relations in videos by proposing a novel   task of visual Relation Grounding in Videos (vRGV). The task takes a relation in   the form of subject-predicate-object as query, and requires the models to localize   the related visual subject and object in a video by returning their trajectories.   As shown in Fig. 1, given a query relation person-jump above-bicycle and an   untrimmed video containing that relation, the task is to \ufb01nd in the video the   subject (person in white) and object (bicycle) trajectory pairs that hold the   query relationship jump above.3 Considering that annotating \ufb01ne-grained regionrelation pairs in videos is complicated and labor-intensive [30], we de\ufb01ne the task   as weakly-supervised grounding where only video-relation correspondences are   available. That is, the models only know that a relation exists but do not know   where and when it is present in the video during training.   vRGV retains and invokes several challenges associated with video visual relation and visual grounding. First, unlike existing video grounding [1,3] which is   to localize a speci\ufb01c object according to its natural language description, in visual relation grounding, the models are required to jointly localize a pair of visual   entities (subject and object) conditioned on their local relationships. Second, unlike a coarsely global description, relations are more \ufb01ne-grained and change over   time, i.e., even a same object would have di\ufb00erent relationships with di\ufb00erent   objects at di\ufb00erent time. For example, the enduring relations (e.g., person-drivecar) may exist for a long time but the transient ones (e.g., person-get o\ufb00-car)   may disappear quickly. Besides, static relationships like spatial locations and   states (e.g., hold and hug) can be grounded at frame level, whereas the dynamic   ones such as lift up and put down can only be grounded based on a short video   clip. Such dynamic nature of relations in videos will cause great challenge for   spatio-temporal modeling. Third, the requirement for weakly-supervision also   challenges the models to learn to ground the relation without reliance on any   spatial (bounding boxes) and temporal (time spans) supervisions.   3 The word \u2019untrimmed\u2019 is regarding to relation. We refer to relation as the complete   triplet subject-predicate-object, and relationship as the predicate only.   Visual Relation Grounding in Videos   3   To address the above challenges, we devise a model of video relation grounding by reconstruction. Concretely, we incorporate a query relation into a given   video which is modeled as a hierarchical spatio-temporal region graph, for the   purpose of identifying the related subject and object from region proposals in   multi-temporal granularity. For weakly-supervised video relation grounding during training, we optimize the localization by reconstructing the query relation   with the subject and object identi\ufb01ed by textual clues and attention shift. During   inference, we dynamically link the subject and object which contribute most to   the re-generated relations into trajectories as the grounding result. Our insight   is that visual relations are data representations of clear structural information,   and there is a strong cross-modal correspondence between the textual subjectobject and the visual entities in the videos. Thus, it is reasonable to ground the   relevant subject and object by re-generating the relation.   Our main contributions are: (1) we de\ufb01ne a novel task, visual Relation   Grounding in Videos (vRGV), to provide a new benchmark for the research   on video visual relation and underpin high-level video-language tasks; (2) we   propose an approach for weakly-supervised video relation grounding, by collaboratively optimizing two sequences of regions over a hierarchical spatio-temporal   region graph through relation attending and reconstruction; and (3) we propose   a novel message passing mechanism based on spatial attention shifting between   visual entities, to spatially pinpoint the related subject and object.   2   Related Work   In this section, we brie\ufb02y recap the history in visual relation, visual grounding   and video modeling, which are either similar in spirit to the task de\ufb01nition or   technically relevant to our approach.   Visual Relation. Early attempts on visual relation either leveraged object   co-occurrence and spatial relationships for object segmentation [4], or focused   on human-centric relationships for understanding human-object interactions [40].   Recently, many works started to study visual relations as a task onto itself to   facilitate cognitive visual reasoning. Lu et al. [20] \ufb01rstly formulated visual relations as three separated parts of object 1-predicate-object 2, and classi\ufb01ed visual   relationships as spatial, comparative, preposition and verb predicates. Krishna et   al. [16] formalized visual relations as a scene graph for image structural representation, in which visual entities are corresponding to nodes and connected by   edges depicted by object relationships. Shang et al. [31] introduced visual relations from images to videos (video scene graph). Apart from the relations in   static images, they added relationships that are featured with dynamic information (e.g., chase and wave ), so as to emphasize spatio-temporal reasoning   of \ufb01ne-grained video contents. According to their de\ufb01nition, a valid relation in   videos requires both the subject and object to appear together in each frame of   a certain video clip.   While a handful of works have successfully exploited relations to improve visual grounding [8] and visual question answering [21], relation as an independent   4   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   problem is mostly tackled in the form of detection task and the advancements   are mostly made in the image domain [17,20,45]. In contrast, relation as a detection task in video domain has earned little attention, partly due to the great   challenges in joint video object detection and relation prediction with insu\ufb03cient video data [26,35]. In this paper, instead of blindly detecting all visual   objects and relations in videos, we focuses on the inverse side of the problem by   spatio-temporally grounding a given relation in a video.   Visual Grounding. Visual grounding has emerged as a subject under intense study in referring expression comprehension [8,15,22,23,28,41,42,44]. Mao   et al. [22] \ufb01rst explored referring expression grounding by using the framework of   Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM)   network [7] for image and sentence modeling. They achieved grounding by extracting region proposals and then \ufb01nding the region that can generate the   sentence with maximum posterior probability. Similarly, Rohrbach et al. [28]   explored image grounding by reconstruction to enable grounding in a weaklysupervised scenario. Krishna et al. [15] explored referring relationships in images   by iterative message passing between subjects and objects. While these works   focus on image grounding, more recent e\ufb00orts [1,3,10,32,39,46,47] also attempted   to ground objects in videos. Zhou et al. [47] explored weakly-supervised grounding of descriptive nouns in separate frames in a frame-weighted retrieval fashion. Huang et al. [10] proposed to grounding referring expression in temporally   aligned instructional videos. Chen et al. [3] proposed to perform spatio-temporal   object grounding with video-level supervision, which aims to localize an object   tube described by a natural language sentence. They pre-extracted the action   tubes, and then rank and return the tube of maximal similarity with the query   sentence. Instead of grounding a certain object in trimmed videos by a global   object description [3], we are interested in localizing a couple of objects conditioned upon their relationships in untrimmed videos, which is more challenging   and meaningful in reasoning real-world visual contents.   Video Modeling. Over the decades, modeling the spatio-temporal nature   of video has been the core of research in video understanding. Established handcrafted feature like iDT [37] and deep CNN based features like C3D [34], twoStream [33] and I3D [2], have shown their respective strengths in di\ufb00erent models. However, all these features mainly capture motion information in a short   time interval (e.g., 16 frames as the popular setting in C3D). To enable both   long and short-term dependency capturing, researchers [36,43] also attempted   to model the video as an ordered frame sequence using Recurrent Neural Networks (RNNs). While RNN can deal with dynamic video length in principle, it   was reported that the preferable number of frames with regard to a video should   be ranged from 30 to 80 [24,36]. As a result, Pan et al. [24] further proposed a   hierarchical recurrent neural encoder to achieve temporal modeling in multiple   granularity. Yet, they focused on generating a global description of the video by   extracting frame-level CNN feature, which can hardly be applied to relation understanding where \ufb01ne-grained regional information is indispensable. Recently,   there is a tendency of modeling videos as spatio-temporal graphs [11,26,35,38],   Visual Relation Grounding in Videos   5   where the nodes correspond to regions and edges to spatial-temporal relationships. Nonetheless, all of them model the video as a \ufb02at and densely connected   graph. Instead, we retain the temporal structure (ordered frames and clips) of   videos by modeling it as a hierarchical spatio-temporal region graph with sparse   directed connections.   3   Method   Recall that our goal is to ground relations in the given videos, which is formulated by giving a relation coupled with videos containing that relation, and   returning two trajectories for each video, corresponding to the subject and object   participating in the query relation4. We formally de\ufb01ne the task as follows.   Task De\ufb01nition: Given a set of query relations in form of R = {< S \u2212   P \u2212O >} and a set of untrimmed videos V (where S, P, O denote the subject,   predicate and object respectively), and each speci\ufb01c query relation Ri is coupled with several videos from V which contain that relation, the task is to   spatio-temporally localize in the videos the respective subjects and objects by   returning their trajectories Ts, To. The trajectory T is given by a sequence of   bounding boxes tied to a certain visual entity across a video segment. For weaklysupervised grounding, there is no spatial (bounding box) and temporal supervisions (time spans) from the dataset during training.   3.1   Solution Overview   Given a video of N frames, we \ufb01rst extract M region proposals for each frame.   Thus, a video can be represented by a set of regions V = {Bi,j | i \u2208[1, N], j \u2208   [1, M]}, and a trajectory T = {Bi | i \u2208[k, l], k \u2208[1, N], l \u2208[k, N]} can be a   sequence of bounding boxes in the video. Our approach will learn to ground a   given relation R by \ufb01nding two trajectories Ts, To that indicate the subject and   object of the relation. According to the task de\ufb01nition, we resolve the problem   by maximizing the following posterior probability:   T \u2217   s , T \u2217   o = arg max   Ts, To   P(R | Ts, To) \u2217P(Ts, To | V, R),   (1)   where P(Ts, To | V, R) aims to attend to the most relevant trajectories in   V given the relation R, and P(R | Ts, To) attempts to reconstruct the same   relation R based on the relevant trajectories it attended to. During inference,   our approach will output the trajectories (T \u2217   s and T \u2217   o ) that contribute mostly to   the re-generated relation to accomplish grounding.   The key idea of our approach is to ground the relation through reconstruction   by capturing the intuition that there is a clear correspondence between subjectobject in textual relation and visual instances. However, unlike image grounding   [28] which can directly model and return the region proposals, e\ufb00ective trajectory proposals are unavailable in this task due to the complicated dynamics of   4 If there are more than one instances that match the query relation in a video, it is   a correct grounding by returning any one of them.   6   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   O   S   C   O   S   C   O   S   C   O   S   C   O   S   C   O   S   C   O   S   C   O   S   C   O   S   C   C   C   C   person follow car   P   P`   P   P`   P   P`   P   P`   P   P`   P   P`   P   P`   P   P`   P   P`   Decoder   person follow car   F1   FL   FN   FN-L+1   \u2026   \u2026   \u2026   S: Subject   P: Predicate   O: Object   C: RNN Cell   F: Frame   L: Clip Length   N: # Frames   Fig. 2: Schematic diagram of video relation grounding by reconstruction. The   model takes the query relation as guidance to pinpoint regions of subjects and   objects over the hierarchical spatio-temporal region graph, where the regions   correspond to nodes which are spatially connected by visual relationships and   are temporally connected by hierarchical RNN over di\ufb00erent frames and clips.   the relations in videos. To achieve the optimization in Equ. (1), we model the trajectory proposals implicitly over a hierarchical spatio-temporal region graph and   online-optimize them through relation attending and reconstruction. As shown   in Fig. 2, two sequences of regions corresponding to the query subject and object will be identi\ufb01ed from the video (region graph) and jointly embedded into   the \ufb01nal graph representation which will be fed to the decoder to reconstruct   the relation. The reconstruction loss from the decoder will be back-propagated   to the graph encoder, to penalize the incorrect object pairs with respect to the   relation. During inference, we dynamically link the regions which response significantly to the reconstructed relations into explicit trajectories to accomplish the   grounding, where the importance of the regions are determined by spatial and   temporal attention over the hierarchical space-time region graph. In this way,   our model can in principle ground visual relations in multi-temporal granularity   without the bottleneck of o\ufb00-line trajectory proposal extraction [3].   We next elaborate how to learn to spatio-temporally attend to the correct   sequences of regions for a given relation, and then how to obtain the \ufb01nal trajectories based on the attention values, to accomplish the grounding.   3.2   Message Passing by Attention Shifting   Spatial Attention. This unit takes as input all the region proposals B =   {Bj | j \u2208[1, M]} in a frame and the query relation R =< S \u2212P \u2212O >. It learns   two spatial attentions (\u03b1M\u00d71   s   , \u03b1M\u00d71   o   ) corresponding to the subject and object.   Concretely, the spatial attention unit (SAU) is formulated as:   \u03b1s = SAU( f(B), g(S) ), \u03b1o = SAU( f(B), g(O) ),   (2)   in which g(\u00b7) returns the textual word feature for subject (S) or object (O), and is   achieved by embedding the respective GloVe [25] vector: g(S) = Emb(GloV e(S)).   Visual Relation Grounding in Videos   7   Besides, f(\u00b7) means feature extraction for the region proposals. In our implementation, we utilize several kinds of feature related to the object appearances,   relative locations and sizes, which are not only important in visual relation understanding, but also crucial in identifying the same object in di\ufb00erent frames.   The object appearance is captured by the ROI-aligned feature from object   detection models, i.e., fapp = CNN(Bj). The object relative location and size   are useful to identify the spatial and comparative relationships, and are given by   fB = [ xmin   W , ymin   H , xmax   W , ymax   H , area   W \u2217H ], in which W, H are respectively the width,   height of the frame and area is the area of the bounding box represented by   the top-left (xmin, ymin) and bottom-right (xmax, ymax) coordinates. The \ufb01nal   feature of a region proposal f(Bj) is thus obtained by element-wise addition of   the transformed visual appearance feature fapp and bounding box feature fB.   The transform operation is achieved by linear mapping with ReLU activation.   We take the subject S as an example to introduce how to obtain the representation fs and attention distribution \u03b1s for it (similar way to obtain \u03b1o and   fo for object O). Given the textual subject representation g(S) and each region   proposal f(Bj), the attention score sj is obtained by   sj = W2tanh(W1[f(Bj), g(S)] + b1),   (3)   in which W1, b1, W2 are model parameters. Then, the attention distribution over   di\ufb00erent region Bj, and the \ufb01nal representation for subject S are give by   \u03b1sj = softmax(sj) =   exp(sj)   PM   z=1 exp(sz)   , fs =   M   X   j=0   \u03b1sjf(Bj).   (4)   Attention Shifting. Although the aforementioned attention unit is capable of   identifying the subjects and objects semantically related to the query relations,   they are not necessarily the exact visual entities that hold the relationships. Take   the instance in Fig. 2 as an example, there is one person but several cars on the   street, and only one car match the relationship follow with the person. It is the   relationship follow that helps to disambiguate the car of interest from the other   cars. Another intuition is that, given the subject person and relationship follow,   the searching space of the object car can be narrowed to the areas in front of   the person, and vice verse.   As shown in Fig 2, we capture these insights by modeling the relationships   as attention shifting (message passing) between the visual entities, so as to accurately pinpoint the subject and object participating in the query relation.   Speci\ufb01cally, we learn two independent transfer matrices Wso and Wos tied to   the forward relationship (P, message from subject to object) and backward relationship (P \u2032, message from object to subject) respectively.   fso = ReLU(Wso\u03b1s), fos = ReLU(Wos\u03b1o).   (5)   The transferred location feature from subject (object) to object (subject) will   be added to the attention based object (subject) feature:   fs = fs + fos, fo = fo + fso.   (6)   Finally, the subject and object representations will be concatenated and transformed to obtain the node input at time step i, i.e., fi = W3([fs, fo])+b3, where   W3, b3 are learnable parameters.   8   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   3.3   Hierarchical Temporal Attention   To cope with the temporal dynamics of video relations, we devise two relationaware hierarchical temporal attention units TAU1 and TAU2 (which work in a   way similar to SAU) over the frames and clips respectively. As shown in Fig.   2, a video is \ufb01rstly divided into H = N   L short clips of length L. The frame-wise   temporal attention \u03b2l1 (of dimension N) is obtained by   \u03b2l1 = TAU1(f l1, f l2   H ),   (7)   in which f l1 denotes the sequence of frame-wise feature, and is achieved by   sequence modeling the subject and object concatenated feature f   f l1   i   = LSTMl1(f1,\u00b7\u00b7\u00b7 ,i), i \u2208[1, N].   (8)   Besides, f l2   H denotes the output at the last time step of the clip-level neural   encoder, which is obtained by   f l2   H = LSTMl2((\u03b2l2   i f c   i )i=1,\u00b7\u00b7\u00b7 ,H),   (9)   where f c denotes the sequence of clip-level inputs which are obtained by selecting the output of the \ufb01rst layer of LSTM at every L steps, i.e., f c = {f l1   i   | i \u2208   {1, L, \u00b7 \u00b7 \u00b7 , N}}. \u03b2l2 (of dimension H) is the clip-level temporal attention distribution, and is obtained by   \u03b2l2 = TAU2(f c, fR),   (10)   where fR denotes the query relation which is obtained by concatenating the   GloVe feature of each part in the relation (average for phrase) and further transformed to the same dimension space as feature vectors in f c.   3.4   Train and Inference   During training, we drive the \ufb01nal graph embedding by an attention-guided   pooling of the node representations across the video, i.e., featv = P \u03b2l1f l1. The   graph embedding will be fed to the decoder to re-generate the query relation.   The decoder part of our model is similar to [28] by treating the relation as a   textual phrase and reconstruct it by a single LSTM layer. The model was trained   with the cross-entropy loss   Lrec = \u22121   nvr   nvr   X   n=1   nw   X   t=1   logP(Rt|R0:t\u22121, featv),   (11)   where Rt denotes the tth word in the relation. nvr and nw denote the number   of video-relation samples and number of words in the relation respectively.   During inference, we base on the learned spatio-temporal attention values to   achieve the relation-aware trajectories. First, we temporally threshold to obtain   a set of candidate sub-segments for each relation-video instance in the test set.   The segments are obtained by grouping the successive frames in the remaining   frame set after thresholding with value \u03c3, i.e., B = {Bi,1:M|\u03b2i >= \u03c3}, in which   Bi denotes regions in frame i. \u03b2 is temporal attention value obtained by \u03b2 =   \u03b2l1+\u03b2l2. Note that the clip-level attention value will be propagated to all frames   belonging to that clip. (Refer to appendix for more details.)   Visual Relation Grounding in Videos   9   Table 1: Statistics of ImageNet-VidVRD.   Dataset   #Videos #Objects #Predicates #Relations #Instances   ImageNetVidVRD [31]   Train   800   35   132   2961   25,917   Val   200   35   132   1011   4835   Then, for each sub-segment, we de\ufb01ne a linking score s(Bi,p, Bi+1,q) between   regions of successive frames (after sampling)   s(Bi,p, Bi+1,q) = \u03b1i,p + \u03b1i+1,q + \u03bb \u00b7 IoU(Bi,p, Bi+1,q),   (12)   where \u03b1 is the spatial attention value, it can be \u03b1s (subject) or \u03b1o (object)   depending on the linking visual instances. IoU denotes the overlap between   two bounding boxes, and \u03bb =   1   D is a balancing term related to the distance   (D \u2208[1, 10]) of the two successive frames. By de\ufb01ning \u03bb, we trust more on the   attention score when the distance between the two frames are larger. Our idea   is to link the regions which response strongly to the subject or object, and their   spatial extent overlaps signi\ufb01cantly. The \ufb01nal trajectory can thus be achieved   by \ufb01nding the optimal path over the segment   T \u2217= arg max   T   1   K \u22121   K\u22121   X   i=1   s(Bi,p, Bi+1,q),   (13)   where T is a certain linked region sequence of length K for the subject or object. Similar to [5], we solve the optimization problem using Viterbi algorithm.   Finally, the linking scores associated with the subject and object are averaged to   obtain the score for the corresponding sub-segment. The grounding is achieved   by returning the segment (subject-object trajectory pair) of maximal score.   4   Experiments   4.1   Dataset and Evaluation   We conduct experiments on the challenging video relation dataset ImageNetVidVRD [31]. It contains 1000 videos selected from ILSVRC-VID [29], and is   annotated with over 30,000 relation instances covering 35 object classes and 132   prede\ufb01ned predicates. Our preliminary investigation shows that over 99% of relations do not appear throughout the video, with 92% (67%) appearing in less than   1/2 (1/5) length of the video, and the shortest relation only exists in 1 second,   while the longest relation lasts for 40 seconds. Besides, each video contains 2 to   22 objects (3 on average), excluding those un-related objects which also matter   due to the weakly-supervised setting. The dataset statistics are listed in Table   1, others details are given in the appendix. Note that the object trajectories are   provided but are not used during training.   We report accuracy (in percentage) as the grounding performance. Speci\ufb01cally, for each query relation, there might be one or more videos, with each having   one or more visual instances corresponding to that relation. For a video-relation   10   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   pair, a true-positive grounding is con\ufb01rmed if the returned subject-object trajectory has an overlap ratio of larger than 0.5 with one of the ground-truth visual   relation instances. The overlap is measured by the temporal Intersection over   Union (tIoU), which is based on the average number of three di\ufb00erent spatial   IoU thresholds (i.e., sIoU = 0.3, 0.5 and 0.7). Aside from the joint accuracy for   the whole relation (AccR), we also separately report the accuracy for the subject   (AccS) and object (AccO) for better analysis of algorithms.   4.2   Implementation Details   For each video-relation instance, we uniformly sample N=120 frames from the   video and further divide them into H=10 clips of length L=12 frames, and extract M=40 region proposals for each frame. We apply Faster R-CNN [27] with   ResNet-101 [6] as backbone (pretrained on MS-COCO [18]) to extract the region proposals, along with the 2048-D regional appearance feature. The \ufb01nal   dimension of each region representation is transformed to 256. For each word   in the textual relation, we obtain the 300-D GloVe feature and then embed it   to 256 dimension space. The hidden size for the encoder and decoder LSTM is   512. Besides, the models are trained using Adam [14] optimization algorithm   based on an initial learning rate of 1e-4 and batch size of 32. We train the model   with maximal 20 epochs, and use dropout rate 0.2 and early stopping to alleviate over-\ufb01tting. During inference, we \ufb01rst obtain the spatial and temporal   grounding results on the basis of all the sampled frames, and then propagate   the adjacent bounding boxes to the missing frames based on linear interpolation   (see appendix for details). The temporal attention threshold is set to 0.04 , and   is greedily searched on a validation split of the training data.   4.3   Compared Baselines   As there is no existing method for the vRGV task, we adapt several related   works as our baselines. (1) Object Co-occurrence, it was applied in [15] for   referring relationship in images. We can equivalently achieve it in the video scenario by removing all the predicates in our model and only grounding the two   categories. This baseline is to study how much the object co-occurrence will   contribute to the relation grounding performance. (2) Trajectory Ranking.   We adapt the method proposed in video language grounding (namely WSSTG   [3]) to the relation scenario. This can be achieved by regarding the relation as   a natural language sentence and transforming grounding to sentence matching   with the pre-extracted object tubes. Speci\ufb01cally, we consider two implementation variants: (a) V1, which optimizes the similarity between each trajectory   proposal and the query relation during training, and outputs the top-2 ranked   trajectories as the grounded subject and object during inference; and (b) V2,   which concatenates the trajectories pair-wisely to compare their similarity with   the query sentence during training, and returns the top-1 trajectory-pair as   grounded results during inference. (More details can be found in the appendix.)   Visual Relation Grounding in Videos   11   Table 2: Results of visual relation grounding in videos. We add bold and underline to highlight the best and second-best results under each metric respectively.   Methods   sIoU=0.3   sIoU=0.5   sIoU=0.7   Average   AccS AccO AccR AccS AccO AccR AccS AccO AccR AccS AccO AccR   T-Rank V1 [3] 33.55 27.52 17.25 22.61 12.79   4.49   6.31   3.30   0.76   20.27 10.68   3.99   T-Rank V2 [3] 34.35 21.71 15.06 23.00   9.18   3.82   7.06   2.09   0.50   20.83   7.35   3.16   Co-occur* [15] 27.84 25.62 18.44 23.50 20.40 13.81 17.02 14.93   7.29   22.99 19.33 12.80   Co-occur [15]   31.31 30.65 21.79 28.02 27.69 18.86 21.99 21.64 13.16 25.90 25.23 16.48   vRGV* (ours) 37.61 37.75 27.54 32.17 32.32 21.43 21.34 21.02 10.62 31.64 30.92 20.54   vRGV (ours) 42.31 41.31 29.95 37.11 37.52 24.77 29.71 29.72 17.09 36.77 36.30 24.58   4.4   Result Analysis   Table 2 shows the performance comparisons between our approach and the baselines, where vRGV* (similar for Co-occur*) denotes our model variant that   greedily links the regions of maximal attention score in each frame by setting   \u03bb in Equ. (12) to 0. We conduct this experiment to validate that our model is   capable of learning the object identity across di\ufb00erent frames, because we implicitly model and optimize the trajectories on the spatio-temporal graph. When   the model is complemented with explicit object locations during post-linking, it   can achieve better performances as shown in the bottom row.   From the results, we can see that our methods signi\ufb01cantly outperform the   baselines, and both methods adapted from WSSTG [3] (i.e., T-Rank V1 and TRank V2) perform poorly on this task. We speculate the reasons are two folds: (1)   the method in WSSTG is designed for single object grounding, they fail to jointly   ground two visual entities and further to disambiguate between subject and   object (see Fig. 3). In our approach, we collaboratively optimize two sequences   of objects on the spatio-temporal graph with relation attending and message   passing mechanisms, and thus cope well with the joint grounding problem. This   is supported by the observation that the two baselines [3] obtain relatively closer   results to ours on separate grounding accuracy (AccS), but much lower results   than ours regarding the joint accuracy AccR; (2) The method in WSSTG aims   for object grounding in trimmed video clips, and it pre-extracts relation agnostic   object tube proposals and keeps them unchanged during training. In contrast,   our approach enables online optimization of object trajectories regarding relation   and post-generates relation-aware trajectory pairs. Thus, we can generate better   trajectories tailored for relations. This is supported by the observation that the   two baselines can get closer results to ours at a relatively lower overlap threshold,   but their results degenerate signi\ufb01cantly at higher thresholds. (Please also refer   to our results on di\ufb00erent temporal overlap thresholds shown in the appendix.)   Another observation is that T-Rank V2 performs better than T-Rank V1 in   grounding the subject (AccS), but gets much worse results in terms of object   (AccO) and hence acts poorly on the joint grounding results for relations (AccR).   This indicates that the two objects in the top-ranked trajectory pair usually   do not correspond to the subject and object mentioned in the sentence, and   12   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   T-Rank V1   Co-occur   T-Rank V2   Ours   Fig. 3: Qualitative results on the query relation bicycle-move beneath-person.   Table 3: Model ablation results on ImageNet-VidVRD.   Models   sIoU=0.3   sIoU=0.5   sIoU=0.7   Average   AccS AccO AccR AccS AccO AccR AccS AccO AccR AccS AccO AccR   vRGV   42.31 41.31 29.95 37.11 37.52 24.77 29.71 29.72 17.09 36.77 36.30 24.58   w/o Msg 34.72 33.23 23.96 31.60 29.15 19.43 22.56 21.36 11.78 29.41 27.46 17.63   w/o Clip 41.08 39.64 27.15 36.31 35.05 21.77 28.19 27.11 13.72 35.05 34.03 20.58   w/o TAU 32.99 32.76 20.34 22.36 19.99   7.61   15.29 13.27   4.83   21.75 19.26   7.06   they are more likely the redundant proposals of the main objects. As shown in   Fig. 3, the model T-Rank V2 can successfully \ufb01nd the subject bicycle, but fails   to localize the object person. We think the reason is that WSSTG [3] is oriented   for grounding the main object in a natural language sentence (i.e., the subject),   and when concatenating the representations of two trajectories, it can enhance   the representation for the main object, but not sure for other supportive objects.   According to the results, it even confuses the model and thus jeopardizes the   grounding performance for the object (AccO).   Relatively, the co-occurrence baseline performs better than [3] on this task.   Yet, its performances are still worse than ours (Co-occur* v.s. vRGV* and Cooccur v.s. vRGV). This demonstrates that the \u201cpredicate\u201d in the relation is   crucial in precisely disambiguating the subjects and objects. As shown in Fig. 3,   the occurrence baseline wrongly grounds the person sitting as the object, whereas   our method successfully grounds the object to the person riding the bicycle. We   also note that the co-occurrence baseline beats our weak model variant under   the metric with threshold 0.7 (Co-occur v.s. vRGV*), which in fact, shows the   superiority of our overall framework for joint grounding of two objects. Also, it   indicates the importance of object locations in generating better trajectories.   4.5   Model Ablations   We ablate our model in Table 3 to study the contribution of each component.   Results in the 2nd row are obtained by removing the message passing module. We   Visual Relation Grounding in Videos   13   (b) person ride bicycle   (a) person feed elephant   \ud835\udefd   \ud835\udefd   Fig. 4: Qualitative results based on temporal threshold 0.04.   can see that the performance AccR drops from 24.58% to 17.63%. This is mainly   because the model without explicit message communication between subject and   object cannot cope well with the scenario where there are multiple visual entities   of same category present in the video. Another reason could be that the ablated   model is weak in detecting objects under complex conditions (e.g., occlusion and   blur) without the contextual information from their partners.   Results in the 3rd row are obtained by removing the clip-level attention.   We do this ablation to prove the importance of the hierarchical structure of   our model. Note that the temporal threshold \u03c3 for this experiment is set to   0.0001, because we only have the frame-level attention \u03b2l1. Comparing with   results in the \ufb01rst row, we can see the results drop under all criteria without the   hierarchical structure. When we further remove the frame-level attention (shown   in the 4th row), and thus delete the whole temporal attention unit (TAU), the   results degrade signi\ufb01cantly from 24.58% to 7.06%. This is because our model   will blindly link the objects throughout the video regardless of relation without   the temporal grounding module. These \ufb01ndings demonstrate the importance of   relation-aware temporal grounding in untrimmed videos.   We also analyze the temporal threshold \u03c3 by changing it from 0.01 to 0.05,   the corresponding results AccR are 24.51%, 24.76%, 24.43%, 24.58% and 22.94%.   From the results, we can see that there is no signi\ufb01cant di\ufb00erence when the   threshold changes from 0.01 to 0.05, and the best result is achieved at the   threshold of 0.02. We show some qualitative results in Fig. 4 based on temporal threshold of 0.04 (More results can be found in the appendix), from which   we can see that our model can ground the subjects and objects in the videos   when the query relations exist.   4.6   Zero-shot Evaluation   In this section, we analyze the models\u2019 capability of grounding the new (unseen   during training) relation triplets. Speci\ufb01cally, in zero-shot relation grounding, we   consider the case that the complete relation triplet is never seen, but their separate components (e.g., subject, predicate or object) are known during training. For   example, the model may have seen the relation triplets person-ride-bicycle and   person-run behind-car during training, but it never knows bicycle-run behind14   Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua   Table 4: Results of zero-shot visual relation grounding.   Methods   AccS AccO AccR   T-Rank V1 [3] 4.05   4.08   1.37   T-Rank V2 [3] 7.09   4.13   1.37   Co-occur [15]   11.60 10.99   7.38   vRGV (ours) 18.94 17.23 10.27   car. As a result, we can \ufb01nd 432 relation instances of 258 unseen relation triplets   in 73 videos from the test set.   As shown in Table 4, our approach still outperforms the baselines in the   zero-shot setting. We attribute such strength of our approach under zero-shot   scenario to two reasons. First, we decompose the relation and separately embed   the words corresponding to the subject and object into a semantic space during   relation embedding. This is di\ufb00erent from modeling the relation holistically using   LSTM as in [3], which lacks \ufb02exibility and is hard to learn with limited training   data. Second, we treat the relation as a natural language in the reconstruction   stage, which enhances the model\u2019s ability in visual reasoning through forcing it   to infer the remaining words conditioned on the related visual content and the   previously generated words in the relation.   5   Conclusion   In this paper, we de\ufb01ned a novel task of visual relation grounding in videos   which is of signi\ufb01cance in underpinning other high-level video-language tasks.   To solve the challenges in the task, we proposed a weakly-supervised video relation grounding method by modeling the video as hierarchical spatio-temporal   region graph, and collaboratively optimizing two region sequences over it by incorporating relation as textual clues and passing messages by spatial attention   shift. Our experiments demonstrated the e\ufb00ectiveness of the proposed approach.   Future e\ufb00orts can either be made on how to jointly ground the subject and   object in videos conditioned on their interactions, or how to better capture the   temporal dynamics of relations in videos. In addition, it is also important to   explore how to better optimize the video graph model based on video-level supervisions only. Another promising direction could be utilizing relation to boost   video language grounding and video question answering.   Acknowledgement   This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, \ufb01ndings and conclusions or recommendations expressed in this material are   those of the author(s) and do not re\ufb02ect the views of National Research Foundation, Singapore.   Visual Relation Grounding in Videos   15", "conf": "ECCV", "year": "2020", "index": 46}, {"title": "Co-Grounding Networks with Semantic Attention for Referring Expression   Comprehension in Videos   Sijie Song1, Xudong Lin2, Jiaying Liu1, Zongming Guo1\u2217, Shih-Fu Chang2   1 Wangxuan Institute of Computer Technology, Peking University, Beijing, China   2 DVMM Lab, Columbia University, New York, NY, USA", "abstract": "In this paper, we address the problem of referring expression comprehension in videos, which is challenging due to   complex expression and scene dynamics. Unlike previous   methods which solve the problem in multiple stages (i.e.,   tracking, proposal-based matching), we tackle the problem   from a novel perspective, co-grounding, with an elegant   one-stage framework. We enhance the single-frame grounding accuracy by semantic attention learning and improve   the cross-frame grounding consistency with co-grounding   feature learning.   Semantic attention learning explicitly   parses referring cues in different attributes to reduce the   ambiguity in the complex expression. Co-grounding feature   learning boosts visual feature representations by integrating temporal correlation to reduce the ambiguity caused by   scene dynamics. Experiment results demonstrate the superiority of our framework on the video grounding datasets VID   and LiOTB in generating accurate and stable results across   frames. Our model is also applicable to referring expression comprehension in images, illustrated by the improved   performance on the RefCOCO dataset.   Our project is   available at https://sijiesong.github.io/cogrounding.   1.", "content": "Referring expression comprehension has attracted much   attention recently. It aims to localize a region of the image/video described by the natural language.   This topic is of great importance in computer vision to support a   variety of research problems such as image/video captioning [2, 27], visual question answering [3] and image/video   retrieval [31, 9]. It also plays a key role in machine intelligence for a wide range of applications from humancomputer interaction, robotics to early education.   In the past years, most of the previous work for referring   expression comprehension focus on the grounding for static   \u2217Corresponding author.   Figure 1. Referring expression comprehension in videos. Due to   dynamic scenes and ambiguity in the expression, per-frame inference (in blue) with state-of-the-art grounding method [36] would   lead to unstable results across frames, while our co-grounding   networks achieve accurate and consistent predictions (in red).   Ground-truth annotations are denoted in green.   images [32, 34, 38, 18, 36, 23, 24, 12, 1] and have achieved   promising results. However, referring expression comprehension for videos is less explored, which is challenging   yet important. Different from several threads of referring   expression comprehension in videos, such as referring all mentioned entities [42], we localize the spatio-temporal   tube that semantically corresponds to the whole sentence.   That is, we output bounding box for each frame as shown   in Figure 1.   The work in [17] treats referring expression comprehension for videos as a tracking problem.   We argue that it   would suffer from template selection error, because it is   hard to tell from the grounding results from multiple frames   which is the right one to track. The other work [6] \ufb01rst   proposes spatio-temporal tube candidates and then matches   them with textual features from expression. However, the   performance is limited by the proposal quality. Inspired by   the advance in one-stage image grounding methods [36] that   get rid of proposal detectors, another solution is to conduct   per-frame inference with [36], but there are still two problems. Firstly, the entities in the expression (such as \u2018boat\u2019,   \u2018men\u2019, \u2018whale\u2019, \u2018sea\u2019 in Figure 1) would cause ambiguity   1346   when encoded into textual features, making the model confused about which entity is the correct one to ground. Secondly, the dynamic scenes across frames would also interrupt the grounding process (see the drifting blue bounding   boxes in Figure 1). Therefore, the key challenge is to generate robust textual and visual features to reduce ambiguity, then further achieve accurate and stable results across   frames.   To tackle the aforementioned issues, we propose to   solve referring expression comprehension in videos with a   new perspective, i.e., co-grounding, with semantic attention learning in an elegant one-stage framework. The basic structure of our model is based on YOLO [28], which   predicts the bounding box and con\ufb01dence simultaneously.   The con\ufb01dence re\ufb02ects the matching score between the textual and visual features. We design a semantic attention   mechanism to obtain attribute-speci\ufb01c features both for vision and language. Speci\ufb01cally, a proposal-free subject attention scheme is proposed to parse the words for subject from the expression. An object-aware location attention   scheme is developed to parse the words for location from   the expression. The interaction between attribute-speci\ufb01c   textual and visual features determines the subject score and   location score for each visual region (see the visualization   examples in Figure 6). Besides, to improve cross-frame   prediction consistency, we develop the co-grounding feature learning. Taking multiple frames as input, it utilizes   the correlation across frames to enhance visual features and   stabilize the grounding process in training and testing. A   post-processing strategy is further employed to improve the   temporal consistency during inference.   Our contributions are summarized as follows:   \u2022 We propose to solve referring expression comprehension in videos by co-grounding in an one-stage framework.   \u2022 We propose semantic attention learning to parse referring cues, including a proposal-free subject attention and   object-aware location attention.   \u2022 Our networks are applicable to both video/image   grounding, and achieve state-of-the-art performance on referring expression comprehension benchmarks.   2. Related Work   2.1. Referring expression comprehension   Bene\ufb01ting from the advances in object detection [29, 28,   10], most methods [32, 34, 38] perform referring expression comprehension in two stages. Region candidates are   proposed in the \ufb01rst stage with an object detector and then   matched with the expression in the second stage. The best   matching region is selected as the grounding result. However, these two-stage methods are limited by the proposal quality of the of\ufb02ine object detector.   The missing of   ground-truth regions in the \ufb01rst stage would lead to the failure of the second stage. To address the issue, more recent   works are proposed to get rid of the of\ufb02ine object detector [18, 36]. Built upon the current one-stage object detection method, i.e., YOLO-v3 [28], Yang et al. [36] \ufb01rst   proposed an one-stage visual grounding framework, which   extracts visual-text features and predict bounding boxes   densely at all spatial locations. Liao et al. [18] reformulate referring expression comprehension as correlation \ufb01ltering, where the \ufb01lter template is generated from language   features. Besides referring expression comprehension for   images, there are a few works [6, 17] exploring the task in   the video domain. However, both of the methods solve referring expression comprehension for videos with multiple   stages (i.e., tracking, proposal-based matching). The failure   in the \ufb01rst stage would directly impact the \ufb01nal results. In   our work, we propose an elegant one-stage framework for   this task.   2.2. Attention mechanisms   Inspired by human perception, attention mechanisms have been widely studied in vision-and-language tasks,   e.g., visual question answering [41, 20, 14, 25], visual dialogue [33] and visual grounding [38].   In these works,   attention is applied to learn the underlying correlation between different modalities [41, 20], which jointly performs language-guided visual attention and vision-guided language attention. Nguyen et al. [25] propose a dense symmetric co-attention to deal with every interaction between   any pair of visual region and each word. To further fully understand the semantic of vision and language, a more   recent work [14] presents a hypergraph model to de\ufb01ne a   common semantic space among different modalities. Attention mechanisms are also popular in the task of visual   grounding to decompose the language into several components [38]. Each component focuses different attributes of   the language and then trigger corresponding visual comprehension. However, the modular network designed in [38] is   based on of\ufb02ine object proposals and requires external annotations. Our work, however, achieves semantic attention   learning without the reliance of object proposals and external labels.   2.3. Temporal consistency   Generating consistent results across adjacent frames is   essential for video applications.The recent works mainly focus on improving performance of per-frame result by exploiting information in the temporal domain [8, 43, 5, 16,   21, 11, 26, 4]. Some of the works aggregate local temporal context to help the inference of current frame. Optical   \ufb02ow is always computed by [7] to propagate features across   frames [43, 4], while some methods integrate temporal context by calculating af\ufb01nity matrix [8, 21] to build temporal   correspondence. Nevertheless, only focusing the locality   1347   Figure 2. Our co-grounding networks with semantic attention for referring expression comprehension. The details for semantic attention   learning and co-grounding feature learnng are presented in Figure 3 and Figure 4, respectively.   may lead to the lack of long-term information. To aggregate   information beyond a small local range, [5] introduces a   global-local aggregation network, taking full consideration   of both global and local information. In addition, [16, 26]   share a similar idea which leverages the merits of recurrent   networks to make use of neighboring results. In our work,   we design a co-grounding module to leverage correlation   across frames, and further stabilize the prediction results   with a post-processing strategy.   3. The Proposed Method   Given an expression Q with N words and a video I with   T frames, our goal is to localize the object region {bt}T   t=1 in   each frame described by Q, where bt represents a bounding   box in the t-th frame.   We build our baseline model following [36], which is   based on the one-stage object detection framework, i.e., YOLOv3 [28]. We conduct bounding box prediction based on   cross-modal features, which are obtained by fusing visual   and textual features. For each spatial position of the crossmodal features, the model outputs bounding box predictions   centered at the current spatial position, with a con\ufb01dence to   indicate the probability of being the \ufb01nal grounding output.   The bounding box with the highest con\ufb01dence is selected as   the \ufb01nal prediction. The basic objective function for training the model consists of the MSE (mean square error) loss   Lreg to regress the bounding box towards the ground-truth,   and a cross-entropy loss Lcls to select the right prediction   from all the bounding boxes. We refer readers to [36, 28]   for more details. Next, we elaborate the semantic attention learning and co-grounding feature learning introduced   to the framework.   3.1. Semantic attention learning   To reduce ambiguity from the expression, we propose   semantic attention learning to parse referring cues from the   input expression. Though the input expression is usually   complex, it is noticed that the words indicating subject and   location play a key role to distinguish the target. Thus, we   aim to decompose the expression into subject and location.   With more attribute-speci\ufb01c textual features, we build the   mapping between language and vision. Note that our semantic attention learning is different from [38] since our   networks are end-to-end trainable without the reliance of   of\ufb02ine proposal detection and external label annotations.   As shown in Figure 2, the expression Q is encoded with   a text encoder consisting of bi-directional LSTM. The representation for the n-th word is the concatenation of the hidden states from both directions:   hn = [\u2212\u2192   h n, \u2190\u2212   h n] = BiLSTM(en, \u2212\u2192   h n\u22121, \u2190\u2212   h n+1),   (1)   where en is the embedding of the n-th word. The attributespeci\ufb01c textual features qm (m \u2208{sub., loc.}) are parsed   by fusing {hn}N   n=1 with learnable weights wm \u2208RN:   \u03b1m   n =   exp (wm   n hn)   PN   i=1 exp (wm   i hi)   ,   (2)   qm =   N   X   n=1   \u03b1m   n en.   (3)   \u2022 Proposal-free subject attention. In this part, our networks learn wsub to parse subject from Q and generate subject attention map St for each frame in a proposal-free manner. The subject attention map St \u2208RH\u00d7W re\ufb02ects visual   feature response to qsub by computing cross-modal similarity as shown in Figure 3:   St(x) = \u03b4(Vt(x), qsub) = qsub[Vt(x)]T,   (4)   where Vt(x) \u2208RD is the feature vector at the position   x from visual feature map Vt \u2208RH\u00d7W \u00d7D of the t-th   frame. Rank loss is exploited to train the network, where   1348   Figure 3. Details for semantic attention learning.   the score of the matched visual and textual features, i.e.,   (Vt(x\u2217), qsub), should be higher than unmatched ones, i.e.,   (Vt(x\u2217), q\u2032   sub) and (Vt(x\u2032), qsub). Therefore, the objective   function is:   Lrank = max (0, \u2206+ \u03b4 (Vt(x\u2032), qsub) \u2212\u03b4 (Vt(x\u2217), qsub))   + max (0, \u2206+ \u03b4 (Vt(x\u2217), q\u2032   sub) \u2212\u03b4 (Vt(x\u2217), qsub)) ,   (5)   where \u2206is a margin and set to 0.5 in our experiments. During training, the visual feature vector corresponding to qsub   can be localized with the ground-truth annotation. The key   issue in the training is how to select negative visual feature   vectors without having object proposals. Though different schemes of hard negative sample mining have been explored, we found it is enough to tackle the problem by random sampling visual features from other training samples   within the same training batch.   \u2022 Object-aware location attention. In this part, our networks learn wloc to parse location from the expression, and   then generate location map Lt \u2208RH\u00d7W to qloc. The key   challenge in learning location attention is how to match coordinates with textual features because location is a relative   concept. When we say something is \u2018on the left\u2019, we have   to give a reference. Thanks to the aforementioned subject   attention map S which roughly identi\ufb01es the subject region,   we design an object-aware location representation. Speci\ufb01cally, we follow [36] to initially encode the coordinate feature as Ut \u2208RH\u00d7W \u00d7D\u2032. A 2D matrix Aloc   t   \u2208RHW \u00d7HW   is computed to model the relation between any two positions x and y:   Aloc   t (x, y) = [Ut(x)]TUt(y).   (6)   With the subject attention map St, we inject the reference   information into the location features as Aloc   t   \u2297Flatten(St),   followed by an FC layer to shape it into HW \u00d7 D. Then   the matrix is reshaped to H \u00d7 W \u00d7 D as the \ufb01nal location   features \u02dcUt. A detailed illustration is shown in Figure 3.   Similar to Eq. 4, we obtain the location response   for position x by computing cosine similarity Lt(x) =   \u03b4( \u02dcUt(x), qloc). We train the location attention with crossentropy loss:   Lce = \u2736loc log   exp(Lt(x))   P   y exp(Lt(y)),   (7)   where \u2736loc \u2208{0, 1}H\u00d7W indicates the ground-truth location.   Figure 4. Details for co-grounding feature learning.   With the subject and location attention maps, the con\ufb01dence map for the t-th frame is generated as Ct = Ot\u2297St\u2297   Lt. Recall that we generate a bounding box prediction for   each position x, Ot(x) indicates how likely the predicted   bounding box contains an object.   3.2. Co-grounding feature learning   For now, we have introduced how to parse referring cues   from expression and build correspondence between text and   visual features. However, the temporal dynamics in videos   would lead to unstable visual feature representations, which   may do harm to the cross-modal matching in both training   and testing. To enhance the visual feature representation   for a more robust learning, we propose co-grounding to integrate the temporal context by utilizing correlation across   frames.   As shown in Figure 2, considering two frames   from the same video, we obtain the initial visual features   Fta \u2208RH\u00d7W \u00d7D and Ftb \u2208RH\u00d7W \u00d7D with a visual encoder. The correlation across the adjacent frames can be   described by a normalized af\ufb01nity matrix M \u2208RHW \u00d7HW ,   providing the measure for similarity of spatial features:   M(x, y) =   exp   \u0000[Fta(x)]TFtb(y)   \u0001   P   y exp ([Fta(x)]TFtb(y)).   (8)   Then we integrate the feature vectors from Ftb with M,   \u02dcFtb(x) =   X   y   M(x, y)Ftb(y).   (9)   The \ufb01nal enhance feature Vta is obtained by:   Vta = Conv(Fta \u2295\u02dcFtb),   (10)   where \u2295indicate concatenation along the channel dimension, and Conv(\u00b7) denotes an 1 \u00d7 1 convolution operation.   The visual feature Vtb can be enhanced in the same way.   Figure 4 shows the details of co-grounding feature learning.   3.3. Post processing   To further stabilize the bounding box prediction for each   frame, we design a post-processing scheme based on the   initial prediction results during inference. Suppose for the   video, we have the initial top K bounding box predictions for each frame {{bi   t, ci   t}K   i=1}T   t=1, where bi   t denotes the   location for the bounding box with the i-th highest con\ufb01dence ci   t for the t-th frame. The visual feature vectors corresponding to the bounding box location for the t-th frame   1349   is Vt = V1   t (b1   t) \u2295... \u2295VK   t (bK   t ) and Vt \u2208RD\u00d7K. Now we   consider referring the neighboring P frames as a window to   stabilize the center frame t\u2217. For the i-th bounding box of   the center frame, we stabilize its con\ufb01dence score by seeking the most similar bounding box in each reference frame.   The similarity is measured by the af\ufb01nity matrix:   Z(t\u2217,t) = VT   t\u2217Vt, t = {t\u2217\u2212\u2206t, ..., t\u2217+ \u2206t},   (11)   where \u2206t = \u230aP/2\u230b, and Z(t\u2217,t) \u2208RK\u00d7K. For each bounding box in {bi   t\u2217}K   i=1, we select the most similar bounding   box from all the reference frames,   pt = [p(1)   t , ..., p(K)   t   ],   (12)   where   p(i)   t   = arg max Z(i)   (t\u2217,t),   (13)   and Z(i)   (t\u2217,t) denotes the i-th row of the matrix. The \ufb01nal   scores for the initial top K bounding boxes are:   \u02dcCt\u2217= 1   N   t\u2217+\u2206t   X   t=t\u2217\u2212\u2206t   \u2736pt \u2217Ct,   (14)   where \u02dcCt\u2217\u2208RK, Ct \u2208RK and Ct = [c1   t, ..., cK   t ]. \u2736pt \u2208   [0, 1]K can be regard as a binary mask to choose the highest   score from Ct. The bounding box with the highest score in   Ct\u2217is treated as the \ufb01nal prediction result. N is a normalization factor.   4. Experiments   In this section, we introduce the datasets and implementation details, then report our evaluation on referring expression comprehension benchmarks. We \ufb01rst show the comparisons to other state-of-the-art methods, then give the ablation study and comprehensive analysis to illustrate the effectiveness of each component. Finally we discuss the failure cases and future work.   4.1. Datasets   To evaluate our model, we conduct experiments on two dynamic video datasets (i.e., VID-Sentence [6], Lingual   OTB99 [17]) and one static image dataset (i.e., RefCOCO).   VID-Sentence (VID) [6]. This dataset consists of 7,654   trimmed videos with language descriptions, and provides   the sequences of spatio-temporal bounding box annotations   for each query. Following [6], the dataset is splited into   6,582/536/536 instances for training/validation/testing.   Lingual OTB99 (LiOTB) [17]. The LiOTB dataset origins from the well-known OTB100 object tracking dataset   in [22]. The videos in [22] are augmented with natural language descriptions of the target object. We adopt the same   protocol as [17] that 51 videos are for training and the rest   are for testing.   RefCOCO [39].   The RefCOCO dataset is collected   from 19,994 images in MSCOCO [19] and 142,210 natural language descriptions. RefCOCO is splited into four   subsets, including train, validation, test A and test B. The   images in test A are with multiple people, while those in   test B are with multiple objects.   4.2. Implementation details   Training settings. Our visual encoder is based on Darknet53 [28] pretrained on MSCOCO [19]. We adopt multi-level   schemes in the grounding process, that we predict bounding boxes on three levels of feature maps, the resolution of   which are 8 \u00d7 8, 16 \u00d7 16 and 32 \u00d7 32. The input images   are resized the long edges to 256 and then padded into the   size of 256 \u00d7 256. Following [28, 36], we adopt the data   augmentation including adding randomization to the color   space, horizontal \ufb02ip, and random af\ufb01ne transformations.   The network is optimized with RMSProp [30] for 100 epochs, the initial learning rate of which is set as 10\u22124 and   decayed under a polynomial schedule. The batch sizes for   VID, LiOTB and RefCOCO are 32, 8, 32, respectively. We   set the weights for Lrank and Lce as 100, 1, respectively.   By default, the top 5 bounding boxes from the neighboring 5 frames are considered in the post-processing for the   VID and LiOTB datasets during inference (i.e., K = 5,   P = 5). Note for the image dataset RefCOCO, we omit the   co-grounding feature learning.   Evaluation metrics. We adopt different metrics to give a   fair and comprehensive evaluation of our framework. Acc@0.5 is widely used to evaluate the grounding results [36, 18], where a predicted bounding box is considered   correct if the IoU with the ground truth region is above 0.5.   Following [37], success and precision scores are reported to   evaluate the performance for videos. The success score is   actually the AUC (area under curve) metric, while the precision score measures the ratio of frames where the predicted   bounding box falls within a threshold of 20 pixels around   the ground-truth. Besides, mIoU is also reported to show   the quality of bounding boxes.   4.3. Comparison to the state-of-the-art   Table 1 shows the referring expression comprehension   results on the video datasets VID and LiOTB, respectively.   We \ufb01rst present the per-frame inference results by the stateof-the-art referring expression comprehension method [36]   in the \ufb01rst two rows. From the 3rd to the 6th rows, we   evaluate the results to see the performance when the problem is solved by per-frame tracking. Speci\ufb01cally, we adopt   the state-of-the art tracker [15] to track the given template.   With the grounding results from One-Stage LSTM [36], we   conduct experiments with the \ufb01rst, middle, last and random   frame as the tracking template, respectively. It is found that   treating referring expression grounding in videos as track1350   Table 1. Referring expression comprehension results on dynamic video datasets VID and LiOTB, respectively.   VID   LiOTB   Accu.@0.5   Success   Precision   Accu.@0.5   Success   Precision   One-Stage BERT [36]   52.39   0.427   0.373   49.13   0.358   0.468   One-Stage LSTM [36]   54.78   0.451   0.393   49.16   0.333   0.414   First-Frame Tracking   36.97   0.334   0.250   50.93   0.391   0.482   Middle-Frame Tracking   44.00   0.384   0.307   43.08   0.356   0.421   Last-Frame Tracking   36.26   0.328   0.239   44.17   0.327   0.391   Random-Frame Tracking   40.20   0.356   0.278   25.16   0.288   0.329   WSSTG [6]   38.20   LSAN [17]   0.259   Ours   60.25   0.495   0.462   52.26   0.392   0.500   Table 2. Referring expression comprehension results (Acc.@0.5)   on the static image dataset RefCOCO.   RefCOCO   Visual encoder   val   testA   testB   MMI [23]   VGG16   64.90   54.51   Neg Bag [24]   VGG16   58.60   56.40   CMN [12]   VGG16   71.03   65.77   SLR [40]   ResNet-101   69.48   73.71   64.96   DGA [35]   ResNet-101   78.42   65.53   MAttN [38]   ResNet-101   76.40   80.43   69.28   CMCF [18]   DLA-34   81.06   71.85   One-Stage BERT [36]   Darknet53   72.05   74.81   67.59   One-Stage LSTM [36]   Darknet53   73.69   75.78   71.32   Baseline   Darknet53   73.72   76.24   71.19   S-Att.   Darknet53   77.42   81.17   72.77   SL-Att.(Ours)   Darknet53   77.65   80.75   73.37   ing leads to poor results compared to per-frame grounding   for almost all the cases. There are mainly two reasons to explain the poor results. On the one hand, we can not guarantee the correct template is selected during tracking. On the   other hand, even given the correct template, the tracker may   fail due to its limitation. The analysis also applies to the unsatisfactory results of LSAN [17]. Besides, we present the   results of WSSTG [6] which relies on spatio-temporal proposal detection. Our results are shown in the last row. It can   be seen our framework outperforms the compared methods   by a large margin.   Table 2 shows the comparisons on the RefCOCO dataset   for referring expression comprehension. Our overall results on RefCOCO are shown in the last row. To give a fair   comparison, we present the backbone structure of each visual encoder. Though MAttN [38] also explores the idea   of modular attention for both language and vision, it requires an of\ufb02ine object detector and external attribute labels. However, our model can learn the semantic attention in   a proposal-free manner, which also makes our results outstanding compared to other one-stage models [18, 36].   Table 3. Referring expression comprehension results for ablation   study on dynamic video datasets VID and LiOTB, respectively.   VID   LiOTB   Acc.@0.5   mIoU   Acc.@0.5   mIoU   w/o co-grounding   Parser   53.19   0.450   49.16   0.397   Baseline   54.41   0.448   49.11   0.405   S-Att.   58.03   0.488   49.66   0.411   SL-Att.   59.22   0.490   50.51   0.418   w/ co-grounding   CG-Baseline   55.88   0.477   50.56   0.405   CG-S-Att.   58.74   0.497   51.67   0.412   CG-SL-Att.   59.48   0.494   50.92   0.418   CG-SL-Att. + pp.   60.25   0.498   52.26   0.418   4.4. Ablation study   To show the effectiveness of each component in our   model, ablation study is conducted on VID, LiOTB and RefCOCO datasets, respectively. We explore different settings   to give a comprehensive analysis. The results are presented   in Table 3 and Table 2. Note that we regard the model of   one-stage LSTM in [36] as our Baseline.   \u2022 Semantic attention learning.   We \ufb01rst explore the   contribution of semantic attention learning without taking   co-grounding feature learning into account.   For all the   datasets, subject attention (S-Att.) brings signi\ufb01cant improvement compared to baseline results. It is largely because the subject attention reduces ambiguity in grounding   process when there are multiple entities in the expression   and images. Location attention (SL-Att.) further improves   the grounding accuracy. Overall, for the VID and LiOTB datasets, the gains from semantic attention learning over   baselines are 4.81% and 1.40%, respectively. For the RefCOCO dataset, our framework outperforms the baseline by   3.93%, 4.93%, and 2.18% under different split settings, respectively. Moreover, we compare our automatic semantic   attention learning with manually semantic parser [13] in Table 3 (see Parser). As analyzed in [38], parsing errors exist for the external parser which is not tuned for referring   1351   Figure 5. Visualization results of video grounding on the VID dataset. We show the ground-truths, baseline results, results of SL-Att. and   results of CG-SL-Att. in the green, blue, orange and red bounding boxes, respectively. The language queries are shown in the sub-captions.   Table 4. Ablation study for post-processing on the VID dataset in   terms of Acc.@0.5 and mIoU.   P=1   P=3   P=5   P=7   P=9   Acc.@0.5   59.48   59.50   60.25   60.16   59.63   mIoU   0.494   0.495   0.500   0.498   0.495   expressions. Therefore, we did not observe improvement   compared to baseline results.   \u2022 Co-grounding feature learning.   We further analyze the contributions of our co-grounding feature learning   through Table 3. Conducting co-grounding feature learning with the baseline structure (CG-Baseline) brings 1.47%   and 1.45% gains in terms of Acc.@0.5 over Baseline on   the VID and LiOTB datasets, respectively. With semantic   attention learning, the co-grounding feature learning helps   further improve the performance on both datasets for each   setting (see CG-S-Att. vs. S-Att., CG-SL-Att. vs. SLAtt.).   \u2022 Post processing. Finally we illustrate the effectiveness of the post processing scheme. The results in the last   row of Table 3 (CG-SL-Att. + pp.) show that our postprocessing scheme is able to further improve the grounding   results in terms of Acc.@0.5 and mIoU. In Table 4, we further explore the in\ufb02uence of different numbers of reference   frames. While the post-processing scheme consistently improve the results compared to those without post-processing   (i.e., P = 1), we set P as 5 in all our experiments for the   tradeoff between ef\ufb01ciency and accuracy.   4.5. Qualitative results   \u2022 Overall grounding results. We choose several videos   from the VID dataset and visualize the grounding results in   Figure 5. We compare the results of the baseline, SL-Att.   and CG-SL-Att. with the blue, orange and red bounding   boxes, respectively. The ground-truths are in green. From   the visualization, it is observed that compared to baseline   results, SL-Att. provides more accurate prediction in most   cases, due to the explicitly parsed referring cues both for   language and vision. However, bounding box drifting is   a problem when we conduct per-frame inference without   taking the temporal context into account (see the drifting   orange bounding boxes). In Figure 5(b), the target bicycle is small and obscure in the \ufb01rst several frames, making the visual features vulnerable for SL-Att. model. In   Figure 5(c), the left stone mislead SL-Att. to ground it as   \u2018antelope\u2019. With co-grounding feature learning, the visual   features are enhanced by integrating temporal context and   become more robust. Therefore, we obtain consistent results across frames (see the red boxes). Please refer to the   supplementary for more grounding results.   \u2022 Visualization on attention. We show the learned attention patterns for language and vision in Figure 6. For   each side, different queries are given for the same frame. It   is found that the semantic attention for the given expression   successfully parses the words for subject entities and location from the language. And the semantic attention for the   visual feature maps generate corresponding response for different attributes. In Figure 6(a) and (b), we show examples   that location attention helps the model handle ambiguities in   the frames and distinguish the correct bounding box. With   the parsed subject \u2018monkey stands branch\u2019 and \u2018monkey staying\u2019, the model pays more attention on both monkeys in   the frame. However, with the guidance of the parsed words   describing location \u2018right stands branch tree\u2019 and \u2018left staying\u2019, the model shows different response on the location   attention maps, providing essential cues to distinguish the   correct bounding box. For the examples on the right side,   there is not dominant location information in the input expressions, resulting in similar location maps in Figure 6(c)   1352   Figure 6. Visualization on attention patterns of our framework. The subject and location attention patterns for language are shown above   the images. We mark the attention values for each word. The attention patterns for visual features are shown as the heat maps overlayed   on the original images. The more reddish, the larger attention. Besides, we show the grounding results of baseline and SL-Att. in the blue   and red bounding boxes, respectively. Ground-truths are shown in green. (Best viewed in color.)   and (d). The multiple entities such as \u2018whale\u2019, \u2018boat\u2019 appearing in the sentences make the baseline model confusing   that which subject is correct to ground (see the blue bounding boxes in Figure 6(c)(d)). In our model, the subject attention for language effectively excludes the effect of other   entities in the expression, making it clear to ground \u2018boat\u2019   in Figure 6(c) and \u2018body\u2019 in Figure 6(d). It further leads to   corresponding high response on the subject attention maps   for visual features and then satisfactory grounding predictions (see the red boxes). More visualizations can be found   in our supplementary.   \u2022 Failure case analysis and future work. We show   some typical failure cases in Figure 7, to illustrate the limitations our model, and the challenges for the topic of video   grounding.   (1) Multi-order reasoning is challenging for   one-stage referring expression comprehension because it always involves multiple entities and relation concepts. As   shown in Figure. 7(a), it is dif\ufb01cult for our model to locate   the airplanes with red smoke and then select the top one.   (2) Motion information is not explicitly explored and utilized as grounding cues. As shown in Figure 7(b), we can   not determine which \u2018zebra\u2019 is \u2018moving\u2019 only by observing   static frames. (3) The language query may not apply to all the frames. In Figure 7(c), the ground-truth is not in the   \u2018middle\u2019 in some frames. How to further tackle the ambiguity caused by dynamic scenes and expression is still worth   exploring. We leave how to solve these failure cases as interesting future works.   5. Conclusion   In this paper, we tackle the problem of referring expression comprehension in videos.   We propose to solve the   problem from a new perspective, co-grounding, with an elegant one-stage framework. To boost single frame results,   Figure 7. Failure cases. Ground-truths are in green and our results   are in red.   our model learns semantic attention to decompose grounding cues into different attributes, which further contribute   to the reasoning of the target described by the input expression. To boost grounding prediction consistency, we propose co-grounding feature learning by integrating neighboring features across frames to enhance visual feature representations. A post-processing scheme is conducted during   inference to further stabilize the predictions. Our model is   applicable to visual grounding both for videos and images.   Experiments on video and image grounding benchmarks illustrate the effectiveness of our model.   Acknowledgements.   This work was supported by the Beijing Natural Science Foundation under Contract No.4192025, the   National Natural Science Foundation of China under Contract No.61772043. This is a research achievement of Key Laboratory   of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). This work was   done in part when Sijie Song was a visiting scholar at Columbia   University, supported by China Scholarship Council.   1353", "conf": "CVPR", "year": "2021", "index": 106}, {"title": "Panoptic Narrative Grounding   Cristina Gonz\u00b4alez1   Nicol\u00b4as Ayobi1   Isabela Hern\u00b4andez1   Jos\u00b4e Hern\u00b4andez1   Jordi Pont-Tuset2   Pablo Arbel\u00b4aez1   1Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, Colombia   2Google Research, Switzerland", "abstract": "This paper proposes Panoptic Narrative Grounding, a   spatially fine and general formulation of the natural language visual grounding problem.   We establish an experimental framework for the study of this new task, including new ground truth and metrics, and we propose a   strong baseline method to serve as stepping stone for future work. We exploit the intrinsic semantic richness in an   image by including panoptic categories, and we approach   visual grounding at a fine-grained level by using segmentations.   In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of   the MS COCO dataset. To guarantee the quality of our annotations, we take advantage of the semantic structure contained in WordNet to exclusively incorporate noun phrases   that are grounded to a meaningfully related panoptic segmentation region. The proposed baseline achieves a performance of 55.4 absolute Average Recall points. This result   is a suitable foundation to push the envelope further in the   development of methods for Panoptic Narrative Grounding.   1.", "content": "Vision and language skills play a key role in humans\u2019 understanding of the world and they are rarely used independently. Their interaction is crucial in achieving high-level   tasks such as describing objects, narrating a visual scene,   or answering questions based on visual cues.   Inspired   by these capabilities of human intelligence, researchers   have formulated tasks at the intersection of computer vision and natural language processing, such as image captioning [5, 18, 25, 40], referring expression comprehension   and segmentation [15, 19, 31, 32], visual question answering [2], among many others.   Current experimental frameworks approach vision and   language tasks at different granularity levels. Image capFigure 1: Panoptic Narrative Grounding. Given an input   image (left) with an associated caption (right), our goal is   to produce a panoptic segmentation that grounds its visual   objects densely (left).   tioning is among the coarsest, aiming at pairing an image with a textual description of its content. With increasing granularity, there are frameworks that intend to assign   specific regions in the image to short descriptions, such   as referring expression comprehension and segmentation,   and region descriptions in the Visual Genome [19] dataset.   The finest approaches tackle visual grounding at word level,   with bounding boxes in the image linked to noun phrases in   the caption, as in the Flickr30k Entities [49, 35] dataset.   Given the general dichotomy in computer vision tasks   between things (countable objects) and stuff (amorphous   regions of similar texture), these datasets mainly focus on   things categories. However, several works [8, 1, 10, 17]   have highlighted the importance of jointly considering both   things and stuff classes towards real-world applications.   Kirillov et al. [17] defined the panoptic segmentation task   as the unified formulation of semantic segmentation, which   recognizes stuff, and instance segmentation, which detects   and segments things.   Recently, Pont-Tuset et al. [36] proposed Localized Narratives, a multimodal dataset for visual grounding based on   the natural human task of describing images while simultaneously pointing with the mouse at the regions being described. Their grounding annotations have the densest sampling of current datasets, in that they visually ground every   word in the caption with mouse trace segments. Moreover,   1364   this dataset naturally contains panoptic descriptions of the   content of each image, including stuff regions.   Although some of the experimental frameworks above   contain sufficiently dense annotations over language, visual   annotations are still very sparse and rough. This paper proposes Panoptic Narrative Grounding (Figure 1), a spatially   finer and more general task formulation of the natural language visual grounding problem in which (i) we propose a   spatially detailed visual grounding that uses segmentations   instead of bounding boxes, and (ii) we include all panoptic   categories to fully exploit the intrinsic semantic richness in   a visual scene. We establish an experimental framework for   the study of this task, including new ground-truth annotations and metrics, and we propose a strong baseline to serve   as stepping stone for future work.   Considering that collecting pixel-wise annotations has a   significant cost, we design a method to transfer the Localized Narrative annotations to regions into the panoptic segmentation annotations provided by the MS COCO [5, 25]   dataset. We select a specific region for each noun phrase   considering the trace segment associated to it. Since humans use different styles to pointing objects (circling, scribbling, and underlining) we evaluate the quality of the annotation by determining if noun phrases are grounded to a   meaningfully related panoptic segmentation region. To do   so, we leverage the underlying semantic information in the   WordNet ontology [34]. To handle trace segments being not   fully synchronized with the object description, we consider   all the regions in the image ranked by relative distance to the   visual grounding and assign the closest region with a strong   meaning relationship. Finally, for plural noun phrases, we   select all regions that (i) are within the tightest bounding   box around the mouse trace and (ii) are from the same category as the main selected region. In terms of evaluation,   in contrast to the traditional metric use in phrase-grounding   task, we introduce a stricter metric that calculates recall at   different Intersection over Union (IoU) thresholds between   the target and the predicted mask. Thus, measuring both   recognition accuracy and segmentation quality.   We propose a strong baseline that builds upon a state-ofthe-art method, Cross-Modality Relevance [52] (CMR), developed for reasoning tasks on language and vision. Specifically, this method is designed to perform two classification   tasks: Visual Questions Answering [2] and Natural Language for Visual Reasoning [43, 44]. We generalize the   model to perform Panoptic Narrative Grounding.   Thus,   our baseline is the first natural language visual grounding   method able to align multiple noun phrases with panoptic   region segmentations within the image.   Our main contributions can be summarized as follows:   (1) We propose Panoptic Narrative Grounding, a new formulation of the natural language visual grounding problem which, by using panoptic segmentation regions as   visual grounding, is spatially denser and more general   in semantic terms.   (2) We establish an experimental framework for the study   of this problem, with annotations coming from the   transfer of Localized Narrative annotations to panoptic   segmentations in the MS COCO dataset.   (3) We introduce the first visual grounding method that   matches segmentation regions to specific noun phrases   in the caption, which serves as a strong baseline for the   task of Panoptic Narrative Grounding.   To ensure the reproducibility of our results and to promote further research on Panoptic Narrative Grounding, we   make all the resources of this paper publicly available in our   project web page1: our benchmark dataset annotations for   the train and validation splits in MS COCO, an implementation of the evaluation metrics, and the pretrained models   and source code for our baseline.   2. Related Work   2.1. Vision and Language Datasets   The first datasets at the intersection of vision and language match images to descriptions without any form of visual grounding [5, 18, 25, 40]. MS COCO Captions [5, 25]   comprises 123,287 images for training, validation, and testing; with five human-annotated captions for each one. Several works, however, have suggested that models developed   for this task do not base their decisions on the visual information contained in the image, but rather on easier-to-learn   language priors [39].   Referring Expressions Comprehension (REC) and Referring Expression Segmentation (RES) are tasks intended   to detect and segment, respectively, a target object instance described by a natural language expression. There   exist three datasets for these tasks, built on top of the   MS COCO dataset: RefCOCO [15], RefCOCO+ [15], and   RefCOCOg [32]. RefCOCO+\u2019s descriptions, in contrast to   those on RefCOCO, focus on appearance attributes rather   than locations of the referent object. RefCOCOg was collected in a non-interactive setup and contains longer descriptions including both appearance and location attributes   of the target instances. These datasets, while having pixellevel annotations, do not ground language at the word-level,   and their annotations\u2019 semantics boil down mostly to salient   objects in images.   Visual Genome [19] is a general-purpose dataset meant   to connect vision and language while not being restricted to   a specific task. It contains 108,077 images with 50 region   descriptions and 35 objects per image on average. Each   short description is localized within the image with a bounding box, making it possible to study visual grounding at the   phrase level. There is no an explicit relationship between   1https://github.com/BCV-Uniandes/PNG   1365   the objects localized within an image and the visual entities included in the textual descriptions. Having multiple   regions descriptions allows to have a more complete understanding of a scene, covering things and stuff categories.   Flickr30k Entities [35] extends the original Flickr30k   dataset [49] with manually annotated bounding boxes   grounding each noun phrase in the caption. Despite its granularity over language, this dataset does not provide pixelwise annotations and focus predominantly on things.   Localized Narratives [36] is a large-scale dataset   that provides annotations for the whole MS COCO,   Flickr30k [50], and ADE20K [53] datasets, and 671K images from Open Images [20]. Its annotations consist of synchronized voice recordings, text transcriptions, and mouse   traces made by human annotators. They describe the image\u2019s contents and the mouse traces visually ground each   word in the narrative, which makes them the densest annotations over language available to date. Grounding goes   beyond nouns and includes visual relationship indicators,   verbs, etc. Annotators were asked to make descriptions that   comprise as much content of the image as possible, which   generally entails a high semantic coverage of the scene. The   visual grounding using mouse trace segments, however, is   spatially very coarse.   Table 1 summarizes the characteristics of existing   datasets at the intersection of vision and language and compares them in terms of: (i) language granularity, (ii) visual granularity, and (iii) semantic generality. Our benchmark dataset is designed to fill the gaps between all previous datasets: (i) It maintains the finest granularity over language by grounding specific words in the narrative, as the   latest developed natural language grounding datasets. (ii)   It provides fine-grained visual grounding annotations using segmentations, as in referring expression segmentation   datasets. This allows studying the problem with a stricter   experimental framework in which the location on the objects is not approximate as in the object detection task. (iii)   Our annotations include panoptic categories [17] instead of   just focusing on things categories, which gives way to a   global analysis of the scene which is relevant for reasoning   about the world around us.   2.2. Natural Language Visual Grounding Methods   The methods developed for the coarsest natural language visual grounding tasks on language, specifically for   the REC and RES tasks, can be grouped into two general approaches: top-down and bottom-up.   The former   paradigm [6, 27, 51] starts from region proposals extracted   by a general method of object detection or segmentation.   From this set of objects, the method selects the one that   is described to a greater extent by the natural language expression. The strengths of this paradigm are that they can   take advantage of what general object detection and segDataset   Language   Granularity   Visual   Granularity   Semantic   Generality   MS COCO Captions [5, 25]   Caption   \u2715   \u2715   Conceptual Captions [40]   Caption   \u2715   \u2715   Stanford Visual Par. [18]   Caption   \u2715   \u2715   ReferIt [15, 32]   Short phrase   Segmentation   Things   Google Refexp [31]   Short phrase   Segmentation   Things   Visual Genome [19]   Short phrase   Bounding box   Things + Stuff   Flickr30k Entities [49, 35]   Noun phrase   Bounding box   Mainly Things   Localized Narratives [36]   Each word   Traces   Things + Stuff   Panoptic Narrative Grounding (Ours)   Noun phrase   Segmentation   Things + Stuff   Table 1: Panoptic Narrative Grounding compared with major captioning and natural language grounding datasets.   mentation methods have learned from the existing largescale datasets for these general tasks, rather than directly   learning how to detect objects within an image. However,   since these methods do not update the region proposals, the   upper bound of these methods is significantly impacted by   the performance of the base method. In contrast, the latter approaches produce the referred object segmentation by   grouping pixels [4, 11, 12, 13, 14, 23, 24, 26, 29, 30, 33, 37,   38, 41, 47, 48]. These methods use single networks, which   leverage high and low-level features to refine segmentation   masks along levels of their architectures. However, complying with a larger search space than its top-down counterpart,   leads to higher rates of false positive segmentations.   Some existing methods approach phrase grounding   through a combination of base networks for visual and linguistic information. To this end, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are   used [11, 33], and their features leveraged to achieve wordto-image interactions. Other works apply Transformer attention [45] to generate contextualized representations of   both modalities. The latter architectures are also used in   cross-modality analyses to aggregate and align visual cues   and linguistic meanings [21, 22, 28, 42, 52].   3. Panoptic Narrative Grounding Benchmark   This section describes our proposed benchmark.   We   describe how we generate the ground-truth annotations   (Sec. 3.1) and report some statistics (Sec. 3.2).   Finally,   we explain the proposed metrics to evaluate results on our   benchmark (Sec. 3.3).   3.1. Ground-Truth Annotations   We transfer the Localized Narrative [36] annotations   to MS COCO panoptic segmentations by synchronizing   timed captions and trace points. We consider a set of utterances U = {u1, . . . , un} in the caption and a set of   trace points T = {t1, . . . , tm} for each Localized Narrative,   where ui and tj are the timestamped verbal and spatial units   that, respectively, compose a caption and a mouse trace.   Synchronization occurs by selecting the trace segments   1366   Figure 2: Ground-Truth Annotations Results. Examples of different Panoptic Narrative Grounding ground truth resulting   from the proposed annotation transfer algorithm (c). We show the input image (a) and Localized Narrative traces (b) and   caption with the matched panoptic segmentation regions (d). Color gradient in the trace, panoptic segmentation and caption   indicates time over the language. The segmentation regions are visualized with the color of their corresponding noun phrases,   according to the last associated spoken word. More qualitative results can be found in the Supplementary Material.   that are drawn over an image between the start and end   times of an utterance, or a subset of contiguous utterances,   containing a noun phrase (uin). Each uin is hence associated with a subset of trace points T \u2032 = {ta, . . . , tb} (where   ta and tb are within the timestamp of ui). T \u2032 provides a   spatial reference for each utterance through their point coordinates. We average these coordinates to obtain a single   point p in the image plane, which we refer to as the Center of Mass (CoM). In the Supplementary Material, we exemplify how this strategy of summarizing the annotator\u2019s   grounding is beneficial for irregular and circling patterns in   the Localized Narrative annotations.   To identify noun phrases, we perform a chunking parsing   using the Natural Language Toolkit (NLTK) for Python [3],   and select sequences of consecutive nouns that may or may   not have a previous adjective or a cardinal digit. These noun   phrases are considered a single grounding unit.   Let S = {s1, . . . , sk} be the regions from the MS COCO   panoptic segmentation of the same image, with corresponding thing or stuff category labels C = {c1, . . . , ck}. We   select the region si \u2208S which contains the location of the   CoM p of the mouse trace and establishes it as a candidate   for the ground truth of the noun phrase uin. This matching,   however, is not guaranteed to be correct, as Localized Narrative mouse traces might not fall exactly on the grounded   object. We, therefore, filter these matches by comparing the   noun phrase in the caption (uin) to the MS COCO object   category associated with si (ci), to which should agree. Intuitively, if the annotator said uin =\u201cred vehicle\u201d, we want   to consider the matching correct if e.g. ci =\u201cvehicle\u201d or   ci =\u201ccar\u201d; but we want to discard it if ci =\u201ctree\u201d, since it   might mean that the annotator was pointing to a tree next to   a vehicle when they pronounced the word \u201cvehicle\u201d. Furthermore, if the caption refers to the same object in the visual scene multiple times, our benchmark will associate it   with all the related noun phrases. Figure 2 shows some examples of our visual grounding annotations.   To evaluate whether the caption utterance(s) uin, or its   composing noun(s), correspond to the matched panoptic   category ci, we consider the following criteria: (1) exact   matches (uin is strictly equal to ci), (2) synonyms (uin is a   synonym of ci), (3) hierarchical relationship (uin is either   a hypernym or hyponym of ci), and (4) meronyms (uin is   a meronym of ci). We evaluate (1) by simple string comparison, and (2), (3) and (4) accessing WordNet [34] using   NLTK [3]. Additionally, (5) we manually relate specific   words to certain MS COCO categories that are omitted by   the WordNet ontology. Examples of these words include   clothing pieces, body parts and female figures, as members   of the MS COCO \u201cperson\u201d category. We consider (1) as   1367   \"...few colorful kites...\"   \"There are many people...\"   Figure 3: Examples of grounded plural noun phrases.   We match multiple regions (red) beyond the seed region   (purple). Increasing the spatial scope of our annotations   agrees with the multi-instance nature of plural noun phrases   and better captures the annotators\u2019 intention of referring to   several instances. Best viewed in color.   the highest rank in matching, given the similarity between   words in this level, followed by (2), (3), (4) and (5). Tag   clouds are reported in the Supplementary Material as examples of criteria (2) - (4).   We find two special cases that require additional measures when transferring annotations. First, there is a possibility of time shifts [36]: an annotator moving their mouse   slightly before or after describing objects. To address this   issue, we consider neighboring segmentation regions of the   selected one via p (the CoM of the mouse traces), as potential matches to each noun phrase uin. If no matching occurs with the center region, we proceed to select the closest   candidate that matches uin, with any of the semantic relationships defined above, as the grounding annotation for the   noun phrase. We consider the minimum distance between   segmentation regions as a measure of their closeness (following a single-linkage concept). An example of vicinity   region analysis can be found in the Supplementary Material.   Second, we consider the implications of plural noun   phrases within narratives. These phrases are inherently related to various instances of an object, which are all globally pointed at during the utterance(s). This motivates the   mapping of a plural noun phrase to S\u2032 \u2286S, which contains   several regions with a common category, as opposed to a   single segmentation si. The common category is defined   using p and the vicinity region analysis, from which we select a seed region. We then augment this set with all the   regions of the same category that are contained in the tightest bounding box around the mouse trace T \u2032. All regions in   S\u2032 are considered the grounding of the plural noun phrase   uin. Figure 3 shows examples of plural groundings.   3.2. Dataset Statistics   Our complete annotation transfer methodology matches   an average of 5.1 noun phrases per narrative and 726,445   noun phrases in the whole Localized Narratives annotations for MS COCO dataset.   Localized Narratives contain an average of 11.3 noun phrases per narrative. This   accounts for 45.1% of noun phrases coverage, which we   ground through panoptic segmentation annotations. In the   visual domain, this translates to a total of 741,697 segments matched, from which 659,298 are unique. Thus, our   benchmark covers 47.5% of all the segments annotated in   MS COCO panoptic with at least one noun phrase match.   In turn, this coverage accounts for 58.5% of all pixels in   123,287 MS COCO images and 65.4% relative to all annotated pixels in MS COCO panoptic annotations.   The proposed Panoptic Narrative Grounding dataset   ground both things and stuff categories. Specifically, 57.0%   of the noun phrases grounded are things and 43.0% are   stuff. 64.7% of matches between grounded noun phrases   and MS COCO panoptic categories are exact matches between the category name and a noun in the phrase. Thus,   using synonyms, manual matching, and hierarchical and   menoronyms relationships significantly extend the grounding scope (35.3%) and allow us to better capture the diversity in natural language expressions in Localized Narratives.   Lastly, 29.3% of the matched noun phrases correspond to   plurals and 44.6% of matches are due to the vicinity region analysis. Consequently, each of the proposed matching steps plays an important role in improving the visual   grounding annotations. A detailed report of these statistics   can be found in the Supplementary Material. Moreover, we   carry out a final manual curation to verify the integrity of   our annotations in the 1% of the annotations.   3.3. Metrics   In contrast to the recall metric, traditionally used for   phrase grounding, we propose to calculate the Average Recall. This metric evaluates the performance of the method   in the Panoptic Narrative Grounding task by considering   different Intersection over Union (IoU) thresholds between   the panoptic segmentation proposal and the ground-truth for   each noun phrase. Hence, the quality of the segmentations   affects the method\u2019s performance since the IoU metric determines whether a detection is considered a true positive   or not. We obtain a curve that at very low IoU values has   recall close to one and at higher IoU values recall drops.   The final metric, named Average Recall, is the area under   the curve described above. For plural noun phrases, we do   not explicitly match the ground-truth instances and the prediction proposals to calculate the Average Recall. Instead,   we aggregate all instances of the ground-truth annotation   into a single segmentation and compute the IoU with respect to the segmentation composed by all the prediction   proposals. With this strategy, we avoid errors or variations   in the matching process between annotations and predictions while assessing the overall segmentation quality.   1368   Relevant regions   per noun phrase   Singlemodality   transformer   Noun   phrases   Panoptic    segmentation   proposals   A\ufb03nity   matrix   max.   (a) Input   (b) Feature extraction   (c) Joint processing   (d) Relevance computation   Singlemodality   transformer   Crossmodality   transformer   Panoptic Narrative   Grounding   This is a picture taken in the    outdoors. The bus is parked on the    road. In the bus a person is holding    the steering. The bus is in white   color. Beside the bus a person is   standing on the path. Behind the bus   there are vehicles, building and   group of people standing on the path    and a sky with clouds.   Panoptic segmentation   proposals extraction   Tokenization   Figure 4: Panoptic Narrative Grounding Baseline. Our model takes an image and a corresponding caption as input (a),   followed by a top-down approach by extracting panoptic segmentation proposals with a backbone segmenter. We then use   single-modality transformers to independently process the segmentation proposals and the tokens from the caption (b) and   jointly process the information with a cross-modality transformer (c). Finally, we calculate an affinity matrix between visual   and textual information and select the more relevant panoptic segmentation proposal for each noun phrase (d).   4. Baseline for Panoptic Narrative Grounding   We   build   upon   the   Cross-Modality   Relevance   (CMR) [52] model developed for reasoning tasks on   language and vision. This method has competitive results   in two classification tasks at the intersection of vision and   language:   Visual Question Answering [2] and Natural   Language for Visual Reasoning [43, 44].   CMR model   introduces entity relevance representation that explicitly   expressed the relevance of textual entities with respect to   visual entities. The model uses this affinity matrix as an   intermediate representation for the final tasks. Their results   suggest using the word alignment with regions in the image   results in an increase in performance for the final task. We   adapt the model and optimize the architecture for our task.   Figure 4 depicts an overview of our baseline method.   Given an image and its caption (a), we extract features from   each of the panoptic segmentation region proposals and process each proposal and word with single modality transformers. (b). We then concatenate the outputs from the   visual modality transformer and the textual modality transformer and use it as input to the cross-modality transformer   that updates the representations of each entity considering   not only those of the same modality but also those of the   other modality (c). Afterwards, we calculate the affinity   matrix from the matrix multiplication between the representations of each word and region proposal. Finally, we make   an average pooling over the language dimension of all the   words included in each noun phrase. This affinity matrix   explicitly indicates which is the most relevant panoptic segmentation region for each noun phrase in the caption (d).   Implementation details: We use a Panoptic Feature Pyramid Network [17] (FPN) with a ResNet-101 [9] backbone   pre-trained on MS COCO [5, 25] with 3\u00d7 schedule using the official implementation [46]. Parameters in Panoptic FPN are fixed.   For stuff proposals, features correspond to the ones extracted from the semantic segmentation   branch after combining the features extracted from each   FPN level and before the final upsampling. For things categories, they are the mask features in the instance segmentation branch. The visual single modality transformer has   3 attention layers. We use the pre-trained BERT \u201cbase\u201d   model [7] to generate the single-modality representation.   The cross-modality transformer has 5 attention layers. During training, we performed a multi-label classification task   to include several regions when the noun phrase is plural. Additionally, for plurals in the final prediction we consider all the panoptic segmentation region proposal with a   match score greater than 0.1. We train our method on an   NVIDIA Quadro RTX 8000 GPU for 25 epochs with an   initial learning rate of 0.000,01, 60 images per batch and   use an Adam [16] optimizer with standard parameters.   5. Experiments   Experimental setup:   We use our Panoptic Narrative   Grounding benchmark with the standard training and validation splits of the MS COCO dataset. We only include the   images for which our annotation transfer process assigns at   least one noun phrase of the Localized Narrative caption to a   panoptic segmentation region. The final splits have 133,103   localized narratives for train and 8533 for validation.   Quantitative results: Table 2 and Figure 5a show the quantitative results of our method using the proposed Average   1369   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   IoU   Recall@IoU   Oracle   Ours   (a) Overall performance   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   IoU   Recall@IoU   Oracle (Things)   Ours (Things)   MCN [30] (Things)   Oracle (Stuff)   Ours (Stuff)   (b) Things and stuff categories   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   0   .1   .2   .3   .4   .5   .6   .7   .8   .9   1   IoU   Recall@IoU   Oracle (Singulars)   Ours (Singulars)   Oracle (Plurals)   Ours (Plurals)   (c) Singulars and plurals   Figure 5: Average Recall Curve for our baseline method performance (a) compared to the oracle, and dissagregated into (b)   things and stuff categories, and (c) singulars and plurals noun phrases.   Method   Average Recall   things + stuff   things   stuff   Oracle   64.4   67.3   60.4   Ours   55.4   56.2   54.3   MCN[30]   48.2   (a) Things and stuff categories.   Method   Average Recall   singulars + plurals   singulars   plurals   Oracle   64.4   64.8   60.7   Ours   55.4   56.2   48.8   (b) Singulars and plurals noun phrases.   Table 2: Results of our method for Panoptic Narrative Grounding task compared to the oracle performance, disaggregated   into (a) things and stuff categories, and (b) singulars and plurals noun phrases.   Recall evaluation metric (Sec. 3.3). Our method\u2019s relative   performance is 86.0% with a drop of only 9.0 absolute Average Recall points with respect to the oracle performance   given by the base panoptic segmentation method (55.4 vs.   64.4). We calculate this oracle by choosing as prediction   the panoptic segmentation proposal with the highest IoU   with the ground-truth annotation. Therefore, the oracle version of our method assumes a perfect match between segments and noun phrases, and only measures segmentation   errors. These results suggest that our proposed baseline is   very strong and a good starting point to promote research   in Panoptic Narrative Grounding. However, our proposed   baseline method is limited by the quality of the proposals\u2019   segmentations and the recall of the base panoptic segmentation method.   Furthermore, to compare our baseline for Panoptic Narrative Grounding with respect to a visual grounding stateof-the-art method, Multi-task Collaborative Network [30]   (MCN). We generalize our task and make a natural extension of RES task. For this purpose, we split the captions   into phrases and include as ground truth all segmentation regions that match a noun phrase within each sentence. Since   this method was developed in the scope of RES task, we   just include things categories. This state-of-the-art method   for RES follows and bottom-up approach and leverages the   complementary relationship between REC and RES tasks   by using their properties to benefit the other task. MCN   achieves a performance of 48.2 Average Recall. This result is comparable with the performance of our disaggregated method in only things, which shows that our baseline   method better address the problem of natural language visual grounding. We hypothesize that approaching this task   from a fine-grained formulation allows the model to easily   abstract information using the semantics of each word.   By disaggregating the performance of our baseline   method into things and stuff (Tab. 2a, Fig. 5b), it is possible   to observe that, even though our method\u2019s performance for   stuff categories is higher, the drop with respect to the oracle   performance for things categories is 5.0 absolute Average   Recall points greater. We hypothesize that this is due to (i)   greater variations in position and appearance in things categories. These results suggest that aligning noun phrases and   panoptic region proposals gets more difficult as the intraclass visual variability increases. Also, (ii) there can be   multiple instances in an image of the same object category   in the case of things, as opposed to stuff categories, so the   1370   Figure 6: Qualitative results for Panoptic Narrative Grounding. Example predictions of our baseline in the validation   split of our benchmark. The inputs are the image (a) and the caption without highlighted noun phrases (b). The outputs are   a set of noun phrases in the caption (b), each with a corresponding region in the predicted segmentation (c). (d) shows the   ground-truth panoptic segmentations. More qualitative results can be found in the Supplementary Material.   model fails to disambiguate between the instances of that   category and select the one to which the noun phrase refers   in the caption\u2019s context.   We also disaggregate the performance on singular and   plural noun phrases (Tab. 2b, Fig. 5c). The drop in performance for plural noun phrases indicates that our model does   not retrieve all the instances of the object to which language   refers. We attribute this limitation to the trace not exhaustively intersecting all the instances in many cases, which   spreads to our annotations and ends up affecting the results.   Figure 6 shows some qualitative results of our baseline   method compared to the ground-truth panoptic segmentations of our benchmark. More qualitative results can be   found in the Supplementary Material.   6. Conclusions   In this work, we present Panoptic Narrative Grounding,   a new formulation for the natural language visual grounding problem that aims at producing a panoptic segmentation   that grounds densely each noun phrase in the caption. This   version of the problem (i) maintains the finest granularity   over language by visually grounding noun phrases, while   (ii) including a spatially detailed visual grounding with segmentations, and (iii) incorporating all panoptic categories to   leverage the intrinsic semantic information in visual scenes.   We establish a strong experimental framework for the study   of this task, including new annotations and evaluation metrics. We design an algorithm to transfer Localized Narratives\u2019 visual grounding to specific regions in the panoptic segmentations of the MS COCO dataset. Furthermore,   we propose a strong baseline method to serve as stepping   stone for future work. The formulation of Panoptic Narrative Grounding, its benchmark and experimental framework   will push the envelope further in the development of fine   and general natural language visual grounding methods. In   turn, advances in this task will impact solutions for other   problems on the intersection of vision and language.   1371", "conf": "ICCV", "year": "2021", "index": 63}, {"title": "MAD: A Scalable Dataset for Language Grounding in Videos   from Movie Audio Descriptions   Mattia Soldan1,   Alejandro Pardo1,   Juan Le\u00b4on Alc\u00b4azar1,   Fabian Caba Heilbron2,   Chen Zhao1,   Silvio Giancola1,   Bernard Ghanem1   1King Abdullah University of Science and Technology (KAUST)   2Adobe Research   {mattia.soldan, alejandro.pardo, juancarlo.alcazar, chen.zhao,   silvio.giancola, bernard.ghanem}@kaust.edu.sa   caba@adobe.com", "abstract": "The recent and increasing interest in video-language research has driven the development of large-scale datasets   that enable data-intensive machine learning techniques. In   comparison, limited effort has been made at assessing the   \ufb01tness of these datasets for the video-language grounding task.   Recent works have begun to discover signi\ufb01cant limitations in these datasets, suggesting that state-ofthe-art techniques commonly over\ufb01t to hidden dataset biases. In this work, we present MAD (Movie Audio Descriptions), a novel benchmark that departs from the paradigm   of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies.   MAD contains   over 384, 000 natural language sentences grounded in over   1, 200 hours of videos and exhibits a signi\ufb01cant reduction in   the currently diagnosed biases for video-language grounding datasets. MAD\u2019s collection strategy enables a novel   and more challenging version of video-language grounding, where short temporal moments (typically seconds long)   must be accurately grounded in diverse long-form videos   that can last up to three hours. We have released MAD\u2019s   data and baselines code at https://github.com/   Soldelli/MAD.   1.", "content": "Imagine you want to \ufb01nd the moment in time, in a movie,   when your favorite actress is eating a Gumbo dish in a New   Orleans restaurant. You could do so by manually scrubbing   the \ufb01lm to ground the moment. However, such a process is   tedious and labor-intensive. This task is known as natural   language grounding [1, 4], and has gained signi\ufb01cant momentum in the computer vision community. Beyond smart   browsing of movies, the interest in this task stems from   multiple real-world applications ranging from smart video   search [26, 27], video editing [18, 19, 32], and helping pa1   10   100   Average Duration per Video (min)   0   5   10   15   20   25   30   Coverage (%)   CharadesSTA   TACoS   DiDeMo   ActivityNet   Captions   MAD   (ours)   Figure 1. Comparison of video-language grounding datasets.   The circle size measures the language vocabulary diversity. The   videos in MAD are orders of magnitude longer in duration than   previous datasets (\u223c110min), annotated with natural, highly descriptive, language grounding (>60K unique words) with very low   coverage in video (\u223c4.1s). Coverage is de\ufb01ned as the average %   duration of moments with respect to the total video duration.   tients with memory dysfunction [2,31]. The importance of   solving this task has resulted in novel approaches and largescale deep-learning architectures that steadily push state-ofthe-art performance.   Despite those advances, recent works [17,34,37] have diagnosed hidden biases in the most common video-language   grounding datasets.   Otani et al. [17] have highlighted   that several grounding methods only learn location priors, to the point of disregarding the visual information   and only using language cues for predictions. While recent methods [34, 37] have tried to circumvent these limitations by either proposing new metrics [35] or debiasing strategies [37, 41], it is still unclear if existing grounding datasets [1, 4, 7, 21] provide the right setup to evalu5026   1:30:00   Time   0:10:00   0:40:00   \u2026   \u2026   Audio    Descriptions   00:09:19   00:09:27   \u201cBearded dad is carrying his other sick son. Mum seems alone    with her thoughts and the daughter is in complete silence.\u201d   00:38:36   00:38:42   \u201cMum hangs up the laundry outside the farmhouse.\u201d   Temporal    Grounding   \u2026   \u2026   Figure 2. Example from our MAD dataset. We select the movie \u201cA quiet place\u201d as representative for our dataset. As shown in the   \ufb01gure, the movie contains a large number of densely distributed temporally grounded sentences. The collected annotations can be very   descriptive, mentioning people, actions, locations, and other additional information. Note that as per the movies plot, the characters are   silent for the vast majority of the movie, rendering audio description essential for visually-impaired audience.   ate progress in this important task. This is partly because   datasets used by video-language grounding models were   not originally collected to solve this task. These datasets   provide valuable video-language pairs for captioning or retrieval, but the grounding task requires high-quality (and   dense) temporal localization of the language.   Figure 1   shows that the current datasets comprise relatively short   videos, containing single structured scenes, and language   descriptions that cover most of the video. Furthermore, the   temporal anchors for the language are temporally biased,   leading to methods not learning from any visual features   and eventually over\ufb01tting to temporal priors for speci\ufb01c actions, thus limiting their generalization capabilities [8,17].   In this work, we address these limitations with a novel   large-scale dataset called MAD (Movie Audio Descriptions). MAD builds atop (and includes part of) the LSMDC   dataset [24], which is a pioneering work in leveraging audio descriptions to enable the investigation of a closely related task: text-to-video retrieval. Similar to LSMDC, we   depart from the standard annotation pipelines that rely on   crowd-sourced annotation platforms. Instead, we adopt a   scalable data collection strategy that leverages professional,   grounded audio descriptions of movies for visually impaired audiences. As shown in Figure 2, MAD grounds   these audio descriptions on long-form videos, bringing   novel challenges to the video-language grounding task.   Our data collection approach consists of transcribing   the audio description track of a movie and automatically   detecting and removing sentences associated with the actor\u2019s speech, yielding an authentic \u201cuntrimmed video\u201d setup   where the highly descriptive sentences are grounded in   long-form videos. Figures 1 and 3 illustrate the uniqueness of our dataset with respect to the current alternatives.   As showcased in Figure 2, MAD contains videos that, on   average, span over 110 minutes, as well as grounded annotations covering short time segments, which are uniformly   distributed in the video, and maintain the largest diversity in   vocabulary. Video grounding in MAD requires a \ufb01ner understanding of the video, since the average coverage of the   sentences is much smaller than in current datasets.   The unique con\ufb01guration of the MAD dataset introduces   exciting challenges. First, the video grounding task is now   mapped into the domain of long form video, preventing current methods to learn trivial temporal location priors, instead requiring a more nuanced understanding of the video   and language modalities.   Second, having longer videos   means producing a larger number of segment proposals,   which will make the localization problem far more challenging. Last, these longer sequences emphasize the necessity for ef\ufb01cient methods in inference and training, mandatory for real-world applications such as long live streaming   videos or moment retrieval in large video collections [3].   Contributions. Our contributions are threefold. (1) We   propose Movie Audio Description (MAD), a novel largescale dataset for video-language grounding, containing   more than 384K natural language sentences anchored on   more than 1.2K hours of video. (2) We design a scalable   data collection pipeline that automatically extracts highly   valuable video-language grounding annotations, leveraging   speech-to-text translation on professionally generated audio descriptions. (3) We provide a comprehensive empirical study that highlights the bene\ufb01ts of our large-scale   MAD dataset on video-language grounding as a benchmark,   pointing out the dif\ufb01culties faced by current video-language   grounding baselines in long-form videos.   5027   2. Related work   Video Grounding Benchmarks. Most of the current video   grounding datasets were previously collected and tailored   for other computer vision tasks (i.e., Temporal Activity Localization [5,21]) and purposes (i.e., Human Action Recognition), then annotated for the video grounding task. This   adaptation limits the diversity of the video corpus towards   a speci\ufb01c set of actions and objects, and its corresponding natural language sentences to speci\ufb01c sets of verbs and   nouns [4,7,17,22]. Currently, ActivityNet-Captions [7] and   Charades-STA [4] are the most commonly used benchmarks   for the task of video grounding. Both datasets have been   collected atop pre-existing video datasets (ActivityNet [5]   and Charades [25]) and have been diagnosed with severe   biases by Otani et al. [17]. These \ufb01ndings show that the   annotations contain distinct biases where language tokens   are often coupled with speci\ufb01c temporal locations. Moreover, strong priors also affect the temporal endpoints, with   a large portion of the annotations spanning the entire video.   As a consequence, current methods seem to mainly rely on   such biases to make predictions, often disregarding the visual input altogether. In comparison, the unique setup of   MAD prevents these drawbacks as keywords are not associated with particular temporal regions, and annotation timestamps are much shorter than the video\u2019s duration. Moreover, different from Charades-STA, MAD de\ufb01nes an of\ufb01cial   validation set for hyper-parameter tuning.   Unlike   Charades-STA   and   ActivityNet-Captions,   TACoS [21] has not been diagnosed with annotation biases.   However, its video corpus is small and limited to cooking   actions recorded in a static-camera setting.   Conversely,   spanning over 22 genres across 90 years of cinema history,   MAD covers a broad domain of actions, locations, and   scenes. Moreover, MAD inherits a diverse set of visual and   linguistic content from the broad movie genres, ranging   from \ufb01ction to everyday life.   Furthermore, DiDeMo [1] was annotated atop Flickr   videos with a discrete annotation scheme (i.e. in chunks   of 5 seconds) for a maximum of 30 seconds, constraining   the problem of video grounding to trimmed videos. Given   these annotations, the grounding task can be simpli\ufb01ed to   choosing one out of 21 possible proposals for each video.   Conversely, MAD provides a setup to explore solutions for   grounding language in long-form videos, whose length can   be up to 3 hours. In this scenario, naive sliding window   techniques for proposal generation could produce hundreds   of thousands of possible candidates. Therefore, developing   ef\ufb01cient inference methods becomes a much more urgent   requirement compared to previous benchmarks.   State-of-the-art video grounding methods [12,13,15,28,   36, 38, 40] have relied on existing benchmarks to design   novel modules (i.e., proposal generation, context modeling,   and multi-modality fusion). However, most of these designs   speci\ufb01cally target grounding in short videos and often rely   on providing the entire video to the model when making   a prediction. As the long-form setup introduced by MAD   prohibits this, new methods will have the opportunity to investigate and bridge previous ideas to these new challenging   and real-world constraints.   Audio Descriptions. The pioneering works of Rohrbach   et al. [23] and Torabi et al. [30] were the \ufb01rst to exploit   audio descriptions to study the text-to-video retrieval task   and its counterpart video-to-text. Rohrbach et al. [23] introduced the MPII-MD dataset, while Torabi et al. [30]   presented M-VAD, both collected from audio descriptions   in movies. Later, these datasets were fused to create the   LSMDC dataset [24], which forms the core of the LSMDC   annual challenge.   Our annotation pipeline is similar in   spirit to these works. However, MAD sizes the potential   of movies\u2019 audio descriptions for a new scenario: language   grounding in videos.   A concurrent work introduced a new benchmark based   on audio descriptions in videos, called QuerYD [16]. It is a   dataset for retrieval and event localization in videos crawled   from YouTube. This benchmark focuses on the short-form   video setup with videos of less than 5 minutes average duration. QuerYD also leverages audio descriptions, which   have been outsourced to volunteer narrators. Similar to our   takeaways, the authors noticed that audio descriptions are   generally more visually grounded and descriptive than previously collected annotations.   3. Collecting the MAD Dataset   In this section, we outline MAD\u2019s data collection   pipeline. We follow independent strategies for creating the   training and testing set. For the former, we aim at automatically collecting a large set of annotations. For the latter,   we re-purpose the manually re\ufb01ned annotations in LSMDC.   We provide detailed statistics of MAD\u2019s annotations and   compare them to existing datasets.   3.1. MAD Training set   MAD relies on audio descriptions professionally created   to make movies accessible to visually-impaired audiences.   These descriptions embody a rich narrative describing the   most relevant visual information. Thus, they adopt a highly   descriptive and diverse language. Audio descriptions are   often available as an alternative audio track that can replace   the original one. Professional narrators curate them, and   devote signi\ufb01cant effort to describe a movie. The audio description process demands an average of 30 work hours to   narrate a single hour of video [24]. In comparison, previous   datasets that have used the Amazon Mechanical Turk service for video-language grounding estimate the annotation   effort to be around 3 hours for each video hour [7].   5028   Videos   Language Queries   Dataset   Total   Duration   Duration   Total   # Words   Total   Vocabulary   Duration   / Video   / Moment   Queries   / Query   Tokens   Adj.   Nouns   Verbs   Total   TACoS [21]   10.1 h   4.78 min   27.9 s   18.2K   10.5   0.2M   0.2K   0.9K   0.6K   2.3K   Charades-STA [4]   57.1 h   0.50 min   8.1 s   16.1K   7.2   0.1M   0.1K   0.6K   0.4K   1.3K   DiDeMo [1]   88.7 h   0.50 min   6.5 s   41.2K   8.0   0.3M   0.6K   4.1K   1.9K   7.5K   ANet-Captions [7]   487.6 h   1.96 min   37.1 s   72.0K   14.8   1.0M   1.1K   7.4K   3.7K   15.4K   MAD (Ours)   1207.3 h   110.77 min   4.1 s   384.6K   12.7   4.9M   5.3K   35.5K   13.1K   61.4K   Table 1. Statistics of video-language grounding datasets. We report relevant statistics to compare our MAD dataset against other video   grounding benchmarks. MAD provides the largest dataset with 1207hrs of video and 384.6K language queries, the longest form of video   (avg. 110.77min), the most diverse language vocabulary with 61.4K unique words, and the shortest moment for grounding (avg. 4.1s).   Data Crawling. Not every commercially available movie is   released with audio descriptions. However, we can obtain   these audio descriptions from 3rd party creators. In particular, we crawl our audio descriptions from a large opensource and online repository1. These audio \ufb01les contain   the original movie track mixed with the narrator\u2019s voice,   carefully placed when actors are not speaking. One potential problem is that the audio descriptions can be misaligned with the original movie. Such misalignment comes   either from a delay in the recording of the audio description   (concerning the original movie) or from audio descriptions   being created from different versions of the movie (with   deleted or trimmed scenes).   Alignment and Cleanup. Since the audio description track   also contains the movie\u2019s original audio, we can resolve this   misalignment by maximizing the cross-correlation between   overlapping segments of the original audio track and the   audio description track. We de\ufb01ne the original audio signal (f), the audio description signal (g), and the time delay (\u03c4delay) between the two signals. The maximum of the   cross-correlation function (denoted with the operator \u22c6) indicates the point in time where the signals exhibit the best   alignment. As a result, the time delay \u03c4delay between f and   g is de\ufb01ned as follows:   \u03c4delay = arg max   t   ((f \u22c6g)(t))   (1)   To verify that our single time delay \u03c4delay de\ufb01nes the best   possible alignment between the audio descriptions and the   original movies, we run our synchronization strategy over   several (i.e., 20) uniformly distributed temporal windows.   We verify that the delays estimated for all windows are consistent with each other, within a maximum range of \u00b10.1   seconds w.r.t the median value of the distribution of the 20   samples. We discard the movies that do not satisfy this criterion to ensure that the whole audio description track correctly aligns with the original movie\u2019s visual content.   Audio Transcriptions and Veri\ufb01cation. After the two au1https://www.audiovault.net/   dio tracks are aligned, we transcribe the audio description   \ufb01le using Microsoft\u2019s Azure Speech-to-Text service2. Each   recognized word is associated with a temporal timestamp.   At this step in our pipeline, we have sentences temporally   grounded to the original video stream. As the Speech-toText contains both the narrations and the actors\u2019 speech, we   set out to remove the latter and only retain the audio description in textual form. To do so, we resort to the movie\u2019s   original subtitles and use their timestamps as a surrogate   for Voice Activity Detection (VAD). Particularly, we discard closed captions and retain subtitles associated with the   actors\u2019 speech or subtitles for songs. Then, we remove from   the Speech-to-Text output every sentence overlapping with   the VAD temporal locations, obtaining our target audio description sentences. We post-process the text for automatic   punctuation re\ufb01nement using the free tool Punctuator [29].   3.2. From LSMDC to MAD Val/Test   As outlined in Section 3.1, the val/test sets of MAD   also relies on audio descriptions for movies. Since the annotations in training are automatically generated, we decided to minimize the noise in the validation and test splits.   Hence, we avoid the automatic collection of data for these   sets and resort to the data made available by the LSMDC   dataset [24]. This dataset collected annotations from audio descriptions in movies targeting the video retrieval   task. LSMDC manually re\ufb01ned the grammar and temporal   boundaries of sentences. As a consequence, these annotations have clean language and precise temporal boundaries.   We reformat a subset of the LSMDC data, adapt it for the   video grounding task, and cast it as MAD\u2019s val and test sets.   LSMDC data for retrieval is made available only as video   chunks, not full movies. To create data suitable for longform video language grounding, we collect 162 out of the   182 videos in LSMDC and their respective audio descriptions. Again, the full-length video data and the chunked   video data provided by LSMDC might not be in sync. To   2https://azure.microsoft.com/en- us/services/   cognitive-services/speech-to-text/   5029   0   20   40   60   80   100   Start Time (%)   0   5000   10000   15000   Counts   (a) Moment start time histogram   0   20   40   60   80   100   End Time (%)   0   5000   10000   15000   (b) Moment end time histogram   0   20   40   60   80   100   120   Duration (s)   101   102   103   104   105   106   MAD   ActivityNet   Captions   CharadesSTA   DiDeMo   TACoS   (c) Moment duration histogram   Figure 3. Histograms of moment start/end/duration in video-language grounding datasets. The plots represent the normalized (by   video length) start/end histogram (a-b) and absolute duration distribution (c) for moments belonging to each of the \ufb01ve datasets. We notice   severe biases in ActivityNet-Captions and Charades-STA, which show high peaks at the beginning and end of the videos. Conversely MAD   does not show any particular preferred start/end temporal location.   align a video chunk from LSMDC with our full-length   movies, we follow a similar procedure as the one described   in Section 3.1 for the audio alignment, but using visual information. We use CLIP [20] to extract \ufb01ve frame-level features per second for both the video chunks from LSMDC   and our full-length movie.   Then, we use the maximum   cross-correlation score to estimate the delay between the   two. We run the alignment on 10 different window lengths   and take the median value as the delay of the chunk.   Once the audiovisual data is aligned, we use the text annotations and re-de\ufb01ne their original timestamps according   to the calculated delays. This process creates full-length   movies with curated grounding data for MAD\u2019s validation   and test sets. In doing so, we obtain large and clean validation and test sets since LSMDC was curated by humans and   has been used for years by the community. Our val/test sets   evaluate video-grounding methods with more than 104K   grounded phrases coming from more than 160 movies.   3.3. MAD Dataset Analysis   We now present MAD\u2019s statistics and compare them   with legacy datasets for the task. Table 1 summarizes the   most notable aspects of MAD regarding both the visual and   language content of the dataset.   Scale and Scalability. MAD is the largest dataset in video   hours and number of sentences. The training, validation,   and test sets consist of 488, 50, and 112 movies, respectively.   Although other datasets have a larger number of   clips, MAD\u2019s videos are full movies which last 2 hours on   average. In comparison, the average clip from other datasets   spans just a few minutes. Overall, MAD splits contain a   combined 50 days of continuous video. We highlight that   our test set alone is already larger than any other video   grounding dataset. We also emphasize that each movie in   MAD is long and composed of several diverse scenes, making it a rich source for long-form video analysis. Also, as   the cinema industry is ever-growing, we expect to periodically expand MAD with subsequent releases to fuel further   innovation in this research direction.   Vocabulary Size.   Besides having the largest video corpus, MAD also contains the largest and most diverse query   set of any dataset. In Table 1, we show that MAD contains the largest set of adjectives, nouns, and verbs among   all available benchmarks. In almost every aspect, it is an   order of magnitude larger.   The number of sentences in   MAD training, validation, and test is 280.5K, 32.1K, and   72.0K, respectively, one order of magnitude larger than the   equivalent set in any other dataset. Overall, MAD contains   61.4K unique words, almost 4 times more than the 15.4K   of ActivityNet-Captions [7] (the highest among the other   benchmarks). Finally, the average length per sentence is   12.7 words, which is similar to the other datasets.   Bias Analysis. Figure 3 plots the histograms for start/end   timestamps of moments in all grounding datasets. We notice clear biases in current datasets: Charades-STA [4],   DiDeMo [1], and ActivityNet-Captions [7]. Charades-STA   and ActivityNet-Captions are characterized by tall peaks at   the beginning (Figure 3a) and end (Figure 3b) of the video,   meaning that most temporal annotations start at the video\u2019s   start or \ufb01nish at the video\u2019s end. This bias is learned quickly   by modern machine learning algorithms, resulting in trivial groundings that span a full video. The smaller dataset   TACoS also exhibits a similar bias, although it is less prominent. DiDeMo is limited by its annotation strategy, where   chunks of 5 seconds are labeled up to a maximum of 30   seconds. This favors structured responses that roughly approximate the start and end points of a moment. In contrast,   MAD has an almost uniform histogram. This means that   moments of interest can start and end at any point in the   video. We only observe a minor imbalance where the end of   the movie has slightly more descriptions than the beginning.   This is related to the standard structure of a \ufb01lm, where the   main plot elements are resolved towards the end, thus creating more situations worth describing. Figure 3c plots the   histograms for moment duration. MAD is characterized by   5030   IoU=0.1   IoU=0.3   IoU=0.5   Model   R@1   R@5   R@10 R@50 R@100   R@1   R@5 R@10 R@50 R@100   R@1   R@5 R@10 R@50 R@100   Oracle   100.00   \u2212   \u2212   \u2212   \u2212   100.00   \u2212   \u2212   \u2212   \u2212   99.99   \u2212   \u2212   \u2212   \u2212   Random Chance   0.09   0.44   0.88   4.33   8.47   0.04   0.19   0.39   1.92   3.80   0.01   0.07   0.14   0.71   1.40   CLIP [20]   6.57   15.05 20.26 37.92   47.73   3.13   9.85 14.13   28.71   36.98   1.39   5.44   8.38   18.80   24.99   VLG-Net [28]   3.64   11.66   17.89 39.78 51.24   2.76   9.31 14.65 34.27 44.87   1.65 5.99   9.77   24.93 33.95   Table 2. Benchmarking of grounding baselines on the MAD dataset. We report the performance of four baselines: Oracle, Random   Chance, CLIP, VLG-Net, on the test split. The \ufb01rst two validate the choice of proposals by computing the upper bound to the performance   and the random performance. CLIP and VLG-Net use visual and language features to score and rank proposals. For all experiments, we   adopt the same proposal scheme as in VLG-Net [28], and use CLIP [20] features for video (frames) and language embeddings.   shorter moments on average, having a long tail distribution   with moments that last up to one minute.   4. Experiments   We proceed with the experimental assessment of videolanguage grounding on the MAD dataset. We \ufb01rst describe   the video grounding task in the MAD dataset along with its   evaluation metrics and then report the performance of four   selected baselines.   Task. Given an untrimmed video and a language query, the   video-language grounding task aims to localize a temporal   moment (\u03c4s, \u03c4e) in the video that matches the query [1,4].   Metric. Following the grounding literature [1,4], we adopt   Recall@K for IoU=\u03b8 (R@K-IoU=\u03b8).   Given a ranked   set of video proposals, this metric measures if any of the   top K ranked moments have an IoU larger than \u03b8 with   the ground truth temporal endpoints. Results are averaged   across all test samples. Given the long-form nature of our   videos and the large amount of possible proposals, we investigate Recall@K for IoU=\u03b8 with K\u2208{1, 5, 10, 50, 100}   and \u03b8\u2208{0.1, 0.3, 0.5}. This allows us to evaluate for loose   alignment (i.e. IoU=0.1) and approximate ranking (i.e.,   K=100), as well as tight predictions (i.e. IoU=0.5) and accurate retrieval (i.e. K=1).   Baselines.   We benchmark MAD using four different   grounding strategies, namely: Oracle, Random Chance,   CLIP [20], and VLG-Net [28]. The \ufb01rst two provide upper bounds and random performance for the recall metric   given a prede\ufb01ned set of proposals. Oracle chooses the   proposal with the highest IoU with the ground-truth annotation, while Random Chance chooses a random proposal   with uniform probability. We also use CLIP, the pre-trained   image-text architecture from [20], to extract frame-level and   sentence-level features. The frame-level features for each   proposal are combined using mean pooling, then we score   each proposal using cosine similarity between the visual   and the text features. Finally, we adopt VLG-Net [28] as   a representative, state-of-the-art method for the grounding   task. VLG-Net leverages recent advances in Graph Convolution Networks (GCNs) [9\u201311,33,39] to model individual   modalities, while also enabling the aggregation of non-local   and cross-modal context through graph convolutions. We   use VLG-Net and adapt it to work with MAD\u2019s long-form   videos. See the supplementary material for details.   Implementation Details. For the lower bound estimation   using Random Chance, we average the performance over   100 independent runs. To favor a fair comparison against   the CLIP baseline, we train VLG-Net using CLIP features   for both modalities. We use the of\ufb01cial VLG-Net\u2019s implementation with a clip size of 128 input frames spanning 25.6   seconds (frames are extracted at 5 fps). We train using the   Adam optimizer [6] with learning rate of 10\u22124. For inference over an entire movie, we adopt a sliding window approach, where we stride the input window by 64 frames and   discard highly redundant proposals through Non Maximum   Suppression (NMS) with a threshold of 0.3.   4.1. Grounding Performance on MAD   Table 2 summarizes the baseline performance on MAD.   The Oracle evaluation achieves a perfect score across all   metrics except for IoU=0.5. Only a negligible portion of   the annotated moments cannot be correctly retrieved at a   high IoU (0.5), this result showcases the suitability of the   proposal scheme.   The low performance of the Random   Chance baseline re\ufb02ects the dif\ufb01culty of the task, given the   vast pool of proposals extracted over a single video. For   the least strict metric (R@100-IoU=0.1), this baseline only   achieves 8.47%, while CLIP and VLG-Net baselines are   close to 50%, a \u223c6\u00d7 relative improvement. An even larger   gap is present for the most strict metric, R@1-IoU=0.5,   with a relative improvement of two orders of magnitude.   The CLIP [20] baseline is pre-trained for the task of   text-to-image retrieval, and we do not \ufb01ne-tune this model   on the MAD dataset. Nevertheless, when evaluated in a   zero-shot fashion, it results in a strong baseline for longform grounding, achieving the best R@K for the least strict   IoU=0.1 at K={1, 5, 10}. Although IoU=0.1 corresponds   to very loose grounding, this result is nonetheless valuable given a large number of negatives in the long-form   setup, and the fact that MAD is characterized by containing short moments (4.1s on average). Although VLG-Net is   5031   IoU=0.1   IoU=0.3   IoU=0.5   Model   R@1   R@5   R@1   R@5   R@1   R@5   Oracle   100.00   \u2212   99.88   \u2212   99.42   \u2212   Random Chance   3.40   15.69   1.47   7.09   0.52   2.61   CLIP [20]   20.98   45.49   9.74   29.63   4.03 15.90   VLG-Net [28]   23.94   51.46   17.51 43.18   10.17 30.35   Table 3. Short video setup. The table showcases the performance   of the selected baselines in a short-video setup, where movies are   chunked into three minutes (non-overlapping windows). VLGNet, which falls behind CLIP in the long-form setup, achieves the   best grounding performance in most metrics. We can conclude that   a new generation of deep learning architectures will have to be investigated to tackle the speci\ufb01c properties of the MAD dataset.   trained for the task at hand, it achieves comparable or better   performance with respect to CLIP only when a strict IoU   (IoU=0.5) is considered. However, it lags behind CLIP for   most other metrics. We believe the shortcomings of VLGNet are due to two factors. (i) This architecture was developed to ground sentences in short videos, where the entire frame-set can be compared against a sentence in a single forward pass. Thus, it struggles in the long-form setup   where we must compare the sentence against all segments   of the movie and then aggregate the predictions in a postprocessing step. (ii) VLG-Net training procedure de\ufb01nes   low IoU moments as negatives, thus favoring high performance only for higher IoUs.   4.2. The Challenges of Long-form Video Grounding   This section presents an in-depth analysis of the performance of the selected baselines in the long-form setup.   We \ufb01rst investigate how the performance changes when   the evaluation is constrained over segments of the movie,   whose length is comparable to current datasets.   Then   explore how methods behave as the size of the movie   chunks changes. To this end, we split each video into nonoverlapping windows (short videos), and assign the annotations to the short-video with the highest temporal overlap.   Short-video Setup. In Table 3, we set the short-video window length to three minutes. This duration is a candidate   representative for short videos. The upper bound performance Oracle slightly decreases to 99.42% for IoU=0.5.   This is a consequence of the division into short videos,   which occasionally breaks down a few ground truth moments. The Random Chance baseline reports increased performance as the number of proposals generated is reduced.   In particular for R@5-IoU=0.1, the performance increases   from 0.44% (Table 2) to 15.69% (Table 3), demonstrating that the short-video setup is less challenging compared   to MAD\u2019s original long-form con\ufb01guration. In a similar   trend, performance substantially increases for both CLIP   and VLG-Net baselines, with the latter now obtaining the   best performances, in all cases.   30   60   120   180   600   1200   1800   3600   Movie   Duration   Window Length (s)   0   10   20   30   40   Recall Performance (%)   VLG-Net R@5 IoU=0.5   CLIP R@5 IoU=0.5   VLG-Net R@1 IoU=0.5   CLIP R@1 IoU=0.5   Figure 4. Performance trend across different windows lengths.   We observe from the graph the decrease in performance for both   CLIP and VLG-Net, as the evaluation window length increases.   This demonstrates that current grounding methods cannot tackle   the task in the long-form video setting.   From Short- to Long-form Results. Figure 4 showcases   the performance trend for the metrics R@{1, 5}-IoU=0.5,   when the window length is changed from a small value (30   seconds) to the entire movie duration (average duration is   2hrs). The graph displays how the performance steadily   drops as the window length increases, showing the challenging setup of long-form grounding enabled by MAD.   Takeaway. This set of experiments veri\ufb01es that VLG-Net   could successfully outperform the zero-shot CLIP baseline   when evaluated in a short-video setup. We can conclude that   current state-of-the-art grounding methods are not ready to   tackle the long-form setting proposed by MAD. This opens   the door to opportunities for the community to leverage previously developed techniques in a more challenging setting   and potentially incorporate new constraints when designing   deep learning architectures for this task.   5. Ablation Study   This section focuses on the empirical assessment of the   quality of MAD training data.   To help the reader navigate the following experiments, Table 4 de\ufb01nes a naming convention associated with different splits of the data.   LSMDC16 refers to the original data collected by [23, 24,   30] for the task of text-to-video retrieval.   LSMDC-G is   our adaptation of this dataset for the grounding task, as described in Section 3.2. We remind the reader that we could   only retrieve 162 out of 182 movies included in LSMDC16.   For these two datasets, we follow the original train/val/test   partitions. Finally, we include MAD\u2019s split details.   Improving Grounding Performance with MAD Data.   We are interested in evaluating the contribution our data can   bring to the grounding task. We investigate the performance   of VLG-Net in the long-form grounding setup when the   5032   Dataset   Task   Videos   Annotations   Name   Train / Val / Test   Train / Val / Test   LSMDC16 [24]   Retrieval   155 / 12 / 17   101.1 K / 7.4 K / 10.1 K   LSMDC-G   Grounding   138 / 11 / 13   89.7 K / 6.7 K /   7.6 K   MAD   Grounding   488 / 50 / 112   280.5 K / 32.1 K / 72.0 K   Table 4.   Data split cheat-sheet.   This table clari\ufb01es the   data splits used in the following experiments (Table 5 and Table 6). LSMDC16 [24] is the original data collected for retrieval.   LSMDC-G is our adaptation to the grounding task. MAD is our   proposed dataset.   training data change. All trained models are evaluated on   the same test split: LSMDC-G test.   The \ufb01rst row of Table 5 shows the performance when   VLG-Net is exclusively trained on the LSMDC-G training   split. This set only contains data that was manually curated   in [24].   The second row is trained with 32% of MADtraining data, which is equivalent to the size of LSMDC-G   training split. We observe a drop in performance, which can   be associated with the presence of noise introduced by the   automatic annotation process in MAD. In the third row, we   use the complete MAD training set. Here, MAD\u2019s scale allows us to overcome the performance issues associated with   noisy data and improve performance to be comparable to   only using the clean LSMDC-G for training. Using 100% of   MAD data yields a relative improvement of 20% for R@5IoU=0.5. Then, we investigate whether the performance of   VLG-Net saturates given the amount of data available. To   this end, we use 100% of LSMDC-G training and gradually   augment it by adding MAD training samples. In these three   experiments (rows 4-6), the performance steadily increases.   These results suggest that current models for video grounding do bene\ufb01t from larger-scale datasets, even though the   automatically collected training data may be noisy. Moreover, designing scalable strategies for automatic dataset collection is crucial, as the drawbacks of noisy data can be offTraining Set   Testing Set   IoU=0.5   % LSMDC-G % MAD   LSMDC-G   R@1 R@5   R@10   100%   0%   Test   1.36   5.18   8.82   0%   32%   Test   0.60   2.60   5.11   0%   100%   Test   1.61   6.23   10.18   100%   32%   Test   2.18   6.63   10.73   100%   64%   Test   2.23   7.79   11.74   100%   100%   Test   2.82 8.74 13.36   Table 5.   Grounding performance with varying training   data.   We investigate VLG-Net [28] grounding performance on   LSMDC-G test, when different data regimens are used for training. This compares our automatically collected data (MAD training) against the manually curated one (LSMDC-G). We conclude   that expensive manual curation can be avoided if large scale data   is available.   Training Set   Testing Set   R@1 R@5 R@10   % LSMDC16 % MAD   LSMDC16   100%   0%   Test   20.9   39.4   48.5   0%   36%   Test   19.2   35.5   44.8   0%   100%   Test   20.5   38.8   48.7   100%   36%   Test   23.3   40.3   48.8   100%   72%   Test   23.6   41.4   49.3   100%   100%   Test   24.8   40.5   50.0   Table 6.   Retrieval performance on LSMDC16 with model   CLIP4Clip [14]. This experiment showcases how MAD data can   be valuable for a related task, beyond grounding.   set and even overcome with more training data.   Improving Retrieval Performance with MAD Data.   Adopting the same ablation procedure, we evaluate the possible contribution of our data in a related task, namely textto-video retrieval. Here, we format our MAD data similarly to LSMDC16, where short videos are trimmed around   the annotated timestamps.   For this experiment, we use   CLIP4Clip [14], a state-of-the-art architecture for the retrieval task, as the baseline. Table 6 reports the performance   when different amounts of data is used for training. Again,   we see that training with the whole LSMDC16 or MAD   leads to a very similar performance. Moreover, our previous argument also holds true in this task. Pouring more data   into the task boosts the performance, motivating the bene\ufb01t   of having a scalable dataset like MAD.   Takeaway. The MAD dataset is able to boost performance   in two closely related tasks, video grounding and text-tovideo retrieval, where we show that scale can compensate   for potential noise present due to automatic annotation.   6. Conclusion   The paper presents a new video grounding benchmark   called MAD, which builds on high-quality audio descriptions in movies. MAD alleviates the shortcomings of previous grounding datasets. Our automatic annotation pipeline   allowed us to collect the largest grounding dataset to date.   The experimental section provides baselines for the task   solution and highlights the challenging nature of the longform grounding task introduced by MAD. Our methodology   comes with two main hypotheses and limitations: (i) Noise   cannot be avoided but can be dealt with through scale.   (ii) Due to copyright constraints, MAD\u2019s videos will not be   publicly released. However, we will provide all necessary   features for our experiments\u2019 reproducibility and promote   future research in this direction.   Acknowledgments. This work was supported by the King   Abdullah University of Science and Technology (KAUST)   Of\ufb01ce of Sponsored Research through the Visual Computing Center (VCC) funding.   5033", "conf": "CVPR", "year": "2022", "index": 251}, {"title": "PATRON: Perspective-Aware Multitask Model for Referring Expression   Grounding Using Embodied Multimodal Cues   Md Mofijul Islam, Alexi Gladstone, Tariq Iqbal   School of Engineering and Applied Science, University of Virginia   {mi8uu,abg4br,tiqbal}@virginia.edu", "abstract": "Humans naturally use referring expressions with verbal utterances and nonverbal gestures to refer to objects and events.   As these referring expressions can be interpreted differently   from the speaker\u2019s or the observer\u2019s perspective, people effectively decide on the perspective in comprehending the expressions. However, existing models do not explicitly learn perspective grounding, which often causes the models to perform   poorly in understanding embodied referring expressions. To   make it exacerbate, these models are often trained on datasets   collected in non-embodied settings without nonverbal gestures and curated from an exocentric perspective. To address these issues, in this paper, we present a perspectiveaware multitask learning model, called PATRON, for relation   and object grounding tasks in embodied settings by utilizing verbal utterances and nonverbal cues. In PATRON, we   have developed a guided fusion approach, where a perspective grounding task guides the relation and object grounding   task. Through this approach, PATRON learns disentangled   task-specific and task-guidance representations, where taskguidance representations guide the extraction of salient multimodal features to ground the relation and object accurately.   Furthermore, we have curated a synthetic dataset of embodied   referring expressions with multimodal cues, called CAESARPRO. The experimental results suggest that PATRON outperforms the evaluated state-of-the-art visual-language models.   Additionally, the results indicate that learning to ground perspective helps machine learning models to improve the performance of the relation and object grounding task. Furthermore, the insights from the extensive experimental results   and the proposed dataset will enable researchers to evaluate   visual-language models\u2019 effectiveness in understanding referring expressions in other embodied settings.", "content": "Humans naturally use multimodal cues, such as verbal utterances and non-verbal signals (gazes and pointing gestures),   to refer to objects and events, known as referring expressions   (McNeill 2012; Arbib, Liebal, and Pika 2008; Liszkowski   et al. 2006, 2004; Tomasello 2010; Tang et al. 2020; Stacy   et al. 2020; Kratzer et al. 2020). In prior work, understanding   referring expressions has been generally modeled as grounding relations and objects in visual scenes using verbal utCopyright \u00a9 2023, Association for the Advancement of Artificial   Intelligence (www.aaai.org). All rights reserved.   terances, which is known as referring expression comprehension (REF) (Yang, Li, and Yu 2019; Yu et al. 2016; Kamath et al. 2021; Akula et al. 2021). These models are often   trained in non-embodied settings, where the visual scenes   contain objects but disregard human nonverbal signals. Consequently, these models cannot generalize well in comprehending real-world human interactions.   Several recent works have attempted to address the task   of comprehending referring expressions by incorporating   nonverbal gestures with verbal utterances in embodied settings (known as embodied referring expression comprehension (E-REF)) (Chen et al. 2021; Schauerte and Fink 2010).   However, some crucial issues remain unaddressed in these   recent works. Particularly, most embodied referring expression datasets only capture human interactions from an observer perspective with exo-centric views. People innately   use an understanding of perspective, which can be observed   in how humans interchangeably use perspectives from the   speaker and the observer when referring to objects during   interactions. For example, a person can refer to an object as   \u201cthe red lamp to the left of the black hat\u201d from the speaker\u2019s   perspective or \u201cthe red lamp to the right of the black hat\u201d   from the observer\u2019s perspective (Fig. 1). Thus, understanding perspectives can help a model to ground relations and   objects. However, the existing datasets do not contain data   from other perspectives (e.g., speaker, observer, neutral) and   visual views (e.g., exo, ego, top) to train such a model.   Recent works studied REF and E-REF by designing two   separate tasks: a relation grounding task (Goyal et al. 2020;   Dai, Zhang, and Lin 2017; Zhuang et al. 2017; Zhang et al.   2017) and an object grounding task (Chen et al. 2021;   Achlioptas et al. 2020; Lee et al. 2022; Kazemzadeh et al.   2014). In a non-embodied setting, the relation grounding   task is defined as determining whether a verbal utterance   appropriately describes the spatial relationships between objects in a visual scene. In an embodied setting, this relation   grounding task is defined as determining whether a verbal   utterance and nonverbal signals (gazes and pointing gestures) refer to the same object. The object grounding task   aims to identify a referred object using a verbal utterance   and nonverbal gestures. These tasks have many use-cases in   real-world interactions. For example, if a person verbally describes an object but nonverbally points to another object,   an AI-driven agent can identify these incoherent multimodal   The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)   971   Figure 1: Comprehending embodied referring expressions requires an understanding of the perspective, i.e., whether an object   is verbally described from the speaker\u2019s or observer\u2019s perspective. In these scenarios, nonverbal signals (gaze and pointing   gesture) can complement verbal utterance to ground an object (a & c). However, sometimes people verbally describe an object   and point to or gaze at another object (b & d). Thus, it is also crucial to ground relation for comprehending referring expressions.   cues, using the relation grounding task, and request clarification. In another case, if a person points to an object and asks,   \u201cwhat is the object to the right of the black hat?\u201d, then the   AI agent can use the object grounding task to identify the referred object (presented in Fig. 1). Thus, training models on   these two related tasks (relations and objects grounding) and   the previously mentioned perspective grounding task can enable achieve seamless human-AI interactions (HAI).   To address these challenges, we have developed a novel   perspective-aware multitask model, PATRON, for the relation and object grounding task using multimodal cues. In   PATRON, we have designed two cooperative tasks, one for   the perspective grounding (auxiliary task) and another for   the relation and object grounding (target task). In the auxiliary task module, PATRON learns disentangled representations (task-specific and task-guidance) to learn perspective grounding. In the target task module, PATRON uses our   proposed guided fusion approach that utilizes task-guidance   representations from the auxiliary task as prior information   to extract guided multimodal representations. PATRON uses   a self-attention-based fusion approach to extract supplementary target task-specific representations. Finally, PATRON   fuses task-guided and target task-specific disentangled representations to learn relation and object grounding.   Additionally, to overcome the shortcomings of the existing datasets, we have developed a dataset, called CAESARPRO, to train and evaluate models for comprehending embodied referring expressions. In CAESAR-PRO, each embodied referring expression is captured from three visual   views (ego, exo, and top), and the verbal utterances are   generated from three perspectives: speaker, observer, and   neutral. We have evaluated the performance of PATRON   and state-of-the-art visual-language models by applying on   the CAESAR-PRO dataset for perspective and relationobject grounding tasks. Our extensive experimental analysis suggests that perspective learning can improve the performance of visual-language models, including PATRON,   for the relation-object grounding task. Moreover, our proposed perspective-aware guided fusion approach helps PATRON to outperform all the evaluated models by achieving the highest accuracy of 74.13% and 81.15% in relationobject and perspective grounding tasks, respectively. Moreover, our ablation study indicates that disentangling multitask representations can help extract salient multimodal   features and significantly improve the performance of the   relation-object grounding task. Our proposed perspectiveaware E-REF model, the dataset, and the insights from our   studies open new research directions in HAI.   Related Work   Embodied Referring Expression Comprehension:   Several datasets and models have been developed for REF (Mao   et al. 2016; Liu et al. 2019; Viethen and Dale 2008). For   example, Kazemzadeh et al. (2014) developed a dataset of   referring expressions to ground objects in photographs of   natural scenes. Lee et al. (2022) curated another dataset   to develop model for comprehending referring expressions   through visual question-answering. One of the crucial limitations of these datasets is that the data samples are curated   in non-embodied settings, where human presence and nonverbal gestures are not considered in referring expressions.   As a result, the models trained on these datasets cannot perceive the nonverbal cues that are often necessary for E-REF.   Recently, a few datasets have been developed for E-REF   with verbal utterances and nonverbal gestures. For example,   Chen et al. (2021) developed a dataset of embodied referring   expressions where a human refers to an object using a verbal utterance and nonverbal gestures. However, one of the   crucial limitations of this dataset is that the data samples are   captured solely from an exocentric perspective. Thus, models trained on these datasets will be biased towards an exocentric perspective and will not comprehend embodied referring expressions from different perspectives.   Multimodal Representation Learning:   Several multimodal representation learning models have been proposed   for various tasks, such as human activity recognition (Islam and Iqbal 2020, 2021; Samyoun* et al. 2022; Islam,   Yasar, and Iqbal 2022; Feichtenhofer et al. 2019), motion   prediction (Yasar*, Islam*, and Iqbal 2022; Yasar and Iqbal   2021), visual-question answering (Lu et al. 2019; Li et al.   2019), and referring expression comprehension (Yu et al.   2016; Mao et al. 2016). Existing models for REF predominately use similar cross-attention or self-attention methods   to fuse multimodal representations (Goyal et al. 2020; Lee   et al. 2022). However, due to lack of perspective diversity   and nonverbal gestures, these models do not explicitly learn   the perspective taking and the understanding of human nonverbal interaction necessary to comprehend REF.   972   Multitask Learning:   Several multitask models have been   designed to learn multiple tasks (Ruder 2017; Hashimoto   et al. 2016; Zhang and Yang 2017; Guo et al. 2018; Gagn\u00b4e   2019; Zhou et al. 2020a). The aim of designing these models is two-fold: maximizing shared representations among   the tasks and compressing the size of models by maximizing shared learnable parameters across tasks (Ruder 2017;   Xu et al. 2018; Zhou et al. 2020b; Achille et al. 2019; Zamir   et al. 2018). Moreover, since tasks in these models are occasionally independent and competitive, training approaches   can determine which tasks should be learned together (Guo,   Lee, and Ulbricht 2020; Jang, Gu, and Poole 2016), further   improving model performance. For example, Standley et al.   (2020) uses cooperative and competitive task relationships   to optimize the shared representations and improve the performance of multiple tasks. A few works use cooperative relationships among tasks to fuse multimodal representations.   For example, Islam and Iqbal (2022) uses activity-group information (auxiliary task) to fuse representations for activity recognition (target task), where the classes of the activity recognition task can be directly mapped to the classes   of the activity-group recognition task. However, if there is   no direct mapping of classes between the tasks, then using   the auxiliary task\u2019s representation to fuse representations for   target tasks can degrade the overall tasks\u2019 performance.   PATRON: Perspective-aware Multitask Model   In PATRON, we have designed two tasks: an auxiliary task   (perspective grounding) and a target task (relations and objects grounding). We combine relation and object grounding   task in a single task (relation-object grounding), where the   models identify the referred object if the verbal and nonverbal cues refer to the same object; otherwise, it will report a   failed condition. In PATRON, the auxiliary task learns disentangled representations, auxiliary task-specific and taskguidance, where task-guidance representations are used to   guide the target task to extract complementary representations. PATRON also learns disentangled representations for   target tasks, task-guided and target task-specific, where taskguided representations are learned using task-guidance representations from the auxiliary task. In the following subsections, we present different modules of PATRON.   Unimodal Feature Encoders   PATRON uses modality-specific encoders to encode data   from visual and verbal modalities. Visual modalities capture   nonverbal gestures in three image views (Xego, Xexo, and   Xtop). Verbal utterances (Xverbal) refer to an object from a   perspective (ego, exo, and neutral). As different modalities   have different feature characteristics, PATRON uses separate encoders to encode visual and verbal modalities. This   architecture design enables PATRON to utilize state-of-theart models (Fm) to extract salient unimodal representations   (Em). In our implementation of PATRON, we use ResNet   and DistilBERT to extract unimodal representations:   Em = Fm(Xm)   ,   m \u2208(ego, exo, top, verbal) (1)   Figure 2: PATRON: Perspective-aware Multitask Learning   Model. PATRON learns disentangled representations (i.e.,   auxiliary task-specific and task-guidance representations)   for the auxiliary task (perspective grounding) and disentangled representations (i.e., task-guided and target taskspecific) for the target task (relation and object grounding).   Here, the proposed guided fusion approach extracts the taskguided representations using the task-guidance representations as prior information from the auxiliary task.   Here, Em \u2208R(B\u00d7Dm), B is the batch size, and Dm is the   representation dimension of modality m.   Auxiliary Task Module   In PATRON, the auxiliary task module extracts task-specific   and task-guidance disentangled representations from unimodal representations Eu = (Eego, Eexo, Etop, Everbal) (u   indicates for unimodal). These disentangled representations   are used together to learn perspective grounding, whereas   task-guidance representations are also used to guide the target task module to extract perspective-aware complementary   representations for relations and objects grounding.   Auxiliary Task-Specific Representation Learning:   In   PATRON, we have designed a guided fusion approach to   fuse unimodal representations. In the auxiliary task module,   PATRON uses verbal representation (Everbal) as queries   to fuse visual modalities (Evisual = (Eego, Eexo, Etop))   and produce task-specific representations. At first, PATRON   projects (Everbal) to produce queries (Q = EverbalW Q)   and projects Evisual to produce key (K = EvisualW K) and   value (V = EvisualW V ) representations. Here, W Q, W K,   and W V are learnable parameters. Finally, queries are used   to extract multimodal representations from keys and values   in the following way:   E   \u2032   =   \u03c3   \u0012QKT   \u221a   Du   \u0013   V   (2)   Eaux   task specific   =   W oE   \u2032   (3)   Du is the unimodal representation dimension and W o is a   973   learnable parameter. As we also use guided fusion approach   in the target task module, we can summarize this as,   Eaux   task specific = Guided Fusion(Query, Eu)   (4)   Task-Guidance   Representation   Learning:   PATRON   uses self-attention approaches to fuse unimodal representations   and   extract   task-guidance   representations   (Eaux   task guidance), which is disentangled from Eaux   task specific:   Eaux   task guidance = Self Attn(Eu) =   X   m\u2208M   \u03b1mEm   (5)   \u03b1m =   exp(\u03b2m)   P   m\u2208M   exp(\u03b2m)   ,   m \u2208M   (6)   \u03b2m = (W aux)T Em   ,   m \u2208M   (7)   Here, M is the modality list (ego, exo, top, verbal), W aux is   a learnable parameter, and \u03b1m is the attention weights.   Perspective Grounding Task:   PATRON fuses disentangled representations ([Eaux   task specific; Eaux   task guidance]) using   a self-attention approach (Self Attn: Eq. 5) to learn perspective. PATRON uses Eaux   task guidance to learn perspective   for ensuring that it contains perspective-aware information,   which PATRON uses in the target task module:   Eaux   fused = Self Attn([Eaux   task specific; Eaux   task guidance]) (8)   yP = FP erspective(Eaux   fused)   (9)   Here, Fperspective is a multi-layer perceptron (MLP).   Target Task Module   In PATRON, the auxiliary task module (perspective grounding) guides the target task module to extract salient multimodal representations for grounding relations and objects.   PATRON uses Guided and Self Fusion modules to extract   representations for target task learning.   Task-Guided Representation Learning:   PATRON uses   our Guided Fusion approach (Section: Task-Specific Representation Learning and Eq.4), to fuse unimodal representations (Eu). In the target task module, the guided fusion approach aims to extract perspective-aware multimodal   representations that can be used for grounding relations   and objects. PATRON utilizes the guidance representations   (Eaux   task guidance) from the auxiliary task module as prior   information to extract salient multimodal representations:   Etarget   guided = Guided Fusion(Eaux   task guidance, Eu)   Target   Task-Specific   Representation   Learning:   Although a guided fusion approach helps PATRON to extract perspective-aware representations (Etarget   guided), verbal   and visual modalities can provide additional information   to Etarget   guided. PATRON uses Self Attn (described in Section Task-Guidance Representation Learning and Eq. 5)   to extract supplementary representations for relation-object   grounding: Etarget   task specific = Self Attn(Eu).   Relation-Object Grounding Task:   PATRON grounds relations and objects together. PATRON identifies the target   object that is referred to by multimodal cues - verbal utterances and nonverbal gestures (gazes and pointing gestures). If the verbal utterance and nonverbal gestures refer to two different objects, then the model should identify these inconsistencies (invalid embodied referring relations) and should not ground any objects. To accomplish   this, PATRON fuses guided representations (Etarget   guided) and   target task-specific representations (Etarget   task specific) through   a self-attention approach (Self Attn: Eq. 5):   Etarget   fused = Self Attn([Etarget   task guided; Etarget   task specific]) (10)   yOR = FOR(Etarget   fused )   (11)   Here, FOR is a MLP to learn target task.   Multitask Learning   We use a multitask learning loss to train PATRON   for jointly learning auxiliary (perspective grounding) and   target tasks (relations and objects grounding). We use   cross-entropy to calculate the loss for auxiliary task   (LP (yP , \u02c6yP ) =   1   B   PB   i=1 y(P,i) log \u02c6y(P,i)) and target task   (LRO(yRO, \u02c6yRO) =   1   B   PB   i=1 y(RO,i) log \u02c6y(RO,i)). These   losses are used to calculate the multitask loss (Lmultitask =   \u03b3pLP +\u03b3ROLRO), where, \u03b3p and \u03b3OR are task loss weights.   LP helps to learn perspective-aware representations for   grounding the perspective. LP is also used to learn disentangled representation (Eaux   task guidance) for guiding the target   task to learn perspective-aware multimodal representations.   CAESAR-PRO Dataset   We have used an embodied simulator, CAESAR (Islam,   Gladstone, and Iqbal 2022) to develop a dataset of embodied   referral expression. CAESAR allows to automatically generate datasets and synthesizing human gaze and gestures from   multiple perspectives (ego, exo, and top). Moreover, CAESAR can generate contrastive situations where the person   verbally and nonverbally referring two different objects.   We have developed an additional embodied environment   in CAESAR, called a shelf environment, where various objects are located on a shelf (Fig. 1-right), whereas the original CAESAR simulator contains only a table-top environment (Fig. 1-left). These two environments allow us to generate diverse data samples with more spatial relations, such   as above and below, enabling the model to understand spatial   relations in three dimensions. Moreover, due to the locations   of cameras in the shelf environment, the observer\u2019s point of   view differs from the table-top environment (Fig. 1), where   the observer is always placed in front of the speaker.   Dataset Generation   To accomplish a realistic and sufficiently variable synthesis of human gaze and pointing gestures, we have used   the CAESAR simulator, which uses a gesture synthesis algorithm on real-world data collected using a motion capture system. CAESAR uses inverse kinematics applied to   974   Figure 3: A visualization of referred object locations from   different views in the table-top environment is presented   here. These locations indicate that the CAESAR-PRO   dataset has little to no bias toward object locations in visual   scenes and is evenly distributed for a given left and right   spatial relations in verbal utterances.   both the chest and head of the human to generate gestures. To construct verbal referring expressions, we used   several templates from CAESAR. We have also included   additional spatial relations, such as the above and below   relation, which allows for generating more diverse data   samples and training models to learn 3D spatial relations.   The general structure of these templates: <referred object   location><referred object properties><spatial relation><   reference object location><reference object properties>.   We have varied this template structure and created eight   unique sub-templates. Additionally, we have varied the object names, colors, sizes, locations, and spatial relations to   generate diverse verbal expressions to identify one of up to   ten objects in a scene from multiple perspectives. These templates are further described in the supplementary document.   Dataset Analyses   The CAESAR-PRO dataset consists of 128, 100 samples,   with a train (79, 431), validation (24, 597), and test split   (24, 072). The CAESAR-PRO dataset is composed of   229, 036 images, which are mixed with different verbal expressions from multiple perspectives. Images are rendered at   a resolution of (480\u00d7320) pixels, and an object library of 61   objects is used. We randomly sampled 10 objects which are   used as referred objects. Each data sample consists of RGB   images, skeletal images, depth map images, and object segmentation mask images for three camera views and a verbal   utterance. We also generated task labels for perspective, relation, and object grounding tasks.   We aim to generate a dataset that is not biased to object   locations and spatial relations. For example, the term \u201con   the left\u201d always refers to objects on the left side of the scene   from the observer\u2019s perspective would cause trained models to bias towards only using verbal cues, resulting in a   model not being aware of the perspective-taking necessary   to ground real-world referring expressions with verbal and   nonverbal signals. In Fig. 3, it is evident that the object locations in CAESAR-PRO (table-top environment) are not tied   to the spatial relations in the verbal utterances. These diverse data ensure that models use nonverbal cues to ground   objects rather than solely relying on verbal cues. We present   additional dataset analysis in the supplementary document.   Experimental Setup   We have compared the performance of PATRON by comparing its performance against the following models for the   perspective and relation-object grounding tasks: MuMu (Islam and Iqbal 2022), VisualBERT (Li et al. 2019), CLIP   (Radford et al. 2021), Dual-Encoder (similar to Zhai et al.   (2022)), and Late-Fusion (similar to Islam and Iqbal (2020)).   The evaluated models produce visual-language (VL) representations from a single visual image and verbal utterance. We extend these models to process multiple visual   views (ego, exo, and top) and a verbal utterance. For VisualBERT, we extract visual representations of multiple views   using ResNet-101 and pass these representations with a verbal utterance to learn VL representations. For the CLIP and   Dual-Encoder models, we pair the verbal utterance to each   visual view and pass each visual-verbal pair through the   model to extract VL representations, which are later concatenated. For the Late-Fusion and MuMu models, we extract visual and verbal representations using ResNet-101   and DistillBERT (Sanh et al. 2019), respectively. Late Fusion model fuses these representations using the Multi-Head   Self-Attention approach (Vaswani et al. 2017), whereas   MuMu uses a guided fusion approach.   We have evaluated these models\u2019 performances by applying on the CAESAR-PRO dataset. As some classes contain more data samples than others, we used macro-accuracy   metrics to evaluate perspective, object, and relation grounding tasks. We trained each model for eight epochs with a   learning rate set to 1e\u22125 in a distributed cluster environment   with eight A100 GPUs in each cluster node. We train all   the models using Pytorch-lightning environment with a fixed   seed to ensure reproducibility. For more implementation details, please check the supplementary materials.   Results and Discussion   Comparison of Multitask Learning Approaches   We evaluated the performance of PATRON and other models by applying on the CAESAR-PRO dataset in single and   multitask learning settings. In these experiments, a model   takes multiple visual views (ego, exo, and top) and a verbal utterance from multiple perspectives (speaker, observer,   and neutral) to learn two tasks: (i) perspective grounding   task and (ii) relation-object grounding task. In the multitask model, we chose either perspective or relation-object   grounding task as the auxiliary task (the first task in the   model architecture) and another task as the target task (the   second task in the model architecture). For example, in Task   Order I, we chose perspective grounding as the auxiliary   task and relation-object grounding as the target task. In Task   Order II, we chose relation-object grounding as the auxiliary task and perspective grounding as the target task. We   have also evaluated state-of-the-art visual-language models   in single-task learning settings, where we trained perspective   and relation-object grounding tasks using two separate models. We did not evaluate MuMu and PATRON in single-task   learning settings, as these models are designed for multitask   learning. We present the results of single task and multitask   models in Table 1 (a) & (b), respectively.   975   (a) Single task models trained separately   Models   Perspective (Pers.) Relation-Object (RO)   Late Fusion   74.90   65.50   Dual Encoder   77.87   54.47   CLIP   71.23   65.26   VisualBERT   77.20   66.53   (b) Multitask models with different task order in model   Models   Task Order I   Task Order II   Pers   \u2192   RO   RO   \u2192   Pers   Late Fusion   72.30   61.80   65.40   75.12   Dual Encoder   75.67   64.99   43.66   75.77   CLIP   74.52   68.14   56.82   73.02   VisualBERT   74.52   65.90   62.15   69.44   MuMu   73.65   67.48   63.22   75.27   PATRON   79.85   74.13   67.63   81.15   Table 1: Top-1 macro accuracy of various models of perspective and relation-object grounding tasks.   Results:   The results in Table 1 suggest that PATRON outperforms all the single and multitask models for grounding   perspective and relation-object tasks by achieving 81.15%   and 74.13% in macro-accuracy, respectively. Among the   other visual-language multitask models, CLIP and Dual Encoder achieve the next highest accuracy for relation-object   and perspective grounding tasks by achieving 68.14% and   75.77%, respectively. However, among the single task models, VisualBERT and Dual Encoder achieve the next highest   accuracy for relation-object and perspective grounding tasks   by achieving 66.53% and 77.87%, respectively.   Discussion:   The results in Table 1 indicate that for both   Task Orders (I & II), the performance of PATRON improves   compared to the single and multitask models. Although   MuMu uses a guided fusion approach and outperforms single task models for relation-object grounding, it fails to outperform PATRON. However, when considering the task order, some multitask models show improved results compared to their single task models. For example, when Task   Order I was considered, the CLIP model showed better accuracy than its single task counterpart for both grounding   tasks. Similarly, for Task Order II, the CLIP model showed   improved performance for the perspective grounding task;   however, the performance degrades for the relation-object   grounding task compared to the single task model. One can   also observe performance degradation of several models in   some multitask settings compared to single task settings.   For example, the accuracy of the perspective grounding task   degrades for the Dual Encoder and VisualBERT models,   whereas the accuracy of the relation-object grounding task   degrades for the Late Fusion and the VisualBERT models.   The reasoning behind the performance degradation of the   multitask models compared to their single-task counterparts   is that the baseline models try to learn a shared representation for all tasks in the multitask setting. As multiple tasks   compete to maximize their task-specific representations, a   shared representation can discard salient representations of   Models   Non-Embodied   Embodied   V   V+NH   V+G   V+P   V+G+P   BERT   26.44   Late Fusion   56.33   54.91   55.30   61.80   Dual Encoder   51.53   53.51   56.93   64.99   CLIP   52.63   57.38   60.67   68.14   VisualBERT   54.45   58.87   57.05   65.90   PATRON   54.24   65.24   66.65   74.13   Table 2: Impact of nonverbal signals (gaze and pointing gesture) on the performance (Top-1 macro accuracy) of the multitask models in the relation and object grounding task. The   results suggest that nonverbal signals improve the performance of the models. (V: Verbal, NH: Visual without Human, G: Gaze, P: Pointing Gesture).   individual tasks. On the other hand, PATRON extracts taskspecific and task-guidance disentangled representations. In   this process, PATRON uses the task-guidance representations to guide other tasks using our proposed guided fusion   approach to extract salient multimodal representations. In   the same way, PATRON also learns to extract disentangle   representations for the target task and trains these tasks cooperatively, whereas most of the other models train these   tasks independently. These findings indicate that a multitask model can improve the tasks\u2019 performance if the model   can disentangle visual-language representations while training the model in a cooperative learning setting, where one   task can guide the learning of other tasks.   Impact of Nonverbal Gestures   We aim to investigate how nonverbal cues impact the performance of the models in the relation-object grounding   task. We have conducted this analysis in different settings   by varying nonverbal gestures: two non-embodied settings   (only verbal (no visual), verbal + visual (scenes without human)), and three embodied settings (verbal + gaze, verbal   + pointing gesture, and verbal + gaze + pointing gesture).   We trained the models in a multitask learning setting (auxiliary task: prospective grounding, target task: relation-object   grounding). We used visual scenes captured from multiple   views (ego, exo, and top) and multiple verbal perspectives   (speaker, observer, and neutral) to train the models. Table 2   shows the top-1 macro accuracy of the target task.   Results and Discussion:   The results in Table 2 suggest   that PATRON outperforms all the baseline models in all the   evaluated settings for the target task (achieving the highest accuracy of 74.13%). The results also indicate that PATRON achieves the highest accuracy when both gaze and   pointing gestures were used, compared to when only gaze   or only pointing gestures were used in the embodied setting,   and only verbal + visual (scenes without humans) were used   in the non-embodied setting. Similarly, other baseline models\u2019 performances were also improved when nonverbal cues   were used compared to the same model trained with a partial   set of nonverbal cues or without any nonverbal cues. Additionally, when only verbal utterances were used, without   976   Models   Training Perspectives   Speaker   Observer   Neutral   All   Late Fusion   60.42   53.71   60.53   61.80   Dual Encoder   59.43   45.23   57.95   64.99   CLIP   62.36   58.04   60.99   68.14   VisualBERT   55.71   43.46   49.68   65.90   PATRON   60.36   47.23   57.85   74.13   Table 3: Top-1 macro accuracy of the multitask learning   models when trained on data samples from single and multiple verbal perspectives and tested on data samples from   multiple visual and verbal perspectives.   visual scene (i.e., BERT model), the model achieved only   26.44% accuracy. As the dataset contains verbal expressions   that can be interpreted differently from different perspectives, nonverbal gestures can help the models disambiguate   and accurately perform the relation-object grounding task.   These findings suggest that using nonverbal gestures can improve a model\u2019s performance in comprehending E-REF.   Importance of Multi-Perspectives   Here, we investigate how varying verbal perspectives   (speaker, observer, and neutral) can impact the performance   of the models. We trained PATRON and baseline models   on the CAESAR-PRO dataset by varying the verbal perspectives while utilizing all the visual views (ego, exo, and   top). During testing, we used all the verbal perspectives and   visual views. These models are trained in a multiple-task   learning setting (auxiliary task: prospective grounding, target task: relation-object grounding). We have reported the   top-1 macro accuracy of the target task in Table 3.   Results and Discussion:   The results in Table 3 suggest   that all the models demonstrated the highest performance   in comprehending E-REF when the models were trained   utilizing the data with all the perspectives. For example,   training PATRON on multiple perspectives improves the   performance of relation-object grounding tasks (achieved   74.13% accuracy) compared to training the same model only   on a single perspective. Baseline models also gain similar   performance improvement when training the models with   data from multiple perspectives. These findings indicate that   training models on data samples from multi-perspective can   help the models to comprehend E-REF more accurately.   Ablation Study and Significance Analysis   We have conducted ablation studies to evaluate whether   our proposed disentangle representation-based guided fusion approach can significantly improve the performance of   the relation-object grounding task. We evaluated PATRON   and the baseline models on our CAESAR-PRO dataset in the   multitask setting (auxiliary task: prospective grounding, target task: relation-object grounding). These models disentangle representations for auxiliary task (task-specific and taskguidance) and target task (task-guided and task-specific). We   have conducted a significance analysis (\u03b1 = 0.05) by evaluating these models five times with different parameters iniModels   Auxiliary   Task   Target   Task   Guided   Fusion   Acc.   Std.   Dev.   Significant   Over \u00a7   M1   \u2717   \u2717   \u2717   61.38   0.97   None   M2: MuMu   \u2717   \u2717   \u2713   64.21   2.27   M1   M3   \u2717   \u2713   \u2713   64.25   1.07   M1   M4   \u2713   \u2717   \u2713   70.38   0.72   M1-3   PATRON   \u2713   \u2713   \u2713   74.09   0.56   M1-4   Table 4: The results (Top-1 macro accuracy) of the ablation   study, where various components of the model are evaluated   on the relation-object grounding task. The results of five runs   with different initial parameters are presented. \u2713and \u2717denote whether a task learns disentangled representations or   not, respectively. \u00a7 Significance analysis at level \u03b1 = 0.05.   tialization (Following Dror, Shlomov, and Reichart (2019)).   The results are presented in Table 4.   Results and Discussion:   The results in Table 4 suggest   that the models with guided fusion can improve the performance of relation-object grounding tasks compared to   the model that does not use guided fusion. For example,   MuMu (M2) can improve the performance of relation-object   grounding by 2.83% compared to a model which does not   use guide fusion (M1). Additionally, the models can significantly improve performance if they can disentangle the   representation for auxiliary and target tasks (e.g., M3, M4,   and PATRON) compared to the models that cannot (e.g.,   M1). For example, PATRON improves the performance of   relation-object grounding tasks by 12.71% by disentangling   multiple task representations and using these representations   in the guided fusion approach compared to M1. The reasoning behind this significant performance improvement is that   learning disentangled representations allows these models to   learn task-specific and task-guidance salient representations,   which can be used to guide other tasks. On the other hand,   models learning non-disentangle representations need to use   the same representations for task learning and guiding other   tasks. Consequently, the shared representations neglect taskspecific salient representation for learning generalized representations for all tasks and degrade the task performance.   Conclusion   We developed a perspective-aware multitask learning model,   PATRON, for comprehending referring expressions in embodied settings. We also curated a dataset of embodied referring expressions, CAESAR-PRO, to develop and evaluate   learning models. Our extensive experimental results suggest   that our perspective-aware guided fusion approach can extract salient multimodal representations for relation and object grounding. Additionally, the results provide valuable insights into developing robust learning models, such as the   effects of the order of the tasks, non-verbal cues, verbal   perspective, and disentanglement of representations. Finally,   we believe the proposed perspective-aware learning model   and dataset will be useful in other embodied tasks, such   as embodied question answering, embodied navigation, and   conversational human-AI interactions.   977", "conf": "AAAI", "year": "2023", "index": 614}, {"title": "UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding   Dave Zhenyu Chen1   Ronghang Hu2   Xinlei Chen2   Matthias Nie\u00dfner1   Angel X. Chang3   1Technical University of Munich   2Meta AI   3Simon Fraser University   Figure 1: We present UniT3D, a unified transformer for 3D dense captioning and visual grounding. UniT3D is pre-trained   with both bidirectional and seq-to-seq objectives on point-cloud-text pairs. Afterwards, it is further fine-tuned for 3D visual   grounding and dense captioning. We show that our proposed architecture and pre-training scheme largely improve the   performance on both downstream tasks.", "abstract": "Performing 3D dense captioning and visual grounding   requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with   highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them   simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture   for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint   pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows   expanding the pre-training scope to more various training   sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.   1.", "content": "The 3D vision-language field has been drawing increasing research interest in jointly understanding 3D scenes [11,   34, 26, 35, 17, 14, 36, 18, 9] and natural language [41, 13,   28, 46, 2], such as 3D visual grounding [5] and 3D dense   captioning [8]. The task of 3D visual grounding takes as   input a point-cloud-text pair and outputs a bounding box of   the referred object. As its sibling task, 3D dense captioning   expects a point cloud as input and densely generates object   bounding boxes and descriptions in the scene. Both tasks   enable applications such as assistive robots and natural language control in AR/VR systems.   Although these two 3D vision-language tasks are naturally complementary to each other, previous attempts to   connect them [6, 3] only exploit partly shared object spatial   information, leaving the joint nature between object relationships and textual semantics underdeveloped. More concretely, to localize the target object in the scene, 3D visual   This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   18109   grounding methods require a joint understanding of object   attributes and spatial relationships. This requirement persists for 3D dense captioning when densely describing the   appearance and spatial aspects of the objects. Therefore,   it is naturally desirable to develop a shared representation   between two tasks with a unified task-agnostic framework,   where the two input modalities are jointly encoded and enhanced. Moreover, such generic multimodal representation   is not only beneficial for sharing knowledge between visual   grounding and dense captioning, but could also enable 3D   vision-language research beyond specific domains, as in the   case of VisualBERT [27] or CLIP [38] on 2D images.   To this end, we propose UniT3D, a joint transformerbased solution to facilitate vision-language representation   learning for 3D visual grounding and dense captioning, as   illustrated in Fig. 1. Unlike prior works with two distinct   and task-specific neural modules [3, 6], our method tackles   the tasks of 3D dense captioning and visual grounding via   a task-agnostic unified transformer with light-weight output   heads. To enable joint vision-language representation learning, we design a supervised training scheme that combines   the bidirectional objective with query-aware object matching supervision and the seq-to-seq objective with objectaware sequence generation supervision.   Through finetuning on specific tasks with corresponding output heads,   the joint vision-language representation learned by our taskagnostic multimodal transformer is well capable of supporting both the localization of the referred objects in the scenes   and the dense generation of object descriptions.   One challenge of the joint 3D vision-language representation learning is that existing 3D vision-language   datasets [5, 1] are relatively limited in size and variety compared to their 2D counterparts such as MSCOCO [7] and   Conceptual Captions [39]. To address this challenge, we   build a large-scale 3D vision-language dataset with text annotations generated from an image captioner learned on   abundant 2D image and text datasets. Concretely, we apply   the image captioner from Mokady et al. [32] to ScanNet images to obtain synthetic image-text pairs and convert them   to point-cloud-text pairs by cropping the reconstructed point   clouds in ScanNet within the image frustum using camera   parameters as our synthetic 3D vision-language data. We   show that along with jointly pre-training on such synthesized data with the proposed bidirectional and seq-to-seq   objectives, our UniT3D model obtains significant performance gains on the downstream 3D vision-language tasks.   To summarize, our contributions are threefold:   \u2022 We introduce a multimodal transformer architecture to   solve 3D visual grounding and dense captioning in a   fully unified fashion.   \u2022 We propose a supervised joint pre-training scheme   with bidirectional and seq-to-seq objectives to facilitate multimodal feature learning.   \u2022 We construct a large-scale synthetic point-cloud-text   dataset, showing that the distilled 2D prior knowledge   is beneficial to 3D vision-language tasks.   2. Related work   3D visual grounding and dense captioning.   Recently,   there has been a thriving research interest in 3D visionlanguage [5, 1]. Seminal works [5, 1] concurrently propose   ScanRefer and ReferIt3D dataset consisting of free-form   descriptions of 3D real-world object from ScanNet [12]   scenes.   Chen et al. [5] propose the 3D visual grounding   task to localize a target object in a 3D environment using   text queries. The majority of the previous work on 3D visual grounding [5, 1, 20, 47, 50, 3, 6] concentrate on distinguishing the multimodal relationships among objects and   text queries. As its reversed task, the task of 3D dense captioning predicts the bounding boxes and the associated descriptions for all objects in the input 3D scene [8]. Similar to 3D visual grounding, most prior works in this track   [8, 48, 24] learn the multimodal relationships among the   objects in the 3D scene and decode them as text outputs.   We observe that these two tasks are complementary in   nature, with minor discrepancies in their outputs (boxes vs.   descriptions). There are several initial attempts in bridging   them together. Chen et al. [6] apply a speaker-listener architecture to self-critically improve the overall performance   of visual grounding and dense captioning. However, there is   no shared representation between the visual and text modalities. Cai et al. [3] link both tasks with a shared detection backbone attached to two task-specific neural modules. Despite the shared object relationships within the visual modality, the multimodal representations between objects and text inputs are not explicitly modeled. In contrast,   our method directly builds multimodal representations with   a joint multimodal fusion transformer, enabling both visual grounding and dense captioning prediction from shared   multimodal knowledge.   Vision-language pre-training. In light of the shared nature of vision-language tasks, developing a joint multimodal   representation between the vision and language modalities   has become a popular research domain in the 2D visionlanguage community [27, 30, 51, 44, 19, 40, 43]. Inspired   by the masked language modeling in Devlin et al. [13], seminal works such as VisualBERT [27] and ViLBERT [30] introduce the masked vision-language modeling scheme for   learning the multimodal representation. However, such pretraining schemes only concentrate on multimodal encoding, but neglect multimodal decoding, which is essential for   generation-based tasks (e.g. image captioning). Based on   masked vision-language modeling, Zhou et al. [51] design   the bidirectional and seq-to-seq pre-training objectives to   18110   (a) Training flow for 3D visual grounding   (b) Training flow for 3D dense captioning   Figure 2: Task-specific training schemes for UniT3D. The input point cloud is fed into a PointGroup [23] detection backbone   to generate box tokens {hi   v}. Parallelly, the text query is processed with a pre-trained BERT [13] to produce text tokens {hi   t}.   For visual grounding, as shown in 2a, the multimodal fusion module takes in the concatenated box and text tokens to produce   the fused box tokens {hi   mv}. Then, a lightweight grounding head predicts the grounding confidence scores for each proposal.   Similarly, for dense captioning, the current target box token from PointGroup (such as h2   v) is added to all BERT outputs as   the captioning cue. The captioning head then takes in the fused text tokens {hi   mt} from the multimodal fusion module, and   predicts the next tokens in the input sequence, as shown in 2b. Note that a multimodal triangular masking scheme is applied   here to enforce the left-to-right sequence modeling.   empower both multimodal encoding and decoding. Hu and   Singh [19] further expand the spectrum of the applicable   downstream tasks using the learned multimodal representation. However, due to the limited amount of existing visionlanguage data in 3D, such pre-training strategies entangled   with large transformer architectures cannot be directly migrated to the 3D vision-language domain. Hence, We propose a synthetic vision-language data generation scheme   featuring the distilled 2D multimodal knowledge to facilitate the multimodal pre-training in the 3D domain.   3D vision with 2D priors. Leveraging 2D distilled knowledge such as CLIP [38] for 3D vision is a trending research topic. PointCLIP [49] transfers 2D knowledge to   3D by conducting alignments between the CLIP-encoded   point cloud image and category texts. This scheme enables   zero-shot classification on point cloud without training on   any 3D datasets such as ShapeNet [4]. CLIP2Point [21]   aligns depth image features to CLIP-encoded point cloud   image features to facilitate image-depth pre-training for   3D shape understanding. SemAbs [15] extracts relevancy   maps from CLIP for open-vocabulary 3D scene understanding and CLIP-NeRF [42] leverages CLIP embedding space   for 3D scene manipulation. More advanced methods utilize pixel-to-point correspondence [29, 16] or kernel inflation technique [45] to tackle 3D shape/scene understanding tasks leveraging 2D distilled knowledge. In this work,   we propose to use 2D vision-language priors to address the   limited scale and variety of existing 3D data, enabling multimodal representation learning for 3D vision-language.   3. Method   Our architecture stem consists of three main opponents   as shown in Fig. 2: a PointGroup detection backbone, a pretrained BERT [13] encoder, and a transformer-based multimodal fusion module. Given a point-cloud-text pair as input, PointGroup takes in the point cloud P \u2208RN\u00d7(3+K) to   produce a sequence of M box tokens {h1   v, ..., hi   v, ..., hM   v }.   We follow Chen et al. [5] to construct the auxiliary point   features with multi-view features and point normals (k =   134).   In the meantime, the BERT encoder encodes the   input text of length L into a sequence of text tokens   {h1   t, ..., hj   t, ..., hL   t }. We use [CLS] and [SEP] as the start   and end tokens of the input text in the BERT encoder, and   apply a triangular mask to enforce the left-to-right sequence   modeling. We also add a visual context token (from a global   box for grounding or a target box for captioning) to every   text token hj   t. Finally, the multimodal fusion module enriches the concatenated box-text input sequence and produces a sequence of fused tokens as output.   3.1. A unified model with task-specific objectives   3D visual grounding with bidirectional objective.   As   shown in Fig. 2a, the input point cloud is processed by   PointGroup to produce the instance masks and the box to18111   kens {h1   v, ..., hM   v }. In the meantime, an object text query   is fed into BERT to produce the text tokens {h1   t, ..., hL   t }.   Here, we average all box tokens as the global box token \u02c6hv   and add it to all text tokens as the global visual cue. To   be consistent with the captioning task, we constantly apply a triangular mask to enforce the left-to-right modeling.   Then, the box tokens are concatenated with the text tokens   and fed into the multimodal fusion module to acquire the   fused multimodal features. We apply a bidirectional mask   in the multimodal fusion module to enable the sequence encoding in both directions. Afterwards, we feed the fused   box tokens {h1   mv, ..., hM   mv} to the lightweight grounding   head to predict the grounding confidence scores for each   object proposal. The box with the highest grounding confidence will be taken as the grounding output. Here, we   use a conventional cross-entropy loss LG for supervision,   where the object proposal with the highest IoU with the   GT queried box is treated as training GT. Inspired by Chen   et al. [5], we also attach a lightweight MLP taking the encoded [CLS] token as input to predict the object semantic   class from the text query. We supervise the language object   classification with another cross-entropy loss Lcls. Since   the PointGroup is fine-tuned end-to-end, we apply the original PointGroup loss identical to Jiang et al. [23], where   LPG = Lsem +Lo dir +Lo reg +Lc score. Overall, we combine   the overall visual grounding loss as: LFG = LPG+LG+Lcls.   3D dense captioning with seq-to-seq objective.   We follow the next word prediction strategy to train our model,   where a lightweight MLP is attached to the task-agnostic   multimodal fusion module as the captioning head.   As   shown in Fig. 2b, the input text is padded with [CLS] and   [SEP] token at the beginning and the end of the sentence,   respectively. Following the teacher-forcing scheme, for a   word sequence {w1, ..., wL} of length L, we take the 1st to   the (L \u22121)th words as input and choose the 2nd to the Lth   words as the modeling target. Note that a triangular-style   mask is applied to the multimodal fusion module. This way,   each text token can only attend to the context box tokens   and the other text token before itself in the sequence. We   apply the seq-to-seq objective loss LC on the predicted sequence, where a word-level cross-entropy loss is applied on   each predicted next word against the target word. Similar to   Sec. 3.1, we also apply the same PointGroup loss LPG for   fine-tuning PointGroup. In the end, we combine the overall   dense captioning loss as: LFC = LPG + LC.   Joint training with both objectives.   On top of the taskagnostic multimodal fusion module, our architecture can   easily enable joint training with both bidirectional and seqto-seq objectives. In this case, both output heads are attached to the multimodal fusion module. We first input the   point cloud and the padded text into PointGroup and BERT   to encode the box and text tokens, respectively. To accumulate gradients for both objectives before back-propagation,   Figure 3: We propose a novel vision-language data synthesis method to enable more generic vision-language representation learning for 3D visual grounding and dense   captioning, In particular, we crop out the dominant objects from the ScanNet frames and feed the image crops to   CLIPCap [32], an off-the-shelf image captioner equipping   CLIP [38] and GPT-2 [37]. In the end, we obtain the pointcloud-text pairs by combining the cropped point clouds in   the camera frustums and the generated object captions.   we apply two forward passes through the multimodal fusion   module. In the first pass for the bidirectional objective, the   global box token and bidirectional mask are applied. Similarly, the target box token and seq-to-seq mask are applied   for the seq-to-seq objective in the second pass. We combine   all losses for both objectives as: LFJ = LPG+LG+Lcls+LC.   3.2. 3D pre-training with synthetic data   Synthesize   3D   vision-language   data   from   images.   Since the existing 3D vision-language data [5] is relatively   limited in scale and variety, augmenting the training data   with vision-language knowledge acquired from existing 2D   datasets and models can potentially enable more generic   vision-language representation learning for aforementioned   3D vision-language tasks. As illustrated in Fig. 3, given a   ScanNet frame, we crop out the dominant objects and feed   the image crops to CLIPCap [32], an image captioner empowered by pre-trained CLIP [38] and GPT-2 [37], to generate its description. Then, we crop out the points in the   camera frustum from ScanNet point clouds as the 3D context. We pair the generated description and the cropped   point cloud as a synthetic point-cloud pair for this view.   To ensure the quality of the created point-cloud-text pairs,   we remove pairs whose CLIP similarities between the original image crops and descriptions are lower than 0.3. We   crop multiple objects from a single ScanNet frame, and repeatedly perform the description generation and point cloud   cropping on every 20 frames in ScanNet. This process produces a large synthetic 3D vision-language dataset for the   pre-training purpose.   18112   Pre-training on synthetic data with both objectives.   We feed the point cloud into a PointGroup [23] detection   module to produce a sequence of encoded object tokens.   To secure the quality of each object token, the PointGroup   module is first trained on the ScanNet instance segmentation task until convergence. During pre-training, we freeze   the PointGroup module and use the GT instance masks for   encoding the object tokens. In the meantime, the object text   query is fed into the BERT [13] to form a sequence of encoded text tokens. These two sets of tokens are concatenated as a sequence for vision-language modeling. Then, a   multimodal transformer module takes the concatenated sequence to produce a sequence of fused multimodal tokens.   To uniformly supervise the joint representation learning, we   apply bidirectional objective (LG + Lcls) and seq-to-seq objective (LC) introduced in Sec. 3.1. We combine the overall   pre-training loss function as: LPT = LG + Lcls + LC.   As the synthetic 3D vision-language data contain many   noisy samples, after convergence on the synthetic data, we   continue the joint training with both objectives on the ScanRefer data with clearer annotations as a second pre-training   stage. Then, we separately fine-tune the model on the 3D   grounding task and 3D dense captioning task with bidirectional objective and seq-to-seq objective, respectively. We   show that this pre-training scheme further boosts the overall   accuracy on the downstream tasks.   3.3. Inference   We use task-specific heads for decoding visual grounding and dense captioning outputs from the task-agnostic   multimodal fusion module.   When inferencing the object captions, we autoregressively generate words from the   [CLS] token until the [SEP] token or reaching the maximum   length. Following Chen et al. [6], we take the minimum   and maximum coordinates in the predicted instance masks   to construct the object bounding boxes. For the bounding   boxes that are assigned to the same GT box, we keep only   the box with the highest IoU with the GT box. Note that the   bounding boxes used for validating the detection and dense   captioning performance are deliberately kept identical.   4. Experiments   4.1. Implementation details   Following Chen et al. [6], we use PointGroup implemented with the Minkowski Engine [10]. The PointGroup   backbone is trained on the ScanNet [12] train set for 500   epochs using the Adam optimizer [25] with a learning rate   of 2e-3 and a batch size of 4. In the pre-training stage on   synthetic data, we acquire the object features with a trained   and frozen PointGroup. For computational simplicity, we   feed the PointGroup with GT instance masks.   We consider up to 16 instances in the cropped point clouds. The   pre-training takes 40k iterations to converge, with a learning rate of 1e-5 and a batch size of 128.   During finetuning, we set the learning rate of PointGroup and the rest   of the UniT3D to 1e-4 and 1e-5, respectively. To boost the   fine-tuning speed, we pair each point cloud with 16 descriptions, as in Chen et al. [6]. Our full architecture contains   120M trainable parameters. All our experiments are conducted on an RTX A6000 GPU with PyTorch [33].   4.2. Dataset   We use the ScanRefer [5] dataset consisting of around   51k descriptions for over 11k objects in 800 ScanNet [12]   scans for the visual grounding and dense captioning tasks.   The descriptions include information about the appearance   of the objects, as well as the object-to-object spatial relationships. We follow the official splits of ScanRefer for   training and validation, and report visual grounding and   dense captioning results on the validation split.   Synthetic data. We repeatedly generate the object descriptions in every 20 frames from ScanNet [12]. In each frame,   we arrange the objects in descending order by the instance   mask areas. Up to 3 most dominant objects are cropped for   description generation. For consistency, we only generate   synthetic data from scans in the ScanNet training split. In   the end, we generate 265 693 synthetic samples from 92 766   frames in 1 199 ScanNet [12] scans. We visualize some synthetic samples in Fig. 4.   4.3. Evaluation metrics   Localization.   Following Chen et al. [5], we measure the   thresholded accuracy Acc@kIoU where the positive predictions have a higher intersection over union (IoU) with   the ground truths than the thresholds. We set the threshold   value k for IoU to 0.25 and 0.5 in our experiments.   Dense captioning.   To jointly measure the quality of the   generated descriptions and the detected bounding boxes, we   evaluate them by evaluating standard image captioning metrics such as CIDEr and BLEU under different Intersectionover-Union (IoU) scores between predicted bounding boxes   and the matched ground truth bounding boxes.   For N pred predicted bounding boxes and N GT ground   truth bounding boxes, we define the captioning precision   M P@kIoU as M P@kIoU =   1   N pred   PN pred   i=1 miui, where ui \u2208   {0, 1} is set to 1 if the IoU score for the ith box is greater   than k, otherwise 0. We use m to represent the captioning   metrics such as CIDEr, BLEU, METEOR, and ROUGE.   Similarly, we define the captioning recall M R@kIoU as   M R@kIoU =   1   N GT   PN GT   i=1 miui. Note that previous dense   captioning metrics proposed in Chen et al. [8] are analogous   to M R@kIoU. It solely measures the caption quality against   the matched grounding truth, without taking the false positive predictions into account.   18113   Figure 4: Our generated synthetic samples. Objects in clear views can be well captured by CLIPCap [32] as shown in the   first three boxes in green. However, CLIPCap also fails to describe blurry or incomplete objects as shown in the last red box.   Figure best viewed in color.   Val Acc@0.25IoU   Val Acc@0.5IoU   Unique   Multiple   Overall   Unique   Multiple   Overall   ScanRefer [5]   76.33   32.73   41.19   53.51   21.11   27.40   TGNN [20]   68.61   29.84   37.37   56.80   23.18   29.70   InstanceRefer [47]   75.72   29.41   38.40   66.83   24.77   32.93   3DVG-Trans [50]   81.93   39.30   47.57   60.64   28.42   34.67   3DJCG [3]   78.75   40.13   47.62   61.30   30.08   36.14   D3Net [6]   72.04   27.11   35.58   3D-SPS [31]   84.12   40.32   48.82   66.72   29.82   36.98   BUTD-DETR [22]   82.77   44.01   49.69   63.81   33.51   38.01   Ours (from scratch)   85.84   32.21   42.31   74.78   27.60   36.49   Ours (w/ pre-training)   82.75   36.36   45.27   73.14   31.05   39.14   Table 1: Quantitative results on 3D visual grounding. We   follow the evaluation setting in Chen et al. [5]. All accuracies are thresholded by the IoU 0.25 and 0.5. In comparison   with the previous SOTA methods, our method has a notably   higher overall grounding accuracy thresholded by IoU 0.5.   Note that the grounding results from BUTD-DETR [22] are   re-evaluated by removing the GT object labels in the text   queries from the original implementation.   Finally, we adopt the captioning F1-score combining   both captioning precision and captioning recall as the final   metric for dense captioning:   M@kIoU = 2 \u00d7 M P@kIoU \u00d7 M R@kIoU   M P@kIoU + M R@kIoU   (1)   Object detection.   We also report the mean average precision (mAP) of the detected objects from our PointGroup   backbone on ScanRefer val split. These detected boxes are   the same as the ones used for dense captioning evaluation.   4.4. Comparison with the state-of-the-art methods   We compare our proposed UniT3D method with several state-of-the-art methods for 3D visual grounding and   dense captioning tasks on ScanRefer dataset [5] in Tab. 1   and Tab. 3. For 3D visual grounding, we divide the validation set into \u201cUnique\u201d, \u201cMultiple\u201d, and \u201cOverall\u201d, following the evaluation protocol in Chen et al. [5]. For 3D dense   captioning, we report the aforementioned dense captioning   F1-score and the object detection mAP.   3D visual grounding.   Tab. 1 compares our method with   the prior state-of-the-art 3D visual grounding methods on   ScanRefer.   For our method trained from scratch (Ours   (from scratch)), we observe that it already achieves on-par   performance with the previous SOTA method [22]. In this   case, our method even achieves the best grounding accuracy in the \u201cUnique\u201d subset, where there is only one unique   object belonging to a specific semantic class in the scene.   After pre-training the network on the synthetic 3D visionlanguage data and fine-tuning on the visual grounding tasks   (Ours (w/ pre-training)), we observe a clear performance   boost in the visual grounding accuracies. Despite a drop   in the \u201cUnique\u201d subset, our method performs clearly better   in the more challenging \u201cMultiple\u201d subset, resulting in an   improvement in the overall grounding accuracy. The improvement in \u201cMultiple\u201d subset indicates that pre-training   on a large amount of synthetic data provides more multimodal knowledge for disambiguating the objects in the   scene with language cues.   Note that BUTD-DETR [22]   is re-evaluated by removing the GT object labels in the   text queries for a fair comparison (more details in supplementary material).   Besides the modulated object detector, BUTD-DETR also takes pre-computed object bounding boxes as extra visual cues. In contrast, our method follows a much simpler design philosophy and relies purely   on the end-to-end fine-tuned object detector. We note that   the baseline joint 3DJCG [3] and D3Net [6] involves either   sophisticatedly designed heavy neural heads or complicated   self-critical training strategy. Our method outperforms both   of them with a significantly simpler architecture design.   3D dense captioning.   We present a comparison of our   method against the previous state-of-the-art 3D dense captioning methods on ScanRefer in Tab. 3.   To keep the   comparison consistent and fair, all methods presented here   are trained with the cross-entropy objective only, including   D3Net [6]. Even without any pre-training, our architecture (Ours (from scratch)) already achieves similar dense   captioning results in comparison with the previous SOTA   D3Net [6]. We observe a significant performance boost if   we initialize the network with the pre-trained weights on   18114   Visual Grounding Accuracy   Dense Captioning F1-Scores   # objects   Unique@0.5IoU   Multiple@0.5IoU   Overall@0.5IoU   CIDEr@0.5IoU   BLEU-4@0.5IoU   ROUGE-L@0.5IoU   METEOR@0.5IoU   up to 1   26.72   7.24   11.02   3.90   0.53   18.70   8.17   up to 3   30.84   9.34   13.51   5.91   0.69   18.91   8.32   up to 5   29.54   7.62   11.87   2.81   0.29   18.22   7.89   Table 2: Comparison of 3D visual grounding and dense captioning results evaluated on ScanRefer [5] val set with   UniT3D pre-trained on different amount of synthetic data. When using synthetic data generated from up to 3 object crops in   the ScanNet [12] frames, the pre-trained UniT3D demonstrates the best initial performance in comparison with other object   cropping configurations.   Captioning F1-score   Detection   C@0.5IoU   B-4@0.5IoU   R@0.5IoU   M@0.5IoU   mAP@0.5   Scan2Cap [8]   15.71   9.01   14.92   7.18   32.09   X-Trans2Cap [48]   17.64   9.68   15.25   7.21   35.31   MORE [24]   16.46   8.86   14.71   7.12   31.93   3DJCG [3]   21.17   14.18   19.49   10.19   39.75   D3Net [8]   26.13   16.18   27.48   13.06   50.93   D3Net [8] (CIDEr loss)   41.32   22.75   35.30   15.87   53.85   Ours (from scratch)   26.68   14.64   27.10   12.92   53.91   Ours (w/ pre-training)   30.28   18.23   30.72   14.74   54.03   Table 3: Quantitative results of 3D dense captioning on   ScanRefer [5]. We measure the dense captioning F1-scores   and the PointGroup detection mAP against the ground truth   bounding boxes and descriptions. All reported metrics are   thresholded by IoU 0.5. Our method outperforms the previous SOTA methods even without any pre-training, and our   proposed pre-training scheme on the synthetic data gives   a further improvement the dense captioning performance.   Note that we compare to D3Net [6] trained only with the   cross-entropy objective for a fair comparison.   the synthetic data (Ours (w/ pre-training)). We observe that   our method clearly outperforms previous joint architectures   D3Net and 3DJCG [3] without any complex design in the   architecture or training objectives. Note that D3Net [6] applies the same PointGroup detection backbone as in our   method. Compared to D3Net [6], the results show that our   method is more capable of modeling the multimodal and   spatial relationships from similar visual cues.   4.5. Ablation and analysis   Does more synthetic data help?   Since synthetic data can   be acquired at almost zero cost in terms of human annotation, there is no theoretical upper bound for the amount of   synthetic data we can generate. We conduct an ablation experiment where we pre-train the proposed network on the   different amounts of synthetic data. We crop up to 1, 3, or 5   dominant objects from the ScanNet [12] frames to generate   synthetic descriptions. The proposed network is pre-trained   on these three synthetic datasets and evaluated on the ScanRefer [5] validation set afterwards. As shown in Tab. 2,   we empirically find that the network pre-trained with the   dataset where up to 3 objects are cropped from the frame   achieves the best performance for both visual grounding and   dense captioning. This indicates that more synthetic data is   not necessarily helpful since more noise resulting from it   Figure 5:   Compared to training from scratch, the pretraining scheme on synthetic data with joint training objectives shows notable benefits on both 3D dense captioning   and visual grounding.   could overwhelm the network in the pre-training stage.   Do pre-training and joint training objectives help?   We   report the ablation results on different pre-training schemes   for visual grounding and dense captioning in Tab. 4 and   Tab. 5, respectively. In general, we validate 5 scenarios:   (a) training the network from scratch on each task in ScanRefer with target objective; (b) training the network from   scratch on both tasks in ScanRefer with joint objectives; (c)   pre-training the network on synthetic data without further   any fine-tuning; (d) fine-tuning the network from (c) directly on the downstream objective; and (e) our full setting   \u2013 fine-tuning the network from (c) joint training objectives   on ScanRefer.   For visual grounding in Tab. 4, our model in (a) already   demonstrates strong performance on par with the previous   SOTA even without pre-training. And the visual grounding   accuracy is further improved by having the joint training   objectives on ScanRefer in (b), showing that our unified architecture effectively connects the two tasks through joint   learning. After being pre-trained only on the synthetic data,   our model in (c) demonstrates non-trivial results when directly evaluated on the ScanRefer [5] validation split. Then,   the model fine-tuned with only the bidirectional objective   in (d) achieves the highest visual grounding accuracy in   the \u201cUnique\u201d split, indicating a great capability of distinguishing the objects via semantic information. Fine-tuned   jointly with both training objectives, our final model in (e)   18115   Training Dataset(s)   Training Objective(s)   Visual Grounding Accuracy   Training setup   Synthetic   ScanRefer   Bidirectional   Seq-to-Seq   Unique@0.5IoU   Multiple@0.5IoU   Overall@0.5IoU   (a) direct from scratch   \u2713   \u2713   74.78   27.60   36.49   (b) joint from scratch   \u2713   \u2713   \u2713   73.68   28.84   37.45   (c) initial pre-trained   \u2713   \u2713   \u2713   30.84   9.34   13.51   (d) direct fine-tuned   \u2713   \u2713   75.51   29.63   38.45   (e) joint fine-tuned   \u2713   \u2713   \u2713   73.14   31.05   39.14   Table 4: 3D visual grounding results on ScanRefer [5] with different training schemes. (a) Without any pre-training, our   model already has a strong performance on par with the previous SOTA. (b) The visual grounding accuracy is further improved by having the joint training objectives, indicating the effectiveness of our architecture in unifying two downstream   tasks. (c) Pre-trained on the synthetic data alone gives non-trivial visual grounding performance on the ScanRefer [5] validation split. (d) Fine-tuned with bidirectional objective from weights in (c), it achieves the highest visual grounding accuracy   in the \u201cUnique\u201d subset. (e) Fine-tuned jointly with both objectives, our final setting in achieves the highest visual grounding   accuracy in the \u201cMultiple\u201d subset, leading to the best overall visual grounding results.   Training Dataset(s)   Training Objective(s)   Dense Captioning F1-Scores   Training setup   Synthetic   ScanRefer   Bidirectional   Seq-to-Seq   CIDEr@0.5IoU   BLEU-4@0.5IoU   ROUGE-L@0.5IoU   METEOR@0.5IoU   (a) direct from scratch   \u2713   \u2713   26.68   14.64   27.10   12.92   (b) joint from scratch   \u2713   \u2713   \u2713   27.28   17.22   29.12   13.74   (c) initial pre-trained   \u2713   \u2713   \u2713   5.91   0.69   18.91   8.32   (d) direct fine-tuned   \u2713   \u2713   28.13   17.69   30.33   14.30   (e) joint fine-tuned   \u2713   \u2713   \u2713   30.28   18.23   30.72   14.74   Table 5: 3D dense captioning results on ScanRefer [5] with different training schemes. (a) Our model without pre-training   already demonstrates competitive performance against the previous SOTA. (b) The dense captioning accuracy is further   improved by having the joint training objectives, indicating the effectiveness of our architecture in unifying two downstream   tasks. (c) Due to the domain gap, the descriptions generated by the model pre-trained only on the synthetic data obviously   deviate from the GT samples in ScanRefer [5] validation split. (d) Plausible dense captioning performance is achieved after   being directly fine-tuned on ScanRefer [5] with the seq-to-seq objective. (e) A further performance boost is observed if the   network is fine-tuned jointly with both training objectives in our final setting.   achieves the highest visual grounding accuracy in the \u201cMultiple\u201d subset, leading to the best overall visual grounding   results. Compared with directly training from scratch in   (a), this pre-training and joint optimization scheme clearly   demonstrates its advantage.   For dense captioning in Tab. 5, our method in (a) already   demonstrates competitive performance against the previous   SOTA without pre-training, and can be further improved by   having the joint training objectives on ScanRefer in (b), indicating the effectiveness of our architecture in unifying two   downstream tasks. Due to the domain gap, the descriptions   generated by the model after pre-training only on the synthetic data in (c) obviously deviate from the GT samples in   ScanRefer [5] validation split. Plausible dense captioning   performance is achieved after the network is directly finetuned on ScanRefer [5] with the seq-to-seq objective in (d).   A further performance boost is observed if the pre-trained   network is fine-tuned jointly with both the bidirectional and   the seq-to-seq objectives in (e). Here, we also demonstrate   the strong advantage of having the pre-training and joint   optimization scheme in Fig. 5. Compared to training from   scratch, our pre-training scheme on synthetic data with joint   training objectives effectively improves the overall accuracy   on 3D dense captioning and visual grounding.   5. Conclusion   We present UniT3D, a unified transformer architecture to connect 3D dense captioning and visual grounding. UniT3D enables learning a strong joint multimodal   representation across two tasks through a supervised joint   pre-training scheme with bidirectional and seq-to-seq objectives. The generic representation of UniT3D expands   pre-training scope to more various training sources such as   the synthesized data via 2D priors, showing that the distillate 2D knowledge is beneficial to 3D vision-language   tasks. Extensive experiments and analysis demonstrate the   strength of our UniT3D model for 3D dense captioning and   visual grounding. We hope our work can inspire more future work in exploring the 3D vision-language field.   Limitations. Although UniT3D takes a promising step towards unifying the two discussed tasks, UniT3D could be   still expanded to other tasks. Further, in addition to generating synthetic data from a 2D captioner, more sources of   distillate 2D data could be explored in the future.   18116   Acknowledgements   This work is funded by the ERC Starting Grant   Scan2CAD (804724), a Hans Fischer Fellowships (Focus   Group Visual Computing), as well as the the German Research Foundation (DFG) under the Grant Making Machine   Learning on Static and Dynamic 3D Data Practical. This   work is also supported in part by the Canada CIFAR AI   Chair program and an NSERC Discovery Grant.", "conf": "ICCV", "year": "2023", "index": 254}, {"title": "Scene-Intuitive Agent for Remote Embodied Visual Grounding   Xiangru Lin1   Guanbin Li2\u2217   Yizhou Yu1,3   1The University of Hong Kong   2Sun Yat-sen University   3Deepwise AI Lab   xrlin2@cs.hku.hk, liguanbin@mail.sysu.edu.cn, yizhouy@acm.org", "abstract": "Humans learn from life events to form intuitions towards the understanding of visual environments and languages.   Envision that you are instructed by a high-level instruction,   \u201cGo to the bathroom in the master bedroom and replace the   blue towel on the left wall\u201d, what would you possibly do   to carry out the task? Intuitively, we comprehend the semantics of the instruction to form an overview of where a   bathroom is and what a blue towel is in mind; then, we   navigate to the target location by consistently matching the   bathroom appearance in mind with the current scene. In   this paper, we present an agent that mimics such human   behaviors. Speci\ufb01cally, we focus on the Remote Embodied Visual Referring Expression in Real Indoor Environments task, called REVERIE, where an agent is asked to   correctly localize a remote target object speci\ufb01ed by a concise high-level natural language instruction, and propose   a two-stage training pipeline. In the \ufb01rst stage, we pretrain the agent with two cross-modal alignment sub-tasks,   namely the Scene Grounding task and the Object Grounding   task. The agent learns where to stop in the Scene Grounding task and what to attend to in the Object Grounding   task respectively. Then, to generate action sequences, we   propose a memory-augmented attentive action decoder to   smoothly fuse the pre-trained vision and language representations with the agent\u2019s past memory experiences. Without bells and whistles, experimental results show that our   method outperforms previous state-of-the-art(SOTA) significantly, demonstrating the effectiveness of our method.   1.", "content": "Vision and Language tasks, such as Vision-and-Language   Navigation   (VLN)   [2],   Visual   Question   Answering   \u2217Corresponding author is Guanbin Li. This work was supported in   part by the National Key Research and Development Program of China   (No.2020YFC2003902), in part by the Guangdong Basic and Applied Basic Research Foundation under Grant No.2020B1515020048, in part by the   National Natural Science Foundation of China under Grant No.61976250   and No.U1811463. This work was also sponsored by CCF-Tencent Open   Research Fund.   (VQA) [3, 4] and Referring Expression Comprehension   (REF) [17, 40, 39] etc., have been extensively studied in the   wave of deep neural networks. In particular, VLN [2, 5] is a   challenging task that combines both natural language understanding and visual navigation. Recent works have shown   promising performance and progress. They mainly focus on   designing agents capable of grounding \ufb01ne-grained natural   language instructions, where detailed information is provided, to \ufb01nd where to stop, for example \u201cLeave the bedroom   and take a left. Take a left down the hallway and walk straight into the bathroom at the end of the hall. Stop in   front of the sink\u201d [10, 23, 37, 36, 34, 18]. However, a practical issue is that \ufb01ne-grained natural language instructions   are not always available in real life and human-machine interactions are mostly based on high-level instructions such   as \u201cGo to the bathroom at the end of the hallway\u201d. In other words, designing an agent that could perform high-level   natural language interpretation and infer the probable target   location using knowledge of the environments is of more   practical use.   In this paper, we focus on the REVERIE task [30] which   is an example of the above mentioned high-level instruction task. Here, we brie\ufb02y introduce the settings. Given a   high-level instruction that refers to a remote target object at   a target location within a building, a robot agent spawns at   a starting location in the same building and tries to navigate   closer to the object. The output of the task is a bounding box   encompassing the target object. The success of the task is   evaluated based on explicit object grounding at the correct target location. A straightforward solution is to integrate   SOTA navigation model with SOTA object grounding model. This strategy has proven to be inef\ufb01cient in [30] and   instead, they proposed an interactive module to enable the   navigation model to work together with the object grounding model. Although the performance is improved, we observe that such method has a key weakness: it is unreasonable to discern high-level instruction by directly borrowing   the \ufb01ne-grained instruction navigation model that consists   of simple trainable language attention mechanism based on   the fact that the perception of high-level instruction primarily depends on commonsense knowledge prior as well as   past experiences in memory. Therefore, the overall design   7036   is not in line with human intuitions in high-level instruction   navigation.   Go to the closet on the second level    and bring me the clothes bag   Go to the closet on the second level    and bring me the clothes bag   Object Grounding Task   Scene Grounding Task   Figure 1. The overview of two pre-training tasks, the Scene   Grounding task and the Object Grounding task.   The Scene   Grounding task empowers the agent the ability to reason where   the target location is and the Object Grounding task learns what to   attend to.   Designing an agent to solve the problem like the   REVERIE task is still under explored and there are still no systematic ways to design such an agent. Then, how   does human wisdom solve this task? Human beings have instincts to understand surrounding visual environments and   languages. Intuitively, given a high-level instruction, we   would \ufb01rst extract high-level what and where information   and then form an overview of the appearance of the target   location in mind based on common sense knowledge. During navigation, we would consistently match current scene   and objects in the scene to the instruction semantics and   decide where to navigate next. According to such intuitions, we approach this problem from a new perspective and   present an agent that imitates such human behaviors. Concretely, we de\ufb01ne our problem as designing an agent that   is able to solve where and what problem in the REVERIE task. We propose a two-stage training pipeline. In the   \ufb01rst stage, we design two pre-training tasks, mimicking the   aforementioned two human intuitions. The second stage is   training the agent with a memory-augmented attentive action decoder, further increasing the agent\u2019s navigation capability under high-level instructions.   Pre-training Stage. As is shown in Fig. 1, we introduce a new subtask called the Scene Grounding task that is   trained to recognize which viewpoint in a set of viewpoints   is best aligned with the high-level instruction and another   subtask called the Object Grounding task that helps the agent identify the best object that matches to the instruction   among a set of candidate objects located at a target viewpoint. Experimental results show that the Scene Grounding   model recognizes the target viewpoint with a high accuracy   and the Object Grounding model outperforms the previous   best model used in [41, 30] by more than 10%.   Action Decoding Stage.   In this stage, with the pretrained models serving as scene and language encoders, we   propose a memory-augmented attentive action decoder that   leverages a scene memory structure as the agent\u2019s internal   past state memory. This design is based on the fact that   the computation of action at a speci\ufb01c time step could depend on any provided information in the past. Experimental   results indicate that the proposed structure is effective and   achieves new state-of-the-art performance.   To sum up, this paper has the following contributions:   \u2022 We propose a new framework that borrows human intuitions for designing agent capable of understanding   high-level instructions, which closely integrate navigation and visual grounding in both training and inference. Speci\ufb01cally, the visual grounding models are   pre-trained and serve as vision and language encoders   for training navigation action decoder in the training   phase. In inference, the action is predicted by considering logits from both the visual grounding models and   the navigation decoder.   \u2022 We introduce two novel pre-training tasks, called   Scene Grounding task and Object Grounding task, and   a new Memory-augmented attentive action decoder in   our framework. The pre-training tasks attempt to help   the agent learn where to stop and what to attend to,   and the action decoder effectively exploits past observations to fuse visual and textual modalities.   \u2022 Without bells and whistles, our method outperforms all previous methods, achieving new state-of-the-art   performance on both seen and unseen environments on   the REVERIE task.   2. Related Work   Vision-and-Language Navigation and REVERIE. In VLN, an agent is required to navigate to a goal location in a   3D simulator based on \ufb01ne-grained instructions.   [2] proposed the Matterport3D Simulator and designed the Roomto-Room task. Then, a lot of methods have been proposed   to solve this task [10, 37, 36, 34, 18]. On the other hand,   the recently proposed REVERIE task [30] is different from   traditional VLN in that it requires an agent to navigate and   localize target object simultaneously under the guidance of   high-level instruction. The model they proposed trains the   navigation model with the interactive module that works together with the object grounding model [41], in the   hope that the model could learn to understand high-level   instruction in a data-driven manner. However, our motivation is essentially different in that we inject commonsense   knowledge prior and past memory experiences into the action policy taking into consideration the human perception   in dealing with such high-level instruction navigation problems. Speci\ufb01cally, we introduce two pre-training tasks and   a memory based action policy to make the agent become   scene-intuitive. Moreover, our pre-training tasks differ from   the ones proposed in [10, 42, 24] in that their motivation is   based on the fact that the ground truth navigation path is   actually hidden in the \ufb01ne-grained instruction, which is not   the case in high-level instruction navigation.   Memory-based policy for navigation tasks.   Various   7037   memory models have been extensively studied for navigation agents, including unstructured memory [14, 26, 38,   15, 25, 7], addressable memory [27, 28], topological memory [31], and metric grid-based maps [11, 1], etc.   Unstructured memory representations, such as LSTM memory, have been used extensively in both 2D and 3D environments. However, the issue of RNN based memory is that it   does not contain context-dependent state feature storage or   retrieval and does not have long time memory [1, 16, 9]. To   address these limitations, more advaneced memory structures, such as addressable, topological, and metric based   memory are proposed. In this paper, we adopt a simple   adressable memory structure. The aim of using such a simple design is 1) to intentionally make it lightweight, thus   reducing computational overhead, since the computational cost is important in REVERIE and our pipeline already   contains heavy models; 2) to improve the performance of   the overall pipeline rather than designing a more advanced   memory superior to others. Besides, in VLN, the metric   map memory construction requires \ufb01negrained language instruction as guidance, which is not available in our task, and   building the topological memory requires pre-exploration of   the environment, a technique that is certainly helpful to our   agent but is beyond the discussion of this paper.   Vision-and-Language BERT based referring expression   comprehension. Recent years have witnessed a resurgence   of active research in transferrable image-text representation learning. BERT-based models [8, 33, 32, 22, 6, 21]   have achieved superior performance over multiple visionand-language tasks by transferring the pre-trained model on   large aligned image-text pairs to other downstream tasks. In   BERT-based VLN, the most related agents to ours are [12]   and [24]. [12] treats VLN as a vision-and-language alignment task and utilizes a pre-trained vision-and-language   BERT model to predict action sequence while [24] formulates VLN as an instruction and path alignment task and   adopts a pre-trained vision-and-language BERT model to   \ufb01nd the best candidate path that matches to the instruction given. However, our work differs from others in that   we propose a generalized pipeline that mimics human intuitions to solve the high-level instruction navigation task   where vision-and-language BERT model is a building block   which can be customized to other vision-language alignment block. Experimental results show that the main performance gain comes from our proposed pipeline.   3. Method   In the REVERIE task, an agent placed at a starting location navigates to the target location to localize an object speci\ufb01ed by a high-level instruction. To carry out this   dif\ufb01cult task, we propose a novel pipeline that contains a   scene grounding model, an object grounding model, and a   memory-based action decoder. We make two claims of our   design choice: \ufb01rst, to better grasp the semantics of highlevel instructions, we choose ViLBERT model as our basic building block to serve as vision-and-language encoder;   second, since scene grounding task and object grounding   task are two essentially different tasks, we do not share the   basic building blocks for these two tasks. In general, we   decompose our method into two stages, as shown in Fig. 2,   namely the pre-training stage and the action decoding stage.   In the following sections, we \ufb01rst introduce the pre-training   tasks; then we illustrate the memory-based attentive action   decoder and \ufb01nally, the loss function used to train the agent.   3.1. ViLBERT introduction   In this section, we brie\ufb02y introduce the input and output   arguments of a ViLBERT model [21] as shown in Fig. 3.   A ViLBERT model is a BERT-based model that consists of   two input streams, vision encoding stream and language encoding stream, followed by a cross-modal alignment Transformer block. The inputs to ViLBERT model are sequence   of words and visual features respectively and the outputs   are corresponding encoded word sequence features as well   as visual sequence features. We use ViLBERT as our base   model (basic building block) for the Scene Grounding task   and the Object Grounding task. In Scene Grounding task, a   panorama viewpoint image is discretized into 36 view images and the inputs are sequence of words in the instruction   and 36 mean-pooled features extracted from 36 view images by a ResNet-152 CNN pre-trained on ImageNet [19].   In Object Grounding task, the inputs are sequence of words   in the instruction and all annotated bounding boxes features   extracted by Mask R-CNN [13] in a target viewpoint.   3.2. Overview of the proposed method   Settings. To formalize the task, we denote a given highlevel instruction as L = {lk}Nl   k=1 where Nl is the number   of words in the instruction L and a set of viewpoints as   \u03bd = {Vk}Nv   k=1 where Nv is the number of viewpoints in   the environment. At each time step t, the agent observes   a panoramic view Vt, a few navigable views Ot and a set   of annotated bounding boxes Bt. The panoramic view is   discretized into 36 single views by perspective projections, each of which is a 640 \u00d7 480 size image with \ufb01eld of   view set to 60 degrees, and is denoted by Vt = {vt,i}36   i=1.   Ot = {vt,i}No   i=1 \u2286Vt where No is the maximum navigable directions at a viewpoint Vt. Each vt,i is represented as   vt,i = ResNet(vt,i). Thus, V t = {vt,i}36   i=1. Besides, the   set of annotated bounding boxes at viewpoint Vt is denoted by Bt = {bt,i}Nb   i=1 where Nb is the number of bounding   boxes. Mask R-CNN [13] is used to extract bounding boxes   features Bt = {bt,i}Nb   i=1, where bt,i = MRCNN(bt,i).   Stage 1(a): Scene Grounding Task. We formulate the   task as \ufb01nding a viewpoint that best matches to a highlevel instruction L in a set of candidate viewpoints \u03bds.   7038   Go to the closet on the second level and    bring me the clothes bag   Current Viewpoint   Navigable View   Extract   Feature   \ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf   \ud835\udc94\ud835\udc94\ud835\udc2d\ud835\udc2d   \ud835\udc1a\ud835\udc1a   \u0de5\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95   \ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95   \ud835\udc82\ud835\udc82\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf   \ud835\udc94\ud835\udc94\ud835\udfcf\ud835\udfcf   \ud835\udc94\ud835\udc94\ud835\udc2d\ud835\udc2d   \u2026   ViL   Encoder   Memory   Update   Soft   Attend   Transfo   rmers   Soft   Attend   Extract   Feature   ViL   Pointer   Soft   Attend   \u0de5\ud835\udc90\ud835\udc90\ud835\udc95\ud835\udc95   \u2032   LSTM   Action   Select   Orient   Encode   Concat   \ud835\udc8d\ud835\udc8d\ud835\udc95\ud835\udc95   \ud835\udc82\ud835\udc82\ud835\udc95\ud835\udc95   argmax   Memory   \ud835\udc49\ud835\udc49\ud835\udc61\ud835\udc61   \ud835\udc63\ud835\udc63\ud835\udc61\ud835\udc61,\ud835\udc56\ud835\udc56   Action to Take Next   \ud835\udc54\ud835\udc54\ud835\udc60\ud835\udc60\ud835\udc54\ud835\udc54   \ud835\udc54\ud835\udc54\ud835\udc5c\ud835\udc5c\ud835\udc54\ud835\udc54   +   +   Vision   Language   Cross-Modal Alignment   \ud835\udc97\ud835\udc97\ud835\udc2d\ud835\udc2d,\ud835\udfcf\ud835\udfcf   \ud835\udc97\ud835\udc97\ud835\udc2d\ud835\udc2d,\ud835\udfd1\ud835\udfd1\ud835\udfd1\ud835\udfd1   \u2026 .   \ud835\udc99\ud835\udc99\ud835\udc2d\ud835\udc2d,\ud835\udfcf\ud835\udfcf   \ud835\udc99\ud835\udc99\ud835\udc2d\ud835\udc2d,\ud835\udc73\ud835\udc73   \u2026 .   \u2026   \u2026   Vision   Language   Cross-Modal Alignment   \ud835\udc83\ud835\udc83\ud835\udc2d\ud835\udc2d,\ud835\udfcf\ud835\udfcf   \ud835\udc83\ud835\udc83\ud835\udc2d\ud835\udc2d,\ud835\udc75\ud835\udc75\ud835\udc83\ud835\udc83   \u2026 .   \ud835\udc99\ud835\udc99\ud835\udc2d\ud835\udc2d,\ud835\udfcf\ud835\udfcf   \ud835\udc99\ud835\udc99\ud835\udc2d\ud835\udc2d,\ud835\udc73\ud835\udc73   \u2026 .   \u2026   \u2026   ViLEncoder   ViLPointer   Figure 2. The overall pipeline of our method. The green part of the \ufb01gure denotes the memory module where current viewpoint feature   V t and previous action feature at\u22121 are embedded and stored in the Memory. Transformer blocks are used to generate sa   t . The red   rectangles represent two pre-trained models, namely Scene Grounding model and Object Grounding model. V iLEncoder consists of   V iLBERT and BiLSTM and V iLPointer is V iLBERT trained on viewpoint-based object grounding task. At each time step t, the   agent perceives the instruction with viewpoint features and object features simultaneously. Action prediction is made by the Action Select   part where an attentive structure is applied. The \ufb01nal action is generated by considering scene grounding score gsg, object grounding score   gog and action logit lt. The dashed dot lines are used only for illustration purposes.   \u03bds = {Vk|Vk \u2208\u03bd} \u2286\u03bd. Concretely, we de\ufb01ne a mapping   function gsg(, ) that maps (L, Vk) to a matching score. The   formula is de\ufb01ned as follows,   V \u22c6   k = arg max   Vk\u2208\u03bds   gsg(L, ResNet(Vk))   (1)   Stage 1(b): Object Grounding Task. The goal of this   task is to identify the best matching object among a set of   candidate objects located at a target viewpoint. We denote   VT as a target viewpoint and its corresponding annotated   bounding boxes set is BT . We de\ufb01ne another compatibility   matching function gog(, ) that produce matching scores for   all objects with a high-level instruction L. Thus, the problem is de\ufb01ned as follows,   b\u22c6   T,i = arg max   bT,i\u2208BT   gog(L, MRCNN(bT,i))   (2)   Stage 2: Memory-augmented action decoder. To mitigate the memory problem presented in previous section,   a scene memory structure M t is implemented to store the   embedded observation and previous action at each time step   t. The memory is updated by,   \u02dcvt = softmax(V t(W 1ht\u22121))T V t   st = FC([at\u22121, \u02dcvt]),   M t = Update(M t\u22121, st)   (3)   where st is current state representation; \u02dcvt is attentive visual feature;ht\u22121 and at\u22121 are last time step hidden state and action embedding respectively;W 1 \u2208R2048\u00d7Dh   is a trainable parameter.   FC stands for fully connected layer.   The Update operation appends st to M t.   V t \u2208R36\u00d72048, \u02dcvt \u2208R1\u00d72048, at\u22121 \u2208R1\u00d73200, st \u2208   R1\u00d7Dh, ht \u2208RDh\u00d71, M t \u2208Rt\u00d7Dh.   \ud835\udc49\ud835\udc491   +   Vision Branch   IMG   \ud835\udc7d\ud835\udc7d\ud835\udc8c\ud835\udc8c= {\ud835\udc97\ud835\udc97\ud835\udc58\ud835\udc58,\ud835\udc56\ud835\udc56}\ud835\udc56\ud835\udc56=1   36   Language Branch   CLS   Walk   \u2026.   Stairs   SEP   Cross-Modal Alignment Layers    \ud835\udc89\ud835\udc89\ud835\udc3c\ud835\udc3c\ud835\udc3c\ud835\udc3c\ud835\udc3c\ud835\udc3c   \ud835\udc58\ud835\udc58   Fully Connected Layer   \ud835\udc89\ud835\udc89\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36   \ud835\udc58\ud835\udc58   Walk   \u2026.   Stairs   SEP   Matching Score \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc58\ud835\udc58   \ud835\udc49\ud835\udc492   \u2212   \ud835\udc49\ud835\udc493   \u2212   \ud835\udc49\ud835\udc494   \u2212   L: walk into the living room and sit    down in the chair closest to the stairs   \ud835\udc7a\ud835\udc7a\ud835\udc7a\ud835\udc7a= {\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc601   +, \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc602   \u2212, \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc603   \u2212, \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc604   \u2212, \ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc605   \u2212}   = {0.75, 0.10, 0.05, 0.05, 0.05}   \u2026   \u2026 .   \u2026 .   \ud835\udc49\ud835\udc495   \u2212   Figure 3. The pipeline of the Scene Grounding Task.   We formulate this task as a 5-way multiple choice problem.   Each   (L, ResNet(Vk)) pair is sent to the V iLBERT model separately   to generate alignment score sck. The panorama viewpoint image   here denotes the discretized 36 view images in a viewpoint. We   mark the beginning of the image sequence with a special token   IMG and the language with CLS.   3.3. Scene Grounding Task   The goal of this task it to help the agent infer where the target location is. Given a high-level instruction, \u201cBring me   7039   the jeans that are hanging up in the closet to the right\u201d, humans \ufb01rst locate the where information, the key word closet, by capturing the semantics of the instruction according   to the language context and commonsense knowledge and   then form an overview of the appearance of the closet in   mind; then, humans navigate to the target location by consistently matching the closet appearance in mind with current scene. In fact, humans have gradually formed intuitions   towards the understanding of scenes, instructions and tasks   in life. For language instructions in relatively simple life   scenes that do not involve complex reasoning, they usually   directly merge the above two processes for direct perception and understanding. We call this process as contextdriven scene perception. In this section, we propose Scene   Grounding task to imitate such human behavior.   Based on the observation, we believe that a model that   could evaluate the alignment between an instruction and a   viewpoint is able to localize the target viewpoint. Therefore, to implement this idea, we create a dataset from the   REVERIE training set and \ufb01ne-tune a ViLBERT model   on the dataset.   Speci\ufb01cally, we adopt a 5-way multiple   choice setting. We eliminate subscript for simplicity concern.   Given an instruction L, we sample 5 viewpoints   \b   V +   1 , V \u2212   2 , V \u2212   3 , V \u2212   4 , V \u2212   5   \t   , out of which only one is aligned   to the instruction (or in other words, positive). In detail,   we choose the ending viewpoint in the ground-truth training path as V +   1 , the second last viewpoint along the groundtruth path as V \u2212   2 which is a hard negative sample and random sample V \u2212   3 , V \u2212   4 from the rest of the viewpoints along   the path, and V \u2212   5 from other path . Then, we run the ViLBERT model on each of the (L, Vk) pair. As is shown in   Fig. 3, the output tokens CLS and IMG encode instruction representation hCLS as well as viewpoint representation hIMG respectively. We de\ufb01ne the matching scores as   Sc and train the model with cross entropy loss Lsr.   Sc = {sc1, sc2, sc3, sc4, sc5}   sck = gsg(L, ResNet(Vk)) = W 2(hk   CLS \u2299hk   IMG)   Lsr = CELoss(softmax(Sc), I(V +   1 ))   (4)   where W 2 \u2208R1\u00d71024 is a trainable parameter and I(.) is   indicator function. hk   CLS \u2208R1024\u00d71, hk   IMG \u2208R1024\u00d71   are the encoded language and visual representations of the   language and vision encoding streams from our pre-trained   ViLBERT model for kth (L, Vk) pair respectively.   3.4. Object Grounding Task   The aim of this task is to help the agent learn what to attend   to. For each ground-truth target viewpoint VT , we formulate   this task as \ufb01nding the best bounding box b\u22c6   T,i in bounding   boxes set BT given (L, BT ) pair. A straightforward method   to implement this idea is to construct a single image based   grounding task, where each training sample consists of instruction L and a subset of bounding boxes in BT that belong to view vT,i. However, according to our experiment,   this strategy produces moderate performance since objects in 3D space could span multiple views in corresponding   projected 2D image space. The cross-image objects relationships in each viewpoint are not well captured by the   model. Therefore, we propose a two-stage training strategy, namely a single image based grounding and a viewpoint based object grounding. In single image grounding,   we \ufb01ne-tune the ViLBERT model from   [22, 21] on the   aforementioned single image grounding dataset where each   training sample is (L, BvT,i) (all annotated bounding boxes in vT,i are collected) and BvT,i \u2282BT ; then, we further   \ufb01ne-tune trained model on a new viewpoint based object grounding dataset. Concretely, each training sample in   the viewpoint based dataset is a (L, BT ) pair (all annotated   bounding boxes in vT are collected) and the corresponding label is a vector containing 0s and 1s where 1 indicates   the IoU of a bounding box with the target bounding box is   higher than 0.5. In inference, we represent an object score   as the averaged scores from all bounding boxes that share   the same object id at a viewpoint that the agent stops.   3.5. Action Decoder   With the pre-trained grounding models, the action decoder   generally adopts Encoder-Decoder structure to produce action prediction. Speci\ufb01cally, the Scene Grounding model   is accompanied by a BiLSTM network to construct a vision and language grounding encoder V iLEncoder and the   Object Grounding model is formulated as an object level   grounding encoder V iLPointer. The inputs to action decoder are L, Bt and V t and it outputs predicted action distribution lt.   First. At each time step t, to perceive current scene and   instruction, we obtain \u02dcxt by grounding L with V t through   V iLEncoder and then selecting the fused language sequence as output. The formula is de\ufb01ned as follows,   Xt = V iLEncoder(L, V t)   = BiLSTM(V iLBERT(L, V t))   \u02dcxt = softmax(Xt(W 3ht\u22121))T Xt   (5)   where W 3 \u2208R1024\u00d7Dh is a trainable parameter and Xt   is encoded language feature taking current scene V t into   consideration. Xt \u2208RNl\u00d71024, \u02dcxt \u2208R1\u00d71024, ht\u22121 \u2208   RDh\u00d71.   Second.   To decide which navigable direction to   go next,   we perform object level referring expression comprehension.   The object level referring comprehension helps the agent infer whether a navigable   view vt,i contains possible target object.   In particular, the set of bounding boxes in view vt,i is denoted   by \u02c6Bt,i = {bt,k|bt,k \u2208Bt, Inside(bt,k, vt,i) = 1} where   7040   Inside(, ) function decides whether bt,k is inside view   vt,i. V iLPointer is V iLBERT pre-trained on the Object Grounding task and we select the fused bounding boxes   features as the output. Then,   F t,i = V iLPointer(L, MRCNN( \u02c6Bt,i))   \u02dcvt,i = gtop\u2212k(F t,i)   (6)   where F t,i is the set of aligned bounding boxes features   at view vt,i and gtop\u2212k(, ) selects top-k aligned bounding   boxes and averages the corresponding aligned bounding   boxes features from F t,i to produce view comprehension   \u02dcvt,i \u2208R1\u00d71024.   Third. We de\ufb01ne the representation of each navigable   view as v\u2032   t,i:   v\u2032   t,i = [vt,i, (cos \u03b8t,i, sin \u03b8t,i, cos \u03c6t,i, sin \u03c6t,i), \u02dcvt,i]   (7)   where the agent\u2019s current orientation (\u03b8t,i, \u03c6t,i) represents   the angles of heading and elevation and is tiled 32 times   according to   [10].   (cos \u03b8t,i, sin \u03b8t,i, cos \u03c6t,i, sin \u03c6t,i) \u2208   R1\u00d7128 and v\u2032   t,i \u2208R1\u00d73200. The set of navigable view representation is denoted as O\u2032   t =   \b   v\u2032   t,i   \tNo   i=1. The grounded   navigable visual representation \u02dco\u2032   t is represented as follows:   \u02dco\u2032   t = softmax(g(O\u2032   t)(W 4ht\u22121))T g(O\u2032   t)   (8)   where W 4 \u2208R1024\u00d7Dh is a trainable parameter and g(, ) is   a number of Fully Connected layers accompanied by ReLU   nonlinearities. \u02dco\u2032   t \u2208R1\u00d71024, O\u2032   t \u2208RNo\u00d73200.   Fourth. The new context hidden state ht is updated by a   LSTM layer taking as input the grounded text \u02dcxt and navigable view features \u02dco\u2032   t as well as the current state representation feature sa   t .   (ht, ct) = LSTM([\u02dcxt, \u02dco\u2032   t, sa   t ], (ht\u22121, ct\u22121))   (9)   where sa   t is memory augmented current state representation   and is de\ufb01ned as,   M a   t = [Transformer(M t, M t)]\u00d7Nmem   sa   t = [Transformer(st, M a   t )]\u00d7Nstate   (10)   where Nmem and Nstate are number of memory transformer blocks used and number of state transformer blocks used respectively.   sa   t   \u2208   R1\u00d7Dh, M a   t   \u2208   Rt\u00d7Dh.   Transformer is the standard version Transformer block   from [35].   Finally. The action logit lt is computed in an attentive   manner.   lt,i = g(O\u2032   t,i)(W 5[ht, \u02dcxt])   (11)   where W 5 \u2208R1024\u00d7(1024+Dh) is a trainable parameter and   lt \u2208RNo\u00d71. In training stage, at = Categorical(lt) is   selected based on categorical policy and in inference stage,   it is selected by at = arg max(lt). Action embedding is   selected based on at = O\u2032   t[at].   3.6. Inference   We propose to use a combined logit Pt   \u03c4=0 l\u03c4 + g\u03c4   og + g\u03c4   sg   that sums action logits, object grounding logits and scene   grounding logits to perform navigation, where g\u03c4   og and g\u03c4   sg   denote object grounding score and scene grounding score at   time step \u03c4 respectively. Experimental results indicate that   our strategy shortens the search trajectories while maintaining a good success rate. The \ufb01nal output bounding box is   obtained by running V iLPointer at the stop viewpoint that   the agent predicts.   3.7. Loss Functions   To train the agent, we use a mixture of Imitation Learning (IL) and Reinforcement Learning (RL) to supervise the   training. Speci\ufb01cally, In IL, at each time step, we allow the   agent to learn to imitate the teacher action by using a cross   entropy loss Lce and a mean squared error loss Lpm for   progress monitor [23]. In RL, we follow the idea of [34]   and allow the agent to learn from rewards. If the agent stops   within 3 meters near the target viewpoint, a positive reward   +3 is assigned at the \ufb01nal step; otherwise a negative reward   \u22123 is given.   Lfinal = \u03b1Lce + \u03b2Lpm + \u03b3LRL   Lce = \u2212   T   X   t=1   y\u22c6   t log(lt,\u22c6)   Lpm = \u2212   T   X   t=1   (ypm   t   \u2212ppm   t   )2   (12)   where y\u22c6   t is the teacher action at step t; ypm   t   \u2208[0, 1] is the   shortest normalized distance from current viewpoint to the   target viewpoint; ppm   t   is the predicted progress; \u03b1, \u03b2 and \u03b3   are all set to 1.   4. Experiments   In the REVERIE dataset, the training set contains 59 scenes   and 10466 instructions over 2353 objects; the val seen split consists of 53 scenes and 1371 instructions over 428   objects and the val unseen split include 10 scenes and   3573 instructions over 525 objects. The test set contains 16 scenes and 6292 instructions over 834 objects.   In   this section, we conduct extensive evaluation and analysis   of the effectiveness of our proposed components. For the   Implementation Details and the Qualitative Examples,   please refer to the supplementary document.   4.1. Evaluation Metrics   Following [30], we evaluate the performance of the model based on REVERIE Success Rate (RGS) and REVERIE   Success Rate weighted by Path Length (RG SPL). We also   7041   Table 1. Ablation Study experiments performed to verify the effectiveness of the proposed method. In different ablation study block, the   best performing result is marked in bold.   Experiments   ID   Methods   Val Seen   Val Unseen   Encoder   Pointer   Policy   Nav. Acc.   RGS\u2191   RG   SPL\u2191   Nav. Acc.   RGS\u2191   RG   SPL\u2191   Lenc   Bertenc   V iLRawenc   V iLenc   MNptr   V iLptr   Cpol   MApol   Succ.\u2191   OSucc.\u2191   SPL\u2191   Length\u2193   Succ.\u2191   OSucc.\u2191   SPL\u2191   Length\u2193   Component   Effectiveness   1   \u221a   \u221a   \u221a   50.53   55.17   45.50   16.35   31.97   29.66   14.40   28.20   7.19   45.28   7.84   4.67   2   \u221a   \u221a   \u221a   54.18   58.68   48.99   12.46   33.87   21.23   18.66   29.51   10.44   32.95   11.13   6.32   3   \u221a   \u221a   \u221a   33.73   39.14   30.72   14.56   23.82   21.94   15.22   31.64   8.44   42.62   8.89   4.84   4   \u221a   \u221a   \u221a   39.00   43.85   35.00   13.71   28.95   25.98   13.80   31.33   8.21   37.31   9.17   5.54   5   \u221a   \u221a   \u221a   37.32   43.08   31.71   18.29   24.88   21.70   19.06   44.39   7.10   79.88   11.08   4.17   6   \u221a   \u221a   \u221a   56.36   60.93   52.24   13.21   36.33   33.92   21.61   31.98   12.21   36.05   13.21   7.31   7   \u221a   \u221a   \u221a   54.25   56.08   50.49   13.56   39.56   37.16   26.98   37.86   13.70   42.50   17.32   8.71   8   \u221a   \u221a   \u221a   59.52   64.23   55.30   14.00   43.57   40.42   28.17   40.41   14.77   43.12   19.60   10.27   Memory   Blocks   (Nmem, Nstate)   9   (1, 1)   55.24   58.61   52.29   12.42   40.90   38.76   28.97   39.56   13.28   44.10   20.51   9.19   10   (3, 3)   61.91   65.85   57.08   13.61   45.96   42.65   31.53   44.67   16.28   41.53   22.41   11.56   11   (5, 5)   60.01   63.38   54.99   17.44   44.69   41.10   25.84   38.20   13.09   44.00   18.23   9.19   12   (7, 7)   57.27   62.26   52.78   13.96   42.66   39.38   23.66   35.61   11.67   45.73   16.79   8.43   13   (9, 9)   57.06   60.15   53.35   14.16   42.38   39.67   28.15   39.45   14.92   41.53   19.54   10.13   Logit   Fusion   14   lt   60.92   65.78   56.14   15.28   45.61   42.19   32.35   49.08   14.74   60.89   22.35   10.54   15   lt+gsg   61.49   65.78   56.72   13.67   45.47   42.31   31.20   47.80   15.90   45.82   21.68   11.08   16   lt+gog   61.14   65.77   55.21   16.82   44.48   40.04   32.12   46.54   15.73   52.14   21.98   11.02   17   lt+gsg+gog   61.91   65.85   57.08   13.61   45.96   42.65   31.53   44.67   16.28   41.53   22.41   11.56   Table 2. Comparison with state-of-the-art methods on the REVERIE task. The best performing result is marked in bold.   Methods   Val Seen   Val Unseen   Test (Unseen)   Nav. Succ.   RGS\u2191   RG   SPL\u2191   Nav. Succ.   RGS\u2191   RG   SPL\u2191   Nav. Succ.   RGS\u2191   RG   SPL\u2191   Succ.\u2191   OSucc.\u2191   SPL\u2191   Length\u2193   Succ.\u2191   OSucc.\u2191   SPL\u2191   Length\u2193   Succ.\u2191   OSucc.\u2191   SPL\u2191   Length\u2193   RCM [36] + MattNet   23.33   29.44   21.82   10.70   16.23   15.36   9.29   14.23   6.97   11.98   4.89   3.89   7.84   11.68   6.67   10.60   3.67   3.14   SelfMonitor [23] + MattNet   41.25   43.29   39.61   7.54   30.07   28.98   8.15   11.28   6.44   9.07   4.54   3.61   5.80   8.39   4.53   9.23   3.10   2.39   FAST-short [18] + MattNet   45.12   49.68   40.18   13.22   31.41   28.11   10.08   20.48   6.17   29.70   6.24   3.97   14.18   23.36   8.74   30.69   7.07   4.52   REVERIE [30]   50.53   55.17   45.50   16.35   31.97   29.66   14.40   28.20   7.19   45.28   7.84   4.67   19.88   30.63   11.61   39.05   11.28   6.08   Human   81.51   86.83   53.66   21.18   77.84   51.44   Ours   61.91   65.85   57.08   13.61   45.96   42.65   31.53   44.67   16.28   41.53   22.41   11.56   30.8   44.56   14.85   48.61   19.02   9.20   report the performance of Navigation Success Rate, Navigation Oracle Success Rate, Navigation Success Rate weighted by Path Length (SPL), and Navigation Length. Please   refer to the supplementary document for more details.   4.2. Ablation Study   In this section, we aim to answer the following questions:   (a) Does the performance gain mainly come from BERTbased structure? (b) How effective is each of the proposed   component?   (c) Does the memory blocks number matter? (d) Why do we need logit fusion? For simplicity concern, we de\ufb01ne the following experiment settings: (1) our   proposed V iLEncoder is V iLenc; (2) the V iLRawenc is   V iLEncoder not pre-trained on the Scene Grounding task   but pre-trained on the Conceptual Captions dataset [29] as   well as the 12 tasks speci\ufb01ed in [22]; (3) the BERTenc   is a BERT language encoder pre-trained on the BookCorpus [43] and English Wikipedia datasets; (4) our proposed   V iLPointer is V iLptr; (5) previous SOTA MattNet pointer is MNptr; (6) our action policy is MApol; (7) previous   action policy is Cpol; (8) previous simple language encoder   is Lenc composed of a trainable embedding layer with a Bidirectional LSTM layer.   Performance Gain. To answer question (a), we perform   experiments 1, 2, 3 and 6 as is shown in Table 1. All agents   are trained under Lce and Lpm with \u03b1 and \u03b2 both set to   0.5. It is clear that the agent\u2019s overall performance is incrementally improved by changing the language encoder from   the simple Lenc to our proposed V iLenc, which proves our   analysis that previous language encoder does not well capture the semantics of high-level instructions. The experimental results of 3 and 6 clearly suggests that the BERTbased structure is not the root cause of our performance   gain and our proposed Scene Grounding task signi\ufb01cantly increase the RG SPL metric to 33.9% on Val Seen and   7.31% on Val Unseen, even higher than the strong baseline   in experiment 2.   Component Effectiveness.   To answer question (b),   based on the statistics from Table 1, we train six models   in experiments from 3 to 8 and ablate the proposed component one by one to demonstrate the effectiveness. For fair   comparison, we follow the settings of [30]. All agents are   trained under Lce and Lpm with \u03b1 and \u03b2 both set to 0.5. We   start from the baseline experiment 3 and replace each component by our proposed ones. Speci\ufb01cally, in experiments   3 and 6, the proposed V iLEncoder improves the RG SPL   (and SPL) by a large margin, 11.98% (and 21.52%) higher   in Val Seen and 2.47% (and 3.77%) higher in Val Unseen   than the baseline respectively, which proves that the Scene   Grounding task is effective; in experiments 3 and 4, our   pointer V iLPointer outperforms the MattNet counterpart   by shortening the length of the search trajectory while maintaining a high RG SPL (and SPL), which demonstrates the   effectiveness of the Object Grounding task; in experiments   3 and 5, the results show that the overall search trajectory   of our action policy is longer than that of the baseline while   our action policy achieves higher RGS and Navigation Success Rate, which demonstrates that the memory structure   in our policy guides the agent to the correct target location   at the cost of long trajectory; in experiments 7 and 8, we   demonstrate that by integrating all our proposed methods,   our agent improves previous SOTA in terms of RG SPL by   10.76% on Val Seen and 5.6% on Val Unseen.   Memory Blocks. To answer question (c), we train \ufb01ve   models with different Nmem and Nstate values. In these experiments, we train the agents with Lce, Lpm and LRL and   7042   Table 3. Pointer Task: REVERIE Success Rate at the ground truth   target viewpoint; Encoder Task: given ground truth path, the success rate of identifying the target viewpoint among a set of candidate viewpoints along the path.   Tasks   Methods   Val Seen   Val Unseen   Pointer   MattNet [41]   68.45   56.63   CM-Erase [20]   65.21   54.02   ViLPointer-image-based   65.72   55.53   ViLPointer-vp-based   73.26   67.45   Encoder   ViLEncoder   85.67   66.43   \u03b1, \u03b2 and \u03b3 set to 1.0. In general, according to the experiments from 9 to 13 in Table 1, all pairs of (Nmem, Nstate)   exhibit superior performance compared to previous SOTA method in experiment 1 and the strong BERT baseline   model in experiment 2. Moreover, the best performance   model is achieved by setting (Nmem, Nstate) to (3, 3) in   these \ufb01ve models, which suggests that using small values of   (Nmem, Nstate) limits the agent\u2019s memorization ability and   using large values of (Nmem, Nstate) enables the agent to   achieve good performance on Val Unseen while maintains   good performance on Val Seen.   Logit Fusion. To answer question (d), we report two   accuracies to verify the effectiveness of gog and gsg. In   the Encoder Task of Table 3, given ground-truth path, our   proposed V iLBERT model achieves competitive performance on both Val Seen and Val Unseen, demonstrating the   strong ability of gsg to identify a target viewpoint. In the   Pointer Task of Table 3, the performance of V iLPointervp-based is signi\ufb01cantly higher than previous image-based   pointers because it is able to capture cross-image objects   relationships, suggesting that gog has the ability to \ufb01nd the   target location if the target object exists. According to experiments from 14 to 17, where the agents are trained with   Lce, Lpm and LRL and \u03b1, \u03b2 and \u03b3 set to 1.0, summing l\u03c4,   g\u03c4   og, and g\u03c4   sg shortens the search trajectory and maintains   a high RGS(Navigation Success Rate) and RG SPL(SPL).   The motivation behind the summing strategy is to use model   ensemble to reduce bias when searching for target locations   considering the fact that the agent has no prior knowledge   of the surrounding environments and the guidance of the   high-level instructions is weak.   4.3. Compared to previous state-of-the-art results   We \ufb01rst show what kind of cases our method improves compared to previous SOTA and our BERT-based strong baseline in experiment 2. Speci\ufb01cally, we divide the shortest   distance lengths of all ground-truth paths into three groups, namely short path(5 meters to 9 meters with 462 sample   paths on Val Seen and 1400 sample paths on Val Unseen),   middle path(9 meters to 14 meters with 703 sample paths on Val Seen and 1869 sample paths on Val Unseen), and   long path(14 meters to 18 meters with 247 sample paths   on Val Seen and 250 sample paths on Val Unseen). Then,   we count the cases that the agent successfully navigates to   RGS Successful Cases Percentage Under Different Length of Ground Truth Paths     RGS Successful Cases Percentage   Short Path   5 meters to 9 meters   462 samples   Middle Path   9 meters to 14 meters   703 samples   Long Path   14 meters to 18 meters   247 samples   (b) Val Seen   RGS Successful Cases Percentage Under Different Length of Ground Truth Paths     RGS Successful Cases Percentage   Short Path   5 meters to 9 meters   1400 samples   Middle Path   9 meters to 14 meters   1869 samples   Long Path   14 meters to 18 meters   250 samples   (d) Val Unseen   Navigation Successful Cases Percentage Under Different Length of Ground Truth Paths     Navigation Successful Cases Percentage   Short Path   5 meters to 9 meters   1400 samples   Middle Path   9 meters to 14 meters   1869 samples   Long Path   14 meters to 18 meters   250 samples   (c) Val Unseen   Navigation Successful Cases Percentage Under Different Length of Ground Truth Paths     Navigation Successful Cases Percentage   Short Path   5 meters to 9 meters   462 samples   Middle Path   9 meters to 14 meters   703 samples   Long Path   14 meters to 18 meters   247 samples   (a) Val Seen   Figure 4. Percentage of successful Navigation and RGS cases under different length of ground-truth paths on Val Seen and Val Unseen datasets for previous state-of-the-art method, BERT baseline   in experiment 2, and our method.   the target locations and the cases that the agent successfully navigates and localizes the target objects for the three   groups. In Fig. 4, we report the corresponding successful   cases percentage. It is obvious that our proposed method   improves all kinds of sample paths by a clear margin.   Then, we compare our \ufb01nal model with previous SOTA   models in Table 2. As is clearly shown in Table 2, our model   outperforms all previous models by a large margin. Specifically, in terms of SPL, our agent increases previous SOTA   by 11.58% on Val Seen, 9.09% on Val Unseen and 3.24%   on Test respectively; for RG SPL, our agent increase previous SOTA by 12.99% on Val Seen, 6.89% on Val Unseen   and 3.12% on Test. The overall improvements indicate that   our proposed scene-intuitive agent not only navigates better   but also localizes target objects more accurately.   5. Conclusion   In this paper, we present a scene-intuitive agent capable   of understanding high-level instructions for the REVERIE   task. Different from previous works, we propose two pretraining tasks, Scene Grounding task and Object Grounding   task respectively, to help the agent learn where to navigate   and what object to localize simultaneously. Moreover, the   agent is trained with a Memory-augmented action decoder   that fuses grounded textual representation and visual representation with memory augmented current state representation to generate action sequence. We extensively verify the   effectiveness of our proposed components and experimental results demonstrate that our result outperforms previous   methods signi\ufb01cantly. Nevertheless, how to bridge the performance gap between seen and unseen environments and   how to shorten the navigation length ef\ufb01ciently remains an   open problem for further investigation.   7043", "conf": "CVPR", "year": "2021", "index": 136}, {"title": "Published as a conference paper at ICLR 2023   PROGRAMMATICALLY   GROUNDED,   COMPOSITIONALLY GENERALIZABLE ROBOTIC MANIPULATION   Renhao Wang1\u2217   Jiayuan Mao2\u2217   Joy Hsu3   Hang Zhao1,4,5   Jiajun Wu3   Yang Gao1,4,5   1Tsinghua University   2MIT   3 Stanford University   4 Shanghai Arti\ufb01cial Intelligence Laboratory   5 Shanghai Qi Zhi Institute", "abstract": "Robots operating in the real world require both rich manipulation skills as well   as the ability to semantically reason about when to apply those skills. Towards   this goal, recent works have integrated semantic representations from large-scale   pretrained vision-language (VL) models into manipulation models, imparting them   with more general reasoning capabilities. However, we show that the conventional   pretraining-\ufb01netuning pipeline for integrating such representations entangles the   learning of domain-speci\ufb01c action information and domain-general visual information, leading to less data-ef\ufb01cient training and poor generalization to unseen objects   and tasks. To this end, we propose PROGRAMPORT, a modular approach to better   leverage pretrained VL models by exploiting the syntactic and semantic structures   of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action   across different modalities. Each functional module is realized as a combination   of deterministic computation and learnable neural networks. Program execution   produces parameters to general manipulation primitives for a robotic end-effector.   The entire modular network can be trained with end-to-end imitation learning   objectives. Experiments show that our model successfully disentangles action and   perception, translating to improved zero-shot and compositional generalization in a   variety of manipulation behaviors. Project webpage at: https://progport.github.io.   1", "content": "Robotic manipulation models that map directly from raw pixels to actions are capable of learning   diverse and complex behaviors through imitation. To enable more abstract goal speci\ufb01cation, many   such models also take as input natural language instructions. However, this vision-language manipulation setting introduces a new problem: the agent must jointly learn to ground language tokens to its   perceptual inputs, and correspond this grounded understanding with the desired actions. Moreover, to   fully leverage the \ufb02exibility of language, the agent must handle novel vocabulary and compositions   not explicitly seen during training, but speci\ufb01ed at test time (Fig. 1).   (a) Training Examples   (b) Zero-Shot Gen. Test   \u201cPack the yellow flower   into the brown box\u201d   \u201cPush the green blocks    into the blue square\u201d    \u201cPack the ring into the    brown box\u201d   \u201cPush the yellow flower   into the blue square\u201d   (c) Compositional Gen. Test   Figure 1: Zero-Shot and Compositional Generalization: Our framework, PROGRAMPORT, is   capable of generalizing to combinations of unseen objects and manipulation behaviors at test time.   \u2217equal contribution   1   arXiv:2304.13826v1  [cs.AI]  26 Apr 2023   Published as a conference paper at ICLR 2023   \u2018put the heart in the hole\u2019   \u2018put the    heart in    the hole\u2019   Control   Parameter   Decoder   Control   Parameters   do   goal   HEART   HOLE   IN   filter   PUT   HEART   IN   HOLE   filter   filter   goal   do   Program   (a) Pretraining-Finetuning Paradigm   filter   PUT   Text    Encoder   Image    Encoder   (b) Program-based Modular Paradigm (ProgramPort)   Figure 2: (a) Existing manipulation models entangle visual grounding from pretrained VL models with task-speci\ufb01c policy learning. (b) Our disentangled approach leverages program-based   representation of language semantics to yield faithful VL-grounding and improved genearlization.   To these ends, many recent works have relied on large pretrained vision-language (VL) models such   as CLIP (Radford et al., 2021) to tackle both grounding and zero-shot generalization. As shown   in Fig. 2a, these works generally treat the pretrained VL model as a semantic prior, for example,   by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn   et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2022a). These models are then updated via   imitating expert demonstrations. However, this training scheme entangles the learning of domainspeci\ufb01c control policies and domain-independent vision-language grounding. Speci\ufb01cally, we \ufb01nd   that these VL-enabled agents over\ufb01t to their task-speci\ufb01c data, leveraging shortcuts during training   to successfully optimize their imitation learning (IL) objective, without learning a generalizable   grounding of language to vision. This phenomenon is particularly apparent when the agent is given   language goals with unknown concepts and objects, or novel compositions of known concepts and   objects. For example, an agent that over\ufb01ts to packing shapes into a box can fail to generalize to   other manipulation behaviors (e.g. pushing) involving the same shapes.   In this work, we introduce PROGRAMPORT, a program-based modular approach that enables more   faithful vision-language grounding when incorporating pretrained VL models for robotic manipulation. Given the natural language instruction, we \ufb01rst use a Combinatory Categorial Grammar   (CCG) (Steedman, 1996) to parse the sentence into a \u201cmanipulation program,\u201d based on a compact but   general domain-speci\ufb01c language (DSL). The program consists of functional modules that are either   visual grounding modules (e.g., locate all objects of a given category) or action policies (e.g., produce   a control parameter). This enables us to directly leverage a pretrained VL model to ground singular,   independent categories or attribute descriptors to their corresponding pixels, and thus disentangles   the learning of visual grounding and action policies (Fig. 2b).   Our programmatically structured, modular design enables PROGRAMPORT to successfully learn more   performant imitation policies with fewer data across 10 diverse tasks in a tabletop manipulation   environment. We show that after training on a subset of objects, visual properties, and actions, our   model can zero-shot generalize to completely different subsets and reason over novel compositions   of language descriptors, without further \ufb01netuning (Fig. 1).   2   BACKGROUND   Problem formulation. We take a manipulation primitive-based approach to robot learning. At a   high level, we assume our robot is given a set of primitives P (e.g., pick, place, and push). Such   primitives are usually de\ufb01ned by interaction modes between the robot and objects, parameterized by   continuous parameters, and their composition spans a wide range of tasks. Throughout the paper, we   will be using the simple primitive set P = {pick,place} in a tabletop environment as the example,   and extend our framework to other primitives such as pushing in the experiment section.   Therefore, our goal is to learn a policy \u03c0 mapping an input observation ot at time t to an action at.   Each at is a tuple of two control parameters (Tpick,Tplace), which, in the pick-and-place setting,   2   Published as a conference paper at ICLR 2023   parameterizes the poses for the robot end effector when picking and placing objects, respectively.   This formulation also generalizes to other primitives such as pushing, in which case the control   parameters are the pre and post-push locations. More speci\ufb01cally, in a language-guided tabletop   manipulation task, our observation is given by ot = (xt,lt). Here, xt \u2208RH\u00d7W \u00d74 is the top-down   orthographic RGB-D projection of the 3D visual scene, while lt is a natural language description of   the desired goal. The control parameters Tpick,Tplace both lie in SE(2).   To learn such a policy,   we assume access to a set of expert demonstrations D   =   {d1,d2,...,dn}, each consisting of discrete time-indexed scene-language-action tuples so that   di = {(x1,l1,a1),(x2,l2,a2),...,(xj,lj,aj)}. The policy \u03c0 can be trained with an imitation   learning objective by randomly sampling a particular tuple at time t from any demonstration.   Vision-language manipulation. Akin to Transporter Networks (Zeng et al., 2020), our policy \u03c0 is   composed of two functions, Qpick and Qplace, outputting dense pixelwise action values over RH\u00d7W ,   and operating in sequence. Qpick \ufb01rst decides a particular pixel location Tpick in the top-down visual   \ufb01eld to pick the target object; Qplace then conditions on the pick location to decide a particular place   pose Tplace for the object:   Tpick = arg max   (u,v)\u2208H\u00d7W   Qpick ((u,v) \u2223xt,lt),   Tplace = arg max   \u03c4i\u2208{\u03c4}   Qplace (\u03c4i\u2223xt,lt,Tpick)   (1)   where poses representing the manipulation end effector primitives are apriori discretized into the   set {\u03c4}, composed of discrete pixel locations and a discrete set of 2D rotation angles. Recently,   CLIPort (Shridhar et al., 2022a) proposed to integrate pretrained vision-language (VL) models into   such frameworks. Shown in Fig. 2a, in CLIPort, both Qpick and Qplace are based on two-stream fullyconnected network (FCN) architectures. The encoders of the FCNs are initialized from pretrained   weights of CLIP (Radford et al., 2021), while the control parameter decoders are randomly initialized   and optimized. Similar paradigms have also been applied to other manipulation environments such as   Khandelwal et al. (2022) and Ahn et al. (2022).   While the pretraining-\ufb01netuning framework can leverage features from the pretrained CLIP encoders   to inform the model on a wide array of vision-language semantics, its monolithic structure entangles   the learning of vision-language grounding and action policies. This has two drawbacks: poor data   ef\ufb01ciency and over\ufb01tting. More speci\ufb01cally, since training data will only contain a \ufb01nite number   of compositions of action, category, and property terms, the model usually fails to generalize to   manipulation instructions that contain novel compositions of seen terms and unseen terms.   3   PROGRAMPORT   The key idea of our framework PROGRAMPORT is that reliably utilizing pretrained vision-language   (VL) models in manipulation involves leveraging the structure of natural language. Given the visual   input and natural language instruction, PROGRAMPORT parses the instruction into a program based on   a domain-speci\ufb01c language (DSL). Each generated program corresponds to a hierarchical structure   of functional modules implemented by neural networks, each ful\ufb01lling a speci\ufb01c operation over the   vision or action representation. The program executor executes the program upon the visual input   and derives the control parameters to action primitives. This compositional design disentangles the   learning for visual grounding modules (e.g., locating the objects being referred to) and the action   modules (e.g., predicting the control parameters). Concretely, it enables us to directly leverage   pretrained VL models such as CLIP as the visual grounding module without further \ufb01netuning, and   to only focus on learning action modules, which improves the transparency, data ef\ufb01ciency, and the   generalization of the system. A visual overview of our framework is available in Fig. 3.   In the remainder of this section, we \ufb01rst describe the structure of our manipulation programs, and how   they are derived from natural language instructions (Section 3.1). We then detail the neural modules   which realize their execution (Section 3.2). Finally, we specify the training paradigm (Section 3.2).   3.1   PROGRAMMATIC REPRESENTATIONS OF MANIPULATION INSTRUCTIONS   At the core of the proposed PROGRAMPORT framework is a programmatic representation for instructions. Illustrated in Fig. 2b, we use a program to represent the underlying semantics of the natural   language instruction. Each program has a hierarchical structure composed of functional modules.   Each functional module takes the outputs from its predecessors, the visual input, and optionally   additional concepts from the language (e.g., red) and computes the outputs (e.g., an attention mask).   3   Published as a conference paper at ICLR 2023   filter(hexagon)   filter(box)   goal(in)   do(pack)   Grounding Modules   Action Modules   \u201cPack the blue    hexagon in the    orange box\u201d   CCG    Parser   Instruction   Scene   Inputs   filter(blue)   filter(orange)   Figure 3: The execution of a manipulation program. A CCG parser \ufb01rst generates a manipulation   program. The task-agnostic visual grounding modules then identify visual semantics associated with   the desired objects and targets. Finally, the task-speci\ufb01c action modules leverage these grounding   results to output the manipulation control parameters.   We present the detailed de\ufb01nition of our domain-speci\ufb01c language (DSL) in Appendix A.1. All   functional modules can be categorized into two groups: visual grounding modules and action   modules. Visual grounding modules take as input language descriptors, and return spatial masks over   the corresponding locations in the visual scene. In this paper, the only visual grounding module we   consider is the \ufb01lter operation, which localizes objects that have certain properties (as in \ufb01lter(blue,   ...) will return blue objects in the scene). In contrast, action modules generate control parameters   for robot primitives. They take as input the outputs from visual grounding modules. For example,   the goal module generates a 2D pose that has a speci\ufb01ed spatial relationship with another object (as   in goal(in, ...) will return a location in the stated object). The do(pack, ..., ...) operator takes three   arguments: the action name pack, the object to be packed, and the goal location, both speci\ufb01ed as   masks over the input image, and generates the control parameter for the robot\u2019s next move.   Semantic parsing with a combinatory categorial grammar. To parse language instructions into a   program structure, we use the Combinatory Categorial Grammar (CCG) formalism (Steedman, 1996).   A CCG parser is composed of a small (less than 10) set of universal categorial grammar rules and a   domain-speci\ufb01c lexicon. Each lexicon entry maps a natural language word (e.g., red) to its syntactic   type (e.g., N/N) and semantic form (e.g., \u03bbx.\ufb01lter(red,x)). The notation N/N indicates that this word   can be combined with a word on its right with type N (noun), and the semantic form indicates that   this word corresponds to a visual grounding module that selects red objects from the scene.   PROGRAMPORT uses a prede\ufb01ned lexicon, manually speci\ufb01ed by the programmers. We choose   to use a prede\ufb01ned CCG for three main reasons. First, CCG enables a compact de\ufb01nition of a   domain-speci\ufb01c semantic parser. For each word in the vocabulary, we only need to annotate 1 or   2 entries. Second, the CCG parser is naturally compositional: given only the syntax and semantic   forms for individual words, it is able to parse whole sentences with complex linguistic structures   such as nested prepositional phrases. Third, and most importantly, the CCG parser can handle   unknown words at test time. In particular, following work on syntactic bootstrapping (Abend et al.,   2017; Alishahi & Stevenson, 2008), for a novel word (e.g., pack the daxy shape into the box), the   parser \ufb01rst \u201cguesses\u201d its syntactic type by ensuring that the entire sentence parses successfully   (in this case, the inferred syntactic type is N/N), and predicts its semantic form following a prior   distribution of p(semantics\u2223syntax), under the current lexicon. In this case, the predicted semantic   form is \u03bbx.\ufb01lter(daxy,x).   3.2   PROGRAMPORT ARCHITECTURE   PROGRAMPORT executes the parsed programs by following their hierarchical structures. Overall,   both visual grounding and action modules consume outputs from their predecessors and individual   concepts parsed by the CCG, instead of the entire language instruction, and produce outputs. By   disentangling manipulation in such a structured fashion, our architecture can learn to \ufb02exibly leverage   vision-semantic priors across a myriad of speci\ufb01c manipulation contexts.   4   Published as a conference paper at ICLR 2023   Visual grounding modules. The key functionality of a visual grounding module is to localize visual   regions corresponding to objects with a given language-speci\ufb01ed property (i.e., \ufb01lter(\ufb02ower,...)). To   imbue our manipulation model with semantic understanding of diverse object properties and spatial   relationships, including those unseen in the manipulation demonstration data, we borrow from the   recent zero-shot semantic segmentation model MASKCLIP (Zhou et al., 2022). Given the input image   xt and a singular concept c (e.g., \ufb02ower), we use the image encoder G and text encoder H from a   pretrained CLIP model to obtain deep features G(xt) \u2208RH\u2032\u00d7W \u2032\u00d7D1 and H(c) \u2208RD2, respectively*.   Here, we denote the spatial dimensions of the visual features as H\u2032 \u00d7 W \u2032, and the feature dimensions   of the visual and language streams as D1,D2, respectively.   Recall that vanilla CLIP uses attention pooling to pool the visual features at different spatial locations   into a single feature vector, and apply a linear layer to align the vector with the language encoding.   We adopt a simple modi\ufb01cation from MASKCLIP to achieve a similar semantic alignment of features,   while preserving the spatiality of G(xt). In particular, we initialize two 1 \u00d7 1 convolution layers   Cv and Cl with weights from the penultimate and last linear layers of the CLIP attention pooling   mechanism, respectively. The Cv layer is initialized from the value encoder in the QKV-attention   layer (Vaswani et al., 2017), while the Cl layer is initialized from the last linear layer. Applying them   sequentially will project the visual feature into the same space as the language feature. Finally, our   grounding module Fgrounding generates a dense mask over image regions of concept c via:   Fgrounding(xt,c) = \u2211[TILE (H(c)) \u2299Cl (Cv (G(xt)))] \u2208RH\u2032\u00d7W \u2032,   (2)   where TILE de\ufb01nes a spatial broadcast operator, in this case for the language embedding vector to   the spatial dimensions of the visual features, the \u2299is the Hadamard product, and the summation   is over the feature dimension D2. This formulation of our semantic module allows us to leverage   CLIP\u2019s large-scale vision-language pretraining to generalize semantics to open vocabulary categories,   including objects and their visual properties. Moreover, since our semantic program has decomposed   the input language phrase lt into individual visual concepts, we can compose multiple \ufb01lter programs   with the intersection operation. Speci\ufb01cally, this is implemented as in NS-CL (Mao et al., 2019) by   taking the element-wise min of the generated masks, parsing out image regions that do not satisfy   the combination of language descriptors.   Action modules. At their core, our action modules extend the architecture from CLIPORT, and \ufb01nally   generate the control parameters (Tpick,Tplace). In the pick-and-place setting, the visual grounding of   the object to be picked (i.e., the mask output by the corresponding \ufb01lter operations) can be directly   interpreted as a distribution for Tpick. Therefore, in this section, we will focus on how PROGRAMPORT   predicts Tplace conditioned on Tpick, the visual grounding of the target location (e.g., the orange box),   and the desired relation (e.g., in). The corresponding module is the \u201cgoal\u201d operator. There are two   design principles. First, besides the image and text, the action modules should also be conditioned   on the grounding results from visual grounding modules (i.e., a spatial grounding map indicating   the position of target objects). Second, we want to prevent over\ufb01tting the action module to visual   categories presented at training time. Therefore, we extend the architecture for Qplace to take the   spatial grounding maps from previous visual modules with the following modi\ufb01cations.   First, we bilinearly upsample or downsample the input masks to the spatial dimensions of different   layers within the CLIPort feature backbone, and multiply the interpolated grounding map with these   intermediate layers. In contrast to other fusion strategies such as concatenation, this fusion strategy   ensures that the grounding map will not simply be ignored by the action modules. Second, we   upsample the grounding map to the spatial dimensions of the output action maps, and perform direct   element-wise matrix multiplication with the action map. Since the grounding map can be viewed as   a mask of the objects being referred to, this multiplication effectively forces the action modules to   attend to regions where pick objects and place locations are described in the manipulation instruction.   Overall, the end-effector poses in SE(2) can be output by appropriately adapting Eq. (1):   Tplace = arg max   \u03c4i\u2208{\u03c4}   Up(Fgrounding) \u2299Qplace (\u03c4i\u2223xt,lt,Tpick,Fgrounding),   (3)   where Up(\u22c5) is an upsampling operator, and \u2299is the Hadamard product. More details are available   in Appendix A.6. In the simple SE(2) pick-and-place setting under the Transporter or CLIPort   framework, simply specifying the locations of the object and its target pose is suf\ufb01cient, and thus   do(pack, ..., ...) operates as an identity function.   *In practice we use a language prompt instead of a single word, following MASKCLIP (Zhou et al., 2022).   5   Published as a conference paper at ICLR 2023   Example.   Recall that the program for the instruction \u201cpack the blue hexagon in the orange box\u201d   is given by do(goal(\ufb01lter(\ufb01lter(hexagon), blue), \ufb01lter(\ufb01lter(box), orange), in), pack). As shown   in Fig. 3, our \ufb01rst set of \ufb01lter modules take the input image and output masks corresponding to   \u201chexagon\u201d shapes and \u201cbox\u201d objects. The next stage similarly \ufb01lters for \u201cblue\u201d and \u201corange\u201d objects.   To compose these adjectives with their corresponding nouns from the previous stage, we take their   elementwise min, returning masks corresponding to \u201cblue hexagon\u201d and \u201corange box\u201d, respectively.   Next, our goal module takes the masks for the referred objects, which correspond to the picked   object and its general place region, and outputs a distribution over SE(2) pose for the object that   is semantically consistent with the \u201cin\u201d target location. Finally, our do module outputs the \ufb01nal   control parameters to the robot end-effector. In this case, it contains Tpick from the execution of   \ufb01lter(\ufb01lter(hexagon), blue), and the Tplace output by the goal module.   Training. Our model can be trained in an end-to-end fashion, given the observation-action tuples   (xt,lt,at), via an imitation learning objective. Note that each action at is itself a tuple described by   (Tpick,Tplace). Thus, given a random observation-action tuple (xt,lt,at) sampled from some expert   demonstration di \u2208D at time t , we \ufb01t our model by minimizing the cross-entropy loss:   L = \u2212ETpick [log softmax(Qpick((u,v)\u2223xt,lt)] \u2212ETplace [log softmax(Qplace(\u03c4i\u2223xt,lt,Tpick)]   (4)   4   EXPERIMENTS   In this section, we will show that the modularity of PROGRAMPORT enables better vision-language   grounding and generalization, which empirically translates to strong zero-shot generalization (i.e.,   understanding instructions with unseen visual concepts), and compositional generalzation (i.e.,   understanding novel combination of previously seen visual and action concepts). We also provide   validation of our approach on real world experiments in Appendix A.4.   Dataset. Our dataset extends the benchmark proposed by CLIPORT (Shridhar et al., 2022a), in turn   based on the Ravens benchmark from Zeng et al. (2020). The CLIPORT benchmark consists of 10   language-conditioned manipulation tasks in a PyBullet simulation environment. Tasks are performed   by a Universal Robot UR5e with a suction gripper end-effector. Every episode in a task is constructed   by sampling a set of objects and properties (such as object type, color, size etc.), and manipulating a   subset of the objects to some desired conformation, which is described by a language goal.   Model details, training, and evaluation. Our visual grounding modules use the weights from the   frozen, pretrained ViT-B/16 CLIP encoder. We train for 200k iterations with AdamW and a learning   rate of 0.0002. We use either n = 100 or n = 1000 training demonstrations, and evaluate on 100   test demonstrations. Our evaluation metric follows Shridhar et al. (2022a) based on the same CLIP   encoders, where a dense score between 0 (failure) and 100 (success) represents the fraction of the   episode completed correctly. For baselines, we compare to CLIPORT across two settings: the single   setting, where the model trains and tests on a single task at a time, and the multi setting, where the   model trains on all tasks at once, and is independently evaluated on each individual task.   4.1   ZERO-SHOT GENERALIZATION   To evaluate zero-shot generalization, all of our main results in Table 1 use the \u201cunseen\u201d variants of the   tasks, as described in Shridhar et al. (2022a). Concretely, the train and test splits of each task involve   different objects and object properties, with minimal overlap. For example, out of 11 total possible   colors for objects, only 3 are shared for both train and test episodes. Or for tasks involving real-world   objects from the Google Scanned Objects dataset (Downs et al., 2022), a total of 56 household   items are split into mutually exclusive sets of 37 train items and 19 test items. Crucially, for a   fair comparison, these test language descriptors are also entirely missing from the prede\ufb01ned CCG   lexicon, and so semantic goal parsing must rely on the probabilistic approach described in Section 3.1.   Our results show that our model effectively grounds object concepts described in language with the   visual scene, and with the exception of the align-rope task, outperforms our baselines across the   benchmark (Table 1). Align-rope is challenging for PROGRAMPORT because it is hard for the CLIP   model to ground abstract spatial concepts used in this task (e.g. front and back).   Data ef\ufb01ciency. Not only do we outperform the baselines by a large margin, but on many tasks,   our method does signi\ufb01cantly better with 10\u00d7 fewer demonstrations. For example, on the \u201cpackingshapes\u201d task, our best-performing model trained on 100 demonstrations outperforms its corresponding   baseline trained on 1000 demonstrations by 25.0 points.   6   Published as a conference paper at ICLR 2023   packingbox-pairs   packing-googleobjects-seq   packing-googleobjects-group   separatingsmall-piles   separatinglarge-piles   # demonstrations   100   1000   100   1000   100   1000   100   1000   100   1000   CLIPORT (single)   67.1   72.0   64.1   70.8   79.3   82.1   75.9   74.3   62.4   66.9   CLIPORT (multi)   77.1   72.3   66.9   71.6   77.0   81.7   65.9   63.7   54.2   59.1   PROGRAMPORT (single)   72.6   78.0   78.4   79.7   79.0   81.2   76.8   77.9   63.8   69.5   PROGRAMPORT (multi)   82.3   85.7   73.5   84.0   80.7   83.9   77.0   78.8   63.0   68.2   align-rope   packing-shapes   assembling-kits   put-blocks-in-bowls   towers-of-hanoi   # demonstrations   100   1000   100   1000   100   1000   100   1000   100   1000   CLIPORT (single)   84.8   94.1   42.0   38.0   32.2   34.7   36.8   29.9   94.6   99.0   CLIPORT (multi)   81.2   70.3   38.0   32.0   30.2   26.9   55.9   46.7   75.2   69.1   PROGRAMPORT (single)   77.5   86.9   52.0   54.0   40.6   42.4   38.2   38.7   97.6   98.4   PROGRAMPORT (multi)   78.4   89.1   57.0   63.0   39.2   46.8   54.2   57.7   97.0   96.3   Table 1: Zero-Shot Generalization. Training and validation splits for all tasks involve different   objects, object properties, or other spatial descriptors.   Input   \u201cPack the lion figure    in the brown box\u201d   ProgPort   CLIPort   Figure 4: Qualitative pick and place affordances for PROGRAMPORT and CLIPORT. PROGRAMPORT   compositionally reasons over independent visual semantics in a zero-shot fashion, correctly picking   and placing the target object. CLIPORT identi\ufb01es both pick object and place location incorrectly.   This data ef\ufb01ciency is derived from our improved grounding properties: by semantically parsing   and visually grounding one object concept at a time, we more effectively leverage the pretrained VL   model. Moreover, note that the best-performing CLIPORT models for different tasks are mostly the   single variants. This suggests the zero-shot performance of CLIPORT relies heavily on over\ufb01tting   to the actions of the speci\ufb01c task, as opposed to leveraging the general visual grounding enabled   by CLIP. With PROGRAMPORT, the gap between our single and multi model variants is much   smaller, and indeed the multi variants are often the stronger manipulators. PROGRAMPORT thus   leverages diverse visual and action information across tasks more effectively, and possesses scalability   with respect to both the number of demonstrations as well as the number of tasks.   Improved vision-language grounding. Qualitatively, we show more interpretable and robust affordance maps compared to the baseline (Fig. 4). By grounding a single object or property at a time,   and then leveraging our DSL operations to compose together these masks, we ground the different   concepts. For Tpick, the lion \ufb01gure is extremely small within the visual scene, and positioned in a   crowded area; our visual grounding modules are still able to compose the concepts of \ufb01gure and   lion together in a zero-shot manner, informing an accurate pick affordance. Similarly, for Tplace, the   baseline is confused by the visual similarity between the box and the porcelain plate. By contrast,   our visual grounding modules compose box and brown together to focus on the left box-like object.   4.2   COMPOSITIONAL GENERALIZATION OF VISUAL GROUNDING AND BEHAVIORS   To evaluate compositional generalization, we introduce 4 new tasks which re-combine basic unary   object concepts from the original 10 tasks (Table 2). We do not train directly on these tasks; by   evaluating on them only, we assess the ability to reason compositionally about the distinct visual   properties, spatial relationships, and manipulation primitives learned at training time. For example,   the packing-location-box task contains instructions of the form \u201cpack the {shape} into the {loc}   box\u201d, where both shape and loc are objects and spatial descriptors seen independently throughout   7   Published as a conference paper at ICLR 2023   packing-color-box   packing-location-box   separating-location-piles   pushing-shapes   100   1000   100   1000   100   1000   100   1000   CLIPORT (single)   23.0   24.0   11.0   13.7   48.1   43.8   2.4   3.2   CLIPORT (multi)   19.0   18.2   10.0   8.3   39.9   41.0   2.7   7.6   PROGRAMPORT (single)   64.0   68.0   56.0   62.5   69.2   72.4   32.3   36.9   PROGRAMPORT (multi)   72.0   76.3   62.0   67.1   68.4   70.5   36.6   40.2   Table 2: Compositional Generalization. Models can learn different object shapes, colors, and   manipulation properties from different tasks. We show that only PROGRAMPORT hierarchically   composes these learned concepts and behaviors together to perform well in more complex tasks.   Method   Semantic   Supervision   packingshapes   packing-googleobjects-seq   assemblingkits   separatingsmall-piles   put-blocksin-bowls   100   1000   100   1000   100   1000   100   1000   100   1000   PROGRAMPORT (single)   MaskCLIP   52.0   54.0   78.4   79.7   40.6   42.4   76.8   77.9   38.2   38.7   GT   97.0   98.0   95.8   95.9   92.0   93.4   80.6   86.4   97.2   96.3   PROGRAMPORT (multi)   MaskCLIP   57.0   63.0   73.5   84.0   40.6   42.4   77.0   78.8   54.2   57.7   GT   96.0   98.0   96.2   96.4   91.0   94.5   81.2   87.2   99.3   97.7   Table 3: Disentanglement of visual semantics and action. Replacing MASKCLIP grounding maps   with ground truth segmentation maps of objects and targets yields near-perfect manipulation.   various tasks at training time, but now appear in a test-time context that requires the model to reason   semantically over their combination. For a full speci\ufb01cation of these tasks, please refer Appendix A.2.   Length-based generalization to new action settings. Our fourth generalization task (pushingshapes) is much more dif\ufb01cult, involving language instructions of the form \u201cpush the {color} {shape}   into the {location} {color} square\u201d. Note that this task evaluates both compositional generalization   and \u201clength generalization\u201d. The latter assesses the ability of the model to generalize beyond the   assumption of at most one visual descriptor per object in similar training tasks. Furthermore, this   task requires the model to zero-shot generalize to entirely new actions; only pushing of simple,   regularly-structured blocks is learned at training time, whereas shapes are much larger, irregular, and   have different friction properties. PROGRAMPORT exhibits much better performance here (+32.6   mean reward), compared to the CLIPORT baseline, which fails entirely.   4.3   ABLATION: DISENTANGLEMENT OF ACTION AND PERCEPTION   To illustrate the disentanglement of action and perception in our modular design, we perform an additional ablation study. Speci\ufb01cally, we train our model normally on all 10 main tasks from Section 4.1,   and at test time, we choose 5 diverse tasks out of the 10. For each test episode, we replace the attention   maps generated by the semantic modules with ground truth segmentation maps corresponding to the   picked objects and place locations. This effectively poses the question: given a perfect perception of   the scene, how would our action modules behave? A properly disentangled model would be able to   leverage the noiseless ground truth visual grounding and decide the appropriate actions to perform.   As apparent in Table 3, performance across all tasks improves dramatically when provided the ground   truth information. This suggests that our action modules do not over\ufb01t to task-speci\ufb01c shortcuts   when optimizing the imitation objective, but actually learn to leverage the attention maps provided by   the semantic modules in a disentangled manner. Such disentanglement has also been translated to   improved compositional generalization for our model.   5   RELATED WORK   Learning language-guided robot behavior. Many prior works have explored enhancing robotic   behavior through language-conditioned imitation learning (Lynch & Sermanet, 2021; Stengel-Eskin   et al., 2022; Stepputtis et al., 2020) and reinforcement learning (Andreas et al., 2018; Luketina et al.,   2019; Misra et al., 2017). With the rise of powerful transformer-based architectures, recent works   have used large-scale pretrained vision-language representations to improve performance for tabletop   manipulation (Shridhar et al., 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill   learning and planning (Ahn et al., 2022; Huang et al., 2022). For example, CLIPORT shows that by   8   Published as a conference paper at ICLR 2023   integrating embeddings from the CLIP VL model into the Transporter manipulation framework (Zeng   et al., 2020), a robot can better generalize to manipulating new objects in a zero-shot manner.   However, in contrast with our method PROGRAMPORT, these works do not leverage the semantic   structure in natural language to ground individual visual and action concepts. Rather, prior works use   the pretrained language model as a monolith that consumes the entire language input and outputs a   latent embedding. This embedding is not grounded to individual concepts within the language inputs,   which leads to poor generalization when faced with novel (combinations) object attributes.   Neurosymbolic learning. Prior works have shown neurosymbolic learning, which refers to the   integration of deep neural networks for pattern recognition and symbolic structures such as programs   for reasoning, as an effective mechanism for enforcing VL grounding (Mao et al., 2019; Wu et al.,   2019). Mao et al. (2019) is a representative work that learns to jointly disentangle independent   visual concepts and parse natural language sentences, from natural supervision in VQA. The learned   concepts can be composed for novel downstream tasks via programmatic (re)composition of neural   network modules, enabling zero-shot and compositional generalization. More generally, a structured   approach involving neural perception of vision and language, and higher-order symbolic reasoning   has appeared previously (Li et al., 2020; Nottingham et al., 2021; Sun & Alexandre, 2013). Beyond   improved VL-grounding, this approach has also translated to improved performance and data ef\ufb01ciency on many downstream tasks, such as reinforcement learning (Cheng et al., 2019; Verma et al.,   2019), scienti\ufb01c discovery (Cranmer et al., 2020; Shah et al., 2020) and robotics (Zellers et al., 2021).   Our work is similarly inspired to combine structured representations with deep learning, in the new   setting of robot manipulation. However, instead of learning a small set of visual concepts, we leverage   pretrained VL models to cover a larger, open-ended set of concepts, and improve our zero-shot   capabilities. Moreover, we ground not just visual concepts with their language descriptors, but also   abstract actions for robotic primitives, while preserving compositional generalization capability.   Vision-language (VL) grounding. The goal of VL grounding is to identify correspondences between visual input and descriptors in natural language. Common tasks include visual question   answering (Antol et al., 2015), VL navigation (Anderson et al., 2018), or VL manipulation (Shridhar   et al., 2022a;b). Recent works show that the CLIP (Radford et al., 2021) model trained on 400M   image-caption pairs from the web possesses zero-shot VL grounding capabilities for a wide array of   visual concepts (Kim et al., 2021; Shen et al., 2022). In robotics, this has translated to grounding of   robotic affordances with natural language in models such as CLIPORT (Shridhar et al., 2022a).   However, we \ufb01nd that the na\u00a8\u0131ve approach taken by these works, which leverage only the frozen,   pretrained CLIP embeddings, cannot successfully disentangle task-agnostic visual grounding with   task-speci\ufb01c action policies. Other works show that this na\u00a8\u0131ve integration of pretrained VL models   also affects grounding abilities in 3D space (Thomason et al., 2022), or temporal settings such as   video understanding (Le & Hoi, 2020). In particular, VL grounding is often adversely affected by endto-end training (e.g. via an imitation objective), and the resultant models are unable to reason about   novel concepts independently, or compose them de novo for more complex tasks (Lin et al., 2022).   This shortcoming is addressed by PROGRAMPORT through a program-based modular approach.   6   CONCLUSION   In this work, we propose PROGRAMPORT, a program-based modular framework for robotic manipulation. We leverage the semantic structure of natural language to programmatically ground individual   language concepts. Our programs are executed by a series of neural modules, dedicated to learning   either general visual concepts or task-speci\ufb01c manipulation policies. We show that this structured   learning approach disentangles general visual grounding from policy learning, leading to improved   zero-shot and compositional generalization of learned behaviors.   Future work of PROGRAMPORT includes generalization to more free-form natural language instructions and challenging scenarios where the syntactic context is not suf\ufb01cient to predict semantics of   novel words. Alternate techniques such as extending human-annotated CCG banks (Bisk & Hockenmaier, 2013; Zettlemoyer & Collins, 2012), and leveraging the wealth of pretrained language-to-code   models (Chen et al., 2021) can be a concrete path to alleviating this issue. Better representations   that can capture interactions between robots and objects (e.g. grasping poses for different objects)   is another important extension. Finally, subsequent work may also consider jointly learning and   leveraging pretrained representations that are more suitable for grounding interaction modes between   9   Published as a conference paper at ICLR 2023   robots and objects, especially for manipulating dif\ufb01cult articulated and deformable objects in our 3D   world (Shridhar et al., 2022b).   REPRODUCIBILITY STATEMENT   Our simulated dataset details are provided in the Appendix A.2. The most salient training details and   hyperparameters are presented in Section 4 of the main text. DSL details can be found in Appendix A.1   and architecture details are available in Appendix A.6. Our code will also be publicly available.   ACKNOWLEDGMENTS   This work is in part supported by the Stanford Institute for Human-Centered AI (HAI), Analog   Devices, JPMC, and Salesforce. This work is also supported by the Ministry of Science and   Technology of the People\u00b4s Republic of China, the 2030 Innovation Megaprojects \u201cProgram on New   Generation Arti\ufb01cial Intelligence\u201d (Grant No. 2021AAA0150000, No. 2022ZD0161700), as well as   a grant from the Guoqiang Institute, Tsinghua University.", "conf": "ICLR", "year": "2023", "index": 178}, {"title": "Grounding Answers for Visual Questions Asked by Visually Impaired People   Chongyan Chen1, Samreen Anjum2, Danna Gurari1,2   1 University of Texas at Austin   2 University of Colorado Boulder", "abstract": "Visual question answering is the task of answering   questions about images.   We introduce the VizWiz-VQAGrounding dataset, the first dataset that visually grounds   answers to visual questions asked by people with visual impairments.   We analyze our dataset and compare it with   five VQA-Grounding datasets to demonstrate what makes   it similar and different. We then evaluate the SOTA VQA   and VQA-Grounding models and demonstrate that current   SOTA algorithms often fail to identify the correct visual evidence where the answer is located. These models regularly struggle when the visual evidence occupies a small   fraction of the image, for images that are higher quality,   as well as for visual questions that require skills in text   recognition.   The dataset, evaluation server, and leaderboard all can be found at the following link: https:   //vizwiz.org/tasks-and-datasets/answergrounding-for-vqa/.   1.", "content": "Visual question answering (VQA) is the task of providing a natural language answer to a question about an image.   While most VQA services only return a natural language   answer, our work is motivated by the belief that it is also   valuable for a VQA service to return the region in the image used to arrive at the answer. We call this task of locating   the relevant visual evidence answer grounding.   Numerous applications would be possible if answer   groundings were provided in response to visual questions.   First, they enable assessment of whether a VQA model reasons based on the correct visual evidence. This is valuable   as an explanation as well as to support developers in debugging models. Second, answer groundings enable segmenting the relevant content from the background. This is a valuable precursor for obfuscating the background to preserve   privacy, given that photographers can inadvertently capture   private information in the background of their images [14]   (exemplified in Figure 1b). Third, users with low vision   could more quickly find the desired information if a service instead magnified the relevant visual evidence. This is   Figure 1. (a) We introduce a new dataset challenge that supports   the task of grounding the visual evidence needed to answer visual   questions asked by people with vision impairments. This enables   valuable use cases including (b) background obfuscation to limit   inadvertent privacy leaks and (c) automatic magnification to expedite low vision users\u2019 abilities to answer their questions.   valuable in part because answers from VQA services can be   insufficient, including because humans suffer from \u201creporting bias\u201d meaning they describe what they find interesting   without understanding what a person/population is seeking.   This is exemplified in Figure 1c, where the most popular response from 10 answers is the generic answer \u2018pasta\u2019 rather   than the specific flavor, \u2018Creamy Tomato Basil Penne\u2019.   While datasets have been introduced to encourage   progress on the answer grounding problem, all proposed   dataset challenges originate from contrived visual questions [6, 9, 11, 17, 18, 22, 26, 37, 42]. This includes scraping images from photo-sharing websites (e.g., Flickr) and   then generating questions automatically [6], by using image   annotations paired with question templates to create questions about the images or (2) manually [9, 11, 18, 42], by   asking crowdworkers to make up questions about an image that would stump a robot. Yet, prior work has shown   that such contrived settings can manifest different characteristics from authentic VQA use cases [15, 39].   This   can cause algorithms trained and evaluated on contrived   19098   datasets to perform poorly when deployed for authentic use   cases [15,16]. Moreover, this can limit the designs of algorithms since developers are oblivious to the additional challenges their algorithms must overcome.   We introduce the first answer grounding dataset that   originates from an authentic use case.   We focus on visual questions originating from blind people who both took   the pictures and asked the questions about them in order to   overcome real visual challenges [5]. This use case has been   shown to manifest different challenges than contrived settings, including that images are lower quality [8], questions   are more conversational [15], and different vision skills are   needed to arrive at answers [39]. For approximately 10,000   image-question pairs submitted by this population, we collected answer groundings. Then, we analyzed the answer   groundings to reveal their characteristics and show how they   relate/differ to five existing answer grounding datasets. Finally, we benchmarked state-of-the-art VQA and answer   grounding models on our dataset and demonstrate what   makes this dataset difficult for them, including smaller answer groundings, images that are of higher quality, and visual questions that require skills in text recognition.   We offer this work as a foundation for designing models that are robust to a larger range of potential challenges   that can arise in real-world VQA settings. Challenges observed in our dataset can generalize to other scenarios, such   as robotics and lifelogging, which similarly encounter varying image quality and textual information (e.g., grocery   stores). To encourage community-wide progress on such   challenges, we have organized a dataset challenge with public evaluation server and leaderboard. Details can be found   at the following link: https://vizwiz.org/tasksand-datasets/answer-grounding-for-vqa/.   2. Related Work   VQA Datasets.   Many large-scale VQA datasets have   been proposed over the past six years [19, 32, 33, 38]. A   key challenge the community has faced in developing such   datasets is the language bias problem [12, 23, 27, 30]. In   particular, models can learn to exploit superficial correlations observed in datasets by identifying common pairings   of answers and questions and never looking at the images.   For example, when given an image of a green banana with   an associated question asking \u201cwhat is the color of the banana?\u201d, the VQA model might answer \u201cyellow\u201d instead of   green because the answer \u201cyellow\u201d appears more frequently   in the answers for questions related to bananas. This problem has been possible, in part, because common evaluation   metrics [4] only assess model performance based on textual   answers. To combat such biases, new VQA datasets have   been created to more equally capture the range of possible   answers for each question [13] and many new models have   been developed using these balanced datasets. Our work   contributes to this body of work by enabling algorithm developers to go beyond only evaluating the textual answers   and also directly evaluate whether algorithms rely on the   correct visual evidence.   Answer Grounding Datasets.   Towards disentangling   the vision problem for VQA, several answer grounding   datasets already have been introduced that locate the visual content needed to arrive at each language-based answer [6, 9, 11, 18, 42]. While some were created by tracking where humans look when presented with a visual question [6,9] or collecting bounding boxes around the relevant   visual evidence [17, 22, 42], our work more closely aligns   with those where humans who annotated the relevant visual   evidence using a segmentation [11,18,26]. That is because   we also collect segmentations. We propose the first answer   grounding dataset that reflects an authentic VQA use case   and conduct extensive analysis to demonstrate how it relates/differs to five existing answer grounding datasets.   VQA and Answering Grounding Algorithms.   Modern   VQA algorithms often rely on attention maps to determine   where to look to find the answers to visual questions, with   this having been the dominant approach for a number of   years [9, 29, 41]. Recently, some researchers have shifted   their focus towards delivering the best VQA models possible under the constraint that the model should outperform   all other VQA models in attending to the correct visual evidence [37]. We benchmark the state-of-the-art VQA and   answer grounding models on our dataset to examine to what   extent they succeed in correctly grounding answers. Experiments reveal that our new dataset is challenging for modern   algorithms, and show what aspects make it challenging.   Assistive Technology for People with Vision Impairments.   Many people with visual impairments rely on visual assistance devices to learn about their surroundings.   For instance, people with low vision often rely on magnification tools to better observe content of interest [20,28,31],   given that they have limited sight that can\u2019t be regained fully   with corrective measures such as glasses. In addition, people with low vision and no vision rely on on-demand technologies [1, 2, 5] that deliver answers to submitted visual   questions. A challenge for this latter use case is that blind   people have no way to check if they inadvertently capture   private information in their images. Yet, roughly 12% of   pictures taken for a VQA use case contained privacy information [14] and visually impaired people have expressed   their discomfort with leaking private information [3,34,35].   Our work can contribute to a wide range of interests for people with vision impairments since answer groundings can   serve as a valuable precursor to smartly magnify visual answers to visual questions, mitigate biases in VQA services,   and support increased privacy measures with obfuscation.   19099   3. VizWiz-VQA-Grounding Dataset   We now introduce our dataset for grounding answers to   visual questions asked in an authentic use case where people who are blind were trying to learn about their visual surroundings. We call this dataset \u201cVizWiz-VQA-Grounding\u201d.   3.1. Dataset Creation   Dataset Source.   Our work builds upon the VizWiz-VQA   dataset [15], which consists of 32,842 image question pairs   where each comes with 10 crowdsourced answers. In addition to the VQA triplets for the publicly-available train   and validation splits, we also consider those from the test   split since the authors of [15] provided us with answer annotations for this purpose. The images and questions come   from visually impaired people who shared them to solicit   visual assistance in their daily lives.   Dataset Filtering.   We designed our dataset to focus on   grounding answers for visual questions that could unambiguously be grounded to a single region. Towards achieving this, we filtered the initial dataset using a combination   of automated and manual techniques that are described in   the Supplementary Materials.   In summary, we removed   all questions that were non-answerable, embedded multiple sub-questions, referred to multiple regions in an image   due to ambiguity, could not be grounded, or for which the   majority of crowd did not agree on a single answer. This   process left a total of 9,998 VQAs, which we use for our   new dataset. We focus on grounding only the single most   popular answer for each visual question.   Grounding Task Design.   Through iterative pilot studies,   we designed a user interface for grounding answers to visual questions. A person is shown the question-answer pair   just above the image and must then demarcate the answer   grounding by clicking a series of points on the image to create a connected polygon. We provide extensive instructions   to cover many annotation scenarios that may be tricky. For   instance, when multiple regions containing the same thing   need to be annotated (e.g., a bunch of flowers), we instruct   the annotator to demarcate all relevant content with a single   polygon when the regions are connected. When the question asks about a property of a visual entity (e.g., color of   clothes), we instruct the annotator to comprehensively annotate all regions that would lead to the answer rather than   a minimum viable region.   Annotation Collection.   We implemented a number of   techniques to support collecting high quality results. First,   we developed a rigorous three-step filtering process to recruit expert crowdworkers from Amazon Mechanical Turk   to complete our annotation tasks, which we describe in   the Supplementary Materials. In summary, we limit which   users can complete our tasks, provide a qualification test,   and then select several expert workers who we identified as   excelling at our task for a larger number of tasks. Next,   for all work submitted by these expert workers, we conduct automated and manual quality control to verify that we   can continue to trust their work. Finally, we collected two   answer groundings per VQA instance (i.e. image-questionanswer triplet). Based on our subsequent analysis for when   and why answer groundings differed, we decided to select   the larger of the two annotations as our ground truth annotation because we found that annotation differences typically   occurred because one was a sub-region of the other.   3.2. Dataset Analysis   We now analyze the VizWiz-Visual Grounding dataset,   which consists of 9,998 groundings for the 9,998 VQA   triplets. To do so, we compute for each grounding its:   \u2022 Location: position of its center of mass relative to the   entire image; i.e., a (x,y) coordinate. Each coordinate   can range from 0 to 1.   \u2022 Boundary complexity: entropy of the histogram of   the normalized centroid contour distance [7], where   centroid contour distance is the distance of every point   on the segmentation boundary to the segmentation\u2019s   center of mass. Values can range from 0 to 1.   \u2022 Image coverage: fraction of pixels it occupies from   all pixels in the image. Values can range from 0 to 1.   To compare our dataset to the current focus of the research community, we also evaluate answer groundings   from existing datasets that were similarly generated through   a manual segmentation annotation process:   VQS [11],   VQA-X [18], and TextVQA-X [26].   For completeness,   we also include CLEVR-Answers [37] and GQA [17], as   these groundings were used to validate the state-of-the-art   answer grounding method [37].   Unlike our dataset, the   images for all these datasets are either scraped from the   Internet\u2014including VQS, VQA-X, and GQA which leverage images from COCO [25] and TextVQA-X which leverages images from Open Images v3 [21]\u2014or computer generated, as is the case for CLEVR-Answers. Another difference from our dataset is that the questions for these datasets   were either generated by crowd workers \u2014for VQS, VQAX, TextVQA-X\u2014or computer generated, as is the case   for GQA and CLEVR-Answers. To support fair comparison, we only consider visual questions in these datasets for   which there is exactly one answer grounding region.   Overall Results.   For all datasets, statistics about the answer groundings\u2019 location are shown in Table 1 and boundary complexity and image coverage are shown in Figure 2.   Overall, we observe that all datasets have answer   groundings that typically lie close to the center of the image   (Table 1). This is evident from the combination of mean   19100   Relative location of groundings   Ours   (0.48\u00b10.14, 0.51\u00b10.15)   VQA-X   (0.50\u00b10.16, 0.53\u00b10.19 )   VQS   (0.50\u00b10.20, 0.52\u00b10.21)   TextVQA-X   (0.49\u00b10.21 , 0.48\u00b10.25)   CLEVR-Ans   (0.50\u00b10.19, 0.45\u00b10.13)   GQA   (0.50\u00b10.20, 0.54\u00b10.19)   Table 1. Shown is the mean and standard deviation of the location of all answer groundings with respect to the images for each   dataset. Across all datasets, answer groundings tend to be located   near the center of the image.   centroids around the coordinates (0.5, 0.5) and relatively   small standard deviations from those coordinates. We found   this result surprising for our dataset since visually impaired   photographers cannot verify they center the content of interest when taking the pictures.   Regarding the complexity of the boundaries for answer   groundings, our dataset lies in the middle of all datasets. On   one extreme lies both datasets for which bounding boxes are   collected: CLEVR-Ans and GQA. That is because the value   computed for boundary complexity is 0 when the boundary is a rectangle. Interestingly, we observe for VQS that   over half of the answer groundings also are rectangles, as   shown by its median score of 0. At the other extreme, lies   the following datasets: VQA-X and TextVQA-X. We suspect the higher boundary complexity for TextVQA-X is due   to its annotation collection approach. Specifically, it is the   only dataset which collected groundings using a paintbrush   rather than a series of points clicked around the boundary   of an object. Implicitly, this annotation approach leads to   a seemingly more complex boundary when a simple polygon might suffice. Our dataset is most similar to VQA-X,   which has the highest median complexity, but our dataset   also exhibits a smaller range of complexity values.   The key criteria that makes our dataset different from   the other datasets, overall, is that its answer groundings occupy a larger range of image coverage values than all other   datasets. This is evident when comparing the box sizes for   all datasets in Figure 2(b). TextVQA-X occupies the smallest region. We suspect that VQA-X, VQS, and GQA share   similar sizes because they all were generated from the same   image source: COCO images. We attribute our dataset\u2019s   inclusion of many considerably larger answer groundings   than the other datasets to the possibility that zooming into   the content of interest when taking the picture may be a   more realistic approach photographers would take in realworld scenarios as they try to only photograph the pertinent   content for answering the question. This distinction is exemplified in Figure 3, where the visually impaired photographer took a photo by positioning a keyboard close to the   hand which held the camera. Altogether, this finding unFigure 2. The box plot shows, for each dataset, the range of values for the answer groundings\u2019 (a) boundary complexity and (b)   image coverage. For each box, the central mark denotes the median score, the box edges denote the 25th and 75th percentiles   scores, the whiskers denote the most extreme data points not considered outliers, and the outliers are plotted individually. Overall,   our dataset shows a moderate range for boundary complexity and   the greatest range for answer grounding size.   Figure 3. Answer grounding for a \u201ckeyboard\u201d in our dataset and in   the VQS dataset. This exemplifies that answer groundings in our   dataset can pertain to large regions in the images since photographers in an authentic use case zoom into the content of interest to   try and only photograph the pertinent content.   derscores a unique benefit of our dataset in that it motivates   the design of algorithms that can simultaneously locate very   large and very small regions.   Most Common Questions.   We next evaluate how our   dataset compares with existing answer grounding datasets   for the five most common visual questions from our   VizWiz-VQA-Grounding dataset. The most common questions are: \u201cWhat is this\u201d, \u201cWhat color is this\u201d, \u201cWhat color   is this shirt\u201d, \u201cWhat is in this box\u201d, and \u201cWhat does this   say\u201d.1 Note that none of these questions are observed in   the VQA-X, CLEVR-Answers, and GQA datasets. Consequently, we exclude these three datasets from comparison.   For each dataset, analysis of the groundings with respect to   these questions are shown in Table 2 for location and Figure 4 for boundary complexity and image coverage.   1We group the following questions together: \u201cwhat is this\u201d, \u201cwhat is it\u201d,   \u201cwhat is this item\u201d, \u201cwhat is that\u201d, \u201cwhat\u2019s that\u201d, \u201cwhat is this please\u201d into   the \u201cWhat is this\u201d group. We also group the following questions together:   \u201cwhat color is this\u201d and \u201cwhat color is it\u201d.   19101   What is this   What color is this   What color is this shirt   What is in this box   What does this say   Ours   (0.48\u00b10.11,0.51\u00b10.11)   (0.50\u00b10.07,0.51\u00b10.08)   (0.50\u00b10.06,0.50\u00b10.07)   (0.55\u00b10.12,0.64\u00b10.15)   (0.48\u00b10.14,0.47\u00b10.16)   VQS   (0.69\u00b10.09,0.45\u00b10.09)   (0.8,0.46)   TextVQA-X   (0.47\u00b10.37,0.56\u00b10.13)   (0.62,0.21)   Table 2. Shown is the mean and standard deviation of the location of all answer groundings with respect to the images for each dataset   for the five most common questions observed in our dataset. \u201c-\u201d denotes that no statistics were computed because the dataset lacks the   question. The results highlight that different questions often have different typical locations, especially across different datasets.   Figure 4. For each dataset, the box plot shows the range of values   for the answer groundings for the most common questions in our   dataset with respect to: (a) boundary complexity and (b) image   coverage. (Figure 2 describes the box plot visualization)   Across all datasets, we observe different types of questions manifest different typical locations, boundary complexities, and sizes from each other. For instance, in Figure 4(b), we see that the average boundary complexity for   \u201cWhat does this say\u201d is uniquely near zero for our dataset   and so usually can be grounded with a rectangle. We suspect algorithms will be able to take advantage of these differences to learn predictive cues for grounding answers.   We also observe that the characteristics of answer   groundings for the same question are considerably different   across the different datasets. This is evident when examining the results for \u201cWhat color is this\u201d and \u201cWhat does   this say\u201d; i.e., there are considerable differences for the typical location of the answer grounding (Table 2), the typical   range of boundary complexity values (Figure 4a), and the   typical range of image coverage values (Figure 4b). Consequently, if models trained on other datasets are learning biases between specific questions and answer grounding locations without truly understanding the question, they would   generalize poorly to our new dataset (and vice versa).   Figure 5. For each dataset, the box plot shows the range of values   for the answer groundings for the most common answers in our   dataset with respect to: (a) boundary complexity and (b) image   coverage. (Figure 2 describes the box plot visualization)   Most Common Answers.   We perform parallel analysis   to what we conducted for the most common questions but   now with respect to five of the most common answers   from the VizWiz-VQA-Grounding dataset: \u201cyes\u201d, \u201cwhite\u201d,   \u201cno\u201d, \u201ckeyboard\u201d, and \u201cdog\u201d.2 For each dataset, we analyze groundings with respect to these specific answers. We   observe that the relative location is similar for all answers,   with answers typically grounded at the center of the images. Due to space constraints, we include these results in   the Supplementary Materials. In contrast, we observe that   different answers manifest different statistics for the boundary complexity (Figure 5a) and relative sizes (Figure 5b),   both within each dataset and across different datasets. This   reinforces our findings from the most common questions.   Answer differences can yield valuable predictive cues for   grounding answers and cross-dataset evaluation could be   valuable for preventing models from learning superficial   correlations between answers and answer grounding characteristics in a specific dataset.   2We restricted our analysis to one color-related answer to support   greater diversity in our analysis.   19102   Figure 6. The box plot shows the range of values for (a) boundary   complexity and (b) image coverage for all answer groundings in   our dataset with respect to visual questions that require different   vision skills. (Figure 2 describes the box plot visualization)   Vision Skills Needed to Answer Visual Questions.   We   also conduct fine-grained analysis of our dataset with respect to the types of skills needed to answer visual questions, using the labels provided in the VizWiz-VQA-Skills   dataset [39]. Overall, we again observe that answer groundings typically are positioned at the center of the image with   respect to vision skills. Due to space constraints, we include   these results in the Supplementary Materials. In contrast,   we observe that different vision skills are correlated with   different statistics for image coverage (Figure 6b). Consequently, vision skills also could provide valuable predictive cues that algorithms could latch onto when trying to   ground answers. For instance, visual questions about trying   to read text tend to have a relatively small visual grounding   area in the image and, often, can be grounded with a simple   bounding region (e.g., with four points). In contrast, questions related to recognizing color tend to have a larger visual   grounding area and more complex boundary. We attribute   this distinction to text being a consistently well-structured   entity with clear printing boundaries whereas color can be   used to describe less structured entities such as articles of   clothing which can manifest a variety of shapes.   4. Automatically Grounding Answers for VQA   Using our grounding dataset, we now quantify to what   extent state-of-the-art VQA models look at the correct regions in images to answer the questions.   Dataset Splits.   Our VizWiz-VQA-Grounding dataset\u2019s   train/val/test/ splits match the train/val/test splits of the original VizWiz-VQA dataset [15].   This resulted in 6,494,   1,131 and 2,373 visual questions in the training, validation,   and test sets respectively.   Baseline Models.   We benchmark a total of six models.   First, we chose the top-performing VQA algorithms with   publicly-available code for the 2021 VizWiz-VQA dataset   challenge [15] and the mainstream 2021 VQA dataset challenge [13], that is LXMERT [36] and OSCAR [24] with   VinVL image features [40] respectively. Both models are   pretrained on the train splits of their respective challenge   datasets. In order to generate attention masks, we follow   the process described in [36] to analyze attention maps extracted from each model. Using the default parameters, attention weights across the multiple attention heads are extracted and averaged to obtain the final attention map. A   threshold of 0.5 is then applied to generate the final binary   segmentation mask.   We also chose the state-of-the-art model for answer   grounding: MAC-Caps [37]. Given an image and question, MAC-Caps predicts an answer and attention weights   on the image. As done in [37], we obtain the final binary   segmentation mask by applying a threshold of 0.5 to the attention weights extracted from the last reasoning step. We   benchmark four variants of MAC-Caps. We use the two   models that were pretrained on GQA and CLEVR respectively, as described in the original paper. Next, we train the   MAC-Caps algorithm from scratch using the train split of   the VizWiz-VQA [15] dataset. Finally, we also train the   MAC-Caps algorithm from scratch using the train split of   the VQA-v2 dataset [13], since this dataset was by design   intended to prevent models from learning language biases   and to instead encourage models to look at the images.   Evaluation Metric.   We employ Intersection over Union   (IoU) to measure the similarity of each binary segmentation   mask to the ground truth segmentation. Values range from 0   to 1, with higher values indicating better performance. We   compute the mean IoU score across all test examples and   report results are percentages (i.e., IoU value x 100).   We also evaluate with the common metric for detection   and localization tasks: mAP@IoU. Following the COCO   evaluation protocol, we use different IoU thresholds, from   0.25 to 0.75, and average AP values with IoU thresholds   in the range of 0.5 to 0.95 with a step size of 0.05. Due to   space constraints, results are provided in the Supplementary   Materials. In summary, these results reinforce our findings   with respect to the IoU metric, as described below.   Overall Results.   The performance for each model on the   VizWiz-VQA-Grounding test split is shown in Table 3. Performance is reported for all visual questions (column 3) as   well as only the subset of visual questions for which each   model correctly predicted the answers (column 4).   We observe that all models performed poorly overall.   For example, the top-performing MAC-Caps model that   was trained on the VizWiz-VQA dataset achieves an IoU   score of 27.43%. The story improves only modestly when   considering just those visual questions for which the model   predicted correct answers; i.e., the IoU jumps \u223c5 percentage points to 32.8%. These findings indicate that existing   19103   (a) Image Coverage   (b) Image Quality   (c) Vision Skills   Figure 7. Comparison of MAC-Caps (pretrained on VizWiz), LXMERT, and OSCAR\u2019s performance on (a) visual questions containing   different sizes of attention areas for the answers, (b) visual questions for images with different severity of quality issues, and (c) visual   questions requiring different vision skills to answer correctly. Overall, we observe (a) smaller answer groundings, (b) images with no   quality issues, and (c) visual questions requiring counting skills are most challenging for the models.   Model   Pretrained   Avg IoU   Avg IoU (correct)   LXMERT   VizWiz-VQA   22.09   26.96 (906)   OSCAR   VQA-v2   15.48   19.79 (693)   MAC-Caps   GQA   12.56   17.77 (270)   MAC-Caps   CLEVR   15.31   10.98 (60)   MAC-Caps   VQA-v2   17.42   19.58 (374)   MAC-Caps   VizWiz-VQA   27.43   32.8 (352)   Table 3.   Performance of six models when evaluated on the   VizWiz-VQA-Grounding test set: two state-of-art VQA models   (LXMERT [36] and OSCAR [24]) and four variants of the stateof-art VQA model for answer grounding (MAC-Caps [37]). IoU   scores (averaged over all 2,373 samples for each model) are reported in percentages. The number in parentheses is the total number of answers correctly predicted by each model.   mechanisms intended to guide models to look at the correct visual evidence are insufficient. This includes both the   state-of-the-art algorithm for answer grounding (i.e., MACCaps) and the mainstream VQA dataset [13] which was designed to encourage models to look at the images.   Our results reveal that the best indicator of better answer   groundings is that models were pre-trained on the VizWizVQA dataset.   In particular, the top two approaches are   LXMERT and MAC-Caps trained on VizWiz-VQA. Interestingly, neither of these models were trained with answer   groundings and so neither could benefit from direct supervision of what answer groundings look like. Altogether, this   finding highlights a considerable domain shift between the   real-world use case for people with visual impairments and   the contrived settings for generating the other datasets.   We also observe that, for the VQA task, the state-ofthe-art VQA algorithms considerably outperform the stateof-the-art answer grounding models that are pretrained on   the same datasets. Specifically, compared to MAC-Caps,   LXMERT predicts over 2.5 times as many correct answers   while OSCAR predicts almost 2 times as many correct answers. This finding suggests that a large part of the success   for state-of-art VQA models still stems from learned biases   that are unrelated to the relevant visual evidence.   In what follows, we conduct fine-grained analysis for the   top-performing visual grounding model (MAC-Caps pretrained on VizWiz-VQA) and the two state-of-the-art VQA   models (LXMERT and OSCAR).   Analysis With Respect to Image Coverage.   We next assess each model\u2019s ability to accurately locate the answer   groundings based on the answer groundings\u2019 relative size.   To do so, we divide the test VQA instances into three buckets of \u201cSmall\u201d, \u201cMedium\u201d, and \u201cLarge\u201d based on their image coverage, specifically whether they occupy up to 1/3 of   the image, between 1/3 and 2/3 of the image, and more than   2/3 of the image respectively. In total, there are 1456 small   examples, 458 medium examples, and 459 large examples   in the test set. Results are shown in Figure 7a.   Overall, we observe that all models struggle most to predict answer groundings for the small set. While this finding   is not necessarily surprising, we believe it is still worthwhile   to be shown experimentally.   Interestingly, the advantage the top-performing answer   grounding model (MAC-Caps) has over the two state-ofthe-art VQA models (LXMERT and OSCAR) in grounding   answers stems from its ability to better ground small and   medium sized regions. This is evident when examining the   median IoU scores, which are roughly double for MACCaps what is observed for the other two models. A valuable area for future work will be to decipher what enables   MAC-Caps to better attend to these smaller answer regions   compared to the mainstream VQA models.   Analysis With Respect to Image Quality.   We next assess each model\u2019s ability to accurately locate the answer   groundings based on the image quality issues defined in [8].   First, we follow the process described in [16] to divide   the VQAs into three buckets of \u201cNone\u201d, \u201cMedium\u201d, and   \u201cSevere\u201d quality issues based on quality ratings from five   crowdworkers, resulting in 1,930, 351, and 92 examples respectively. Results are shown in Figure 7b. We also as19104   sess performance with respect to specific quality issues (i.e.,   poor framing, blurry, too dark, too bright, obfuscations, and   improper rotations) and, due to space constraints, provide   results in the Supplementary Materials.   Surprisingly, we observe that images without any quality   issues (\u201cNone\u201d) are the most challenging for models, particularly the VQA models (LXMERT and OSCAR). Upon   further analysis, we found that this is because these images   have smaller answer regions, and hence show similar performance as observed for images with \u201cSmall\u201d attention   areas for image coverage. Specifically, the average attention areas for \u201cNone\u201d, \u201cMedium\u201d and \u201cSevere\u201d images are   0.28, 0.47 and 0.83 respectively. This also explains why   models performed better on \u201cSevere\u201d images as they tend   to have larger answer regions. Further analysis also shows   82% \u201cSevere\u201d images contain questions asking about color,   which we will see in the next section, are also examples   where the models typically perform better.   Analysis With Respect to Vision Skills.   Next, we assess   each model\u2019s ability to accurately locate the answer groundings based on the vision skills needed to answer the questions, as introduced in Section 3.2. Specifically, these skills   are object recognition, color recognition, text recognition,   and counting. Results are reported in Figure 7c.   We consistently observe across all models that they perform worse for questions involving text recognition and   counting while they perform better for questions involving   object recognition and color recognition. We suspect we   observe improved performance for the latter two skills because color recognition has a relatively simple image analysis component and object recognition models have become   quite advanced compared to many other vision tasks, given   the large focus on the problem which was spurred by the   ImageNet dataset challenge [10]. This finding also could be   due in part to our prior observation that answer groundings   for text recognition tend to have smaller answer groundings while color recognition tends to have larger visual   grounding areas (Section 3.2) and models perform worse   for smaller answer groundings (Section 4).   Analysis of the Presence of Text.   We also analyze how   often answer groundings contain text. We apply the Microsoft Read API to detect and recognize text in grounded   areas. Of the 9,998 answer groundings, about 52% (i.e.,   5,207 images) were detected as containing text. For this   subset, we compared the extracted text to the ground truth   answers. We found that the ground truth answer was present   in the detected text for only 7% (i.e., 372 images) of visual   questions. This suggests that most language answers cannot   be found directly from detected text in answer groundings.   Qualitative Results.   We finally show qualitative results   from the top-performing answer grounding model, MACFigure 8.   Qualitative results exemplifying answer groundings   from the best model, MAC-Caps pretrained on VizWiz-VQA.   Caps pretrained on VizWiz-VQA. Results are shown in Figure 8. These examples illustrate answer groundings for a   large range of sizes (e.g., row 3 vs row 4) as well as visual   questions that require different vision skills, such as text   recognition for rows 1 and 3, object recognition for row   2, and color recognition for row 4. When observing the   model\u2019s predictions, we observe that the results reinforce   our quantitative findings that the model often fails. These   answer grounding failures include for both when the model   predicts correct natural language answers (rows 1 and 4)   and predicts incorrect answers (rows 2 and 3).   5. Conclusions   Our VizWiz-VQA-Grounding dataset offers a strong   foundation for supporting the community to design less   biased VQA models and more accurate answer grounding models which can serve as a valuable precursor for a   range of practical applications.   We will publicly-release   the dataset alongside a public evaluation server and leaderboard to spur community progress on this important answer   grounding problem. Our benchmarking of state-of-the-art   models reveal current limitations for future models to overcome. Future work will need to establish how to ensure   such algorithms truly learn to understand the visual questions rather than learning superficial correlations between   properties of visual questions and their answer groundings.   Acknowledgments.   We are grateful for funding support   from Microsoft AI4A and Amazon Mechanical Turk.   19105", "conf": "CVPR", "year": "2022", "index": 256}, {"title": "Dense Events Grounding in Video   Peijun Bao,1 Qian Zheng,2 Yadong Mu1*   1Peking University, China   2Nanyang Technological University, Singapore   1{peijunbao, myd}@pku.edu.cn, 2zhengqian@ntu.edu.sg", "abstract": "This paper explores a novel setting of temporal sentence   grounding for the \ufb01rst time, dubbed as dense events grounding. Given an untrimmed video and a paragraph description,   dense events grounding aims to jointly localize temporal moments of multiple events described in the paragraph. Our   main motivating fact is that multiple events to be grounded   in a video are often semantically related and temporally coordinated according to their order appearing in the paragraph.   This fact sheds light on devising more accurate visual grounding model. In this work, we propose Dense Events Propagation Network (DepNet) for this novel task. DepNet \ufb01rst adaptively aggregates temporal and semantic information of dense   events into a compact set through a second-order attention   pooling, then selectively propagates the aggregated information to each single event with soft attention. Based on such   aggregation-and-propagation mechanism, DepNet can effectively exploit both the temporal order and semantic relations   of dense events. We conduct comprehensive experiments on   large-scale datasets ActivityNet Captions and TACoS. For   fair comparisons, our evaluations include both state-of-art   single-event grounding methods and their natural extensions   to the dense-events grounding setting implemented by us. All   experiments clearly show the performance superiority of the   proposed DepNet by signi\ufb01cant margins.", "content": "Over the last few years, the computer vision community   has witnessed the success of temporal sentence grounding,   which aims to localize temporal moment described by a sentence description in a given video. A list of promising methods (Anne Hendricks et al. 2017; Gao et al. 2017; Wang, Ma,   and Jiang 2020; Ghosh et al. 2019; Rodriguez et al. 2020;   Wang, Huang, and Wang 2019a) have been proposed for   temporal sentence grounding. Several recent works (Hendricks et al. 2018; Zhang, Su, and Luo 2019; Liu et al.   2018a; Stroud et al. 2019; Yuan et al. 2019; Zhang et al.   2020) further explore to localize more complicated sentence   containing compositional activity like \u201cthe woman takes the   book across the room to read it on the sofa\u201d .   Most existing methods separately ground an individual   event from a video, which we argue is not an optimal op*Corresponding author.   Copyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   The ball goes out of bounds. The man    in green picks up the ball. The man    with red shorts serves the ball. The    man serves the ball again. The ball    goes out of bounds again.   The ball goes out of bounds.   The man in green picks up the ball.   The man with red shorts serves the ball.   The man serves the ball again.   The ball goes out of bounds again.   ..   ..   ..   time   Figure 1: Dense Events Grounding in Video. Given a paragraph description, dense events grounding aims to jointly   localize described dense events in untrimmed video.   tion for contextualized description of multiple events. Compared with single-sentence input, a paragraph of multiple   sentences that describe video events in time order is seemingly more natural and powerful (Krishna et al. 2017; Li   et al. 2018; Zhou et al. 2018). Consider the example in Figure 1, people may be interested in a list of events around the   moment \u201cthe man with red shorts serves the ball\u201d, and they   use a paragraph consisting of multiple sentences to describe   these events. Furthermore, events like \u201cthe man with red   shorts serves the ball.\u201d appear several times in the video, but   people may only be interested in one of them. To avoid ambiguity, they use a paragraph to describe not only the most   interested event but also its contextual events.   To localize dense events in a paragraph, one can simply   apply some single-event grounding model to each individual sentence in the paragraph. However, the temporal order   of dense events occurred in video is often highly correlated   with their locations in the descriptive paragraph, as demonstrated in relevant tasks such as dense captioning (Krishna   et al. 2017; Regneri et al. 2013). Ignoring the temporal clues   in a paragraph tends to lead inferior performance in precisely   \ufb01nding the temporal boundary of an event. To illustrate it,   for a pair of events in a video we investigate the consistency between the visual grounding results and their temThe Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)   920   poral order in the paragraph. Surprisingly, even state-of-art   single event grounding methods (Zhang et al. 2020) have a   more than 20% chance to generate visual grounding results   that contradict with the temporal order in the corresponding   paragraph, which hints a huge space for improvement via   contextual grounding.   Moreover, events described in a same paragraph are usually semantically related to one another. As shown in Figure 1, localizing sentence event \u201cthe man serves the ball   again\u201d requires to understand another sentence event \u201cthe   man with red shorts serves the ball\u201d. Jointly grounding them   can utilize their contextual semantic relation, which also   contributes to precise time boundary prediction.   To this end, we introduce a novel setting of temporal sentence grounding for the \ufb01rst time in the literature, termed as   dense events grounding. Given an untrimmed video and a   paragraph of sentence descriptions, the goal of dense events   grounding is to jointly localize temporal moments described   by these sentence descriptions. Rather than grounding each   event independently, dense event grounding requires to exploit temporal order and semantic relations of dense events   for more accurate localization.   To achieve this, we propose a novel dense events grounding model called Dense Events Propagation Network (DepNet). Our main idea is to adaptively aggregate temporal   and semantic information of dense events into a compact   set, then selectively propagate the aggregated information to   each single event. More speci\ufb01cally, DepNet \ufb01rst generates   visual-semantic moment proposals for each single event in   the paragraph description. A dense events aggregation module then aggregates these moment proposals into a compact   set through a second-order attention pooling. For each moment proposal, a dense events propagation module then selects a desired subset of features from the compact set, and   propagates the selected features to the proposal through soft   attention. In this way, the moment proposals of each single   event can perceive and exploit the temporal order information and context semantic relation from other events in the   paragraph description.   Our contributions are summarized as follows:   \u2022 We de\ufb01ne a new task dense events grounding and develop   Dense Events Propagation Network (DepNet) as the \ufb01rst   attempt of tackling this task. Particularly, DepNet adopts a   novel aggregation-and-propagation scheme, which effectively enables context-guided visual grounding.   \u2022 Experiments on large-scale datasets ActivityNet Captions   and TACoS show that the proposed DepNet outstrips several state-of-the-art single-event grounding methods and   their dense-events variants (implemented by us for fair   comparisons) by signi\ufb01cant margins.   Related Work   Single Event Grounding   Temporal sentence grounding of single event in video is recently introduced by (Anne Hendricks et al. 2017; Gao et al.   2017), which aims to determine the start and end time points   of single event given by a query sentence. (Anne Hendricks   et al. 2017) proposes a moment context network to jointly   model text query and video clips. (Gao et al. 2017) proposes   cross-modal localizer to regress action boundary for candidate video clips. (Liu et al. 2018b,c) then advice to apply attention mechanism to highlight the crucial part of visual features or query contents. (Wang, Huang, and Wang 2019b)   then develops a semantic matching reinforcement learning   framework to reduce the large visual-semantic discrepancy   between video and language.   Several recent works (Zhang et al. 2019a, 2020; Wang,   Ma, and Jiang 2020) propose to model temporal dependencies within sentence to closely integrate language and video   representation. (Zhang et al. 2019a) models temporal dependencies as a structured graph and devises an iterative   graph adjustment network for temporal structural reasoning.   (Zhang et al. 2020) proposes 2D Temporal Adjacent Networks to model the temporal relations between video moments by a two-dimensional map. (Wang, Ma, and Jiang   2020) uses a lightweight semantic boundaries prediction   branch to aggregate contextual information and models the   relationship between the referent and its neighbors.   Some recent works (Zhang, Su, and Luo 2019; Stroud   et al. 2019; Zhang et al. 2019b) further utilize compositional   property of query sentence and decompose sentence as multiple components for better temporal reasoning. (Zhang, Su,   and Luo 2019) proposes temporal compositional network   where a tree LSTM decomposes a sentence into three descriptions with respect to the main event, context event and   temporal signal. Similarly, (Stroud et al. 2019) \ufb01rst grounds   atomic sub-events to short video segments and then establishes the temporal relationships between these segments.   (Zhang et al. 2019b) develops a syntactic Graph Convolution Network to leverage the syntactic structure of sentence   and a multi-head self-attention module to capture long-range   dependencies from video context.   Although compositional activity may be considered, only   single event with single sentence description is grounded   in the settings of existing works. Unlike this, the proposed   dense events grounding aims to jointly localize multiple   events described by a paragraph, which requires to the model   temporal order and semantic relations of the dense events.   Dense Events Understanding in Video   Understanding dense events in video has been popular in   recent years. (Krishna et al. 2017) introduces the task of   dense video captioning, which aims to both detecting and   describing dense events in a video. They use contextual information from past and future events to jointly describe all   events. (Li et al. 2018; Zhou et al. 2018) propose a joint   and global optimization framework of detection and captioning in an end-to-end manner for dense video captioning. (Wang et al. 2018) develops a hierarchical reinforcement learning algorithm for dense video captioning where a   high-level manager module learns to design sub-goals and a   low-level worker module recognizes the primitive actions to   ful\ufb01ll the sub-goal. (Duan et al. 2018) further extends dense   events captioning in a weakly supervised setting and formulate the problem as dual process of event captioning and sentence localization. The dense captioning task in these papers   is to describe dense events in the video with a paragraph. In   921   Video   C3D   LSTM      : A man serves a tennis    ball on a tennis court.   Sentence Feature   Video Clip Feature       : The two men    continuously play tennis     until a point is scored.   K   S   1   S   1   S   f   SK   f   Sentence Feature   LSTM   X   X   Dense Events   Aggregation and Propagation   Visual-Semantic    Proposals Generator   Grounding Result   ...   Language Encoder   Video Encoder   1x1 conv   1x1 conv   Compact Set   ...   ...   ...   ...   ...   Visual Proposals   Proposals for   K   S   Proposals for   1   S   iS   Proposals for   Figure 2: The framework of our proposed Dense Events Propagation Network. It consists of a Language Encoder, a Video   Encoder, a Visual-Semantic Proposals Generator, a Dense Events Aggregation and Propagation Module. In the Dense Events   Aggregation and Propagation Module, visual-semantic information of dense events is aggregated into a compact set, then   selectively propagated to each single event. Only the \ufb01rst and last sentence queries are visualized.   contrast, our dense events grounding task can be viewed as   the inverse problem of dense captioning.   (Bojanowski et al. 2015) proposes to ground multiple   sentences in video with weakly-supervised settings. These   sentence events are assumed to temporally non-overlap,   which does not generalize to most video paragraph descriptions. (Shao et al. 2018) proposes a novel task to retrieve   paragraph query in video collections and then localize these   events. They present a \ufb01nd-and-focus framework where the   top-level matching narrows the search while the part-level   localization re\ufb01nes the results. Our task is different from   their events localization step since we jointly localize dense   events within the video while they treat each event independently.   Method   Problem Formulation   Given an untrimmed video V and K sentence descriptions   {S1, S2, \u00b7 \u00b7 \u00b7 , SK} with temporal order, our goal is to jointly   localize temporal moments {T1, T2, \u00b7 \u00b7 \u00b7 , TK} described by   these sentences. More speci\ufb01cally, the video is presented as   a sequence of frames V = {vi}LV   i=1 where vi is the feature   of i-th frame and LV is the frame number of the video. The   k-th sentence description Sk is presented as Sk = {ski}Lk   i=1   where ski represents i-th word in the sentence and Lk denotes the total number of words. The k-th temporal moment   Tk consists of start and end time point of the event in the   video.   Dense Events Propagation Network   As illustrated in Figure 2, our proposed Dense Events Propagation Network (DepNet) consists of four main components:   a language encoder, a video encoder, a visual-semantic   proposal generator, and a dense-events aggregation-andpropagation module. This section will elaborate on the details of each component.   Language Encoder   Given an input of a natural language   paragraph query, the goal of language encoder is to encode   the sentences in the paragraph such that moments of interest   can be effectively retrieved in the video. Our language encoder extracts feature embedding f Sk of each sentence Sk   in the paragraph descriptions {S1, S2, \u00b7 \u00b7 \u00b7 , SK} separately.   Instead of encoding each word with a one-hot vector or   learning word embeddings from scratch, we rely on word   embeddings obtained from a large collection of text documents. In more details, each word ski in the sentence Sk   is \ufb01rst encoded into Glove word embedding (Jeffrey Pennington and Manning 2014) as wki. Then the sequence of   word embedding {wki}Lk   i=1 is fed to an LSTM (Hochreiter   and Schmidhuber 1997). The last hidden state of LSTM is   passed to a single fully-connected layer to extract the \ufb01nal   sentence feature f Sk \u2208RdS. All parameters of the LSTM   and fully-connected layer are shared across different sentences in the paragraph.   Video Encoder   Video encoder aims to obtain high-level   visual representations of video moment proposals from raw   input frames. Speci\ufb01cally, the input video is \ufb01rst segmented   into small clips where each video clip contains T frames. A   \ufb01xed-interval sampling is performed over these video clips   to obtain N video clips. For each sampled video clip, we   extract a sequence of basic C3D features V = {vi}N   i=1 with   a pretrained C3D (Tran et al. 2015) Network.   The visual feature embeddings for moment proposals are   constructed from these basic C3D features. For a moment   proposal (a, b) with start point at a and end point at b, we   apply boundary-matching operation BM (Lin et al. 2019)   over all C3D features covered by this proposal to get the   feature embedding:   \u02dcf Vab = BM({vi}b   i=a).   (1)   The boundary-matching operation can ef\ufb01ciently generate   proposal-level feature from basic clip-level feature, through   a series of bilinear sampling and convolutional operations.   More algorithmic details are omitted here and can be referred to (Lin et al. 2019). \u02dcf Vab is passed through a fullyconnected layer to obtain the \ufb01nal feature embedding f Vab \u2208   RdV for the moment proposal (a, b). Essentially, this extracted feature f Vab summarizes spatial-temporal patterns   922   from raw input frame and thus represents the visual structure of the moment proposal.   Visual-Semantic Proposals Generator   Visual-semantic   proposals generator constructs visual-semantic features of   moment proposals for each sentence query in the paragraph   description. Speci\ufb01cally, video moment feature f Vab for   all possible moment proposals are computed according to   Eq (1) where 1 \u2264a \u2264b \u2264N.   The features of the visual modality and language modality   are then fused to generate visual-semantic for each sentence   in the paragraph. To interact the language feature of k-th   sentence f Sk with video moment feature f Vab, we multiply   f Sk with video moment clip feature f Vab and then normalize   the fused feature \u02c6   M k   ab with its L2 norm, namely   \u02c6   M k   ab = f Vab \u2299f Sk,   M k   ab = \u02c6   M k   ab/|| \u02c6   M k   ab||2,   (2)   where \u2299denotes Hadamard product.   Inspired by positional encoding of tokens in natural language processing (Vaswani et al. 2017; Devlin et al. 2018),   we encode the relative positions of each proposal to better   perceive the temporal information. We consider three sorts   of the relative positions, i.e., the start point a, end point b   and sentence order k. These positions are encoded by sine   and cosine functions of different frequencies:   PEpos,i =   \u001asin(pos/10000i/dpos),   if i is even   cos(pos/10000i/dpos),   otherwise   (3)   where pos can be any one of the three relative positions, d is   the number of dimensions and i denotes the i-th dimension.   Then these three sorts of positional feature PE \u2208R3dpos   are concatenated with M k   ab and transformed by 1 \u00d7 1 convolutional layer. The output of the convolutional layer summarizes visual-semantic patterns and relative positional information of the proposals for each sentence query in the   paragraph. For simplicity, we still refer it as M k   ab.   Since the number of moment proposals is large, following (Zhang et al. 2020) we adopt sparse sampling strategy   to remove the redundant moment proposals that have large   overlaps with the selected one. Such sampling strategy can   effectively reduce the computational cost and the number of   moment proposals.   Dense Events Aggregation and Propagation   Dense   event grounding requires to exploit temporal order and semantic relations of sentence events for more accurate localization. Inspired by global feature modeling in image/video   classi\ufb01cation (Chen et al. 2018), we design dense events   aggregation and propagation modules. Speci\ufb01cally, a dense   events aggregation module \ufb01rst perceives the entire visualsemantic proposals of sentence queries, and adaptively aggregates global information through a second-order attention pooling. Then a dense events propagation module is   cascaded to selectively propagates the aggregated global information to each event proposal with soft attention.   In more details, the dense events aggregation module adaptively aggregates visual-semantic feature M   =   {M k   ab}(1 \u2264a \u2264b \u2264N, 1 \u2264k \u2264K) of entire   moment proposals into a compact set of global features   A = {a1, \u00b7 \u00b7 \u00b7 , an}. To do this, it \ufb01rst transforms M to   G = {g1, \u00b7 \u00b7 \u00b7 , gn} by a convolutional layer g as attention   weight to densely attend each moment proposal for global   information aggregation in M. Each ai is then calculated by   second-order attention pooling operation as:   ai = f(M) softmax(gi)\u22a4,   (4)   where f is another convolutional layer to transform M   and softmax is used to normalized gi to a valid attention   weight. Above second-order attention pooling adaptively selects informative moment proposals among multiple sentence events and aggregates visual-semantic information of   them.   Then the dense events propagation module selectively   propagates the global features in the compact set A to each   proposal via soft attention. More speci\ufb01cally, a subset of feature vectors are dynamically selected from A for each proposal and propagated to the proposal with soft attention as:   \u02dc   M k   ab =   n   X   j=1   hk   abjaj = Ahab,   (5)   where h is the attention weight and satis\ufb01es Pn   j=1 hk   abj = 1.   Similar to the generation of the attention weight g in the aggregation module, h is generated by applying a convolution   layer and a followed softmax normalizer on M. In this way,   each moment proposal can perceive moment contexts from   other sentence events which are complementary to itself.   Such an aggregation and propagation design enables the   model to not only perceive the entire moment proposals   of the event itself, but also perceive the moment contexts   from other events, resulting in learning the distribution of   the multi-events.   Finally we pass the output of propagation module { \u02dc   M k   ab}   to a fully-connected layer and a sigmoid layer to generate   a temporal-sentence score map {pk   ab}. And each value pk   a,b   in temporal-sentence score map denotes predicted matching   score of the temporal moment (a, b) for k-th sentence. The   maximum of k-th score map pk corresponds to the grounding result for the k-th sentence Sk.   Training Loss   Our training sample consists of an input video, a paragraph   query {S1, S2, \u00b7 \u00b7 \u00b7 , SK} and a set of temporal annotations   {T1, T2, \u00b7 \u00b7 \u00b7 , TK} associated with the sentences in the paragraph. During training, we need to determine which temporal moment in the temporal-sentence score map corresponds   to the annotations and train the model accordingly.   Instead of hard label, we assign each moment proposal   with a soft label according to its overlap with the annotations. Speci\ufb01cally, for each moment in the temporalsentence score map, we compute the IoU score IoU k   ab between its temporal boundary (a, b) and the annotation of the   k-th sentence Tk. Then a soft ground truth label gtk   ab is as923   signed to it according to IoU k   ab:   gtk   ab =   \uf8f1   \uf8f4   \uf8f2   \uf8f4   \uf8f3   0   IoU k   ab \u2264\u00b5min,   IoU k   ab\u2212\u00b5min   \u00b5max\u2212\u00b5min   \u00b5min < IoU k   ab < \u00b5max,   1   IoU k   ab \u2265\u00b5max,   (6)   where \u00b5min and \u00b5max are two thresholds to customize the   distribution of soft labels.   For each training sample, the model can be trained in an   end-to-end manner with a binary cross entropy loss, which   is de\ufb01ned as:   L = \u2212   X   (a,b)\u2208C   gtk   ab log(pk   ab)+(1\u2212gtk   ab) log(1\u2212pk   ab), (7)   where C = {(a, b)|1 \u2264a \u2264b \u2264N} is the set of all valid   moment proposal boundaries.   Experiments   Dataset   ActivityNet   Captions   ActivityNet   Captions   (Krishna   et al. 2017) consists of 19,209 untrimmed videos. Each   video includes multiple sentence descriptions with temporal order and corresponding moment boundary annotations.   The contents of video are diverse and open. It is originally   built for dense-captioning events (Krishna et al. 2017) and   lately introduced for temporal grounding with single sentence setting. For a fair comparison, following the experimental setting in single sentence grounding (Zhang et al.   2020; Yuan et al. 2019), we use val 1 as validation set and   val 2 as testing set. There are 37,417, 17,505, and 17,031   moment-sentence pairs in the training, validation and testing set, respectively.   TACoS   TACOS (Regneri et al. 2013) consists of 127   videos. Each video has several paragraph descriptions and   temporal annotations. It is developed on MPII Compositive (Rohrbach et al. 2012). The main video theme is limited   to cooking scenes, thus lacking diversity. Compared with   ActivityNet Captions, Videos in the TACoS benchmark generally has longer duration and shorter moments. Following   the standard data splitting, there are totally 10,146, 4,589   and 4,083 moment-sentence pairs in the training, validation   and testing set, respectively.   Evaluation Metrics   The commonly-adopted evaluation metric in single-event   grounding (Zhang et al. 2020; Yuan et al. 2019) is known   to be Rank N@\u03b8. For each sentence query in the paragraph, we calculate the Intersection over Union (IoU) between the grounded temporal segment and the ground truth.   Rank N@\u03b8 represents the percentage of top N grounded   temporal segments that have at least one segment with   higher IoU than \u03b8. For ease of comparisons, we borrow   the identical evaluation metric in our proposed novel multievent setting. There are speci\ufb01c settings of N and \u03b8 for   different datasets. To fairly compare with previous single   event grounding methods, we follow (Zhang et al. 2020;   Yuan et al. 2019) to report the results as N \u2208{1, 5} with   \u03b8 \u2208{0.3, 0.5, 0.7} for ActivityNet Captions dataset, and   N \u2208{1, 5} with \u03b8 \u2208{0.1, 0.3, 0.5} for TACoS dataset.   Implementation Details   For a fair comparison, we use pretrained CNN (Tran et al.   2015) as previous methods to extract C3D video features on   both datasets. And we use Glove (Jeffrey Pennington and   Manning 2014) word embeddings pretrained on Common   Crawl to represent each word in the sentences. A three-layer   LSTM is applied to word-embeddings to obtain the sentence   representation. The channel numbers of sentence feature and   video proposal feature dS, dV are all set to 512 . We set the   dimension of positional feature dpos to 128 and the size of   compact set n to 512. The number of sampled clips N is set   to 32, 64 for ActivityNet Captions and TACoS respectively.   For BM operations in the video encoder, we set sampling   number of each proposal to 16, 32 for ActivityNet Captions   and TACoS respectively.   During training, We use Adam (Kingma and Ba 2014)   with learning rate of 1 \u00d7 10\u22124, the momentum of 0.9 and   batch size of 4 as optimization algorithm. For each training sample, we randomly sample K (2 \u2264K \u22648) sentence   queries with temporal order and apply padding them with zeros to 8 sentences. Such random sampling strategy reduces   model to over\ufb01t to moment prior of dense events. We only   calculate the values on the valid location and do not take   zero-padding into calculation. For binary cross entropy loss,   the scaling thresholds \u00b5min and \u00b5max are set to 0.5 and 1.0   for ActivityNet Captions and 0.3 and 0.7 for TACoS.   During inference, we set the whole paragraph as query   description if the annotated paragraph has less than 8 sentences. For annotated paragraph queries with more than 8   sentences, we split them into multiple sub-paragraphs to   meet the limitation of maximum 8 sentences. We independently choose the moment proposals with the highest con\ufb01dence score for each sentence as the \ufb01nal result. If we need   to select multiple moment localizations per sentence (i.e. for   R@5), Non Maximum Suppression (NMS) with a threshold   of 0.5 is applied to remove redundant candidates.   Competing Methods   In this subsection, we compare the proposed DepNet model   with state-of-the-art methods of single event grounding and   two competitive baseline models of dense sentence grounding. We refer the proposed model as DepNet.   The compared single event grounding methods are listed   as followings. CTRL (Gao et al. 2017): Cross-model Temporal Regression Localizer. MCN (Anne Hendricks et al.   2017): Moment Context Network. ACRN (Liu et al. 2018b):   Attentive Cross-Model Retrieval Network. QSPN (Xu et al.   2019): Multilevel Language and Vision Integration. ACLK (Ge et al. 2019): Activity Concepts based Localizer.   GDP (Chen et al. 2020): Graph-FPN with Dense Predictions. SAP (Chen and Jiang 2019): A two-stage approach   based on visual concept mining. SCDM (Yuan et al. 2019):   Semantic Conditioned Dynamic Modulation. CBP (Wang,   Ma, and Jiang 2020): Contextual Boundary-aware Prediction. CMIN (Zhang et al. 2019b): Cross-Modal Interaction   Networks. 2D-TAN (Zhang et al. 2020): 2D Temporal Adjacent Network.   The current literature on temporal grounding is dominated by single-event methods. For a fair comparison, we   924   carefully devise two competitive baseline models for denseevents grounding, including Beam Search (BS) and 3D   Temporal-Paragraph Network (3D-TPN) (a natural extension of the state-of-the-art single-event model 2D-TAN   fully implemented by us). In particular, the BS model \ufb01rst   localizes each sentence in the paragraph independently with   a base single event grounding model, then applies beam   search on the top k grounding results of each event as postprocessing, such that \ufb01nal dense events grounding results   match the temporal order. We set k to 8 in the experiments   and choose 2D-TAN as the base model for constructing a   strong baseline in dense event grounding. In speci\ufb01c, the   3D-TPN model \ufb01rst constructs two-dimensional temporalsentence feature map as in 2D-TAN for each sentence in   paragraph, formulates them as three-dimensional temporalparagraph map according to the sentence order and then applies a stack of 3D-convolutional layers on the formulated   3D temporal-paragraph map to perceive the temporal order   and semantic relations among dense events. Last, a fullyconnected layer is applied on the output of 3D-convolution   to get the dense events grounding result. Since 3D-TPN is a   natural extension of 2D-TAN, more details of its implementation are omitted here and can be referred to (Zhang et al.   2020).   Performance Comparisons   Table 1 and Table 2 show the performance comparisons between DepNet and all above-mentioned baseline methods   on ActivityNet Captions and TACoS, respectively. From the   tables, all three proposed dense events grounding methods   outperform single event grounding methods with a clear   margin. This veri\ufb01es the superiority of the proposed dense   events grounding setting. Furthermore, DepNet achieves the   best performance among all the methods, which veri\ufb01es the   effectiveness of dense events aggregation and propagation   mechanism in DepNet.   In more details, DepNet achieves about 22.3%, 25.6% and   23.4% higher evaluation scores than the best single event   grounding method 2D-TAN in terms of R1@0.3, R1@0.5   and R1@0.7 on ActivityNet Captions. And DepNet achieves   2D-TAN about 17.9%, 10.9% and 7.3% higher scores than   2D-TAN in terms of R1@0.1, R1@0.3 and R1@0.5 on   TACoS. The other two dense events grounding models also   achieve much better performance than the state-of-the-art   single event grounding methods on both datasets. For example, BS and 3D-TPN earn 3 and 8 points higher than the best   single event grounding method 2D-TAN in R1@0.3 on ActivityNet Captions, and 1 and 7 points higher than 2D-TAN   in terms of R1@0.1 on TACoS. This veri\ufb01es the superiority of the proposed dense events grounding setting, which   can use the information of temporal order and semantic relations among the dense events compared to the existing single   event grounding setting.   Moreover, we compare dense events grounding models   with intra-sentence context based single event grounding   models, including QSPN, GDP, CBP and CMIN. These approaches explicitly model the context moment within the   single sentence and achieve better results than the simple sliding window based single events grounding methods,   Method   Rank1@   Rank5@   0.3   0.5   0.7   0.3   0.5   0.7   MCN   39.35   21.36   6.43   68.12   53.23   29.70   CTRL   47.43   29.01   10.34   75.32   59.17   37.54   ACRN   49.70   31.67   11.25   76.50   60.34   38.57   QSPN   52.13   33.26   13.43   77.72   62.39   40.78   GDP   56.17   39.27   \u2212   \u2212   \u2212   \u2212   CBP   54.30   35.76   17.80   77.63   65.89   46.20   SCDM   54.80   36.75   19.86   77.29   64.99   41.53   CMIN   63.61   43.40   23.88   80.54   67.95   50.73   2D-TAN   59.45   44.51   26.54   85.53   77.13   61.96   BS   62.53   46.43   27.12   \u2212   \u2212   \u2212   3D-TPN   67.56   51.49   30.92   87.94   81.53   65.86   DepNet   72.81   55.91   33.46   90.08   83.82   68.80   Table 1: Performance comparison on ActivityNet Captions.   Method   Rank1@   Rank5@   0.1   0.3   0.5   0.1   0.3   0.5   MCN   3.11   1.64   1.25   3.11   2.03   1.25   CTRL   24.32   18.32   13.30   48.73   36.69   25.42   ACRN   24.22   19.52   14.62   47.42   34.97   24.88   QSPN   25.31   20.15   15.23   53.21   36.72   25.30   ACL-K   31.64   24.17   20.01   57.85   42.15   30.66   SAP   31.15   \u2212   18.24   53.51   \u2212   28.11   GDP   39.68   24.14   \u2212   \u2212   \u2212   \u2212   CBP   \u2212   27.31   24.79   \u2212   43.64   37.40   SCDM   \u2212   26.11   21.17   \u2212   40.16   32.18   CMIN   32.48   24.64   18.05   62.13   38.46   27.02   2D-TAN   47.59   37.29   25.32   70.31   57.81   45.04   BS   48.46   38.14   25.72   \u2212   \u2212   \u2212   3D-TPN   55.05   40.31   26.54   78.18   63.60   48.23   DepNet   56.10   41.34   27.16   79.59   64.74   48.75   Table 2: Performance comparison on TACoS.   i.e., CTRL, MCN and ACL-K. However, these approaches   achieve clearly inferior results than the proposed DepNet.   For example, even the best one of them performs about 9   points lower than DepNet in terms of R1@0.3 on ActivityNet Captions, and about 8 points lower than DepNet in   terms of R1@0.1 on TACoS. The performance gap serves   as strong evidence for the necessity of joint multi-event   grounding.   Last, we compare DepNet with two dense events grounding baselines BS and 3D-TPN. In terms of R1@0.3, DepNet outperforms BS and 3D-TPN more than 10 and 3 points   on ActivityNet Captions, and more than 6 and 1 points on   TACoS, respectively. In terms of R5@0.3, DepNet outperforms 3D-TPN more than 2 points on ActivityNet Captions,   and more than 1 point on TACoS. As stated before, 3D-TPN   is a natural extension of 2D-TAN. It adopts vanilla stacked   temporal convolutions for perceiving information of dense   events on the temporal-paragraph map. This represents the   standard treatment in the literature of modern video analysis methods. We attribute its inferior performance to the   relatively low ef\ufb01cacy in cross-event communication (i.e.,   proposals with distant locations on the map can only be perceived after several layers of temporal convolutions). In contrast, our proposed aggregation-and-propagation scheme in   DepNet treats different proposals equally and leads to better   usage of information from dense events.   925   A man is seen speaking to a woman while    showing her how to sharpen a knife   The woman then uses the tool to sharpen the    knife with the man's help   He points to the knife and shows her    again how to do it properly.   Ground Truth   2D TAN   DepNet   Figure 3: Qualitative prediction examples of our proposed model. The \ufb01rst row shows the ground-truths for the given paragraph   queries, and the second and third row shows the grounding results of 2D-TAN and our DepNet.   Method   Rank1@   Rank5@   0.3   0.5   0.7   0.3   0.5   0.7   w/o. AP   62.86   46.76   28.08   86.19   78.16   60.52   w/o. PE   58.29   43.63   26.64   85.66   76.58   60.12   w/o. AP,PE   58.35   42.99   25.12   85.41   76.49   59.95   full   72.81   55.91   33.46   90.08   83.82   68.80   Table 3: Ablation study on ActivityNet Captions.   Ablation Study   In this section, we conduct ablation studies on ActivityNet   Captions to analyze the contributions of our proposed DepNet method. Speci\ufb01cally, we re-train our model with the   following settings: 1) DepNet (w/o. AP): the dense events   aggregation and propagation module is dropped, 2) DepNet   (w/o. PE): the positional encoding of moment proposals is   dropped, 3) DepNet (w/o. AP,PE): both positional encoding of moment proposals and dense events aggregation and   propagation module are dropped.   Table 3 shows the performance comparisons of our proposed full model DepNet (full) with respect to these ablations on the ActivityNet Captions dataset. DepNet (full)   outperforms all ablation models on ActivityNet Captions,   which demonstrates the positional encoding, dense events   aggregation and propagation module is critical to dense   events grounding in videos. Without considering the dense   events aggregation and propagation module, the performance of the model DepNet (w/o. AP) degenerates dramatically compared to DepNet (full). It shows that dense events   aggregation and propagation module is effective to model   temporal order and semantic relations of dense events. DepNet (w/o. PE) improves the grounding performance compared to DepNet (w/o. AP,PE), shows that modeling the semantic dependencies between moment proposals can help.   However, the improvement is limited compared to DepNet (full). This is mainly because without positional encoding, the temporal information of the moment proposals is   dropped by the process of dense events aggregation. This   also veri\ufb01es the importance of modeling the temporal order   of dense events.   Moreover, we explore the impact of the number of sentences in the paragraph descriptions as shown in Figure 4.   Speci\ufb01cally, we evaluate the same well-trained DepNet on   the ActivityNet Captions Dataset with a different number   of sentences in the paragraph descriptions. Here we select   \u201cR@1,IoU=0.3\u201d and \u201cR@1,IoU=0.5\u201d as evaluation metrics.   As can be seen, both metrics achieve higher values when the   59.59   66.19   69.67   71.9972.42 72.67 72.72   72.81   58.0   62.0   66.0   70.0   74.0   1   2   3   4   5   6   7   8   R1@0.3   Number of Sentences   85.41   85.51   87.98   89.31   89.83 89.98 90.05   90.08   85.0   86.5   88.0   89.5   91.0   1   2   3   4   5   6   7   8   R5@0.3   Number of Sentences   Figure 4: Impact of the number of sentences in the paragraph   descriptions on the ActivityNet Captions Dataset.   number of sentences increases. This shows that the temporal   order and semantic relations of dense events can help each   other to obtain more accurate grounding. We further note   that when the number of sentences is larger than 5, the improvement is almost saturated. This could be due to that sentences with more intervals are less semantically relevant and   provide less information to help the dense events grounding.   Qualitative Analysis   To qualitatively validate the effectiveness of the DepNet   method, we display several typical examples of dense events   grounding. Figure 3 shows the grounding results of the DepNet method and the single event grounding method 2D-TAN   on ActivityNet Captions. DepNet is capable of grounding   a diverse set of events, including the one requiring strong   temporal dependencies with other sentences \u201che points to   the knife and shows her again how to do it properly\u201d. DepNet can exploit the temporal order and semantic relations   of dense events based on the aggregation and propagation   mechanism. This makes it perform better than the 2D-TAN.   Conclusion   This work introduces a novel task dubbed as dense events   grounding, a more challenging task than traditional singleevent event grounding. To effectively capture the temporal   context in the accompanying paragraph of a video, we here   propose DepNet that has the unique trait of an aggregationand-propagation scheme. Evaluations on two large-scale   video benchmarks clearly demonstrate the superiority of   DepNet. We strongly believe that our pilot research on this   novel task will inspire more works on temporal context   modeling in visual grounding.   Acknowledgement: The work is supported by National Natural Science Foundation of China (61772037), Beijing Natural Science Foundation (Z190001).   926", "conf": "AAAI", "year": "2021", "index": 492}, {"title": "Learning to Mediate Perceptual   Differences in Situated Human-Robot Dialogue   Changsong Liu and Joyce Y. Chai   Department of Computer Science and Engineering   Michigan State University   East Lansing, Michigan 48824   {cliu, jchai}@cse.msu.edu", "abstract": "In human-robot dialogue, although a robot and its human   partner are co-present in a shared environment, they have   signi\ufb01cantly mismatched perceptual capabilities (e.g., recognizing objects in the surroundings). When a shared perceptual basis is missing, it becomes dif\ufb01cult for the robot   to identify referents in the physical world that are referred   to by the human (i.e., a problem of referential grounding).   To overcome this problem, we have developed an optimization based approach that allows the robot to detect and   adapt to perceptual differences. Through online interaction   with the human, the robot can learn a set of weights indicating how reliably/unreliably each dimension (e.g., object   type, object color, etc.) of its perception of the environment   maps to the human\u2019s linguistic descriptors and thus adjust   its word models accordingly. Our empirical evaluation has   shown that this weight-learning approach can successfully   adjust the weights to re\ufb02ect the robot\u2019s perceptual limitations.   The learned weights, together with updated word models, can   lead to a signi\ufb01cant improvement for referential grounding in   future dialogues.", "content": "A new generation of robots have emerged in recent years to   serve as humans\u2019 assistants and companions (Christensen,   Kruijff, and Wyatt 2010). To allow humans to better interact with these robots, techniques to support situated humanrobot dialogue become crucial. Unlike traditional spoken   dialogue systems (McTear 2002) and conversational interfaces (Johnston et al. 2002; Chai et al. 2004), one signi\ufb01cant challenge in situated human-robot dialogue is that the   robot needs to perceive and make sense of the shared environment simultaneously during conversation. The robot\u2019s   representation of the shared world is often limited by its   preceptual (e.g., computer vision algorithms) and reasoning (i.e., inference algorithms) capabilities. Therefore, although co-present, humans and robots do not share a joint   perceptual experience. The lack of a shared pereptual basis   will jeopadize referential communication between the human and the robot. It will become more dif\ufb01cult for the robot   to identify referents in the physical world that are referred to   by the human, i.e., a problem of referential grounding.   Copyright c\u20dd2015, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   Computational approaches to referential grounding often   consist of two key components (Gorniak and Roy 2004;   Siebert and Schlangen 2008; Liu, Fang, and Chai 2012). The   \ufb01rst component addresses formalisms and methods that connect linguistic terms (e.g., red, left) to the lower level numerical features (e.g., rgb vectors) captured in the robot\u2019s   representation of the perceived environment. We call this   component the word grounding models. The second component extracts all the linguistic terms from a referring expression and combines their grounding models together to   identify referents. For example, given a referring expression   \u201cthe big blue bottle to the left of the red box\u201d, it will recognize that the intended referent has several attributes (e.g.,   color is blue, size is big, type is bottle) and it is to the left   of another object. Then it will combine all relevant sensors\u2019   inputs and apply the corresponding word grounding models   to identify the referent that most likely satis\ufb01es the referring   expression.   Many previous works on referential grounding have focused on the \ufb01rst component, i.e., exploring how to learn and   ground individual linguistic terms to low level physical attributes (e.g., Roy 2002; Barnard et al. 2003; Roy 2005). Although different algorithms have been applied for the second   component (Liu, Fang, and Chai 2012; Liu et al. 2014), little   attention has been paid to the question how to intelligently   combine different attributes to ground", "conf": "AAAI", "year": "2015", "index": 206}, {"title": "Weakly-Supervised Generation and Grounding of Visual Descriptions with   Conditional Generative Models   Effrosyni Mavroudi and Ren\u00b4e Vidal   Mathematical Institute for Data Science, Dept. of Biomedical Engineering, Johns Hopkins University   {emavrou1, rvidal}@jhu.edu", "abstract": "Given weak supervision from image- or video-caption   pairs, we address the problem of grounding (localizing)   each object word of a ground-truth or generated sentence describing a visual input. Recent weakly-supervised   approaches leverage region proposals and ground words   based on the region attention coef\ufb01cients of captioning   models. To predict each next word in the sentence they attend over regions using a summary of the previous words   as a query, and then ground the word by selecting the   most attended regions. However, this leads to sub-optimal   grounding, since attention coef\ufb01cients are computed without taking into account the word that needs to be localized.   To address this shortcoming, we propose a novel   Grounded Visual Description Conditional Variational Autoencoder (GVD-CVAE) and leverage its latent variables   for grounding. In particular, we introduce a discrete random variable that models each word-to-region alignment,   and learn its approximate posterior distribution given the   full sentence. Experiments on challenging image and video   datasets (Flickr30k Entities, YouCook2, ActivityNet Entities) validate the effectiveness of our conditional generative   model, showing that it can substantially outperform softattention-based baselines in grounding.   1.", "content": "Linking words to visual regions provides a \ufb01ne-grained   bridge between vision and language modalities and is a fundamental block of many applications, such as human-robot   interaction [57,60], visual question answering [27,61], and   even unsupervised neural machine translation [58]. Thus,   visual grounding has become a prominent research area   at the intersection of vision and language [12, 16, 29, 51].   Training visual grounding systems typically requires annotations of textual descriptions combined with bounding   boxes for each groundable word (e.g., object nouns). Since   constructing datasets with such \ufb01ne-grained bounding box   annotations is rather time-consuming and costly, we focus   A soccer player    is chasing after    a ball.   A soccer player    is chasing after    a ball.   +   Image   Caption   Grounding   Visual Object Grounding   +   Image   Caption   Grounding   Grounded Visual Description   Vision + Language Tasks   GT: A man is    wearing a hat.   GT: A man is    wearing a jacket.   (a) Soft-Attention-based Grounding (Prior Work)   (b) GVD-CVAE (Ours)   Image   Caption   Grounding   Same   vs   Soft attention:    computed based on previous words   CVAE-posterior:   computed based on whole sentence   GT: A man is    wearing a hat.   GT: A man is    wearing a jacket.   Figure 1. Our proposed framework jointly models visual descriptions and word-to-region alignments conditioned on an input image (or video) and region proposals. Without using any   bounding box annotations during training, it can tackle two tasks:   Visual Object Grounding and Grounded Visual Description. Unlike prior work [74] that leverages soft attention for grounding and   always predicts the same region for two words given the same visual input and partial caption context, our model can ground words   by taking into account the full ground-truth or generated sentence.   on weakly-supervised training of visual grounding systems,   which require only image-caption pairs for training. In particular, we consider two tasks, as illustrated in Fig. 1: (1)   Weakly-Supervised Visual Object Grounding (WS-VOG),   where given an input image (or video) and its visual description, the goal is to localize the referred semantic entities in   the visual input, and (2) Weakly-Supervised Grounded Visual Description (WS-GVD), where given an input image   (or video), we must jointly generate a natural language description and localize the generated words.   Most prior work has focused on learning how to align   words with regions by learning how to correctly match   images and videos to sentences [8, 26, 56, 65].   However, these matching-based approaches can only tackle the   \ufb01rst task (WS-VOG), and cannot generate grounded vi15544   sual descriptions. On the other hand, captioning-based approaches [40, 74] aim to learn how to ground words by   learning how to generate captions based on region proposals, thus they can tackle both tasks. For example, the GVD   captioning-based model [74] grounds words by using the   region attention mechanism of a discriminative, encoderdecoder captioning model to select regions with maximum   attention coef\ufb01cients. Nonetheless, exploiting soft attention   as a grounding mechanism suffers from two major limitations. First, despite being an effective, end-to-end learnable   mechanism for summarizing relevant context, attention is   not explicitly encouraged to capture meaningful alignments   and can result in poor grounding [36], unless it is supervised. Second, each word is generated using attention coef\ufb01cients computed from a query that summarizes the previously generated words. Hence, the coef\ufb01cients do not take   into account the word to be grounded. For example, consider grounding the words \u2018hat\u2019 and \u2018jacket\u2019 given the sentences \u201cA man is wearing a hat\u201d and \u201cA man is wearing a   jacket\u201d, respectively. As shown in Fig. 1, existing attentionbased grounding approaches wrongly predict the same box   for \u2018hat\u2019 and \u2018jacket\u2019, since the partial caption is the same.   To overcome these limitations, we propose a conditional   generative model for the joint probability distribution of   sentences and latent word-to-region alignments given an input image (or video) and a set of region proposals. That is,   we account for the lack of grounding annotations by introducing discrete latent variables that model word-to-region   alignments. We parameterize our model with state-of-theart visual encoders, language decoders and attention modules, and leverage Amortized Variational Inference [30,59]   to learn its parameters.   The resulting Grounded Visual   Description Conditional Variational Autoencoder (GVDCVAE) allows us to both generate sentences and also infer the latent word-to-region alignments based on the whole   sentence, including the word to be grounded. Hence, it can   correctly ground the hat in the motivating example.   In summary, this work makes three key contributions.   First, we introduce the GVD-CVAE, a novel conditional   generative model of visual descriptions with a sequential   discrete latent space and attention-based parameterization   of the prior and approximate posterior alignment distributions. Second, we propose a training objective that encourages our model to learn latent variables that capture meaningful word-to-region alignments. Third, we evaluate our   method on three challenging image and video datasets and   demonstrate that both our \u201cprior\u201d and \u201capproximate posterior\u201d alignment distributions improve upon soft attention.   This leads to a 12% absolute improvement in WS-VOG on   Flickr30k Entities. Our model also achieves state-of-the-art   or competitive grounding and captioning performance compared with a diverse family of state-of-the-art methods that   are tailored to WS-VOG or WS-GVD.   2. Related Work   Grounded Visual Description. Developing models that   can both generate a sentence and link the generated words   to visual regions is a nascent research area, motivated by   a need for more trustworthy and interpretable captioning   models [24,36,50]. Such models can be seen as an evolution   of early image auto-annotation methods [7], methods for   generating visually grounded storylines [20], or methods for   generating descriptions with grounded and co-referenced   people [52].   Zhou et al. [74] ground words by leveraging the region attention coef\ufb01cients of an attention-based   captioning models. However, in contrast to prior work on   phrase grounding that computes attention using the whole   phrase as query [51], the region attention in [74] is computed based on previous words (partially generated sentence), and it is thus agnostic to the word being grounded.   A recent line of work has attempted to mitigate this issue. Ma et al. [40] propose a cyclical training regime for   WS-GVD of images and videos that involves two attention   mechanisms: one based on the partial caption and another   based on the groundable word. By forcing the words generated using these two attention mechanisms to match the   ground-truth words, the mechanisms are implicitly regularized to produce similar attention weights during training.   Other approaches explicitly supervise the region attention   during training on image-caption pairs, either by using attention coef\ufb01cients based on future relevant words [37], or   by leveraging the word-to-region alignments of a separately   trained image-to-text matching model [77]. In summary, a   common thread in prior work is the usage of a regular region attention module of an UpDown [2] captioning model   for grounding, which is regularized only during training   based on auxilliary models or attention mechanisms.   In   contrast, inspired by discrete latent-variable models for image captioning/neural machine translation [13, 45, 54, 66],   our key innovation is to treat word-to-region alignments as   discrete latent variables in a grounded visual description   CVAE model and exploit the prior or approximate posterior   alignment distributions to infer the latent word-to-region   alignments. This enables us to consider the past, future and   current words for localizing each object word in the input   image or video during testing.   Visual Object Grounding. Grounding words (rather than   whole sentences [71] or phrases [21, 65]) in images and   videos is an active research \ufb01eld in the intersection of vision   and language. Early attempts for weakly-supervised visual   grounding given textual descriptions of images and videos   relied on graphical models [47, 68]. Powered by advances   in region proposal generation, a large group of recent methods [11, 29] cast the task as a Multiple Instance Learning   (MIL) problem. These methods de\ufb01ne an image-sentence   matching score determined by word-to-region alignments   and learn how to correctly match images to sentences us15545   ing ranking losses. Such methods have also been extended   to videos [26, 56, 75] with frame-sentence matching scores   and mechanisms to account for missing objects.   However, these MIL-based methods cannot both generate sentences and ground objects. This limitation is lifted by the   captioning-based GVD-Grd method [74], which grounds   each word based on region attention coef\ufb01cients, computed   with the previous words as query, combined with region-toclass similarity coef\ufb01cients. These are obtained by transferring object class knowledge from external datasets. In this   work, we also use captioning as a downstream task, but we   localize words with the distributions of a conditional generative model, leveraging the full sentence context.   Joint Vision-Language Representation Learning.   Inspired by advances in pretrained NLP models [14], researchers have also started to use large-scale vision-text   corpora to learn cross-modal vision-language representations. There exist Transformer-based models [35, 38] that   are also trained using only pairs of images with object proposals and associated textual descriptions.   However, instead of focusing on learning task-agnostic, visiolinguistic   representations using large-scale corpora to facilitate downstream tasks, we are interested in training visual grounding   systems on small-scale datasets. Importantly, we rely on   text as weak supervision for learning how to ground without bounding box annotations directly on the target dataset.   Instead, these pretrained models require \ufb01netuning on a   smaller, fully-annotated dataset to tackle downstream tasks   such as referring expression grounding [38].   Modeling Sequential Data with Variational Autoencoders.   Our proposed CVAE-based captioning model is   also related to regular or Conditional VAEs that are developed for modeling sequential data in NLP applications.   In particular, VAEs with sequences of latent variables [3,   9, 10, 18, 53, 69] instead of a single latent variable driving   the whole sequential generation process [5, 43, 63, 72] are   more closely related to our work. However, the majority   of those have non-interpretable, continuous latent variables,   unlike our discrete latent word-to-region alignments. A notable exception is the approach of Graber et al. [19] that   uses sequential discrete variables to model interactions between entities in interacting systems. Still, all these works   share the same goal of modeling the likelihood of sequential data, while we propose exploiting the latent variables   for grounding. To this end, we need to avoid training an   inference model that produces posteriors almost identical   to the prior, thus ignoring the word to be grounded. Researchers are actively exploring various techniques to mitigate this posterior collapse issue by modifying: the training   objective [1, 17, 34, 42, 48, 55], the training procedure [22]   or the decoder architecture [15]. Similarly, we propose controlling the relative factor between sentence reconstruction   term and the prior regularization term [1,6,55].   Soccer players chasing    after the ___   \u201cattend\u201d   \u201ctell\u201d   \u201cground\u201d   \ud835\udc5d(\ud835\udc67\ud835\udc61|\ud835\udc66<\ud835\udc61, \ud835\udc45, \ud835\udc3c)   \ud835\udc5d(\ud835\udc67\ud835\udc61|\ud835\udc66\u2265\ud835\udc61, \ud835\udc66<\ud835\udc61, \ud835\udc45, \ud835\udc3c)   \ud835\udd3c\ud835\udc67\ud835\udc61\ud835\udc5d(\ud835\udc66\ud835\udc61|\ud835\udc67\ud835\udc61, \ud835\udc66<\ud835\udc61, \ud835\udc45, \ud835\udc3c)   ?   Soccer players chasing    after the _ball_   Soccer players chasing    after the _ball_   after   the   chasing   ball   \ud835\udc3c   R   set of regions   \ud835\udc66<\ud835\udc61partial caption   \ud835\udc3c   input image   \ud835\udc67\ud835\udc61t-th aligned region index   \ud835\udc66\ud835\udc61t-th output word   Figure 2. We propose a deep conditional generative model of   visual descriptions that models each word-to-region alignment   with a discrete latent variable zt. It is able to attend over the region proposals in an input image (or video), tell what it shows by   marginalizing out the latent word-to-region alignments from the   joint distribution and ground each word by leveraging the learned   approximate posterior word-to-region alignment distribution.   3. Method   3.1. Problem Formulation   Let Y denote a visual description of a given visual input   I (i.e., an image or video). We represent Y = {y1, . . . , yT }   as a sequence of T words from a vocabulary V, where yt is   the one-hot encoding of the t-th word, i.e., yt \u2208{0, 1}|V|   and \u2225yt\u2225= 1. In the VOG task, the goal is to ground words   in ground-truth descriptions of a visual input, i.e., we are interested in localizing each mentioned groundable word with   a bounding box bbt. In the GVD task, the goal is to both   generate a visual description bY and localize each generated   groundable word byt with a bounding box bbt.   In this work, we propose to design a model that can   tackle both tasks in both the image and video domains,   and can be trained with weak supervision in the form   of aligned visual input and and visual description pairs   {(I(n), Y (n))}N   n=1. To achieve this, we treat the problem   of grounding as a problem of word-to-region alignment by   leveraging M candidate region proposals R = {rm}M   m=1   obtained by an off-the-shelf object detector [23]. Then, the   localization problem is reduced to identifying the variable   zt \u2208{0, 1}M with \u2225zt\u2225= 1, which denotes which region corresponds to the t-th word. Our key idea is to model   word-to-region alignments as latent variables in a deep conditional generative model. To this end, we propose a novel   Grounded Visual Description Conditional Variational Autoencoder (GVD-CVAE). As illustrated in Fig. 2, learning   such a model allows us to leverage the posterior distribution   of word-to-region alignments for grounding words based on   the entire sentence, unlike attention-based grounding.   15546   \ud835\udc99\ud835\udfcf\ud835\udc99\ud835\udfd0\ud835\udc99\ud835\udfd1   \ud835\udc99\ud835\udc74   \ud835\udc66\ud835\udc61\u22121   <start> soccer   \u2212log \ud835\udc5d\ud835\udf03\ud835\udc66\ud835\udc61= \ud835\udc66\ud835\udc61   \u2217\ud835\udc67\ud835\udc61   (\ud835\udc60), \ud835\udc66<\ud835\udc61, \ud835\udc45, \ud835\udc3c)   \ud835\udc891   \ud835\udc892   \ud835\udc89\ud835\udc61   \ud835\udc9a\ud835\udc95= ball   soccer players   \ud835\udc89\ud835\udc61\u22121   q-attention    network   p-attention    network   \ud835\udf19   \ud835\udf03   X =   Region Features   CNN   \u2026   \ud835\udc3c   Proposals   RoIAlign   embed   ball   \ud835\udc45set of regions   \ud835\udc3cinput image   \ud835\udc67\ud835\udc61t-th aligned region   \ud835\udc66\ud835\udc61t-th word   \ud835\udc5d\ud835\udf03= \ud835\udc36\ud835\udc4e\ud835\udc61(\ud835\udc82\ud835\udf03   \ud835\udc61)   \ud835\udc5e\ud835\udf19= \ud835\udc36\ud835\udc4e\ud835\udc61(\ud835\udc82\ud835\udf19   \ud835\udc61)   \ud835\udc9b\ud835\udc95   (\ud835\udc94)   sampled    region   index   Prior   Approx. Posterior   MLP   UpDown   LSTM    \ud835\udc94\ud835\udc61   \ud835\udc89\ud835\udc61+1   Decoder   \ud835\udc4b   \ud835\udc4b   \ud835\udc4b   \ud835\udc72\ud835\udc73(\ud835\udc92||\ud835\udc91)   \ud835\udf03   \ud835\udf03   Figure 3. Our proposed GVD-CVAE architecture. The input   image and proposals are fed through a visual encoder to produce   region embeddings. The prior word-to-region alignment is computed as a function of only the previous words, while the approximate posterior is computed as a function of the full sentence. During training, a region is sampled from the approximate posterior   and is fed to the language decoder that predicts the next word.   3.2. Attention-based Conditional Variational Autoencoder for Grounded Visual Description   Let Z = {z1, . . . , zT } be the sequence of latent variables corresponding to alignments between words and regions, where zt \u2208{0, 1}M is a binary discrete random   variable with zt,i = 1 when the i-th region proposal corresponds to the t-th word yt. The joint conditional distribution p\u03b8(Y, Z | R, I) of a caption Y and sequence of alignments Z, given the input video (or image) I and candidate   regions R can be factorized in an autoregressive manner:   T   Y   t=1   p\u03b8(yt | y<t, z\u2264t, R, I)p\u03b8(zt | y<t, z<t, R, I),   (1)   where y<t = y1:t\u22121 is the partial caption up to word t \u22121,   and similarly z<t denotes the sequence of word-to-region   alignments up until word t \u22121. We can simplify this joint   distribution by making two assumptions: (a) the t-th word   depends only on the region zt given the partial caption y<t,   and (b) the region-to-word alignments zt for each word are   conditionally independent of each other given the partial   caption. Hence, our joint probability distribution becomes:   p\u03b8(Y, Z|R, I) =   T   Y   t=1   language decoder   z   }|   {   p\u03b8(yt|y<t, zt, R, I)   region prior   z   }|   {   p\u03b8(zt|y<t, R, I) .   (2)   Next, we describe how we parameterize our conditional   generative model with deep networks whose trainable   weights are denoted by \u03b8, as illustrated in Fig. 3.   Visual Encoder. Images are encoded using a pretrained   CNN model with RoI-pooling operations and trainable linear projections [74]. The encoder captures global visual   context in the form of a coarse image-level feature vector, v, as well as \ufb01ne-grained grid features F = {fl}L   l=1,   where l indexes the feature map spatial grid. It also generates grounding-aware region representations X = {xi}M   i=1,   where the representation xi of each region encodes information about appearance, position and object class knowledge [74] transferred from an object detector [23] trained   on an external dataset [31]. Videos are also encoded to a   global video feature v, a sequence of frame-level features   F = {fl}L   l=1, where l indexes the frames, and groundingaware region representations X = {xi}M   i=1, but using different network architectures, as detailed in the appendix.   Language Decoder. The decoder p\u03b8(yt|y<t, zt, R, I) =   Cat(g\u03b8(st, zt, X)) is a categorical distribution over words   in the vocabulary given the partial caption y<t, the wordto-region alignments zt, the regions R, and the visual input   I. We parameterize this distribution with a shallow network   g\u03b8(st, zt, X) = softmax(Wc tanh(Wp   \"   st;   M   X   i=1   zt,ixi   #   )),   (3)   whose inputs are: (a) the state st \u2208Rd of a language model   that summarizes y<t, R and I, and (b) the aligned region   feature PM   i=1 zt,ixi \u2208Rd, where [\u00b7; \u00b7] denotes concatenation, and Wc \u2208Rd\u00d7d, Wp \u2208Rd\u00d72d are learnable weights.   Although st can be chosen as the state of any standard language model [2, 76], we follow prior work on   grounded visual description [37, 40, 74, 77] and adopt a   variant of the UpDown [2] LSTM model. This language   model is composed of a word embedding layer (emb) and   two LSTM [25] layers with hidden states ut and st, respectively. It also uses an additive attention mechanism [4]   f\u03b8(ut, F) = softmax(wT   f tanh(Wf[ut; fl])) over holistic   visual features F, where wf, Wf are learnable attention   weights. Region features X are also summarized with another additive attention mechanism k\u03b8(ut, X).   ut = RNN1   \u03b8 (ut\u22121, [v; emb(yt\u22121)])   (4)   st = RNN2   \u03b8       st\u22121,   \" L   X   l=1   f (l)   \u03b8 (ut, F)fl;   M   X   i=1   k(i)   \u03b8 (ut, X)xi; ut   #!   .   (5)   Prior Model. The prior distribution, p\u03b8(zt|y<t, R, I), is a   categorical distribution over possible word-to-region alignments. We choose to parameterize it with an additive attention mechanism [4] that computes region attention coef\ufb01cients \u03b1\u03b8(st, X) \u2208RM using as a query the top LSTM   15547   state st that summarizes the partial caption and visual input:   zt | y<t, R, I \u223cCat(\u03b1\u03b8(st, X)).   (6)   Variational Posterior. To learn the parameters of our conditional generative model we leverage Amortized Variational Inference (AVI). Therefore, our model becomes a   CVAE [59] with sequential discrete latent space and sentences as observations. In the CVAE framework, a variational distribution q\u03c6(Z|Y, R, I) is introduced to approximate the true posterior and is parameterized via a neural   network with weights \u03c6, also known as the \u201cinference network\u201d. We choose to approximate the true posterior with   the following approximate posterior:   q\u03c6(Z | Y, R, I) =   T   Y   t=1   q\u03c6(zt | Y, R, I).   (7)   Then, we model the approximate posterior distribution   of each word-to-region alignment as a categorical distribution that is parameterized by the attention coef\ufb01cients   \u03b1\u03c6(ht, X) \u2208RM obtained via another attention network,   implemented as additive attention [4] or general dot-product   attention [39]:   zt | Y, R, I \u223cCat(\u03b1\u03c6(ht, X)).   (8)   In this case, the attention query ht \u2208Rd summarizes the   whole sentence. It is obtained by summing the forward and   backward states of a BiLSTM network, whose inputs consist of the global feature v and ground-truth word yt at each   timestep. Optionally, we can augment the unnormalized attention coef\ufb01cients \u02dc\u03b1\u03c6(ht, X) for the object words in the   input sentence with transferred object class knowledge:   zt | Y, R, I \u223cCat(softmax(\u02dc\u03b1\u03c6(ht, X)+   + \u03b3\u03c9t(wT   cto + 1bct))), (9)   where wct \u2208Rd0, bct \u2208R are trainable weights, initialized   with the pretrained object classi\ufb01er for the external dataset\u2019s   object class ct that is closest to the object word yt, o \u2208   Rdo\u00d7M are region object features, 1 \u2208RM is a vector of all   ones, and \u03c9t is a binary word mask with \u03c9t = 1 denoting a   groundable word. The hyperparameter \u03b3 \u2208{0, 1} controls   whether this transferred knowledge will be used or not.   Training. During training, we assume we are given N i.i.d.   pairs of visual inputs and their visual descriptions, without   grounding supervision. To train our Grounded Visual Description CVAE (GVD-CVAE), we minimize the following   loss over the parameters \u03b8 and \u03c6 (omitting the conditioning   of all distributions on I(n) for readability) :   L = 1   N   X   n,t   \u03bbLCV AE(n, t) + (1 \u2212\u03bb)LCE(n, t)   (10)   where LCE = \u2212log p\u03b8(y(n)   t   |   E   zt\u223cp\u03b8   [zt], y(n)   <t , R(n)) and   LCV AE(n, t) =   E   zt\u223cq\u03c6   h   \u2212log p\u03b8(y(n)   t   | y(n)   <t , zt, R(n))   i   + \u03b2KL   \u0010   q\u03c6(zt | Y (n), R(n)) || p\u03b8(zt | y(n)   <t , R(n))   \u0011   .   (11)   For \u03bb = \u03b2 = 1, we recover the negative of the Evidence   Lower Bound Objective (ELBO) for our factorization of the   joint probability distribution and our choice of the approximate posterior. Similar to prior work in generative modeling [6], we observe that optimizing the ELBO often results   in an inference model that produces approximate posteriors   almost identical to the prior. To mitigate this issue, we reweight the KL loss term with a scalar factor \u03b2. We found   that gradually increasing \u03b2 up to a value \u03b2clip < 1 during   training or using a PI-controller [55] to reach a desired KL   divergence value are effective for training our GVD-CVAE.   Moreover, we experimentally observed that optimizing the   CVAE loss jointly with a cross-entropy word prediction loss   (\u03bb = 0.5), that is applied on word predictions obtained   based on the p-attention-based weighted sum of region features, further facilitates training. More details about training   with Gumbel-Softmax [28,41] samples and about approximate inference with our model are included in the appendix.   4. Experiments   4.1. Datasets, Metrics and Implementation Details   Flickr30k Entities (F30k) is a large-scale image dataset,   originally annotated with phrase-to-region alignments [46].   To evaluate our results on object grounding (rather than   phrase grounding), we follow the setup from Zhou et   al. [74] to convert each noun phrase (e.g. her brown hat)   associated with each bounding box to a single groundable   object, such as hat. This results in |Vo| = 480 groundable   words out of the |V| = 8639 words comprising the vocabulary. We use the standard dataset split with 29k/1k/1k images in the training, validation and testing sets, respectively.   ActivityNet Entities (ANet) is a large-scale video dataset,   containing 52k video segments annotated with a caption   each. Following the original setup [74], we use a vocabulary of 4905 words, 431 of which are groundable. Each   groundable word in a sentence is associated with a bounding box in a frame of the video where it can be clearly observed. Since annotations for the testing set are not public   and the evaluation server is closed at the time of submission,   we follow [64] and report results on the validation set.   YouCook2-BB is a video dataset containing YouTube cooking videos with video segments paired with captions and   bounding box annotations [75] at 1 fps for 67 object classes.   We use the same training/validation/test split as in [56].   15548   Table 1. Comparison of grounding performance between the   GVD and GVD-Grd models (baselines) and the GVD-CVAE   on the validation sets of F30k and ANet. We report the box accuracy metric for evaluating grounding given ground-truth sentences   and the F1all metric for evaluating grounding of object words in   generated sentences. GVD-CVAE-p (GVD-CVAE-q) denotes using our learned prior (approximate posterior) alignment distribution for grounding.   Dataset   Method   Box Acc.   F1all   F30k (Image)   GVD [74]   22.0   4.4   GVD-Grd [74]   25.9   4.4   GVD-CVAE-p (Ours)   29.6   6.2   GVD-CVAE-q (Ours)   33.4   7.3   ANet (Video)   GVD [74]   14.9   3.7   GVD-Grd [74]   21.3   3.7   GVD-CVAE-p (Ours)   19.4   4.8   GVD-CVAE-q (Ours)   24.2   6.1   Metrics. Performance for WS-VOG is measured with Box   Accuracy [56, 74, 75], which computes the percentage of   correctly localized words of an object class. A word is considered to be correctly localized when its predicted box has   more than 0.5 Intersection-over-Union (IoU) with groundtruth boxes. Metrics for WS-GVD evaluate both grounding and captioning capabilities. We adopt the F1all and   F1loc grounding metrics [74] for evaluating grounding on   generated sentences, and standard language evaluation metrics, such as Bleu [44], METEOR [32], CIDEr [62], and   SPICE [2], for evaluating generated sentences. In F1all,   a region prediction is considered correct if the object word   is both correctly predicted and localized, while F1loc only   considers correctly predicted object words.   Implementation details. For the F30k and Anet datasets,   our GVD-CVAE receives as inputs the region proposals, region features and image/video global features from Zhou et   al. [74], with 100 region proposals per frame/image. For   YouCook2, we use 20 region proposals and the features extracted by Shi et al. [56]. Hyperparameters such as learning   rate, \u03b2clip, attention mechanisms, number of samples, are   chosen based on the validation sets of F30k and YouCook2.   For evaluating on the ANet validation set, we train a model   with hyperparameters selected based on the F30k validation   set. All other hyperparameters, such as layer sizes, are in   general adopted from prior work [56,74]. Additional training and implementation details are included in the appendix.   4.2. Baselines and Ablation Studies   (1) Are the regions localized via our learned wordto-region alignment distributions better than those localized via soft-attention-based baselines?   Our baseline is the attention-based encoder-decoder GVD captionTable 2. Ablation analysis of the decoder and inference model   design on the F30k validation set. Types of UpDown [2] model attention: Grid: over grid features, Reg.: over region features, Both:   both attention mechanisms. Obj. Cls. denotes inference model   with transferred object class knowledge (\u03b3 = 1).   Approximate Posterior   Box Acc.   F1all   Decoder   Cond.   Obj. Cls.   p   q   p   q   UpDown (Both)   zt|y\u2264T   \u0013   29.6   33.4   6.2 7.3   UpDown (Both)   zt|y\u2264T   \u0017   25.1   32.3   5.4 7.0   UpDown (Both)   zt|y\u2264t   \u0017   26.3   31.4   6.0 7.4   UpDown (Grid)   zt|y\u2264T   \u0013   30.7   34.4   7.3 7.2   UpDown (Reg.)   zt|y\u2264T   \u0013   26.6   33.0   5.8 6.3   LSTM   zt|y\u2264T   \u0013   30.2   34.8   6.9 7.5   ing model, trained with teacher-forcing language generation   cross-entropy loss. We ensure that our GVD-CVAE exactly   mirrors the inputs and the visual encoder/language decoder   modules of this baseline model. Baseline object grounding   is performed either by (a) selecting the region with maximum region attention coef\ufb01cient k(i)   \u03b8 (ut, X) (GVD [74])   given the partial caption y<t, or (b) by combining the attention coef\ufb01cients with region-to-class similarity scores based   on the word yt to be grounded for the VOG task (GVDGrd [74]). In Table 1, we compare our GVD-CVAE\u2019s ability to ground objects in ground-truth or generated sentences   with these two powerful, discriminative baselines. We observe that even grounding based on our learned prior wordto-region alignment distribution (GVD-CVAE-p) improves   upon the soft-attention baseline by a signi\ufb01cant margin in   both benchmarks and tasks (e.g., it improves Box Accuracy from 22% to 29% on F30k), despite similarly capturing   only the history of previous words. The reason for this improvement is that our prior distribution is encouraged during   training to \u201clook ahead\u201d when sampling a region to generate   a word, by mimicking the approximate posterior alignment   distribution which has access to future words. Using the   latter for grounding conditioned on the full sentence further   improves results (from 29.6% to 33.4%), verifying our intuition that leveraging the word to be grounded in its language   context can help us better localize the word. Additionally,   it outperforms the GVD-Grd discriminative baseline which   also takes into account the word to be grounded, demonstrating the bene\ufb01ts of our conditional generative modeling.   (2) How does the choice of the language model and approximate posterior affect grounding performance? Table 2 demonstrates the grounding performance obtained   with different design choices.   First, results suggest that   taking the full sentence into account via a BiLSTM   (q(zt|y\u2264T )) leads to better VOG grounding compared to   only seeing the sentence up to the current word y\u2264t with   an LSTM (e.g., improving Box Acc. from 31.4% to 32.3%)   15549   Table 3.   Impact of various training objectives on weaklysupervised object grounding. Performance measured via Box   accuracy (%) on the F30k validation set.   Training objective   CVAE-p   CVAE-q   ELBO   3.29   3.16   CE + ELBO   25.22   23.99   CE + ELBO + \u03b2 anneal   26.07   25.61   CE + ELBO + \u03b2 anneal + clip   26.31   28.88   CE + ELBO + PI Controller   29.27   31.71   (rows 2-3). Another observation is that explicitly adding   transferred information about object class distributions in   the inference model (\u03b3 = 1) improves grounding given   ground-truth sentences (Box Accuracy) with both the prior   and approximate posterior distributions (rows 1-2). This   further demonstrates that knowledge from the inference   model is distilled to the prior during training via the KL   loss, resulting in a model that is looking at better localized   regions while generating descriptions based on the prior   and decoder modules. Interestingly, results suggest that our   GVD-CVAE is robust to the choice of the language decoder   (rows 1,4-6), and achieves top grounding performance even   when using a simple LSTM in the decoder or an UpDown   LSTM with soft-attention only over grid features, demonstrating the effectiveness of our latent-variable modeling.   (3) What is the effect of the proposed training objective?   We \ufb01rst train our GVD-CVAE with the vanilla CVAE loss,   i.e., with \u03bb, \u03b2 = 1. Without any of our proposed modi\ufb01cations, this results in a very low grounding performance, as   can be seen in the \ufb01rst row of Table 3. By adding the crossentropy loss term that penalizes word predictions based on   soft region context determined by the p-attention network   (CE+ELBO), we are able to improve upon the soft-attention   baseline of 22%. However, learning curves (included in the   appendix) show that the KL loss term has vanished, suggesting that the model\u2019s posterior has collapsed to the prior   and the approximate posterior alignment does not additionally take into account the word being grounded. Applying   known solutions to KL vanishing, such as linearly annealing   the \u03b2 hyperparameter from 0 to 1 (CE+ELBO+\u03b2 anneal),   does not solve the problem. Instead, our proposed clipped   linear annealing schedule leads to overall better grounding   of 28.9% (KL term \u22480.06). Alternatively, after we determine a desirable value for the KL term, we can use the   PI-Controller [55] anneal \u03b2, which we found to be less sensitive to changes in architecture and requires minimal calibration. Note that in this ablation we used a single LSTM   language decoder and an LSTM in the inference model for   faster experimentation.   4.3. Comparison with the State of the Art   As shown in Table 4, our GVD-CVAE improves weaklysupervised object grounding by 12% compared to the GVD   Table 4. Results on the Flickr30k Entities test set. The performance of the fully-supervised GVD model (Sup.) is reported as   an upper-bound to the weakly-supervised approaches. Types of   model inputs during inference: region proposals extracted and encoded following GVD [74] or BUTD [2], or Scene-graphs [70].   \u2020 denotes models trained using auxiliary image-to-text matching   models [33]. RL denotes models \ufb01ne-tuned via Reinforcement   Learning [49]. Note that results in the third block are obtained   with different inputs, and thus they are not directly comparable to   ours. We report average results for our GVD-CVAE after 5 random runs (standard deviations are included in the appendix).   VOG   GVD   Captioning   Grounding   Feat   Acc   B@4 M   C   S   F1all F1loc   GVD [74] (Sup.)   G   41.4   27.3 22.5 62.3 16.5 7.55   22.2   GVD [74]   G   21.4   26.9 22.1 60.1 16.1 3.88   11.7   GVD-Grd [74]   G   25.5   26.9 22.1 60.1 16.1 3.88   11.7   Cyclical [40]   G   26.6 22.3 60.9 16.3 4.85   13.4   DPA [37]   G   27.6 22.6 62.7 16.7 4.79   15.5   SCAN-RL [77] \u2020   G   28.0 22.6 66.2 17.0 6.53   15.8   BUTD [2]   U   24.2   27.3 21.7 56.6 16.0   DPA [37]   U   27.2 22.3 60.8 16.3 5.45   15.3   Sub-GC [73]   S   28.5 22.3 61.9 16.4 5.98   16.5   SCAN-RL [77] \u2020   U   30.1 22.6 69.3 16.8 7.17   17.5   GVD-CVAE   G   33.7   24.0 21.3 55.3 15.7 6.70   19.2   GVD-CVAE-RL   G   31.6   29.8 23.1 67.6 17.2 6.94   17.6   Table 5. Results on the ActivityNet Entities validation set. We   report average results for our GVD-CVAE after 5 random runs.   VOG   GVD   Captioning   Grounding   Acc   B@4 M   C   S   F1all F1loc   GVD (Sup.) [74]   35.7   2.59 11.2 47.5 15.1   7.1   24.1   MIL-based   NAFAE [56]   19.5   STVG [67]   21.1   SCL [64]   23.8   Captioning-based   GVD [74]   14.9   2.28 10.9 45.6 15.0   3.7   12.7   GVD-Grd [74]   21.3   2.28 10.9 45.6 15.0   3.7   12.7   Cyclical [40]   2.45 11.1 46.4 14.8   4.7   15.8   GVD-CVAE   23.9   1.90 10.4 41.8 13.3   5.8   21.7   method (21.4% to 33.7%) on the F30k image dataset.   Thus, it sets the state-of-the-art VOG result, and reduces   the gap with the fully-supervised GVD approach (41.4%).   It also generates more grounded captions (higher F1all   and F1loc scores) than all other methods, given the same   features (from GVD). We even outperform methods using   Scene Graphs [70] for grounding [73]. Note that the F1all   15550   Table 6. Results on the YouCook2 test set following the experimental setup of Shi et al. [56]. GVD* denotes our implementation and training of the GVD model [74].   Box accuracy (%)   macro   micro   Upper Bound   62.41   MIL-based methods   DVSA-frm [29]   37.55   44.16   Zhou [75]   35.08   42.42   NAFAE [56]   40.71   46.33   STVG [67]   41.67   48.22   SCL [64]   42.80   48.60   Captioning-based methods   GroundR [51]   19.94   GVD* [74]   37.40   44.15   GVD-CVAE (Ours)   38.85 \u00b1 0.20   44.62 \u00b1 0.09   scores obtained by both our CVAE-p (6.43%) and CVAE-q   (6.70%) distributions outperform Cyclical [40] (4.85%) and   DPA [37] (4.79%). This suggests that modeling alignments   as latent variables works better than applying attention regularization techniques during training. Despite generating   more grounded captions, our method has lower captioning   metrics than SoTA methods, some of which apply reinforcement learning (RL). However, our language model can also   be \ufb01netuned with a CIDEr-based SCST loss [49] (GVDCVAE-RL), leading to competitive captioning metrics.   Results on the ANet video dataset (Table 5) show similar   trends. Our GVD-CVAE yields better metrics when grounding ground-truth or generated sentences.   It also outperforms video-tailored, video-to-text matching models, such   as NAFAE [56]. Although powerful, these models cannot   tackle the WS-GVD task. Since we evaluate only on the validation set, we did not select the model with best CIDEr   score, or tune the learning rate based on it.   This might   have led to our slightly inferior captioning metrics compared to [40,74], which used the validation set for selecting   a model to be evaluated on the now closed test server.   We also compare our method to MIL-based grounding   approaches in the YouCook2 test split in terms of Box Accuracy (additional metrics are reported in the appendix).   As seen in Table 6, although our method outperformed all   video-to-text-matching methods on the ANet video dataset,   it is, for instance, lagging behind NAFAE [56] by around   2% on the YouCook2 dataset. A possible explanation is   that, while in ANet grounding is evaluated on a single   frame, in YouCook2 grounding predictions are evaluated   in every frame. Therefore, MIL-based methods that model   the consistency between the localized regions at each frame   or model inter-object interactions perform better. We believe that extending our GVD-CVAE to model such relationships will improve these metrics, and we leave that to   future work. Finally, we show qualitative image grounding   results in Fig. 4.   Limitations.   Similar to all other proposal-based approaches, our model\u2019s performance is limited by the quality of the region proposals. Also, our GVD-CVAE does   not model the dependency between alignments for consecutive words. Finally, we applied the same framework for   image and video object grounding to demonstrate its generality and effectiveness, without taking advantage of several   inductive biases in the video domain, such as the visual similarity between grounded regions in consecutive frames.   GT: an adult soccer game, soccer players   chasing after the ball during a live game    GT: one football player in a red jersey jumping    onto a player wearing a white jersey   GT: a woman outside on a street wearing a    yellow shirt and sunglasses   Attn.   Attn.   CVAE-p   CVAE-q   CVAE-p   CVAE-q   Attn.   CVAE-p   CVAE-q   Figure 4. Qualitative WS-VOG results on the F30k validation   set. For each ground-truth caption, we show grounding results   obtained by (a) the soft-attention baseline, (b) our prior, and (c) our   approximate posterior alignment distributions. We observe that   knowing the words to be grounded improves grounding of small   objects. Third row shows a failure case, in which our CVAE-q   predicts the same bounding box for all groundable words.   5. Conclusion   In this paper, we proposed a novel grounded visual   description CVAE. We showed how leveraging the latent   alignment distributions of our model outperforms soft attention for grounding given ground-truth or generated sentences. We also demonstrated the generality and effectiveness of our model by evaluating it on both image and video   datasets. Our novel approach yields competitive results in   both grounding and grounded video description, while comparing against methods optimized for one of the two tasks.   Acknowledgements. The authors thank Benjam\u00b4\u0131n B\u00b4ejar   Haro, Carolina Pacheco O\u02dcnate, Ambar Pal, Paris Giampouras, and the anonymous reviewers for their valuable   comments.   This research was supported by the IARPA   DIVA program via contract number D17PC00345.   15551", "conf": "CVPR", "year": "2022", "index": 179}, {"title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics   Volume 1: Long Papers, pages 10914\u201310932   July 9-14, 2023 \u00a92023 Association for Computational Linguistics   Weakly-Supervised Spoken Video Grounding via   Semantic Interaction Learning   Ye Wang\u2217, Wang Lin\u2217, Shengyu Zhang\u2217,   Tao Jin, Linjun Li, Xize Cheng, Zhou Zhao\u2020   Zhejiang University   {yew,linwanglw,sy_zhang}@zju.edu.cn   {jint_zju,lilinjun21,chengxize,zhaozhou}@zju.edu.cn", "abstract": "The task of spoken video grounding aims to   localize moments in videos that are relevant   to descriptive spoken queries. However, extracting semantic information from speech and   modeling the cross-modal correlation pose two   critical challenges. Previous studies solve them   by representing spoken queries based on the   matched video frames, which require tremendous effort for frame-level labeling. In this   work, we investigate weakly-supervised spoken video grounding, i.e., learning to localize   moments without expensive temporal annotations. To effectively represent the cross-modal   semantics, we propose Semantic Interaction   Learning (SIL), a novel framework consisting   of the acoustic-semantic pre-training (ASP) and   acoustic-visual contrastive learning (AVCL). In   ASP, we pre-train an effective encoder for the   grounding task with three comprehensive tasks,   where the robustness task enhances stability   by explicitly capturing the invariance between   time- and frequency-domain features, the conciseness task avoids over-smooth attention by   compressing long sequence into segments, and   the semantic task improves spoken language   understanding by modeling the precise semantics. In AVCL, we mine pseudo labels with   discriminative sampling strategies and directly   strengthen the interaction between speech and   video by maximizing their mutual information.   Extensive experiments demonstrate the effectiveness and superiority of our method.1   1", "content": "Temporal video grounding (Gao et al., 2017; Hendricks et al., 2017) is an important task in the crossmodal understanding field (Zhang et al., 2020d;   Jin et al., 2020; Xun et al., 2021; Jin and Zhao,   2021; Yin et al., 2022), aiming to retrieve a target   moment within a video based on a given query.   \u2217Equal contribution.   \u2020 Corresponding author   1https://github.com/yewzz/SIL.   Figure 1: An Example of Spoken Video Grounding.   With the progress in deep learning, there has been   significant achievements in this area. While most   previous studies focus on textual queries, recent   work (Xia et al., 2022) introduce the spoken video   grounding task by incorporating the spoken query   into the video grounding, as shown in Figure 1.   However, such video grounding task with spoken queries presents unique challenges compared   to its text-based counterpart. First, the encoding   of speech is inherently difficult due to its weak   and volatile semantic information, making it arduous to extract useful features for video grounding.   Second, even with the acquisition of valuable semantic features, modeling speech-video interaction   and extracting cross-modal content still poses an   inevitable obstacle.   Prior work (Xia et al., 2022) address these two   problems simultaneously through the proposed   video-guided contrastive predictive coding, which   utilizes aligned video clips to learn semantic representations from spoken queries. However, a critical   drawback is its heavy reliance on precise temporal matching between video and spoken queries.   The acquisition of such fine-grained annotations   requires substantial manual labor, hindering the   practicality and applicability of this approach.   In this work, we address the issue of intensive   labor by investigating a novel task called WeaklySupervised Spoken Video Grounding (WSVG),   aiming to localize speech-aligned moments in   videos under a weakly-supervised scheme.   In   this setting, we only have access to aligned video10914   speech pairs during training without any temporal   annotations, which poses a greater challenge.   To tackle the aforementioned problem, we propose a novel framework Semantic Interaction   Learning (SIL), following a progressive pipeline to   first pre-train a speech encoder for semantic encoding and then learn speech-video alignment during   grounding training. It consists of two key components: acoustic-semantic pre-training (ASP) and   acoustic-visual contrastive learning (AVCL).   In the pre-training stage, the ASP module utilizes external speech-text data to train a speech   encoder capable of extracting rich semantic information from speech. To adapt the encoded features for the downstream weakly-supervised video   grounding, ASP includes three specialized tasks   targeting three specific characteristics. (1) The robustness task focuses on the encoder\u2019s ability to   handle complex and noisy speech, which is a practical problem in the real world. Considering time   series data can be split into the time and frequency   domains to provide invariance regardless of varying distributions (Zhang et al., 2022), we utilize   both time- and frequency-based speech feature for   pre-training and forces their encoded semantic information to be consistent. (2) The conciseness   task addresses the issue of long sequence features   with redundant information, which results in scattered distribution of the video-speech attention and   impedes effective interaction. Hence, we compress the encoded features into discrete segments   via I&F algorithm (Dong and Xu, 2020), refining   the feature sequence for effective interaction. (3)   The semantic task emphasizes the extraction of   key semantics for grounding, which is a crucial   requirement in this fine-grained cross-modal understanding task. Unlike the trivial self-supervised   method (Baevski et al., 2020) or knowledge distillation method (Hinton et al., 2015), we draw   inspiration from the human understanding system   that encompasses auditory perception and cognitive processing (Dong et al., 2021). Concretely,   we introduce a connectionist temporal classification (CTC) (Graves et al., 2006) loss to facilitate   training, and further consider both sequence-level   and word-level semantic to ensure the comprehensive semantic transfer.   In the grounding stage, the AVCL module directly enhances the correlation between video and   speech. Despite the effective semantic encoding   of spoken queries, the discrepancy between video   and speech still hinders the cross-modal interaction. As video and speech are from two distinct   feature spaces, AVCL leverages contrastive learning to maximize their agreement. First, we perform   location-based selection and score-based mining to   select pseudo labels with high confidence. With the   located boundary of the predicted pseudo proposal,   we can coarsely select the negative samples from   regions outside and further calculate the clip-level   score inside the boundary to mine positive/negative   samples. Then, based on these discriminative samples, we contrastively maximize the mutual information between speech and positive clips.   Our main contributions are listed as follows:   \u2022 We investigate a new task WSVG to explore the   weakly-supervised spoken video grounding.   \u2022 We propose a novel framework SIL to effectively   model the semantic contents of video-speech interaction, where the ASP module enhances semantic encoding and the AVCL module improves   cross-modal interaction.   \u2022 Extensive experiments verify the superiority of   our approach in terms of both accuracy and efficiency.   2   Related Works   2.1   Temporal Video Grounding   Temporal video grounding aims to localize the moment corresponding to the query. Under the supervised setting, existing methods can be categorized   into the top-down and bottom-up frameworks. The   top-down methods (Gao et al., 2017; Hendricks   et al., 2017; Liu et al., 2018; Chen et al., 2018;   Zhang et al., 2019) first generate proposals and   then estimate cross-modal alignment scores for   them. And the bottom-up methods (Chen et al.,   2019a, 2020; Wu et al., 2020; Zhang et al., 2020a;   Zhao et al., 2021) directly calculate the frame-level   probabilities of being temporal boundaries. Under the weakly-supervised setting, the methods can   be categorized into the multiple instance learning   (MIL) and the reconstruction frameworks. The   MIL methods learn the latent visual-textual alignment by distinguishing the matched video-language   pairs from the unmatched pairs. For example, Gao   et al. 2019 devise an alignment and a detection   module. Zhang et al. 2020d develop contrastive   learning between counterfactual results.   Huang   et al. 2021 explore cross-sentence relational constraints. The reconstruction methods reconstruct   10915   the query from visual contents during training and   utilize intermediate results to localize. For example, Lin et al. 2020 utilize language reconstruction   to rank proposals. Song et al. 2020 further employ   the attention weight. Zheng et al. 2022 mine negatives within the same video. Recent work (Xia   et al., 2022) study the spoken video grounding task   and represent speech with video-guided contrastive   predictive coding. We consider the intensive labor   and introduce the weakly-supervised setting.   2.2   Vision-Audio Learning   Vision-audio learning has attracted researchers\u2019 interest in recent years. Since Harwath and Glass   2015 collect spoken captions for Flickr8k, much   research (Chrupa\u0142a, 2022; Harwath et al., 2019;   Higy et al., 2021; Scholten et al., 2021) begins   to attach importance to this field. Some works   emphasize the cognitive and linguistic questions,   such as understanding how different learned layers correspond to visual stimuli (Chrupa\u0142a et al.,   2017; Gelderloos and Chrupa\u0142a, 2016), learning   linguistic units (Harwath and Glass, 2019; Harwath et al., 2019). Oncescu et al. 2020 propose   QuerYD, a video dataset with both text and audio   descriptions for text-video retrieval and corpus moment retrieval. Recent work (Cheng et al., 2023)   study visual speech translation and recognition.   3   Methods   3.1   Overview   Problem Formulation.   Given an untrimmed   video V and a spoken query S, this task aims to   train a network G(V, S) to localize the most relevant moment proposal p corresponding to the spoken query S, without (p, S) alignment annotations,   i.e. only the video-speech pair (V, S) are available.   Overall Pipeline.   Our SIL follows a two-stage   pipeline. First, we pre-train the speech encoder   with external speech-text data. In this stage, the   ASP module develops three tasks to improve robustness, conciseness and semantic respectively,   which enables the encoder to extract effective information of speech for the downstream task. Then,   we fix the speech encoder and conduct weaklysupervised training on the grounding dataset via   our base network. In this stage, the ACVL module selects contrastive samples via a discriminative   sampling strategy and then maximizes the mutual   information between video and speech.   Figure 2: The Base Network for WSVG.   3.2   Base Network   To illustrate our framework clearly, we first formulate the base grounding network G(V, S) under SIL   as following four common modules:   \u2022 Feature Encoder: The video encoder encodes   video features as \u00afV = {\u00afvi}nv   i=1 \u2208Rnv\u00d7d and   the speech encoder encodes speech features as   \u00afS = {\u00afsi}nc   i=1 \u2208Rnc\u00d7d, where d is the hidden   size, nv and nc are the length of video and speech   features, respectively.   \u2022 Interaction: It develops the cross-modal interaction between \u00afV and \u00afS, then outputs multi-modal   clip features {mi}nv   i=1 \u2208Rnv\u00d7d. The interaction   methods include attention-based aggregation and   feature fusion (Zhang et al., 2019).   \u2022 Proposal Scorer: Based on multi-modal clip   features {mi}nv   i=1, it extracts np proposal features and calculates their alignment scores K =   {ki}np   i=1. The score of each video-speech pair   (V, S) is f(K), where f(\u00b7) is the average of the   top-R proposal scores.   \u2022 Training: We follow the MIL paradigm (Zhang   et al., 2020c) to utilize the score f(K) to train   the model with binary cross entropy loss Lbase,   which distinguishes the matched video-speech   pair (V, S) from two randomly-selected negative   pairs (V \u2032, S) and (V, S\u2032).   The details are introduced in Appendix A.   3.3   Acoustic-semantic Pre-training   In this section, we elaborate on our pre-training   for the speech encoder. Given the external data,   we denote the speech as S and its paired text (i.e.   transcript) as W. We first introduce the overall encoding process and then detail our designed tasks.   Our speech encoder consists of convolutional layers and Na+Ns layers Transformer encoder (Vaswani et al., 2017). (1) First, the robustness task in Section 3.3.1 simultaneously considers   the time-based features Stime and frequency-based   features Sfreq as the speech input. For ease of presentation, we omit superscripts and denote them   10916   Figure 3: The Concrete Design of Semantic Interaction Learning Framework.   as S. We apply convolutional blocks on S to extract their deep features S = {si}ns   i=1 \u2208Rns\u00d7d,   where ns is the downsampled sequence length.   (2) Next, we input these features into Na layers   Transformer encoder to obtain acoustic features   Sa \u2208Rns\u00d7d. The conciseness task in Section 3.3.2   then compresses them into segment-level features   Sc \u2208Rnc\u00d7d, where nc is the length of segments.   (3) Finally, we input Sc into another Ns layers   Transformer encoder to learn semantic features   Ss \u2208Rnc\u00d7d. We train Sa and Ss via the semantic   task in Section 3.3.3.   3.3.1   Robustness Task   As explicit consideration of the frequency domain   provides an understanding of the behavior of time   series that cannot be captured in the time domain   (Zhang et al., 2022), we aim to improve robustness   by considering both domain features and identifying their general property of speech that is preserved across transformations.   For each speech S, we generate its time-based   feature as St (i.e. wave) and frequency-based feature as Sf (i.e. mel-frequency spectrum). They   can be converted to each other through Fourier   transform and inverse Fourier transform. Then we   simultaneously input two features into the speech   encoder and perform the same aforementioned pretraining. Here we adopt different convolutional   layers and Na layers transformer encoder to model   acoustic property for two distinct domain features,   while we remain the rest Ns layers identical for   semantic sharing. Following the encoding process,   we can obtain their corresponding semantic features St   s and Sf   s, which are output from the last   Ns layers encoder. To learn the invariance across   domains, we apply L1 loss to align two features in   a common feature space, given by:   Lrob = |St   s \u2212Sf   s|   (1)   After pre-training, we yield the final semantic features \u02dcS via concatenation as \u02dcS = [St   s, Sf   s]. During   grounding training, we further encode it as \u00afS by a   Bi-GRU network.   3.3.2   Conciseness Task   The long sequence of speech may result in oversmooth attention distribution   (Touvron et al.,   2023). To alleviate ineffective cross-modal interaction caused by this, we design a conciseness task   to compress long acoustic features into segments.   We adopt continuous integrate-and-Fire (I&F)   (Dong and Xu, 2020) algorithm, which is a soft   and monotonic alignment mechanism. First, the   input features Sa = {sa,i}ns   i=1 are fed to a weight   predictor to obtain the weights G = {gi}ns   i=1, representing the amount of information in Sa. We then   scan and accumulate them from left to right until   the sum reaches the threshold(set to 1.0), indicating a semantic boundary bj is detected. Then, we   10917   reset the accumulation and continue to scan the rest   which begins with rj. Finally, we multiply all gi   by corresponding sa,i and integrate them to obtain   segment features Sc = {sc,i}nc   i=1, where nc is the   detected segment number.   To enable each segment carry the complete semantic information, we regard each word in the text   sequence W = {wi}nw   i=1 as an independent supervision signal. Then we develop a non-auto-regressive   decoder to predict word tokens \u00afW = { \u00afwi}n \u00af   w   i=1 from   the segment features. The alignment loss consists   of two terms:   Lcon = (n \u00afw\u2212nw) \u2212   X   (x,y)\u2208( \u00af   W,W)   logPnar(y|x) (2)   where the first item aims to force the length of   predicted token consistent with the target text and   the second item is the cross entropy loss for word   recognition.   3.3.3   Semantic Task   We design the semantic task to transfer the knowledge from the text representation model, e.g. Glove   embedding (Pennington et al., 2014), to the encoded speech features Ss. To stabilize and facilitate   semantic learning, we first utilize an ordinary CTC   loss without considering the syntactic structure   and semantic knowledge of target word sequences.   Next, with the embedding features W = {wi}nw   i=1   of text W, we perform semantic learning with   sequence-level and item-level objectives, where   the sequence-level objective tries to contrastively   align matched speech-text features and the wordlevel objective aims to reconstruct the masked key   word based on the speech. The full semantic loss   consists of three terms Lsem = Lctc+Lseq+Lword.   CTC Warm-up. To model the acoustic structure   for semantic learning, we build a CTC decoder over   the features Sa and optimize it with a CTC loss.   Given the target word sequence W = {wi}nw   i=1,   CTC introduces a set of intermediate paths \u03c6(W),   where each path C \u2208\u03c6(W) is composed of words   and blanks that can be reduced to the target sequence. The loss is computed by:   Lctc = \u2212log   X   C\u2208\u03c6(W)   P(C|Sa)   (3)   Sequence-level Contrastive Objective.   The   sequence-level objective employs contrastive learning to bring the speech closer to its corresponding text in the global feature space. First, we apply mean-pooling on word features W and speech   features Ss to obtain their sequence-level features   Wseq and Sseq   s . For each sequence-level speech   feature Sseq   s , we denote the corresponding text feature as Wseq,+ and randomly sample B unmatched   text features Wseq,\u2212.   We adopt the Info-NCE   loss (Gutmann and Hyv\u00e4rinen, 2010; Sun et al.,   2019) to optimize the alignment by:   Lseq =\u2212log   eSseq   s   \u00b7Wseq,+   eSseq   s   \u00b7Wseq,++PB   i=1eSseq   s   \u00b7Wseq,\u2212   (4)   Word-level Generative Objective. Though the   sequence-level objective ensures the global semantic, it fails to capture the information of crucial   word for grounding. Thus, we further leverage the   speech content to predict the masked words in order   to preserve the word-level knowledge.   We mask x% of the word features W to generate modified word features Wm as (Devlin et al.,   2018). Then we build a bi-directional Transformer   decoder with Wm as queries and Ss as keys and   values. The output o of the decoder is given by   o = TransformerDecoder(Wm, Ss).   We employ a linear layer to predict the word distribution   {ei}nw   i=1 \u2208Rnw\u00d7db, where db is the vocabulary size.   Finally, we compute the negative log-likelihood of   each word and add them up, given by:   Lword = \u2212   nw\u22121   X   i=1   logp(wi+1|ei)   (5)   3.4   Acoustic-visual Contrastive learning   In grounding training, we conduct acoustic-visual   contrastive learning (AVCL). To mine visual samples as guidance, we design two discriminative   sampling strategies.   Location-based Selection. As no temporal annotations are provided under the weakly-supervised   setting, we consider the selected proposal p as the   latent visual guidance and coarsely select negative   samples outside the boundary of the proposal p.   Score-based Mining. To mine high-quality visual samples, we further calculate clip-level scores   {ci}nv   i=1 for clips inside the boundary through the   proposal scorer in Section 3.2, where the proposal   features is replaced with the clip features as input.   Then we select several clips with the highest scores   as positive samples, while reserving a subset with   the lowest scores as negative samples.   With the above strategies, we select T positive   and T negative clips samples.   The inspiration   10918   comes from the observation on experiments. During early training, the predicted boundary tends   to cover a wide temporal range, thus the locationbased selection provides insufficient negative samples and the score-based mining can further select   hard negative clips within the predicted proposal as   a complementary part. As the training goes on, the   predicted boundary will narrow and be more accurate, the location-based selection can select enough   negative samples to avoid introducing noise.   Given \u00afV+ and \u00afV\u2212as features of positive and   negative clips respectively, we maximize the lower   bound of cross-modal mutual information through   Jensen-Shannon estimator (Hjelm et al., 2018; Nan   et al., 2021) as:   LAVCL = E[(\u03d5(\u00afS, \u00afV\u2212))]\u2212E[(\u03d5(\u00afS, \u00afV+))]   (6)   where \u03d5(\u00b7, \u00b7) is the MI discriminator.   3.5   Training and Inference   Pre-Training. We combine the losses of three tasks   to form the overall loss LASP for acoustic-semantic   pre-training by:   LASP = \u03bb1Lrob +\u03bb2Lconc+Lsem   (7)   Grounding Training. We fix the speech encoder   and perform grounding training with the ACVL   module. The full loss LG is given by:   LG = Lbase + \u03bb3LAVCL   (8)   Inference. During the inference, we directly select   the proposal with the highest score as the result.   4   Experiments   4.1   Datasets   We evaluate the weakly-supervised spoken video   grounding on the ActivityNet Speech dataset and   perform pre-training on the LibriSpeech dataset.   ActivityNet Speech (Xia et al., 2022). It is constructed on the ActivityNet Caption (Caba Heilbron et al., 2015), which contains 19,209 videos   annotated with 3.65 textual descriptions on average   and marked with timestamps. ActivityNet Speech   transforms the textual annotations into speech.   LibriSpeech (Panayotov et al., 2015). It is a collection of approximately 1,000 hours of audiobooks   in the LibriVox project (Kearns, 2014).   4.2   Implementation Details   For video features, we follow the previous works   (Gao et al., 2017) to extract C3D (Tran et al.,   2015) features as input. For speech features, we   adopt the raw wave as time-based feature and use   Fourier Transform to obtain 80-dimensional logmel spectrograms as frequence-based feature. In   pre-training, we use the pre-trained Glove (Pennington et al., 2014) embeddings as the word features. The full details are listed in Appendix B.2   4.3   Evaluation Metrics   For a fair comparison, we follow previous temporal video grounding works (Gao et al., 2017) to   employ the R@n,IoU=m as the evaluation metrics,   which is the percentage of at least one of the top-n   moments with the IoU > m. We also report the   mIoU value which is the average IoU between the   top-1 selected moment and the ground truth.   4.4   Performance Comparison   Baseline. Since no existing strategy can be directly   applied to WSVG, we consider baselines under the   cascaded and end-to-end (E2E) setting.   \u2022 Cascaded methods: these methods use textual   input recognized by the ASR model (Baevski   et al., 2020) as input to the grounding network.   We select the following weakly-supervised textbased video grounding methods: WSLLN (Gao   et al., 2019), RTBPN (Zhang et al., 2020c) and   SCN (Lin et al., 2020).   \u2022 E2E methods: these methods use speech as direct input to the grounding network. (1) We consider the supervised approach VSLNet (Zhang   et al., 2020a) and VGCL (Xia et al., 2022) for   reference. (2) We denote the backbone of SIL as   Base and combine it with other pre-training techniques, including Wav2vec2.0 model (Baevski   et al., 2020) that performs self-supervised training, VILT (Kim et al., 2021) that follows multimodal pre-training, and LUT (Dong et al., 2021).   For these pre-training techniques, we adopt the   same 960h LibriSpeech data. (3) Besides, we   combine our semantic task Lsem with the above   text-based grounding backbones for a better comparison. We keep almost all architecture as same   as the original backbones but replace the text   encoder with the speech encoder.   Main Results. Table 1 reports the performance   evaluation results. We also fine-tune the speech encoder on the grounding data and report the results   10919   Table 1: Performance Evaluation on the ActivityNet Speech. The best results are bold. SP: Speech. TXT: Text.   Setting   Backbone   Pre-Training & Data   R@1,IoU=m   R@5,IoU=m   mIoU(finetune)   Method   SP   TXT   0.1   0.3   0.5   0.7   0.1   0.3   0.5   0.7   Supervised Approach   E2E   VSLNet   \u00d7   \u00d7   76.42   49.64   31.98   17.26   35.92   VGCL   \u00d7   \u00d7   75.26   51.80   32.36   18.10   36.83   Weakly-supervised Approach   Cascaded   ASR-WSLLN   \u00d7   \u00d7   74.47   41.76   22.62   11.03   31.42(31.68)   ASR-SCN   \u00d7   \u00d7   71.35   46.64   28.09   13.26   89.55   71.32   55.74   31.14   32.02(32.49)   ASR-RTBPN   \u00d7   \u00d7   74.62   47.75   28.80   13.44   92.31   77.52   61.88   32.37   32.88(33.26)   E2E   Base   \u00d7   \u00d7   65.62   41.99   24.20   11.02   87.28   72.37   55.83   28.08   28.44   Base   Wav2vec2.0   \u2713   \u00d7   65.65   44.61   26.20   10.83   90.01   74.21   58.37   28.89   30.22(30.36)   Base   VILT   \u2713   \u2713   69.71   45.69   26.68   12.43   92.51   76.80   59.56   30.91   31.06(32.13)   Base   LUT   \u2713   \u2713   72.44   46.24   26.97   12.82   92.08   75.64   60.38   31.12   31.34(32.48)   WSLLN   Lsem   \u2713   \u2713   75.11   41.39   22.07   10.96   31.28(31.54)   RTBPN   Lsem   \u2713   \u2713   72.86   46.76   27.89   13.24   92.49   76.80   61.04   32.45   32.10(33.01)   E2E(Ours)   Base   Lsem   \u2713   \u2713   73.11   46.39   27.07   13.06   92.10   76.12   60.24   31.24   31.88   Base   LASP   \u2713   \u2713   74.88   48.14   28.68   13.95   93.44   79.32   61.52   32.17   33.04   Base+LAVCL   LASP   \u2713   \u2713   71.79   49.46   30.26   15.22   94.87   82.28   63.73   35.48   34.02(34.52)   Table 2: The Comparison of Average Inference Latency   and Grounding Performance. The batch size is set to 1.   Method   mIoU   Latency   Speedup   ASR-Base   32.13   0.090s   1.00x   ASR-RTBPN   32.88   0.094s   0.95x   SIL(Ours)   34.02   0.044s   2.04x   in the last column. From the results, we can observe several interesting points: (1) The cascaded   baselines generally perform better than E2E baselines. This phenomenon is reasonable since the   ASR model has a high recognition accuracy (see   Appendix) and weakly-supervised text-based video   grounding methods are mature, validating the difficulty of direct modeling the video-speech interaction. (2) In E2E baselines, our semantic task   (Lsem) outperforms other pre-training approaches,   indicating its effectiveness for semantic modeling. However, existing text-based backbones are   still inferior to their cascaded version, indicating   that speech encoding and video-speech interaction are still insufficient. (3) Our full framework   SIL (Base+LAVCL+LASP) surpasses all weaklysupervised baselines on almost all criteria, verifying its effectiveness. (4) We also observe an interesting point that the stronger weakly-supervised   model tends to obtain a higher value at R@1,   IoU=0.7/0.5 while a lower value at R@1, IoU=0.1.   This is because inaccurate models tend to give a   proposal prediction with a larger temporal range,   which usually covers the ground truth segment but   with a low IoU score.   Figure 4: Robustness Analysis.   Grounding Efficiency. We measure the grounding   efficiency in the average time required to ground   one spoken query in the video. Note the computation cost of data pre-processing is excluded. We   select ASR-Base/ASR-RTBPN as two typical cascaded methods for comparison and list the result in   Table 2. It demonstrates the superiority of SIL in   both performance and efficiency.   Robustness analysis. To showcase the robustness   of our model in real-life scenes, we add augmentation at different levels (low, medium, high), including time stretching, pitch shifting and noise   addition. The full configuration is introduced in   Appendix B.2. We compare our SIL with ASRRTBPN and two ablation methods that only uses   frequency- or time-based features without training of robustness task. As illustrated in Figure 4,   our SIL shows stable performance, validating the   power of our robustness task.   4.5   Ablation Study   Overall Ablation Study. We conduct the overall ablation study and show the result in Table 3. We observe our semantic task (Lsem) im10920   Table 3: Ablation Results on the ActivityNet Speech.   ASP Module   AVCL Module   R@1, IoU=m   R@5, IoU=m   mIoU   Lrob   Lconc   Lsem   LAVCL   0.1   0.3   0.5   0.7   0.1   0.3   0.5   0.7   67.94   41.48   22.99   10.07   88.10   72.12   56.24   29.24   28.52   \u2713   73.11   46.39   27.07   13.06   92.10   76.12   60.24   31.24   31.88   \u2713   \u2713   74.36   47.64   28.39   13.81   93.12   78.96   61.13   32.03   32.93   \u2713   \u2713   72.69   47.61   29.12   14.40   92.76   79.07   62.36   33.64   32.96   \u2713   \u2713   \u2713   71.13   49.37   30.02   15.19   94.56   81.32   63.38   35.27   33.91   \u2713   \u2713   \u2713   \u2713   71.79   49.46   30.26   15.22   94.87   82.28   63.73   35.48   34.02   Table 4: Evaluation Results of Different Amount of   Data for Acoustic-semantic Pre-training.   Data   R@1, IoU=m   mIoU   0.3   0.5   0.7   10%   46.71   28.22   13.33   31.71   50%   49.28   29.42   14.46   33.87   100%   49.46   30.26   15.22   34.02   Table 5: Effect of Conciseness Task.   Fusion   Pre-Training   R@1, IoU=m   mIoU   0.5   0.7   Pool+Add   w/o. Lconc   27.49   13.42   32.36   Pool+Add   w. Lconc   27.24   13.38   32.28   Attention   w/o. Lconc   27.07   13.06   31.88   Attention   w. Lconc   28.68   13.95   33.04   proves performance significantly, e.g.   4.08 on   R@1,IoU=0.5 and 2.99 on R@1,IoU=0.7, since   it directly learns knowledge from word embedding.   Also, the conciseness task (Lconc) and acousticvisual contrastive learning (LAVCL) further bring   gains. Though the robustness task (Lconc) has no   significant effect, we have demonstrated its contribution in the robustness analysis.   4.5.1   Analysis of ASP   Effectiveness under Low-resource Setting. As   shown in Table 4, under a low-resource setting,   our acoustic-semantic pre-training still achieves   comparable performance with limited data (50%),   showing excellent generalization capability.   Effect of Conciseness Task.   To investigate   whether the conciseness task can promote crossmodal interaction, we combine it with two different fusion methods: attention-based fusion (Zhang   et al., 2019) and simple fusion that includes pooling and addition. We remove the AVCL module to   better reflect the impact and list the results in Table 5. It is observed that directly applying attention   leads to inferior results due to insufficient interac(a) Performance   (b) Grouding Loss   Figure 5: Effect of Semantic Task.   Table 6: Extension of AVCL on Text-based Grounding.   Method   R@1, IoU=m   R@5, IoU=m   0.3   0.5   0.3   0.5   RTBPN   49.77   29.63   79.89   60.56   RTBPN+AVCL   50.87   30.96   84.27   64.68   tion. However, our conciseness task successfully   leverages the potential of the attention mechanism,   improving interaction significantly for grounding.   Effect of Semantic Task. We compare three objectives in our semantic learning task with the   method (Dong et al., 2021), which also includes   two similar semantic objectives: the seq-level loss   Ldis-s and the word-level loss Ldis-w. The difference is that they mainly focus on the distance loss   (e.g. MSE) to minimize the semantic gap. The result in Figure 5 (a) reveals that our sequence-level   objective Lseq can outperform Ldis-s +Ldis-w due to   the effectiveness of contrastive learning. And our   word-level objective Lword can also be seamlessly   combined to improve performance by a large margin. Besides, we draw the curve of grounding loss   Lbase during grounding training in Figure 5 (b) to   reflect the video-speech alignment. We find each   loss in semantic task speeds up convergence for   weakly-supervised grounding training.   4.5.2   Analysis of AVCL   Integration with Text-based Video Grounding.   AVCL is also a common module that can be integrated into the weakly-supervised text-based video   10921   Figure 6: Visualization of Grounding Results.   grounding by replacing the speech features with   the word features. As shown in Table 6, our AVCL   module further improves the performance on the   strong basis of the RTBPN method, validating its   versatility and effectiveness.   4.6   Qualitative Analysis   Visualization of Grounding Results. Figure 6 depicts two grounding examples from the ActivityNet   Speech, where Base+SIL localizes the temporal   moments covering the most salient part. Compared   to two methods Base+VILT and ASR-RTBPN, our   SIL can localize more precise moments and achieve   better performance, validating its effectiveness.   Visualization of Speech Representations.   To   qualitatively verify our acoustic-semantic pretraining strategy, we use the pre-trained encoder   to extract the features of speech in the ActivityNet   Speech and visualize them using t-SNE in Figure 7   (a). We show three point pairs staying close in   the feature space. The two red points on the left   both describe the \"jump\" action, and the two yellow points on the top have similar \"gun\" and \"hide\"   meanings. Note each pair contains a few similar   words, indicating the close distance is determined   by semantic rather than acoustic information. Also,   we perform clustering with respect to four specific   words in Figure 7 (b). We observe there is a clear   boundary and symmetric relationship between the   four clusters. The above result demonstrates the   effectiveness of our pre-training strategy.   Visualization of Video-Speech Attention. We visualize the video-speech attention between the target frame and segments in Figure 8 using a thermodynamic diagram, where the darker color means a   higher correlation and the temporal correspondence   between the transcript and speech is also shown.   Figure 7: Visualization of Speech Representations.   Figure 8: Visualization of Video-Speech Attention.   From the result, we observe that the frame can attend to the segments temporally corresponding to   keywords, e.g. \"lay\", \"crunches\", and ignore other   irrelevant ones, e.g. \"the\", \"on\". This fact suggests   that our conciseness task can detect the word-level   segments and boost the cross-modal interaction.   5   Conclusion   In this paper, we propose a new task named WeaklySupervised Spoken Video Grounding and present a   novel framework SIL. Concretely, we conduct an   acoustic-semantic pre-training to achieve effective   and robust semantic encoding. Besides, we develop   an acoustic-visual contrastive learning to optimize   representations for cross-modal interaction. The   extensive experiments demonstrate the superiority   of our proposed method.   6   Limitations   In this section, we make a clear discussion of the   limitation of our work. Our work mainly leverages   a pre-training scheme to enhance the encoding of   speech for video grounding. However, the adopted   audio data (i.e. Libri Speech) for pre-training are   different from the one in the grounding dataset (i.e.   ActivityNet Speech). This could lead to performance degradation due to the domain gap. The findings could inspire the researchers to explore a better   pre-training strategy to learn domain-invariant and   effective speech representations for grounding.   7   Ethics Statement   We adopt the widely-used datasets that were produced by previous researchers. We follow all relevant legal and ethical guidelines for their acquisi10922   tion and use. Besides, we recognize the potential   influence of our technique, such as its application in   human-computer interaction and vision-language   grounding systems. We are committed to conducting our research ethically and ensuring that our   research is beneficial. We hope our work can inspire more investigations for spoken video grounding and wish our framework can serve as a solid   baseline for further research.   Acknowledgments   This work was supported in part by the National Key R&D Program of China under Grant   No.2022ZD0162000, National Natural Science   Foundation of China under Grant No.62222211,   Grant No.61836002 and Grant No.62072397, and   Yiwise.", "conf": "ACL", "year": "2023", "index": 424}, {"title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics   Volume 1: Long Papers, pages 2736 - 2749   May 22-27, 2022 c\u20dd2022 Association for Computational Linguistics   A Meta-framework for Spatiotemporal Quantity Extraction from Text   Qiang Ning\u2217   Amazon   qning@amazon.com   Ben Zhou   University of Pennsylvania   xyzhou@seas.upenn.edu   Hao Wu   Hooray Data   haowu@hooray.ai   Haoruo Peng   Newsbreak   haoruo.peng@newsbreak.com   Chuchu Fan   MIT   chuchu@mit.edu   Matt Gardner\u2217   Microsoft Semantic Machines   mattgardner@microsoft.com", "abstract": "News events are often associated with quantities (e.g., the number of COVID-19 patients   or the number of arrests in a protest), and it   is often important to extract their type, time,   and location from unstructured text in order   to analyze these quantity events.   This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes   the first meta-framework for solving it. This   meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline   models. We demonstrate the meta-framework   in three domains\u2014the COVID-19 pandemic,   Black Lives Matter protests, and 2020 California wildfires\u2014to show that the formalism   is general and extensible, the crowdsourcing   pipeline facilitates fast and high-quality data   annotation, and the baseline system can handle   spatiotemporal quantity extraction well enough   to be practically useful.   We release all resources for future research on this topic.1   1", "content": "Events are often associated with quantities \u2013 how   many COVID-19 patients are on ventilators, how   many people are injured during protests, or how   large is the extent of a wildfire. We often need   to figure out the type of an event, and where and   when it happened for these quantities for coherent discussion of public policy on sociopolitical   events in rapidly evolving situations: \u201c19 deaths\u201d   is different from \u201c19 recoveries;\u201d \u201c19 deaths in a   small city yesterday\u201d apparently describes a more   severe situation than \u201c19 deaths in the whole country last month.\u201d However, until dedicated channels   are established, these quantities are typically first   reported on social media and local news articles,   which then have to slowly make their way to some   \u2217Work started while at the Allen Institute for AI   1https://github.com/steqe   DCT: Thursday, 08/27/2020   Title: Study Sessions, Dinners: 104 New USC Student    Coronavirus Cases    Text: LOS ANGELES , CA -- The number of coronavirus    cases confirmed among USC students continued rising    Thursday, with the university announcing [104] new cases    over the past four days\u2026   Recognition: 104   Type: Confirmed cases   Spatial Grounding: US \u00e0 California \u00e0 Los Angeles \u00e0 USC   Temporal Grounding: [08/23/2020, 08/26/2020]   DCT: Monday, 06/01/2020   Title: Black Lives Matter: 16 Organizations That Are Bailing    Out Protestors   Text: \u2026Police officers have arrested [thousands] of    demonstrators\u2026   Recognition: thousands   Type: Arrests   Spatial Grounding: US   Temporal Grounding: Overall quantity ending on    06/01/2020   Figure 1: Given document creation time (DCT), title,   and text, the STEQE problem is to do quantity recognition, typing, spatial grounding, and temporal grounding   according to the proposed formalism (Sec. 2). Above   are two examples from our COVID-19 dataset and BLM   protest dataset.   aggregate location for decision-makers to use. This   calls for a general framework to extract and analyze   quantities associated with events, so that we can   automatically summarize quantitative information   from news streams, rapidly respond to emergencies, investigate incidents, and potentially combat   misinformation through comparisons with trusted   sources.   Prior work on events focused on extracting event   mentions, attributes, and relationships (ACE, 2005;   Chen and Ji, 2009; Do et al., 2011; UzZaman et al.,   2013; Glava\u0161 et al., 2014; Zhou et al., 2019; Chen   et al., 2021), and paid little attention to quantities   associated with those events, which presents an   opportunity to perform targeted information extraction on these quantity events.   This paper studies spatiotemporal quantity   extraction (STEQE): finding quantities of certain   2736   types and extracting their associated times and locations. We develop a general meta-framework to   help researchers overcome challenges and extend   to new domains easily. Specifically, the contributions of this meta-framework are:   Task Formulation   We draw on ideas from existing NLP tasks to create the first formalism that defines STEQE as four information extraction tasks:   quantity recognition, typing, spatial grounding, and   temporal grounding. While each of these has analogues in the literature, our combination of them   into a complete picture of quantity events is novel.   Annotation Collection   We release a shareable and extensible crowdsourcing pipeline on   CROWDAQ (Ning et al., 2020a) that facilitates fast   and reliable data annotation. We show how this   pipeline facilitates fast and high-quality annotations for three sociopolitical events: the COVID-19   pandemic, Black Lives Matter (BLM) protests, and   2020 California wildfires. These practical STEQE   datasets are also released to foster future research.   Modeling   We propose a T5 baseline model for   its flexibility across tasks and easy domain transfer. This model shows that, while the end-to-end   STEQE problem remains challenging in all domains, temporal grounding is typically the most   difficult task, pointing out a research focus next.   2   STEQE   The STEQE problem aims to extract information   about quantity events in text, consisting of four   parts: determining which numerical expressions   actually correspond to events (\u00a72.1), the type of the   event that a quantity is referring to (\u00a72.2), where   that event happened (\u00a72.3), and the temporal extent   to which the quantity refers (\u00a72.4).   Note that for each of these subparts, there could   have been other definition and formulation choices.   We describe our formalism\u2019s design choices, and   discuss why they would lead to better-defined learning problems and more reliable data collection,   along with their limitations and how to extend our   formalism for more specialised applications.   2.1   Quantity Recognition   Similar to named entity recognition (NER) (Tjong   Kim Sang and De Meulder, 2003), quantity recognition is defined as a text span detection problem.   We discuss two questions regarding the definition   of quantities: (1) how to distinguish between quantities and non-quantities; (2) how to define the span   for quantities to avoid misalignment.   First, quantities are a special type of numbers that are associated with events, either in   digits (e.g., \u201c123\u201d) or in words (e.g., \u201cone hundred   twenty three\u201d). Some non-quantity examples are:   1. Date and time: \u201cMay 8, 2020\u201d and \u201c5:30 pm\u201d   2. Duration: \u201c3 months\u201d and \u201c60 years old\u201d   3. Part of an entity name: \u201cCOVID-19\u201d, \u201cPorsche   911\u201d, and \u201c502 Main Street\u201d   Article words, \u201ca\u201d and \u201can\u201d, require more attention. When we say \u201ca man died,\u201d the \u201ca\u201d does   mean \u201c1\u201d death, while in \u201ca large number of people died,\u201d the \u201ca\u201d itself does not have the meaning   of \u201c1,\u201d and we thus do not consider it a quantity.   Ordinal numbers can also indicate events, but   their spatiotemporal extent can be understood differently: \u201cthe fifth case in Seattle\u201d implies that   there had been 5 cases, and the spatiotemporal extent of \u201cfifth\u201d can be that of the fifth case only, or   all of the five cases. Ordinal-number events are   rare in our study, so comparing to the extra annotation requirement, we decide to consider ordinal   numbers as non-quantities, although the definition   is easily extensible to cover them in the future.   Second, we need to define the boundaries of   these quantity spans. For instance, in \u201cfive cases   in Seattle,\u201d should one label the text span of \u201cfive\u201d   or \u201cfive cases\u201d? What about \u201c4.8 billion\u201d and   \u201c$4.8 billion\u201d? Similar to labeling an event using   its predicate only, our choice is to keep the span   minimal while keeping the numerical semantics:   we will mark \u201cfive\u201d (i.e., drop \u201ccase\u201d), \u201c4.8 billion\u201d (i.e., keep \u201cbillion\u201d), and \u201c4.8 billion\u201d (i.e.,   drop \u201c$\u201d) in these examples. Minimising the span   does not lose information about the quantity\u2014only   marking \u201cfive\u201d in \u201cfive cases\u201d does not prevent us   from identifying its type, unit, and spatiotemporal   extent in subsequent annotation tasks. Below are   some tricky cases, and quantities are in brackets.   1. Rate: \u201c[20 percent] of the tenants were infected\u201d, \u201cthe positive rate is now [200] per   [100,000]\u201d, \u201c[1000] tests per day\u201d   2. Approximation: \u201c[4 or 5] are missing\u201d   3. Range: \u201cthe positive rate is [2 to 3 percent] / at   least [2%] / at most [3%]\u201d   2.2   Quantity Typing   Again, similar to NER, recognized quantities can   have an associated type from a predefined set of   2737   classes.2 A clear event type is important for subsequent spatiotemporal grounding, but some quantities can have multiple types, and some can have   multiple interpretations for their spatiotemporal extent. This work thus makes two design choices to   mitigate these issues.   Enforce single-typing   In this work, we allow   quantities to have only one single type. This ensures annotation quality since multiple types for   a single quantity may complicate the spatiotemporal extent. For instance, in \u201c[three] men were   hospitalized 5 days after being tested positive,\u201d the   time span of hospitalization and that of tested positive are different. We enforce single-typing by   providing an order of importance. For instance,   hospitalization is more important than tested positive, so the spatiotemporal extent of \u201cthree\u201d will   be that of hospitalizations.   Ignore rate and money quantities   Rate and   money quantities are excluded in all of our typing labels, because their spatiotemporal extent can   be interpreted in different ways. For instance, the   spatiotemporal extent of \u201ca bill of $4.8 billion\u201d can   be interpreted either as when and where this bill   was passed, or as when and where the bill will be   used; similarly, to define the time span of the rate   quantity \u201c[20%] of the tenants were infected\u201d, we   can either use the time span from the very first   case to the last case that brought the infection rate   from 0% to 20%, or use the time span when the   infection rate was holding at 20%. For applications where one needs to spatiotemporally ground   rate and money quantities, one could extend our   instructions to clarify the ambiguities above.   2.3   Spatial Grounding   The spatial grounding problem of STEQE is to   ground real-world events to a locale (see Fig. 7 in   Appendix), avoiding complications in applications   like human-robot interactions (e.g., \u201cturn left and   go to the kitchen, and then pick up the fruit on the   table\u201d). Thus we do not need to handle the nuances   of relative spatial relationships like \u201cthe kitchen   is on our left\u201d and \u201cthe table is in the kitchen.\u201d   We describe our formalism in terms of the format,   granularity, and multi-location handling.   2The set of types in a STEQE problem will be domainspecific. We will explain the label set for typing for each of   the 3 domains studied in this work later in \u00a73.2.   Title: Six COVID-19 cases emerge in South Portland   Text: SOUTH PORTLAND, Maine -- A facility for people with    cognitive disabilities reports having [six] COVID-19 cases\u2026   Spatial grounding for [six]: US \u00e0 Maine \u00e0 South Portland \u00e0   A facility for people with cognitive disabilities    Figure 2: The desired spatial grounding annotation is   the most specific location mentioned in the text that   contains all individual cases of a quantity event.   Format   An important decision for spatial grounding is the format: we can use natural language to   describe the locale, select text spans from the original text, or select from a map directory. In this   work, we use a combination of all three for spatial grounding to balance between flexibility and   consistency: we choose from a predefined set of   questions to determine the country (U.S. vs nonU.S.) and state, use free text for the name of the   city, and span selection for more granular locale   information (e.g., \u201ca pork plant\u201d). We leave it for   future work if one wants to extend to other countries, or if one can provide a detailed map directory.   Granularity   We define spatial grounding annotation to be the most specific location mentioned in   the text that contains all individual cases of a quantity event. For instance, in Fig. 2, the title mentions   6 cases in \u201cSouth Portland,\u201d but later we will see   that the 6 cases are all from \u201ca facility for people   with cognitive disabilities.\u201d The annotation should   specify that facility instead of stopping at \u201cSouth   Portland.\u201d This design choice requires annotators   to check the context in addition to the sentence   containing the quantity, and is important for downstream tasks because it is likely that there are cases   in South Portland but not in that facility.   Multi-location   We handle events in multiple locations by broadening the granularity of the spatial   location, as mentioned above. However, there are   cases where the same quantity is explicitly mentioned with two or more separate locations:   1. \u201cBoth Seattle and Tacoma had [10] new cases.\u201d   2. \u201cSeattle and Tacoma together had more than   [10] new cases.\u201d   The \u201c10\u201d in both sentences above are associated   with two cities, Seattle and Tacoma. The semantics   are also different: being shared by two locales, or   the events from both locales combine to make this   quantity. In our pilot studies, we tried to consider   2738   these details in multi-location quantities, but found   that they were very rare and crowd workers could   not capture them reliably. We thus decide to ignore these cases in this work and only allow crowd   workers to select a single location.   2.4   Temporal Grounding   The temporal grounding problem of STEQE is to   ground each real-world quantity event to a single   time span, which reduces the complexities in temporal semantics often encountered in prior datasets   (Pustejovsky et al., 2003; Cassidy et al., 2014;   O\u2019Gorman et al., 2016; Ning et al., 2018a, 2020b)   and improves practicality.   Format   A time span consists of two time points,   and the key is the format for time points. In this   work, we allow a time point to be UNKNOWN if   the text is unclear. For a specific time point, there   are two general ways to describe it: (1) use absolute date and time (e.g., \u201cFeb 1st, 2021\u201d); (2) use   relative time \u2206based on a reference time point T   (e.g., \u201c3 days before lockdown\u201d).   We have chosen the first format in this study,   and when a time point is unclear based on the   text, we allow annotators to simply select \u201cUnknown\u201d. The second method above is strictly more   expressive, but also comes with many degrees of   freedom: the reference point T can be either an absolute date and time Ttime or another event Tevent   (e.g., \u201clockdown\u201d), and the relative time difference   \u2206can be either a specific duration \u2206spec like \u201c3   days before/after\u201d or a rough description \u2206rough   like \u201ca few days before/after.\u201d In our pilot studies allowing for Ttime + \u2206rough, Tevent + \u2206spec,   or Tevent + \u2206rough, we found the T + \u2206method   too flexible to achieve annotation agreement; in   the meantime, using absolute date and time could   reliably estimate those time spans in practice. This   is why we recommend the first format above.   Granularity   Given the nature of news events, it   is often enough to be specific up to days. We define   the time span of a quantity to be from the day of   first event to the day of the last,3 but this exact time   span may not always exist in the text, so STEQE   uses the best over-estimate of this gold time span   based on information in the text (see Table 3).   3If these events are durative, then accordingly, the time   span should change to the day when the first event started to   the day when the last event ended, although we did not find   it necessary to point this out in our data collection guidelines   for crowd workers.   Time   First event   Last event   Time span   Best estimate based on text   Figure 3: We define the time span of a quantity to start   from the first event and end at the last; the desired temporal grounding annotation is the tightest estimate based   on the text that covers all 6 events.   This work also addresses common ambiguities.   (1) Some time expressions are not critical and thus   less specific in text, e.g., \u201cMarch 2020,\u201d for which   we will simply use the entire span of that range,   e.g., [03/01/2020, 03/31/2020]. (2) For time expressions like \u201cmid September\u201d and \u201cend of 2020\u201d,   we choose the closest dates, e.g., \u201c09/15\u201d and   \u201c12/31/2020\u201d. (3) Depending on the actual publication date and the content of an article, there   can be different interpretations for \u201ctoday,\u201d thus   leading to a one-day disagreement among people   regarding time expressions like \u201cyesterday\u201d or \u201cin   the last three days.\u201d We allow our annotators to   use their best judgment in these cases.   Multi-span   Similar to spatial grounding, we handle events in multiple time spans by broadening the   granularity of the time span, as mentioned above,   and as with spatial grounding, we do not label multiple time spans separately in rare cases like \u201c10   arrests on Monday and Wednesday.\u201d   Overall quantity   A special type of temporal grounding phenomenon is overall quantities.   Strictly speaking, this notion exists for spatial   grounding as well (e.g., the overall COVID-19 case   number around the world or the U.S.). While humans easily agree on the spatial extent of these   overall quantities, their time spans are often ambiguous, especially the start time. For instance, in   \u201cthere have been [3 million] cases so far,\u201d the start   time is supposed to be \u201cthe beginning of the pandemic,\u201d but people do not always agree on when   that was. The disagreement comes from (1) the pandemic started at different times in different regions   of the world; (2) one may argue that the pandemic   started either since the first confirmed case, or since   the lockdown. This debate over start-time is not an   NLP problem, so instead of inventing a new mechanism to resolve this, we simply allow \u201coverall\u201d as   a label for the start time of a quantity.   2739   Domain, DCT   Quantity   Type   Spatial Grd.   Temporal Grd.   COVID-19   Sat, 2020-08-15   Tennessee has conducted 1,757,690 tests   with 1,631,297 negative results.   Tested negative   Tennessee   Overall number at DCT   Wildfires   Tue, 2020-09-22   The blaze had more than doubled in size   over the past week to 170 square miles (440   square kilometers), ... from Los Angeles.   Measurements   Los   Angeles,   California   2020-09-15 to   2020-09-22   BLM Protests   Tue, 2020-06-16   Black Lives Matter demonstrators in a tiny   Ohio town...Sunday. The small demonstration has about 80 people, organized by local   Bethel residents.   Participants   Bethel, Ohio   2020-06-14 to   2020-06-14   Table 1: Example annotations with quantity span highlighted. Texts are truncated.   3   Data Annotation   We have walked through the definition of the tasks   in our STEQE framework, with discussions on various design choices. Next we explain how to collect   annotations via this framework in practice. Table 1 shows some example annotations from our   datasets.   3.1   Input Document Filtering   We worked with NewsBreak Inc., a local news aggregation company, to obtain raw newswire texts   from publicly available news outlets.4 We then   made use of NewsBreak\u2019s internal tools to determine the topic of these news articles, i.e., whether   an article is about COVID-19, Black Lives Matter   protests in 2020, or the 2020 California wildfires.   The data also comes with meta information including each article\u2019s source domain and publication   time. Altogether, we obtain 1M articles on COVID19 between 01/01/2020 and 12/31/2020, 100k on   protests from 05/22/2020 to 12/31/2020, and 90k   on California fires from 08/01/2020 to 12/31/2020   as source articles.   3.2   Domain-specific Typing   Following the general guidelines in \u00a72.2, we used   the following domain-specific types in this study.   1. COVID-19 pandemic: deaths caused by COVID19, deaths likely caused by COVID-19, recoveries, confirmed cases, tests, tested negative, hospitalizations, patients on ventilators, and in ICUs.   2. BLM protests: protests, participants, order maintainers, arrests, deaths, injuries, and shootings.   3. California fires: fires, physical measurements,   people impacted, items impacted, and resources.   4https://www.newsbreak.com/   These domain-specific types can be very specific   (see those for the COVID-19 pandemic) or generic   (see those for California fires), which demonstrates   the flexibility of our framework.   3.3   Shareable CROWDAQ Pipeline   CROWDAQ (Ning et al., 2020a) is an open-source   platform that standardizes data annotation pipelines   and provides a customizable annotation interface,   automated annotator qualification exams, progress   monitoring, and annotation agreement monitoring.5 CROWDAQ pipelines have four components:   instruction, tutorial, exam, and main task: an annotator will read the instruction and tutorial, and then   work on a set of multiple-choice exam questions.   CROWDAQ automatically checks their scores and   assigns qualifications. Qualified annotators will   then be able to work on the main task. For each of   the four tasks defined in Sec. 2, we have designed   CROWDAQ pipelines that are general enough to be   used for annotating in all domains.6 We release the   CROWDAQ pipelines for public use.7   3.4   Data statistics   We first show statistics of our qualification exams   in Table 2. We can see quantity recognition expectedly has the fewest hard questions and highest   passing rate, and spatial and temporal grounding   have more hard questions. Note that typing for   California fires seems harder than typing for the   other two domains, likely due to our choice of more   generic types for California wildfires.   We then launched main annotation tasks on Amazon Mechanical Turk (MTurk) that were available   5http://www.crowdaq.com/   6The only change for a new domain is instructions and   exams for quantity typing, which have to be domain-specific.   7Please   see   the   description   at   https://dev2.   crowdaq.com/w/instruction/steqe/readme.   2740   Qual ID   Qual Name   Hard (%)   Passed (%)   Q   Recognition   18   94   SG   Sp. Grd.   47   62   T G   Temp. Grd.   50   57   T -C   Typing (COVID)   27   60   T -B   Typing (BLM)   36   60   T -F   Typing (Fire)   50   53   Table 2: The difficulty of the qualification exams in   this work. Hard: exam questions where less than 70%   attempts were correct. Passed: the ratio of passed in all   attempts. See Table 5 in the appendix for more details.   only to qualified workers. We also required 3 different workers for each single annotation job and   used majority voting to aggregate multiple workers\u2019 annotations. Since quantity recognition is a   relatively easy task and our quantity recognition   system based on BERT (Devlin et al., 2019) for the   COVID domain was reliable enough to be applied   to other domains, we did not further collect quantity   recognition data. Table 3 and Table 6 (Appendix)   show more statistics of these datasets.   Task   QID   #W   #Q   WAWA   Expert   Recog.   - COVID   Q   58   2.6k   92%   98%   Typing   - COVID   Q, T -C   52   1.5k   95%   100%   - BLM   Q, T -B   74   4k   87%   94%   - Fire   Q, T -F   68   2k   91%   96%   Sp. Grd.   - COVID   T -C, SG   91   3.4k   91%   98%   - BLM   T -B, SG   50   1.5k   80%   96%   - Fire   T -F, SG   63   2k   92%   90%   Temp. Grd.   - COVID   T -C, T G   132   4.3k   86%   100%   - BLM   T -B, T G   57   1.6k   77%   96%   - Fire   T -F, T G   63   1.6k   82%   96%   Table 3: The required qualifications (QID), numbers of   actual annotators (#W) and annotated quantities (#Q),   worker agreement with aggregate (WAWA), and expert   evaluation on 50 random samples after worker aggregation. The WAWA metric is for the \u201cstate\u201d choice in   spatial grounding, and the \u201coverall number\u201d judgment   in temporal grounding (reported by CROWDAQ directly).   The expert evaluation scores are all accuracy, except for   F1 for quantity recognition.   Note that we did not enforce full annotation for   all quantities (i.e., one quantity may only receive   typing annotations, and another may only receive   spatial annotations) to cover more documents (Ning   et al., 2019a). Within those reported in Table 3, 500   quantities in each domain are fully labeled with   both typing and spatiotemporal extent, and we use   these as our test sets.   We paid $0.05 for each job in quantity recognition, and $0.15 for those in typing, spatial grounding, and temporal grounding; in the COVID-19   data collection, the average hourly pay of the top   5 annotation contributors was $25 (typing), $13   (spatial grounding), and $12 (temporal grounding).   In total, the cost of 3 datasets was $11k (including   20% overhead paid to MTurk).   We developed our CROWDAQ pipeline for   COVID-19 and applied it on other domains. When   we received news articles in BLM protests and California wildfires from NewsBreak Inc., it only took   us about 2 weeks to obtain the annotations used   in this work, including designing domain-specific   typing instructions and exams, launching tasks to   MTurk, and waiting for crowd workers to finish.   This fast and reliable data collection is appealing   for responding to emerging events in the future.   4   Model   Quantity recognition is a typical span selection   problem and we use the standard token classification model based on BERT (large, cased) (Devlin   et al., 2019) that comes with HuggingFace (Wolf   et al., 2020). For typing, spatial, and temporal   grounding, we use the T5-large language model   (Raffel et al., 2020) for its flexibility across tasks   and easy domain transfer. We format data from   each task to fit into T5\u2019s sequence to sequence (seqto-seq) nature. Specifically, for each quantity, the   input sequence to T5 is the string of the previous   3 sentences, the current sentence with a special   marker token right before the quantity span, the   next 3 sentences, the title, and document creation   time (DCT). For typing, the output sequence is a   single token representing each label mapped from   a reserved vocabulary. For spatial grounding, the   output sequence is the location names from the   highest hierarchy to the lowest ended by an end-ofsentence (EOS) marker. For temporal grounding,   the output sequence is the start time followed by   the end time. Both times are either \u201cunknown\u201d or   a date string in ISO 8601 format (e.g., \u201c2021-0115\u201d). We view the start time of an overall quantity   as \u201cunknown\u201d. To get complete date predictions,   we enforce the decoding length to be at least 12   and use a date parser to find \u201cunknowns\u201d or dates.   2741   System   Task   Typing   Spatial Grounding   Temporal Grounding   End-to-end   Acc   EM-city   EM-state   Binary   S-N   E-N   EM-city, Binary   COVID   44   68   84   68   0   24   3   Naive   BLM   38   74   82   32   0   32   0   Fire   27   58   92   86   0   31   20   COVID   89   81   90   74   53   52   56   T5 (in-domain)   BLM   89   77   89   57   49   43   41   Fire   87   70   94   83   1   32   55   COVID   89   81   91   74   54   57   55   T5 (all domains)   BLM   89   80   91   65   62   57   48   Fire   87   71   94   76   46   61   52   Table 4: System performances on typing, spatial grounding, and temporal grounding (averaged from 3 different runs).   EM-city/-state: exact match scores up to the city-/state-level. Binary: judging if a quantity is an overall-quantity   ending on DCT. S-N/E-N: EM scores when the start/end time is non-trivial. End-to-end: quantities receiving   correct predictions on all steps based on \u201cEM-city\u201d (spatial) and \u201cBinary\u201d (temporal). T5 (all domains) uses the   same typing systems trained in-domain, but combine the spatiotemporal grounding data from all domains in training.   Bold values are best results with respect to each domains and metrics.   5   Experiments   In our evaluation of quantity recognition using the   aforementioned BERT model on a random set of   300 sentences (100 from each domain), we find the   precision 99% for all domains, and the recall 95%   (COVID), 87% (BLM), and 87% (Fire). The recall   is slightly lower because of poor performance on   article words (\u201ca\u201d and \u201can\u201d). However, since most   missed quantities are not associated with event   types that we are interested in (e.g., \u201c[a] post office\u201d or \u201c[a] comment\u201d), the adjusted recall is 98%   (COVID), 94% (BLM), and 93% (Fire) if we do   not consider those irrelevant quantities.   Table 4 shows system performances on typing,   spatial, and temporal grounding on extracted   quantities. Our test set in each domain consists   of 500 fully annotated quantities. The rest of the   data is split into 80% for training and 20% for development, that we use to acquire the learning rate   (5e-3) and batch size (32). We compare T5 with   a naive method, which always predicts the majority type in each domain for \u201ctyping,\u201d the location   mention closest to the quantity in text for \u201cspatial   grounding,\u201d8 and overall quantity ending on DCT   for \u201ctemporal grounding.\u201d For spatial grounding,   we report two exact match (EM) scores, up to the   state-level and city-level, respectively. For temporal grounding, we report the accuracy for judging   whether a quantity is an overall quantity ending on   DCT (\u201cBinary\u201d in Table 4), and two EM scores for   cases where the gold start time is a specific date   8This assumes world knowledge of geo-hierarchies, e.g.,   \u201cL.A.\u201d is in California.   (\u201cS-N\u201d for \u201cStart-Nontrivial\u201d) and where the end   time is not DCT (\u201cE-N\u201d for \u201cEnd-Nontrivial\u201d).   T5 (in-domain)   On quantity typing, T5 improves   by a large margin over the naive baseline in all domains. The naive baseline performs reasonably   well on spatial grounding at the state level (82-92%   EM-state across three domains), but often fails to   provide more granular information at the city level   (58-74% EM-city). This is expected because a city   mentioned close to the quantity does not necessarily mean that the quantity is for the city.9 This phenomenon also varies across domains: BLM protests   were in a few major cities, the EM-city score of the   naive method is thus relatively high (74%), while   for Calfornia wildfires, there were more cities to   choose from, leading to a low EM-city of 58%.   In contrast, T5 can produce more granular information at the city level, and maintain a relatively   stable score across domains (70-81% EM-city). As   for temporal grounding, due to the nature of news   articles, the naive baseline that treats all quantities   as an overall quantity ending on DCT yields reasonably good performances in all domains; but for   quantities with a non-trivial start time or end time,   the naive baseline largely fails.   T5 (all domains)   We also combine the training data for spatiotemporal grounding from all domains and train a single T5 system (but keep T5   in-domain systems for typing), which achieves the   best scores for almost all metrics in Table 4. One   outlier is the Fire domain, where the Binary score   9\u201cThe State Department of Public Health in Springfield   reports a total case of [268].\u201d is a quantity for the state.   2742   for temporal grounding drop, probably due to most   temporal annotations being overall quantities. This   suggests that spatiotemporal phenomena can be   generally transferred across different domains.   Finally, the end-to-end column in Table 4 shows   how many of these quantities have received correct predictions on typing, spatial grounding (based   EM-city), and temporal grounding (based on \u201cBinary\u201d). The reported performance does not count   for quantities that are not recognized, so we view   this as the precision of the system. We see that   the naive baseline has very low performance due   to errors propagated at each step, while with this   framework, T5 is trained to produce significantly   better results. Note that depending on the use case,   one can simply collect more training data, or focus on only a few important event types, to further   improve the end-to-end performance.   6   Related works   Existing NLP works on events have focused on detection (e.g., detecting LIFE and BUSINESS events;   ACE (2005)), common sense (e.g., Rashkin et al.   (2018); Sap et al. (2019); Zhang et al. (2020a)),   and relationships (e.g., coreferential Chen and Ji   (2009), temporal UzZaman et al. (2013), causal   Do et al. (2011), and parent-child relations Glava\u0161   et al. (2014)). There is also a line of recent works   specifically on temporal semantics: time expression extraction and normalization (Laparra et al.,   2018), temporal relation extraction (Ning et al.,   2018a, 2019b, 2020b), temporal common sense   (Zhou et al., 2019, 2020), temoral slot filling (Surdeanu, 2013), and timeline construction (Do et al.,   2012; Ning et al., 2018b; Li et al., 2019). These   tasks may help understanding the temporal aspects   of events in general, but they cannot directly associate temporal values with quantities, and calls   for a dedicated framework such as STEQE. Prior   works on quantities either focus on math calculations (Roy et al., 2015; Roy and Roth, 2018) or   common sense reasoning (e.g., mass distribution   of animals; Elazar et al. (2019)), and not on quantity events and the associated spatiotemporal extent   studied in this work.   Existing works on spatial semantics have focused on natural language navigation (Chen et al.,   2019; Kim et al., 2020), human-machine interaction (Landsiedel et al., 2017; Roman Roman et al.,   2020), dialogue systems (Udagawa et al., 2020),   and clinical analysis (Kordjamshidi et al., 2015;   Datta and Roberts, 2020). Works on geocoding   (Gritta et al., 2018; Kulkarni et al., 2020) map spatial mentions to coordinates, which can be applied   to our work for finer geolocation mapping. Zhang   and Choi (2021) proposes a QA dataset that considers time and location of the question when judging   answer correctness, which may benefit from our   information extraction framework.   A recent work from Zong et al. (2020), which   extracts COVID-19 related events from tweets, is   closely related to our work.   Besides that they   worked on tweets instead of news articles, the key   differences are: (1) instead of span selection used in   Zong et al. (2020), we propose formalisms deeper   into the spatiotemporal extent of quantity events   and capture more nuances in spatiotemporal semantics; (2) we show that our STEQE framework   generally applies to multiple domains and not only   for the COVID-19 pandemic; (3) we release our   entire data collection pipeline on CROWDAQ for   public use and extension.   7   Discussion   As \u00a75 shows, the performance bottleneck of   STEQE is mainly at temporal grounding: with almost perfect quantity recognition and very good   typing and spatial grounding results, temporal   grounding performance is typically much lower   than the other tasks.   While typing and spatial   grounding are ready for practical research into fewand zero-shot settings along the lines of what is   done in entity typing (Zhou et al., 2018; Obeidat et al., 2019; Zhang et al., 2020b), temporal   grounding still requires more investigation even in   in-domain settings.   Why is temporal grounding so challenging?   First, news articles tend to mention many overall quantities ending on publication time, leading   to imbalanced datasets. For instance, 86% in Fire   fall into this category, leaving little training data   for other quantities; in contrast, this number is only   32% in BLM, and the S-N and E-N scores are much   higher in BLM than those in Fire. Second, temporal grounding often requires reasoning, an effect   known to be difficult in many works on temporal   semantics (Ning et al., 2020b; Zhou et al., 2021).   For instance in Fig. 4, to figure out the time span of   \u201c80,\u201d we need to understand that (1) it happened on   \u201cSunday\u201d (2) the \u201cSunday\u201d is a Sunday in the past   instead of in the future, and (3) it is most likely the   most recent Sunday instead of earlier ones.   2743   DCT: Tuesday, 06/16/2020   Text: Black Lives Matter demonstrators in a tiny Ohio    town...Sunday. The small demonstration has about [80]    people, organized by local Bethel residents.   Figure 4: The start time of \u201c80\u201d needs reasoning.   Another direction to improve on STEQE is to aggregate from multiple articles, given that the same   quantity or similar quantities are typically covered   by multiple sources. Cross-document event coreference has many unique difficulties (e.g., see Upadhyay et al. (2016); Bugert et al. (2020)), but knowing the quantity event type, location, and time span   may make it relatively easy to find coreference to   strengthen one\u2019s belief in its prediction, or demote   outliers that are likely wrong predictions.   The proposed STEQE framework may also be   used to detect misinformation and perhaps in social science studies too. For instance, we have   anecdotes where a website mistakenly reported Virginia\u2019s COVID-19 case number on Apr 2, 2020   to be 17k, while the correct number was 1.7k; we   also found signs that news agencies might have   mentioned case numbers in New York city less frequently after a sharp increase, but turned to report   case numbers in New Jersey in April 2020. These   social science analyses are beyond the scope of   this work, but the examples above point to interesting potential uses of these information extraction   systems.   8   Conclusion   Many important news events are associated with   quantities. With practicality in mind, we dive deep   into the semantics of quantity events and propose a   meta-framework for spatiotemporal quantity extraction: we formulate the problem as four information   extraction tasks which lead to quick and reliable   data annotation via crowdsourcing; we also build   a T5 baseline to study the difficulties of the task   and discuss transfer learning opportunities. We   use this meta-framework to build datasets on three   separate sociopolitical events: the COVID-19 pandemic, BLM protests, and California fires. Our   meta-framework is shown to be readily extensible   to different domains of quantity events, an appealing feature for quick response to future events. The   new datasets we collect as examples of this framework can also directly contribute to future studies   on spatiotemporal quantity extraction.", "conf": "ACL", "year": "2022", "index": 279}, {"title": "Open-vocabulary Object Segmentation with Diffusion Models   Ziyi Li\u22171, Qinye Zhou\u22171, Xiaoyun Zhang1, Ya Zhang1,2, Yanfeng Wang\u2020,1,2, and Weidi Xie\u2020,1,2   1Coop. Medianet Innovation Center, Shanghai Jiao Tong University, China   2Shanghai AI Laboratory, China   {599lzy, zhouqinye, xiaoyun.zhang, ya zhang, wangyanfeng, weidi}@sjtu.edu.cn   https://lipurple.github.io/Grounded_Diffusion/   a painting of a phoenix with rainbow beams in the distance   a painting of a unicorn on the snow land   a photograph of a penguin in sakura forest   a photo of a lion on a mountain top at sunset   a painting of a Pikachu on top of a mountain    a photo of a Cyberpunk Transformer next to a river   a photo of a fox playing basketball on the ice   a photograph of a turtle climbing a valcano   aeroplane   bird   horse   person   bicycle   train   dog   cat   pottedplant   tvmonitor   boat   bottle   sheep   car   chair   fox   basketball   Pikachu   turtle    penguin    unicorn   phoenix    lion   Transformer   Figure 1:   Predictions from our guided text-to-image diffusion model. The model is able to simultaneously generate images and   segmentation masks for the corresponding visual objects described in the text prompt, for example, Pikachu, Unicorn, Phoenix, etc.", "abstract": "The goal of this paper is to extract the visual-language   correspondence from a pre-trained text-to-image diffusion   model, in the form of segmentation map, i.e., simultaneously   generating images and segmentation masks for the corresponding visual entities described in the text prompt. We   make the following contributions: (i) we pair the existing   Stable Diffusion model with a novel grounding module, that   can be trained to align the visual and textual embedding   space of the diffusion model with only a small number of   * Both the authors have contributed equally to this project.   \u2020 denotes corresponding author.   object categories; (ii) we establish an automatic pipeline   for constructing a dataset, that consists of {image, segmentation mask, text prompt} triplets, to train the proposed   grounding module; (iii) we evaluate the performance of   open-vocabulary grounding on images generated from the   text-to-image diffusion model and show that the module can   well segment the objects of categories beyond seen ones at   training time, as shown in Fig. 1; (iv) we adopt the augmented diffusion model to build a synthetic semantic segmentation dataset, and show that, training a standard segmentation model on such dataset demonstrates competitive   performance on the zero-shot segmentation (ZS3) benchmark, which opens up new opportunities for adopting the   This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   7667   powerful diffusion model for discriminative tasks.   1.", "content": "In the recent literature, text-to-image generative models   have gained increasing attention from the research community and wide public, one of the main advantages of   such models is the strong correspondence between visual   pixels and language, learned from large corpus of imagecaption pairs, such correspondence, for instance, enables   to generate photorealistic images from the free-form text   prompt [26, 29, 39, 25]. In this paper, we aim to explicitly   extract such visual-language correspondence from the generative model in the form of segmentation map, i.e., simultaneously generating photorealistic images, and infer segmentation masks for corresponding visual objects described   in the text prompts. The bene\ufb01t of extracting such visuallanguage correspondence from generative model is signi\ufb01cant, as it enables to synthesize in\ufb01nite number of images   with pixel-wise segmentation for categories within the vocabulary of generative model, serving as a free source for   augmenting the ability of discriminative segmentation or   detection models, i.e., be able to process more categories.   To achieve such goal, we propose to pair the existing   Stable Diffusion [26] with a novel grounding module, that   can segment the corresponding visual objects described in   the text prompt, from the generated image. This is achieved   by explicitly aligning the text embedding space of desired   entity and the visual features of synthetic images, i.e., intermediate layers of diffusion model. Once trained, objects   of interest can be segmented by their category names, for   both seen and unseen objects at training time, resembling an   open-vocabulary object segmentation for generative model.   To properly train the proposed architecture, we establish a pipeline for automatically constructing a dataset with   {synthetic image, segmentation mask, text prompt} triplets,   in particular, we adopt an off-the-shelf object detector, and   do inference on images generated from the existing Stable   Diffusion model, advocating no extra manual annotation.   Theoretically, such a pipeline enables to generate in\ufb01nite   data samples for each category within the vocabulary of existing object detector, for example, we adopt the Mask RCNN [21] pre-trained on COCO with 80 categories. We   show that the grounding module trained on a pre-de\ufb01ned   set of object categories, can segment images from Stable   Diffusion well beyond the vocabulary of any off-the-shelf   detector, as shown in Fig. 1, for example, Pikachu, unicorn, phoenix, etc, effectively resembling a form of visual   instruction tuning, for establishing visual-language correspondence from generative model.   To quantitatively validate the effectiveness of our proposed grounding module, we initiate two protocols for evaluation: \ufb01rst, we compare the segmentation results with   a strong off-the-shelf object detector on synthetic images;   second, we construct a synthesized semantic segmentation   dataset with Stable Diffusion and our grounding module,   then train a segmentation model on it.   While evaluating zero-shot segmentation (ZS3) on COCO and PASCAL   VOC, we outperform prior state-of-the-art models on unseen categories and show competitive performance on seen   categories, re\ufb02ecting the effectiveness of our constructed   datasets. Even more crucially, we demonstrate an appealing application for training discriminative models with synthetic data from generative model, for example, to expand   the vocabulary beyond any existing detector can do.   2. Preliminary on Diffusion Model   Diffusion models refer to a series of probabilistic generative models, that are trained to learn a data distribution by gradually denoising the randomly sampled Gaussian noises. Theoretically, the procedure refers to learning   the reverse process of a \ufb01xed Markov Chain of length T.   As for text-to-image synthesis, given a dataset of imagecaption pairs, i.e., Dtrain = {(I1, y1), . . . , (IN, yN)}, the   models can be interpreted as an equally weighted sequence   of conditional denoising neural network that iteratively predicts a denoised variant of the input conditioned on the text   prompt, namely \u03f5\u03b8(It   i, t, yi), where It   i denotes a noisy version of the input image, and t = 1, . . . , T refers to the   timestep, i \u2208{1, . . . , N}. For simplicity, we only describe   the training and inference procedure for a single image, thus   ignoring the subscript in the following sections.   In particular, this paper builds on a variant of diffusion   model, namely, Stable Diffusion [26], which conducts the   diffusion process in latent space. We will brie\ufb02y describe   its architecture and training procedure in the following.   Architecture. Stable Diffusion consists of three components: a text encoder for producing text embeddings; a   pre-trained variational auto-encoder that encodes and decodes latent vectors for images; and a time-conditional UNet (\u03f5\u03b8(\u00b7)) for gradually denoising the latent vectors, with   the progressive convolutional operation that downsamples   and upsamples the visual feature maps with skip connections. The visual-language interaction occurs in the U-Net   via cross-attention layers, speci\ufb01cally, a text encoder is used   to project the text prompt y to textual embeddings, that then   are mapped into Key and Value, and the spatial feature   map of the noisy image is linearly projected into Query,   iteratively attending the conditioned text prompt for update.   Training and Inference. The training procedure of Stable   Diffusion can be described as follows: given a training pair   (I, y), the input image I is \ufb01rst mapped to a latent vector   z and get a variably-noised vector zt := \u03b1tz + \u03c3t\u03f5, where   \u03f5 \u223cN(0, 1) is a noise term and \u03b1t, \u03c3t are terms that control the noise schedule and sample quality. At training time,   7668   A photograph of a    dog near a cat   Diffusion Model   Text Prompts   Our Model   Grounding   Synthetic images generated    by diffusion model   Oracle GT masks produced    by off-the-shelf detector   Synthetic images and corresponding masks generated    by our grounded generation model   \u2026   \u088c\u0ada   \u2026   \u088c\u0adb   \u088c\u0878   Visual    Encoder   a photograph of {        }    dog   cat   . . .   Text    Encoder   Fusion Module   . . .   . . .   . . .   Q   \u2297   (K, V)   7   Diffusion Model   Text Prompts   7   Figure 2: Overview. The left \ufb01gure shows the knowledge induction procedure, where we \ufb01rst construct a dataset with synthetic images   from diffusion model and generate corresponding oracle groundtruth masks by an off-the-shelf object detector, which is used to train   the open-vocabulary grounding module. The right \ufb01gure shows the architectural detail of our grounding module, that takes the text   embeddings of corresponding entities and the visual features extracted from diffusion model as input, and outputs the corresponding   segmentation masks. During training, both the diffusion model and text encoder are kept frozen.   the time-conditional U-Net is optimised to predict the noise   \u03f5 and recover the initial z, conditioned on the text prompt   y, the model is trained with a squared error loss on the predicted noise term as follows:   Ldiffusion = Ez,\u03f5\u223cN (0,1),t,y   \u0002   ||\u03f5 \u2212\u03f5\u03b8(zt, t, y)||2   2   \u0003   (1)   where t is uniformly sampled from {1, . . . , T}.   At inference time, Stable Diffusion is sampled by iteratively denoising zT \u223cN(0, I) conditioned on the text   prompt y. Speci\ufb01cally, at each denoising step t = 1, . . . , T,   zt\u22121 is obtained from both zt and the predicted noise term   of U-Net whose input is zt and text prompt y. After the   \ufb01nal denoising step, z0 will be mapped back to yield the   generated image I.   3. Problem Formulation   In this paper, we aim to augment an existing text-toimage diffusion model with the ability of open-vocabulary   segmentation, by exploiting the visual-language correspondence, i.e., simultaneously generating images, and the segmentation masks of corresponding objects described in the   text prompt, as shown in Fig. 2 (left):   {I, m} = \u03a6diffusion+(\u03f5, y)   (2)   where \u03a6diffusion+(\u00b7) refers to a pre-trained text-to-image diffusion model with a grounding module appended, it takes   the sampled noise (\u03f5 \u223cN(0, I)) and language description   y as input, and generates an image (I \u2208RH\u00d7W \u00d73) with   corresponding segmentation masks (m \u2208{0, 1}H\u00d7W \u00d7O)   for a total of O objects of interest. Note that, we expect   the model to be open-vocabulary, that means, it should be   able to output the corresponding segmentation mask for any   objects that can be generated by diffusion model, without   limitation of the semantic categories.   4. Open-vocabulary Grounding   Assuming there exists a training set of N triplets, i.e.,   Dtrain = {(F1, mgt   1 , y1), . . . , (FN, mgt   N, yN)}, the predicted   segmentation mask for all objects, i.e., mi \u2208RH\u00d7W \u00d7Oi   can be obtained by:   mi = \u03a6fuse(\u03a6v-enc(f 1   i , . . . , f n   i ), \u03a6t-enc(g(yi)))   (3)   where yi denotes the text prompt for image generation,   Fi = {f 1   i , . . . , f n   i } refers to the intermediate representation from Stable Diffusion at t = 5 (this has been experimentally validated in supplementary material).   Our   proposed grounding module consists of three functions,   namely, \u03a6v-enc(\u00b7), \u03a6t-enc(\u00b7) and \u03a6fuse(\u00b7), denoting visual encoder, text encoder, and fusion module, respectively. g(\u00b7)   denotes a group of templates that decorate each of the visual   categories in the text prompt, e.g., \u2018a photograph of a [category name]\u2019. At training time, there are totally Oi object   categories in the text prompt, and they fall into a pre-de\ufb01ned   set of vocabularies Ctrain; while at inference time, we expect   the model to be highly generalizable, that should equally be   capable of segmenting objects from unseen categories, i.e.,   Ctest \u2283Ctrain.   In the following sections, we start by introducing the   procedure for automatically constructing a training set in   Sec. 4.1, followed by the architecture design for openvocabulary grounding in Sec. 4.2, lastly, we detail the visual   7669   instruction tuning procedure, that trains the model with only   a handful of image-segmentation pairs as visual demonstrations in Sec. 4.3.   4.1.   Dataset Construction   Here, we introduce a novel idea to automatically construct {visual feature, segmentation, text prompt} triplets,   that are used to train our proposed grounding module. In   practise, we \ufb01rst prepare a vocabulary with common object   categories, e.g., the classes in PASCAL VOC can form a   category set as Cpascal = {\u2018dog\u2019, \u2018cat\u2019, . . . }, |Cpascal| = 20,   then we randomly select a number of classes to construct   text prompts for image generation (e.g., \u2018a photograph of a   dog and cat\u2019). Repeating the above operation, we can theoretically generate an in\ufb01nite number of image (intermediate   visual representation, i.e., F) and text prompt pairs. To acquire the segmentation masks, we take an off-the-shelf object detector, e.g., pre-trained Mask R-CNN [21], and run   the inference procedure on the generated images:   mgt   i = \u03a6detector(Ii), where Ii = \u03a6diffusion(\u03f5, yi),   (4)   where mgt   i \u2208{0, 1}H\u00d7W \u00d7Oi refers to the predicted masks   for a total of Oi objects in the generated image Ii, conditioning on the text prompt yi.   To evaluate the effectiveness of our proposed induction   procedure for open-vocabulary grounding, we divide the   vocabulary into seen categories (Cseen) and unseen categories (Cunseen), the training set (Dtrain) only has images of   seen categories, and the test set (Dtest) has both seen and   unseen categories. Note that, in addition to using the offthe-shelf detector to generate oracle masks and construct   dataset, we also explore to utilize real public dataset, e.g.   PASCAL VOC, to train our grounding module via DDIM   inverse process. We show the details in Sec. ??.   4.2.   Architecture   Given one speci\ufb01c training triplet, we now detail three   components in the proposed grounding module: visual encoder, text encoder, and fusion module.   Visual Encoder. Given the visual representation, i.e., latent features from the Stable Diffusion, we concatenate   features with the same spatial resolution (from U-Net encoding and decoding path) to obtain {f 1   i , . . . , f n   i }, where   f k   i \u2208R   h   2k \u00d7 w   2k \u00d7dk, k \u2208{0, . . . , n} denotes layer indices of   U-Net, dk refers to the feature dimension.   Next, we input {f 1   i , . . . , f n   i } to visual encoder for generating the fused visual feature \u02c6Fi = \u03a6v-enc({f 1   i , . . . , f n   i }).   As shown in Fig 3, visual encoder consists of 4 types of   blocks: (1) 1\u00d71 Conv for reducing feature dimensionality,   (2) Upsample for upsampling the feature to a higher spatial resolution, (3) Concat for concatenating features, and   (4) Mix-conv for blending features from different spatial   \u0ada\u00d7 \u0adaConv   Upsample   Upsample   \u0ada\u00d7 \u0adaConv   Concat   Mix-Conv   Upsample   \u0ada\u00d7 \u0adaConv   Concat   Mix-Conv   Upsample   \u0ada\u00d7 \u0adaConv   Concat   Mix-Conv   \u088c\u0ada   \u088c\u0894\u0b3f\u0adb   \u088c\u0894\u0b3f\u0ada   \u088c\u0894   ...   ...   Visual Encoder   Figure 3: The architecture of visual encoder. The features extracted from U-Net are \ufb01rst grouped according to their resolution,   then gradually upsampled and fused into the \ufb01nal visual feature.   resolutions, that includes two 3 \u00d7 3 Conv operations with   residual connection and a conditional batchnorm   operation, similar to [17].   Text Encoder. We adopt the language model from pretrained Stable Diffusion, speci\ufb01cally, given the text prompt   yi, the text encoder output the corresponding embeddings   for all visual objects: Eobji = \u03a6t-enc(g(yi)).   Fusion Module. The fusion module computes interaction   between visual and text embeddings, then outputs segmentation masks for all visual objects. In speci\ufb01c, we use a   standard transformer decoder with three layers, the text embeddings are treated as Query, that iteratively attend the   visual feature for updating, and are further converted into   per-segmentation embeddings with a Multi-Layer Perceptron (MLP). The object segmentation masks can be obtained   by dot producting visual features with the per-segmentation   embeddings. Formally, the procedure can be denoted as:   Esegi = \u03a6TRANSFORMER-D(W Q\u00b7Eobji, W K\u00b7 \u02c6Fi, W V \u00b7 \u02c6Fi) (5)   mi = \u02c6Fi \u00b7 [\u03a6MLP(Esegi)]T   (6)   where the transformer decoder generates per-segmentation   embedding Esegi \u2208RN\u00d7de for all visual objects described   in the text prompt, W Q, W K, W V refer to the learnable parameters for Query, Key and Value projection.   4.3.   Training   With the constructed dataset, we can now start supervised training the proposed grounding module:   L = \u22121   N   N   \u0002   i=1   [mgt   i \u00b7 log(\u03c3(mi)) + (1 \u2212mgt   i ) \u00b7 log(\u03c3(1 \u2212mi))]   where mgt   i   \u2208RH\u00d7W \u00d7Oi refers to the oracle groundtruth   segmentation from the off-the-shelf object detector, and   mi \u2208RH\u00d7W \u00d7Oi refers to the predicted segmentation from   our grounding module, \u03c3(\u00b7) refers to Sigmoid function.   In practise, while using the off-the-shelf detector to generate segmentation masks, the model may sometimes fail to   7670   Test   Setting   PASCAL-sim   COCO-sim   # Objects   One   Two   One   Two   Categories   Seen   Unseen   Seen   Seen +Unseen   Unseen   Seen   Unseen   Seen   Seen +Unseen   Unseen   DAAM [31]   Split1   61.66   75.63   46.74   51.31   69.94   62.25   55.56   49.68   52.06   43.35   Split2   65.75   59.25   49.08   47.98   41.50   60.08   65.55   48.80   54.66   33.22   Split3   67.11   53.82   48.80   48.28   41.41   62.81   52.48   50.85   49.84   45.80   Average   64.84   62.90   48.21   49.19   50.95   61.71   57.76   49.78   52.19   40.79   Ours   Split1   90.16   83.19   78.93   66.07   57.93   83.35   76.81   64.64   57.15   47.77   Split2   90.08   86.19   78.68   67.10   47.21   82.83   74.93   63.39   57.18   42.82   Split3   90.67   79.86   79.68   70.42   62.07   84.85   67.89   65.70   54.60   42.62   Average   90.30   83.08   79.10   67.86   55.74   83.68   73.21   64.16   56.31   44.40   Table 1: Quantitative result for Protocol-I evaluation on grounded generation. Our model has been trained on the synthesized training   dataset, that consists of images with one or two objects from only seen categories, and test on our synthesized test dataset that consists of   images with one or two objects of both seen and unseen categories. Split1, Split2 and Split3 refer to 3 different splits of C that construct 3   different (Cseen, Cunseen) pairs, respectively. Our model outperforms the DAAM [31], by a large margin, see text for more detailed discussion.   detect the objects mentioned in the text prompt, and output incorrect segmentation masks. Such error comes from   two sources, (i) the diffusion model may fail to generate   high-quality images; (ii) off-the-shelf detector may fail to   detect the objects in the synthetic image, due to the domain   gap between synthetic and real images. Here, we consider   two training strategies, Normal Training, where we fully   trust the off-the-shelf detector, and use all predicted segmentation masks to train the grounding module; alternatively, we also try Training w.o. Zero Masks, as we empirically found that the majority of failure cases come from   false negatives, that is to say, the detector failed to detect the   objects and output an all-zero mask, therefore, we can train   the grounding modules by ignoring the all-zero masks.   5. Experiments   In this section, we present the evaluation detail for validating the effectiveness of grounding module and its usefulness for training discriminative models, speci\ufb01cally, we   consider two protocols: in Sec. 5.1, we train the grounding   module with the constructed training set, and test the segmentation performance on synthetically generated images   from Stable Diffusion, then compare with a strong off-theshelf object detector; in Sec. 5.2, we use our augmented diffusion model to construct a synthesized semantic segmentation dataset and train a semantic segmentation model for   zero-shot segmentation. Lastly, in Sec. 5.3, we conduct a   series of ablation studies on the different training strategies   and effects on the number of seen categories.   5.1.   Protocol-I: Open-vocabulary Grounding   Here, we train the grounding module with our constructed training set, as described in Sec. 4.1. Speci\ufb01cally,   the training set only consists of a subset of common (seen)   categories, while the test set consists of both seen and unseen categories. In the following, we describe the implementation and experimental results in detail, to thoroughly   assess the model on open-vocabulary grounding.   Following the dataset construction procedure as introduced in Sec. 4.1, we make PASCAL-sim and COCOsim, with the same category split of PASCAL VOC [8] and   COCO [19] as in [10, 35, 7]: (i) PASCAL-sim contains 15   seen and 5 unseen categories respectively; (ii) COCO-sim   has 65 seen and 15 unseen categories respectively.   Training Set. For PASCAL-sim or COCO-sim, we generate a synthetic training set by randomly sampling images from pre-trained Stable Diffusion. This exposes our   grounding module to a great variety of data (> 40k) at   training time, and the model is unlikely to see the same   labeled examples twice during training. Unlike BigDatesetGAN [17], where only a single object is considered, we   construct the text prompt with one or two objects at a time,   note that, for training, only the seen categories are sampled.   Although we can certainly generate images with more than   two object categories, the quality of the generated images   tends to be unstable, limited by the generation ability of   Stable Diffusion, thus we only consider synthesized images   with less than three object categories.   Testing Set.   For evaluation purpose, we generate two   synthetic test sets with of\ufb02ine sampling for PASCAL-sim   and COCO-sim, respectively. In total, we have collected   about 1k images for PASCAL-sim, and about 5k images   for COCO-sim, we run the off-the-shelf object detector on   these generated images to produce the oracle segmentation.   For both test sets, the images containing two categories will   be divided into three groups: both seen, both unseen, one   seen and one unseen. We leave the detailed statistics of our   synthetic dataset in the supplementary material. Note that,   we have manually checked all the images and the segmentation produced from the off-the-shelf detector, and only keep   the high-quality ones, thus the performance evaluation of   our grounding module can be a close proxy.   7671   car   car   dog   bottle   sofa   sofa   train   hot dog   hot dog   bear   bear   backpack   apple   frisbee   Image   Image   Image   Image   Ours   Oracle GT   Ours   Ours   Ours   Oracle GT   Oracle GT   Oracle GT   bottle   backpack   apple   motorbike   motorbike   Figure 4: Segmentation results of PASCAL-sim (left) and COCO-sim (right) on seen (motorbike, bottle, backpack, and apple) and   unseen (sofa, car, hot dog, and bear) categories. Our grounded generation model achieves comparable segmentation results to the oracle   groundtruth generated by the off-the-shelf object detector.   dog   cat   bird   train   bus   pottedplant   sheep   cow   boat   boat   car   bottle   horse   chair   sofa   motorbike   person   pottedplant   person   motorbike   boat   pottedplant   bottle   dog   train   bottle   bird   Figure 5: Our synthesized semantic segmentation dataset with one category (left) and two categories (right) for Protocol-II training.   Evaluation Metrics.   We use the category-wise mean   intersection-over-union (mIoU) as evaluation metric, de\ufb01ned as averages of IoU over all the categories: mIoU   = 1   C   \u0004C   c=1 IoUc, where C is the number of all target categories, and IoUc is the intersection-over-union for the category with index is c.   Baseline. DAAM [31] is used as a baseline for comparison,   where the attention maps are directly upsampled and aggregated at each time step to explore the in\ufb02uence area of each   word in the input text prompt.   Implementation Details.   We use the pre-trained Stable   Diffusion [26] and the text encoder of CLIP [24] in our implementation. We choose the Mask R-CNN [21] trained on   COCO dataset as our object detector for oracle groundtruth   segmentation. We fuse features from U-Net and upsample   them into 512 \u00d7 512 spatial resolution, the text and visual   embeddings are both mapped into 512 dimensions before   feeding into the fusion module. We train our grounding   module with two NVIDIA GeForce RTX 3090 GPUs for   5k iterations with batch size equal to 8, ADAM[14] optimiser with \u03b21 = 0.9, \u03b22 = 0.999. The initial learning rate   is set to 1e-4 and the weight decay is 1e-4.   Results. As shown in Tab. 1, we provide experimental results for our grounding module, we change the composition of categories three times and compute the results for   each split. Here, we can make the following observations:   \ufb01rst, our model signi\ufb01cantly outperforms the unsupervised   method DAAM [31] in the mIoU on all test settings. This   is because DAAM tends to result in ambiguous segmentations, as the textual embedding for individual visual entity   will largely be in\ufb02uenced by other entities and the global   sentence at the text encoding stage; second, our grounding   module achieves superior performance on both seen and unseen categories, indicating its open-vocabulary nature, i.e.,   the guided diffusion model can synthesize images and their   corresponding segmentations for more categories beyond   the vocabulary of the off-the-shelf detector.   Visualization. We demonstrate the visualization results in   Fig. 4.   On both seen and unseen categories, our model   can successfully ground the objects in terms of segmentation mask. Impressively, as shown in Fig. 1, our grounding   module can even segment the objects beyond any off-theshelf detector can do, showing the strong open-vocabulary   grounding ability of our model.   5.2.   Protocol-II: Open-vocabulary Segmentation   In the previous protocol, we have validated the ability   for open-vocabulary grounding on synthetically generated   images. Here, we consider to adopt the images and grounding masks for tackling discriminative tasks. In particular,   we \ufb01rst construct a synthesized image-segmentation dataset   with the Stable Diffusion and our grounding module, then   train a standard semantic segmentation model on such a   synthetic dataset, and evaluate it on public image segmentation benchmarks.   Synthetic Dataset.   To train the semantic segmentation   7672   PASCAL VOC   COCO   Method   Training Set / # Categories   mIOU   Training Set / # Categories   mIOU   Real / 15   Synthetic / 15+5   (# Objects)   Seen   Unseen   Harmonic   Real / 65   Synthetic / 65+15   (# Objects)   Seen   Unseen   Harmonic   ZS3 [3]   \u0002   \u0003   78.0   21.2   33.3   \u0002   \u0003   SPNet [35]   \u0002   \u0003   77.8   25.8   38.8   \u0002   \u0003   33.0   21.9   26.3   CaGNet [10]   \u0002   \u0003   78.6   30.3   43.7   \u0002   \u0003   Joint [1]   \u0002   \u0003   77.7   32.5   45.9   \u0002   \u0003   57.9   8.6   14.9   STRICT [23]   \u0002   \u0003   82.7   35.6   49.8   \u0002   \u0003   22.2   20.4   21.3   SIGN [5]   \u0002   \u0003   83.5   41.3   55.3   \u0002   \u0003   ZegFormer[7]   \u0002   \u0003   86.4   63.6   73.3   \u0002   \u0003   53.3   34.5   41.9   Model-A (Ours)   \u0003   \u0002(one)   62.8   50.0   55.7   \u0003   \u0002(one)   28.8   32.6   30.6   Model-B (Ours)   \u0003   \u0002(two)   65.8   60.1   62.8   \u0003   \u0002(two)   37.3   37.0   37.1   Model-C (Ours)   \u0003   \u0002(mixture)   69.5   63.2   66.2   \u0003   \u0002(mixture)   38.3   38.1   38.2   Model-D (Ours)   \u0002   \u0002(mixture)   83.0   71.3   76.7   \u0002   \u0002(mixture)   50.0   38.2   43.2   Model-E (Ours)   (complicated prompt)   \u0002   \u0002(mixture)   83.4   74.4   78.7   \u0002   \u0002(mixture)   53.4   37.8   44.3   Table 2: Comparison with previous ZS3 methods on the test sets of PASCAL VOC and COCO. These ZS3 methods are trained on   real training sets. The results on PASCAL VOC are from [7]. And we retrained these ZS3 methods on COCO using their of\ufb01cial codes,   however, due to missing implementation details, we failed to reproduce ZS3 [3], CaGNet [10] and SIGN [5].   Image   GT Mask   Zegformer   Ours (Maskformer)   Image   GT Mask   Zegformer   Ours (Maskformer)   Figure 6: Qualitative results compared with the state-of-the-art zero-shot semantic segmentation method (Zegformer) on Pascal   VOC. While training on our synthetic dataset and real dataset (only PASCAL VOC seen) together, a standard MaskFormer architecture   can achieve better performance than Zegformer on unseen categories, i.e. pottedplant, sofa and tvmonitor. Note that, the synthetic data   for training MaskFormer are generated from our grounding module, which has only been trained on seen categories, with no extra manual   annotation involved whatsoever.   model, in addition to the provided datasets from public   benchmarks, we also include two synthetically generated   datasets: (i) 10k image-segmentation pairs for 20 categories in PASCAL VOC; (ii) 110k pairs for 80 categories   in COCO, as shown in Fig. 5. All the image-segmentation   pairs are generated by the Stable Diffusion and our proposed grounding module, that has only been trained on the   corresponding seen categories (15 seen categories in PASCAL VOC and 65 seen categories in COCO). That is to say,   generating these two datasets requires no extra manual annotations whatsoever.   Training Details. To compare with other open-vocabulary   methods, our semantic segmentation model uses MaskFormer [4] with ResNet101 as its backbone. The image resolution for training is 224\u00d7224 pix, and we train the model   on our synthetic dataset for 40k iterations with batch size   equal to 8. We use the ADAMW as our optimizer with a   learning rate of 1e-4 and the weight decay is 1e-4.   Comparison on Zero-Shot Segmentation (ZS3). While   evaluating on the test sets of real images (1,449 images   for PASCAL VOC and 5000 images for COCO), we compare with the existing zero-shot semantic segmentation approaches. As shown in Tab. 2, while only trained on synthetic dataset, our model-A,B,C have already outperformed   most of ZS3 approaches on unseen categories. Speci\ufb01cally,   the model-C trained on the mixture of one and two objects achieves the best performance. Additionally, \ufb01netuning on the real dataset with images of seen categories can   7673   Training Type   One   Two   Seen Unseen Seen Seen +Unseen Unseen   Normal Training   89.88   71.18   77.66   57.24   44.22   Training w.o. Zero Masks 90.16   83.19   78.93   66.07   57.93   Table 3: Ablation on training type on the constructed dataset.   Performance is measured by mIoU on PASCAL-sim test set.   further improve the performance, especially on seen categories (model-D). We also try to construct the synthetic   dataset with more complicated prompt, e.g., \u2018A photograph   of a bus crossing a busy intersection in Seoul\u2019s bustling city   center\u2019, and the model-E gets a slight boost on \ufb01nal performance. Qualitative results can be seen in Fig. 6, our model   obtains accurate semantic segmentation on both seen and   unseen categories.   Discussion. Overall, we can draw the following conclusions: (i) the grounding module is capable of segmenting unseen categories despite it has never seen any segmentation mask during the knowledge induction procedure,   validating the strong generalisation of the grounding module; (ii) it is possible to segment more object categories   by simply training on synthesized datasets, and the addition of real datasets with only seen categories can narrow   the data gap thus resulting in better performance; (iii) with   our proposed idea for extracting the visual-language correspondence from generative model, it introduces promising   applications for applying the powerful diffusion model for   discriminative tasks, i.e., constructing dataset with generative model, and use it to train discriminative models, e.g.,   expand the vocabulary of an object segmentor or detector.   5.3.   Ablation study   In this section, we show the effect of different training   loss and different numbers of seen categories. Due to the   space limitation, we refer the reader for supplementary material, for the study on the different timestep for extracting   visual representation, the number of objects in the synthetic   datasets, and the effect of different datasets.   Normal Training v.s. Training without Zero Masks. As   shown in Tab. 3, Normal Training results in unsatisfactory performance on unseen categories, we conjecture this   is because the errors from detector tend to be false negative, that bias our grounding module to generate all-zero   segmentation masks when encountering unseen categories;   in contrast, by ignoring all-zero masks at training, Training w.o. Zero Masks achieves equally good performance   on both seen and unseen categories.   Effect on the Number of Seen Categories. We ablate the   number of seen categories to further explore the generalisation ability of our proposed grounding module. As shown in   Tab. 4, the grounding module can generalise to unseen categories, even with as few as \ufb01ve seen categories; when inTrain Set   # Seen / Unseen   One   Two   Seen   Unseen   Seen   Seen +Unseen   Unseen   5   /   75   94.81   72.42   87.19   49.60   39.00   20   /   60   91.91   73.33   71.59   56.27   41.91   35   /   45   87.23   73.85   66.91   55.99   43.28   50   /   30   84.55   73.20   66.41   54.39   42.71   65   /   15   83.85   76.81   64.64   57.15   47.77   Table 4: Ablation on the number of seen categories on COCOsim. The bolded number indicates the best result. Our model can   generalise to unseen categories, even as few as \ufb01ve seen categories.   troducing more seen categories, the performance on unseen   ones consistently improves, but decreases on seen ones, due   to the increasing complexity on seen categories.   6. Related Work   Image Generation. Image generation is one of the most   challenging tasks in computer vision due to the highdimensional nature of images. In the past few years, generative adversarial networks (GAN) [9], variational autoencoders (VAE) [16], \ufb02ow-based models [15] and autoregressive models (ARM) [32] have made great progress. However, even GANs, the best of these methods, still face training instability and mode collapse issues [2]. Recently, Diffusion Probabilistic Models (DM) demonstrate state-of-theart generation quality on highly diverse datasets [11, 22, 30,   12, 28], outperforming GANs in \ufb01delity [6]. These models   are often combined with a well-designed text encoder and   trained on billions of image-caption pairs for text-to-image   generation task, i.e., OpenAI\u2019s DALL-E 2 [25], Google\u2019s   Imagen [29] and Stability AI\u2019s Stable Diffusion [26]. However, despite being able to generate images with impressive   quality using free-form text, it remains unclear what extent   the visual-language correspondence has been successfully   captured, this paper aims to augment an existing text-toimage diffusion model with the ability to ground objects in   its generation procedure.   Visual Grounding. Visual grounding, also known as referring expression comprehension, expects to understand the   natural language query and then \ufb01nd out the target object   of the query in an image. Early visual grounding works are   trained in two stages [13, 20, 33, 34, 36], by \ufb01rst detecting   the candidate regions, and then ranking these regions. Later,   one-stage approaches [18, 27, 37, 38] attract more attention   due to their superior accuracy and ef\ufb01ciency in fusing linguistic context and visual features. Here, we consider visual   grounding in the image generation procedure.   7. Conclusion   In this paper, we propose a novel idea for guiding the   existing Stable Diffusion towards grounded generation, i.e.,   segmenting the visual entities described in the text prompt   7674   while generating images.   Speci\ufb01cally, we introduce a   grounding module that explicitly aligns the visual and textual embedding space of the Stable Diffusion and train such   module with an automatically constructed dataset, consisting of {image, segmentation, text prompts} triplets. Experimentally, we show that visual-language correspondence can   be established by only training on a limited number of object categories, while getting the ability for open-vocabulary   grounding at the image generation procedure. Additionally,   we generate a synthetic semantic segmentation dataset using the augmented Stable Diffusion and train a semantic   segmentation model. The model can transfer to real images and show competitive performance to existing zeroshot semantic segmentation approaches on PASCAL VOC   and COCO dataset, opening up new opportunities to exploit   generative model for discriminative tasks.   8. Acknowledgement   This research was supported by the National Key R&D   Program of China (No. 2022ZD0161400), National Natural Science Foundation of China (62271308), STCSM   (22511105700, 22DZ2229005), 111 plan (BP0719010),   and State Key Laboratory of UHD Video and Audio Production and Presentation.", "conf": "ICCV", "year": "2023", "index": 228}, {"title": "The Thirty-Third AAAI Conference on Arti\ufb01cial Intelligence (AAAI-19)   A Natural Language Corpus of Common Grounding   under Continuous and Partially-Observable Context   Takuma Udagawa   The University of Tokyo, Tokyo, Japan   takuma udagawa@nii.ac.jp   Akiko Aizawa   National Institute of Informatics, Tokyo, Japan   aizawa@nii.ac.jp", "abstract": "Common grounding is the process of creating, repairing and   updating mutual understandings, which is a critical aspect   of sophisticated human communication. However, traditional   dialogue systems have limited capability of establishing common ground, and we also lack task formulations which introduce natural dif\ufb01culty in terms of common grounding while   enabling easy evaluation and analysis of complex models. In   this paper, we propose a minimal dialogue task which requires advanced skills of common grounding under continuous and partially-observable context. Based on this task formulation, we collected a largescale dataset of 6,760 dialogues   which ful\ufb01lls essential requirements of natural language corpora. Our analysis of the dataset revealed important phenomena related to common grounding that need to be considered.   Finally, we evaluate and analyze baseline neural models on a   simple subtask that requires recognition of the created common ground. We show that simple baseline models perform   decently but leave room for further improvement. Overall, we   show that our proposed task will be a fundamental testbed   where we can train, evaluate, and analyze dialogue system\u2019s   ability for sophisticated common grounding.   1", "content": "One major goal of natural language processing is to develop agents with human-level competency in dialogue. In   the \ufb01eld of human communication and development, the   ability to construct and maintain common ground has been   pointed out to be essential for natural language communication (Clark 1996) and acquisition (Tomasello 2009). Furthermore, in the \ufb01eld of human-computer interaction, it is   important that humans and computers have certain ways of   creating mutual understandings in order to collaboratively   solve problems. Although natural language communication   is not the only option, it is one of the most natural and effective solutions to this problem.   However, existing study of common grounding in dialogue system research is limited in three major ways.   First, existing dialogue tasks are limited in terms of common grounding due to the restricted types of information   that need to be dealt with. Speci\ufb01cally, previous tasks are   focused on either fully-observable or categorical context,   which makes common grounding a relatively trivial task:   Copyright c\u20dd2019, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   A\u2019s view   B\u2019s view   A: I see three in a line going up and to the right. The middle one   is the largest and darkest   B: I don\u2019t see that. I have one large, medium gray dot that\u2019s under   a small, darker gray dot   A: Is the larger dot slightly to the left   B: yes, slightly, let\u2019s choose the larger one.   A: SELECT red   B: SELECT blue   Figure 1: Example dialogue and context of our task. Two   agents have 7 entities in each view, but their views are centered slightly differently and only some entities are in common. Under this setting, agents must \ufb01nd a common entity   through natural language communication.   \u2022 In a fully-observable context (De Vries et al. 2017), it is   usually given that every information about the context is   shared among the agents. This makes common grounding easier because information about the context is already in their common ground, and there could be little   chance of misunderstandings. In contrast, under partiallyobservable context agents typically need to create common ground from minimal shared information, and there   could be a lot more misunderstandings between agents   that need to be resolved.   \u2022 In a categorical context (Bordes and Weston 2016; He   et al. 2017; Lewis et al. 2017), information can be expressed by symbolic natural language without ambiguity.   For example, there could be little ambiguity in describing categorical properties, such as discrete color (red, blue   and yellow). However, in a continuous context natural lan7120   guage usage can be ambiguous and more pragmatic (such   as darker gray and almost black), and this introduces natural dif\ufb01culty in terms of common grounding.   Another problem is the dif\ufb01culty of evaluation and analysis. As the models acquire more \ufb02exibility, automatic evaluation becomes problematic (Liu et al. 2016; Novikova et al.   2017) and interpretation of model behavior becomes more   challenging. Since advanced common grounding requires   high \ufb02exibility, it is expected that we will need reliable evaluation metrics and analysis methods in the process of comparing and improving different methods.   Finally, there are limitations of model capabilities. Although traditional dialogue systems rely on rule-based engineering and predetermined slot-\ufb01lling (Traum 1994; Young   et al. 2013; Williams, Raux, and Henderson 2016), these   models lack \ufb02exibility in terms of representing dialogue   states and generating natural utterances. Since common   ground can be very complex which may include high ambiguity, uncertainty, partial understandings and misunderstandings, we need systems that can better capture such   complexity and resolve them through \ufb02exible dialogues.   In this paper, we make a \ufb01rst step towards addressing   these problems in the following way:   First, we formulate a novel dialogue task which requires   advanced skills of common grounding under continuous and   partially-observable context. Our task is based on a more   general collaborative referring task, where the goal of the   agents is to coordinate attention on the same entity in a given   context. This setting enables clear evaluation based on task   success rate and various error analysis of complex models   since the contexts are simple and completely controllable.   Second of all, to enable training of recent end-to-end dialogue systems with high \ufb02exibility (Bordes and Weston   2016; Lewis et al. 2017), we collected a largescale dataset   of 6,760 human dialogues with over 32K utterances through   crowdsourcing on Amazon Mechanical Turk. During the   dataset collection, we managed to ful\ufb01ll three essential requirements of natural language corpora: interpretability, linguistic/strategic variety and reliability.   Next, we conduct a comparative analysis with the previous dataset to illustrate how continuous and partiallyobservable context introduces dif\ufb01culty in terms of common   grounding. In addition, further analyses of our dataset revealed various common grounding phenomena at different   levels, including nonlinguistic bias towards joint saliency.   Finally, we evaluate and analyze simple neural network   models on our dataset based on an important subtask of   collaborative referring. Due to the complexity of common   grounding, there is still room for further improvement.   We show an example of the collected dialogue and context in Figure 1. Although human players successfully coordinated the selection with relatively short turns, we can \ufb01nd   dif\ufb01cult common grounding strategies such as pragmatic descriptions (\u201cthree in a line going up\u201d), clari\ufb01cation based on   hypothesis testing (\u201cIs the larger dot slightly to the left\u201d) and   nuanced acknowledgement (\u201cyes, slightly\u201d).   Overall, we expect our task to be a fundamental testbed   for developing dialogue systems with advanced common   grounding skills. Our dataset and scripts will be available   at https://github.com/Alab-NII/onecommon.   2   Related Work   In dialogue system research, classical approach models   common grounding based on \ufb01nite states, where information   in a dialogue transitions through \ufb01xed grounding acts (such   as Initiate, Continue, Repair, Request Repair, Acknowledge)   (Traum 1994), and the whole system relies on careful rulebased engineering and predetermined slot-\ufb01lling (Young et   al. 2013). Although they perform reliably in restricted domains such as restaurant information retrieval (Williams,   Raux, and Henderson 2016), they lack \ufb02exibility to deal with   realistic complexity of common ground.   Recently, a new line of data-driven dialogue systems,   which we refer to as end-to-end dialogue systems, has been   gaining attention in both task-oriented (Bordes and Weston 2016; Lewis et al. 2017) and non-task-oriented domains (Vinyals and Le 2015; Serban et al. 2016). In this approach, dialogue state and utterance generation are learned   directly from large raw corpora with little prior constraints,   so they are more suitable for complex common grounding where \ufb02exibility is a requirement. However, few existing tasks focus on the dif\ufb01culty of common grounding   and most are based on either fully-observable or categorical context (De Vries et al. 2017; Bordes and Weston 2016;   Lewis et al. 2017) where dif\ufb01cult common grounding is   not required. A dataset closest to our setting is the MutualFriends dataset (He et al. 2017), which is based on the task   of \ufb01nding a mutual friend from private lists of friends. Although this can be considered as a collaborative referring   task under partially-observable context (due to the privacy   of knowledge), they only include categorical information   and the dif\ufb01culty of common grounding is limited. We give   a precise comparative analysis in Section 4.   Referring expression generation and dialogues based on   realistic visual context have also been studied extensively   (Kazemzadeh et al. 2014; Das et al. 2017a; 2017b). Although these tasks are based on continuous (and sometimes   partially-observable) context, realistic images could be too   complex and costly to deal with modern dialogue systems.   In contrast, we show that abstract and simple context is suf\ufb01cient for making common grounding dif\ufb01cult and preferable for a testbed of common grounding where models can   be easily developed, compared and analyzed. Nevertheless,   we expect experiments on abstract and realistic contexts to   have complementary strengths.   Finally, previous work addressed the dif\ufb01culty of common grounding due to the perceptual difference between humans and machines (Liu, Fang, and Chai 2012; Fang, Doering, and Chai 2015). However, such problems are speci\ufb01c   to human-machine dialogues, and instead we focus on a   more general dif\ufb01culty of common grounding due to complex ambiguity and uncertainty introduced by continuous   and partially-observable context.   7121   3   Task Description   Collaborative referring is the task of creating a mutual understanding about the entity currently under discussion. We   can interpret this as the initial step of common grounding   where agents coordinate attention on speci\ufb01c entity (or entities) in the world, since it is only after collaborative referring   succeeds that agents can exchange useful information about   entities and develop common ground related to the world.   3.1   Task De\ufb01nition   Formally, the task is formulated as a multi-agent cooperative game with two sets of entities E = {e1, e2, ..., em} and   agents A = {a1, a2, ..., an}. Each agent ai \u2208A has an observation of E, and at each timestep they can send natural   language messages to coordinate their selections from E.   The game is considered completed when all the agents have   made at least one selection action, and it is successful if and   only if all the agents selected the same entity.   We propose our task based on a minimal formulation   of the collaborative referring task under continuous and   partially-observable context. Speci\ufb01cally, we consider two   agents located slightly differently in a 2-dimensional grid.   Entities are also located in the same grid with two additional   properties: size and color. However, agents can only observe   entities within a \ufb01xed radius r from their location, which   makes this setting partially-observable. Furthermore, each   observation is continuous, since all the properties of the entities (location, size and color) are continuous.   For the sake of simplicity, the number of entities observable by each agent is \ufb01xed at 7. This ultimately reduces our   task to a simple classi\ufb01cation problem, which can be evaluated based on simple metrics, such as accuracy.   3.2   Dataset Collection   We basically followed the dataset collection procedure of the   MutualFriends dataset (He et al. 2017). We used Amazon   Mechanical Turk to pair up two human workers and gave   each worker 20 seconds to read an example, followed by a   maximum of 6 minutes session to complete the task. Our   chat interface is shown in Figure 2.   During the dataset collection, we were concerned on three   essential requirements of natural language corpora: interpretability, linguistic/strategic variety and reliability. In this   section, we show why these properties need consideration   and how we managed to ful\ufb01ll these requirements.   Interpretability   We de\ufb01ne the interpretability of the   dataset to be the ease of interpreting its language and strategy, which is critical for additional annotations and error   analysis. However, lack of discipline and complexity of the   vocabulary can make a corpora dif\ufb01cult for interpretation.   In free-formed dialogues, lack of discipline can cause unnecessary dif\ufb01culty in terms of interpretation. For instance,   cross-talk (conversation which does not progress sequentially) can occur frequently (He et al. 2017) and complicate important structures of dialogues, such as discourse   segments, adjacency pairs and contributions in common   grounding (Clark 1996). Thus, we tried to minimize them   Figure 2: Screenshot of our chat interface. Workers are given   a maximum of 6 minutes session to communicate through   the chatbox and select the entity they found in common.   by forcing workers to take turns. Also, chit-chat could occur occasionally, which adds undesirable noise for analyzing   strategies. Therefore, we explicitly instructed the workers to   avoid talking about things that are irrelevant to this task.   Keeping the vocabulary simple is also important for interpretability, especially for people unfamiliar with the domain.   For instance, the MutualFriends dataset (He et al. 2017) includes up to 7 attributes with approximately 3K complex   named entities and technical terms. In contrast, we kept the   attributes minimal with only 4 intuitive scalar attributes (xvalue, y-value, size and color). As a result, this greatly reduced the complexity of the vocabulary as we describe in   detail in Section 4.   Linguistic and Strategic Variety   Linguistic and strategic   variety of the dataset is fundamental for developing dialogue   systems with broad coverage. To elicit this property, we sampled all attributes of the entities uniformly at random, with   the only restriction that the entities cannot be too close to   each other. As the previous work with similar idea con\ufb01rmed   (Suhr et al. 2017), we found rich varieties of linguistic phenomena, including cardinalities (\u201cthree gray dots\u201d), existentials (\u201cThere is another small dark ..\u201d), universals (\u201call of the   other dots are larger\u201d), coordinations and negations (\u201cfurther   to the right and not as far down\u201d).   During the dataset collection, we assigned 6,759 unique   contexts to 6,760 dialogues we collected. We collected two   dialogues based on the exact same context and con\ufb01rmed   that they solved the task in different ways: thus there could   be various effective solutions and agents must adapt \ufb02exibly   to their partners\u2019 strategies.   In addition, we gave variation according to the degree of   partial observability. Speci\ufb01cally, only 4, 5 or 6 out of the 7   entities in view are shared among the agents. As a result, we   found further variation of common grounding strategies, as   we discuss in detail in Section 4.   Reliability   Finally, we regard the reliability of the dataset   to be crucial, especially when crowdsourcing data through   untrained, possibly low-motivated workers. Speci\ufb01cally, in   7122   our preliminary experiment we found many cases where   workers did not follow the instruction carefully or solve the   task effectively, especially on dif\ufb01cult cases.   As a solution, we manually reviewed all work and rejected   ones which clearly did not follow the instruction. Our instruction is kept brief and explicit so that it is easier to follow, and we also gave manual feedback about general solutions to improve their work. To discourage premature guessing, we prohibited workers from selecting within the \ufb01rst   minute and instructed them to make it very sure they found   one entity in common. We also incentivized task success   with $0.05 bonus for all successful dialogues, in addition   to the base reward of $0.30.   As a result, we found signi\ufb01cant improvement in terms   of task success rate, which is an important evidence of the   reliability of our dataset.   Based on the above procedure, we collected 6,839 completed dialogues and accepted 6,760 dialogues in total. Overall, we received positive feedback about our task and the   workers seemed to enjoy it.   4   Dataset Analysis   In this section, we verify the dif\ufb01culty of common grounding   introduced in our task and conduct further analyses of our   dataset.   4.1   Dif\ufb01culty of Common Grounding   Our hypothesis is that continuous and partially-observable   context makes common grounding dif\ufb01cult compared to categorical or fully-observable context. Since our task can be   solved trivially with full-observability (e.g., by always uttering \u201cselect the darkest dot\u201d), we focus on testing how continuous context adds dif\ufb01culty in terms of common grounding.   As a comparison, we use the MutualFriends dataset which   is based on a similar collaborative referring task under   partially-observable but categorical context (He et al. 2017).   However, several differences make a direct comparison dif\ufb01cult: for instance, we only gave one chance for \ufb01nal selection, while the MutualFriends dataset allowed multiple   chances in the given time. Therefore, we focused on the following factors which are less affected by the differences.   Utterance Length   We compare the average utterance   length because this indicates the syntactical/semantic complexity of utterances required for common grounding. As   shown in Table 1, utterances in our dataset are at least   twice as long as those in the MutualFriends dataset. Therefore, more complex utterances are required under continuous context compared to categorical context. Interestingly,   utterance lengths also slightly increase when the number of   shared entities are smaller: thus, greater degrees of partialobservability also add complexity at the utterance level.   Pragmatic Expressions   In our dataset, we found many   pragmatic expressions whose meaning depend on the context and should not be taken literally. A typical example is   the usage of word black to indicate the darkest dot in the context, even if its color is not completely black. Another common expression triangle is also pragmatic, since in literal   sense there could be numerous triangles in one\u2019s view, and   the speaker actually indicates a group of three dots which is   close to prototypical types of triangles, such as an equilateral   triangle. Examples are shown in Figure 3.   large black dot   a close triangle   Figure 3: Examples of typical pragmatic expressions in our   dataset, marked by bold.   As the previous work pointed out, such pragmatic expressions are characteristic in continuous context (Monroe et al.   2017) and add complex ambiguity that need to be resolved   through common grounding.   Nuanced Expressions   Finally, frequent usage of nuanced   expressions is an important characteristic of our dataset.   Since the context is continuous and partially-observable, we   hypothesize that speakers need to rely on such expressions   to express subtle differences in terms of degree, ambiguity   and uncertainty.   To estimate the frequency of nuanced expressions, we   manually reviewed 100 dialogues from each dataset to   create keyword-based dictionaries of nuanced expressions,   which are further expanded with synonyms/morphologies.   We excluded words which are likely to be used with   different meanings (such as like, about and around). For   simplicity, we do not consider nuances expressed morphologically (such as ish as in smallish) although they are also   common in our dataset. Results in Table 2 show that our   dataset includes signi\ufb01cantly more nuanced expressions of   various types. The dictionaries will be publicly available for   reproducibility.   To summarize the points, utterances are much longer in   our dataset, which indicates the complexity of common   grounding at the utterance level. Secondly, our dataset includes more ambiguity and uncertainty represented by frequent occurrence of pragmatic expressions and nuanced expressions. Thus the process of common grounding is complicated, but regardless of such dif\ufb01culties, human workers   could solve the task reasonably well with little evidence of   confusion. Therefore, we conclude that introducing continuity and partially-observability to the context is a critical solution to add natural dif\ufb01culty in terms of common grounding.   7123   MutualFriends   Ours   (# Shared = 4)   (# Shared = 5)   (# Shared = 6)   # dialogues   10,661   2,189   2,279   2,292   Average tokens per utterance   5.38   12.87   12.37   11.86   Average turns per dialogue   8.97\u2217   4.97   4.77   4.56   Success rate   0.85\u2217   0.66   0.77   0.87   Unique tokens   13,478   3,761   Occupancy of top 10% frequent tokens   91.6%   97.0%   Table 1: Basic statistics of our dataset and the MutualFriends dataset (He et al. 2017). To count tokens and vocabulary size, we   preprocessed the text with the same NLTK word tokenizer (Loper and Bird 2002) and converted each token to its lowercased   form. Statistics with asterisk are not suitable for direct comparison due to the task difference (e.g. the number of chances).   Nuance Type   MutualFriends   Ours   Example Keywords   Example Usage   Approximation (10)   0.10   3.98   almost, nearly, approximately   almost in the middle   Exactness/Con\ufb01dence (33)   0.14   2.71   exactly, completely, de\ufb01nitely   exactly horizontal   Subtlety (12)   0.01   9.37   slightly, bit, somewhat   slightly to the right   Extremity (27)   0.21   9.35   very, really, extraordinary   very light dot   Uncertainty (20)   0.57   5.79   maybe, might, guess   Maybe it\u2019s different   Table 2: Average occurence of nuanced expressions per 100 utterances (dictionary size shown in parenthesis).   4.2   Further Analysis   Next, we conduct further analyses of the collected dataset,   which revealed important phenomena at different levels that   need to be considered.   Basic Statistics   From Table 1, we also found that dialogues get longer in terms of average turn length with fewer   shared entities. This shows that under greater degrees of   partial-observability, it is more likely that the presented information is not groundable and players need more try-anderror to create common ground. Success rate also drops naturally, so in general common grounding becomes more challenging when less information is shared.   In terms of lexical variety, we found 3,761 unique tokens   in total, in contrast to 13,478 in the MutualFriends dataset.   In addition, large portion of the corpora constitutes of common words, and top 10% of the most frequent tokens occupy   97.0% of the whole tokenized corpora, in contrast to 91.6%   in the MutualFriends dataset. Therefore the vocabulary of   our dataset is extremely simple, which is an important evidence of interpretability discussed in Section 3. This may   also be helpful for training dialogue systems because rare   words are less problematic.   Nonlinguistic Phenomena   Language is a coordination   device we use to coordinate our joint actions (Lewis 1969),   but we also use joint saliency to coordinate actions at the   nonlinguistic level (Schelling 1980). In our dataset, we   found that human players have a tendency to focus attention   on perceptually salient entities more often.   We plot the \ufb01nal selection probabilities based on entity\u2019s   color and size in Figure 4. We can clearly see that the selection is biased, and entities with extreme properties (around   the edge) are more likely to be selected. We also found that   darker entities are more likely to be selected (62.7%) compared to lighter entities (37.3%), and larger entities (54.3%)   slightly more likely than smaller entities (45.7%).   There could be other types of joint saliency such as geometric relations between entities, but the point is that such   bias exists and needs consideration: for example, just by   taking advantage of such bias, we can predict human selections signi\ufb01cantly better than random (Section 5). However,   due to partial-observability joint saliency is not suf\ufb01cient to   solve our task and communication is critical.   Figure 4: Comparison of the \ufb01nal selection probabilities   based on color and size. The total range of size is 7 and color   is split into 30 equal-sized bins based on RGB scale (smaller   is darker).   7124   Function   Type   Example Utterances   Information Providing   Inform (Init.)   I have very dark small dot in the center   Inform (Cont.)   It also has a small light grey one further down from the group   Agreement   Yes I have one like that. / same here.   Agreement (Strong)   Exactly! / perfect. mine too.   Agreement (Partial)   not sure its the one / more of a line.   Yes, but the small is medium dark, not completely black   Disagreement   I don\u2019t have that one. / mine are not in those locations.   Information Seeking   Question (Prop.)   the middle one is the darkest of the 3?   Question (Set)   where is it in relation to the large med grey?   Question (Choice)   Which should we choose? / the black or the grey?   Question (Check)   It\u2019s the darkest dot in the circle, right?   Commissives   Offer   lets click the upper left one that\u2019s bigger and darker gray   Directives   Request   tell me about your tiniest dot? / pick one at the bottom   Suggestion   Please describe it in relation to other dots in the circle   Table 3: Illustrative utterances in the dataset, grouped by the task dimension of communicative functions (Bunt et al. 2017)   Utterance Level Phenomena   Understanding speaker\u2019s   meaning at the utterance level is critical in dialogue (Grice   1957): especially the idea of speech acts (Austin 1962;   Searle 1969) has been applied widely in dialogue system   research for improving utterance understanding and generation.   In our setting, we allowed free-formed chat with few restrictions, as long as they are relevant for the accomplishment of the task: as a result, we found a wide variety of   speech acts in various forms related to common grounding.   We show illustrative examples of the collected utterances in   Table 3. Utterances are grouped by the task dimension of   communicative functions (Bunt et al. 2017), including information transfer functions (Information Providing/Seeking)   and action discussion functions (Commissives/Directives).   With additional annotations, our dataset can be extended for   other dialogue tasks, such as dialogue act recognition.   Discourse Level Phenomena   Naturally, we found many   coreference and anaphoric expressions in our dataset. Coreference resolution is the task of mapping mentions of entities   to their referents. In our dataset, we found two characteristics that complicate this task. First, due to continuous and   partially-observable context, mentions are usually ambiguous and referents may be missing. Thus players must keep   track of various possibilities and investigate them through   interaction. Secondly, players often use groupings (such as   three in a line, a cluster of 4 dots) where mentions refer   to sets of entities. This strategy could be effective but adds   complexity to coreference resolution.   On the other hand, anaphoric relation is the relation between a mention and following mentions which refer to the   previous mention. This can occur both within utterances (\u201ca   medium size black one, with a very light slightly smaller   one to it\u2019s left\u201d) and across utterances (\u201cDoes the lighter dot   appear to be slightly larger?\u201d). Similar to coreference resolution, this is a challenging subtask of common grounding at   the discourse level which can be studied on our dataset.   5   Experiments   5.1   Experiment Overview   In this experiment, we formulate a natural language understanding task based on target selection: speci\ufb01cally, we try   to predict which target a player selected, given the player\u2019s   observation and the corresponding dialogue. This is an essential subtask of collaborative referring, where players   choose their \ufb01nal selection based on the created common   ground. Since the number of entities in view is \ufb01xed at 7, we   can formulate this as a simple classi\ufb01cation problem. Our   baseline models are kept as simple as possible, with minimal preprocessing and hyperparameter tuning.   5.2   Methods   Two main components of the models are as follows:   Context Embedding   The structured form of the context   is represented as a 28 dimensional real-valued vector, where   each of the 7 observable entities is represented as a 4 dimensional vector (x-value, y-value, size, color). Each dimension   is further normalized in the range of (-1,1).   The simplest way to embed context is to directly apply   a multi-layered perceptron (MLP) over the context vector.   However, without feature engineering this simple approach   may have dif\ufb01cultly in capturing relevant information, such   as relations between entities. Therefore, in the second approach we use the Relation Network module (Santoro et al.   2017) to create additional features about relations between   entities. Speci\ufb01cally, we embed each combination of the entities (total of 21 pairs) with a shared MLP and append the   sum of these vectors as additional input.   Dialogue Embedding   Utterances are all tokenized and   lowercased, and tokens which occur less than 10 times are   treated as a unique unknown token. We insert tokens which   represent speaker id to each utterance at the beginning,   and another token to indicate the end of the dialogue.   Then, we embed these tokens with a shared MLP and run   7125   Full   Uncorrelated   Success Only   Random   14.28   14.28   14.28   Context Only (MLP)   27.90 \u00b1 0.6   28.74   29.59   Context Only (RN)   31.94 \u00b1 0.9   30.22   32.40   Context + Dialogue (MLP)   40.27 \u00b1 1.3   40.89   43.82   Context + Dialogue (RN)   43.09 \u00b1 0.8   44.00   49.44   Humans   82.50   90.79   Table 4: Results of the target selection experiment. Models are trained 10 times initialized with different seeds for the Full   testset, and the models with best validation loss are used for the additional testset results (Uncorrelated and Success Only).   a bidirectional GRU (Cho et al. 2014) over the embedded   tokens. Finally, we take the last output of the bi-GRU as the   \ufb01nal representation of the dialogue.   For prediction, we simply concatenate the context and   dialogue embeddings and run another MLP. However, as   we\u2019ve seen in Section 4, there are nonlinguistic selection   bias in our dataset, so it is possible to make predictions without dialogue embeddings. Therefore, we also train models to   predict only from the context embeddings using MLP.   Following common practice, we split the dataset into   training, validation and test set with a proportion of 8:1:1,   and all models are tuned on the validation set. The loss function is calculated using cross entropy. All components of   the neural networks consist of single layer with 128 hidden   units, and dropout rate of 0.5 is applied at each layer to avoid   over\ufb01tting. All parameters are initialized uniformly within   the range of (-0.01, 0.01). Models are trained with the Adam   optimizer (Kingma and Ba 2014) with initial learning rate of   0.001, and we clip gradients whose L2 norm is greater than   0.1. The experiment is run 10 times initialized with different   seeds, and we report the mean and standard deviation of the   selection accuracies on the full testset.   For further analyses, models with the best validation loss   in the previous experiment are also tested on two variants   of the testset. First, we create an uncorrelated testset by randomly removing one from each correlated pair in the current testset (same dialogue but different context). Secondly,   we further removed dialogues where players failed to coordinate on the same entity from the uncorrelated testset,   since this may affect target selection performance. The statistical signi\ufb01cance of the results for each pair of methods   are tested on the uncorrelated testset using paired student\u2019s   t-test. Finally, we take 100 random samples from the uncorrelated testset (including 76 successful) to report human   performance based on average accuracy of two annotators.   5.3   Results   We show the results of our experiment in Table 4. As we   can see, models trained only with the context embeddings   perform signi\ufb01cantly better than random (p-value < 10\u22127).   This veri\ufb01es that we can indeed take advantage of selection   bias to make better predictions.   In addition, we found that embedding context with Relation Network consistently outperforms MLP, but not at a   statistically signi\ufb01cant level (p-value > 0.1). Therefore, the   simplest strategy of using MLP works decently, but a better   architecture may improve the overall performance.   Finally, models trained with both context and dialogue   embeddings signi\ufb01cantly outperform models trained only   with the context embeddings (p-value < 10\u22129). This indicates that even our simplest models can learn to ground linguistic meanings based on the context to make better predictions. When the testset only includes successful cases, models perform better but human performance improves even   more achieving over 90% accuracy. Overall, our target selection task is challenging due to the complexity of common   grounding, and we still have a huge room for improvement.   6   Conclusion and Future Work   The main contributions can be summarized as follows:   \u2022 We proposed a simple and general idea of incorporating   continuous and partially-observable context to the dialogue tasks, which makes common grounding dif\ufb01cult in   a natural way.   \u2022 Following this idea, we formulated a novel dialogue task   based on collaborative referring which enables clear evaluation and analysis of complex models.   \u2022 We collected a largescale dataset of 6,760 dialogues,   which ful\ufb01lls essential requirements of natural language   corpora and will be publicly available online.   \u2022 Our analysis of the dataset veri\ufb01ed the dif\ufb01culty of common grounding and revealed various phenomena that need   to be considered.   \u2022 We evaluated and analyzed simple baseline models on an   important subtask of collaborative referring and showed   that there is still room for further improvement.   In future work, we will evaluate and analyze dialogue   models based on our task, especially to identify the current limitations of end-to-end approaches in terms of common grounding. Models can be trained in a variety of   ways, including supervised learning, reinforcement learning   with humans, and reinforcement learning based on self-play   (Lewis et al. 2017). Overall, we expect our task to be a fundamental testbed for developing dialogue systems with sophisticated common grounding abilities.   7126   Acknowledgements   This work was supported by JSPS KAKENHI Grant Numbers 16K12546,18H03297.", "conf": "AAAI", "year": "2019", "index": 347}, {"title": "Investigating Compositional Challenges in Vision-Language Models for Visual   Grounding   *Yunan Zeng 1,2,3   Yan Huang 1,2   Jinjin Zhang 3   Zequn Jie 3   Zhenhua Chai 3   \u2020Liang Wang 1,2   1Center for Research on Intelligent Perception and Computing (CRIPAC)   2Institute of Automation, Chinese Academy of Sciences (CASIA)   3Meituan   yunan.zeng@cripac.ia.ac.cn   {yhuang, wangliang}@nlpr.ia.ac.cn   {zhangjinjin05, chaizhenhua}@meituan.com   zequn.nus@gmail.com", "abstract": "Pre-trained   vision-language   models   (VLMs)   have   achieved high performance on various downstream tasks,   which have been widely used for visual grounding tasks   in a weakly supervised manner. However, despite the performance gains contributed by large vision and language   pre-training, we find that state-of-the-art VLMs struggle   with compositional reasoning on grounding tasks.   To   demonstrate this, we propose Attribute, Relation, and Priority grounding (ARPGrounding) benchmark to test VLMs\u2019   compositional reasoning ability on visual grounding tasks.   ARPGrounding contains 11,425 samples and evaluates the   compositional understanding of VLMs in three dimensions:   1) attribute, denoting comprehension of objects\u2019 properties;   2) relation, indicating an understanding of relation between   objects; 3) priority, reflecting an awareness of the part of   speech associated with nouns. Using the ARPGrounding   benchmark, we evaluate several mainstream VLMs.   We   empirically find that these models perform quite well   on conventional visual grounding datasets,   achieving   performance comparable to or surpassing state-of-the-art   methods but showing strong deficiencies in compositional   reasoning. Furthermore, we propose a composition-aware   fine-tuning pipeline, demonstrating the potential to leverage cost-effective image-text annotations for enhancing the   compositional understanding of VLMs in grounding tasks.   Code is available at link.   1.", "content": "Vision-language models (VLMs) have achieved high   performance on various downstream tasks, including many   zero-shot learning and text-guided vision tasks [2, 4, 19,   *Work was done during the author\u2019s internship at Meituan.   \u2020Corresponding author.   Figure 1. Typical examples of testing compositional understanding of CLIP on visual grounding task. CLIP encounters challenges   in discerning the authentic object from deceptive ones.   (Left)   CLIP is misled by a dog of a distinct color. (Right) CLIP is misled   by an object within the phrase. Both instances suggest a deficiency   in CLIP\u2019s grasp of compositional structure.   21, 34, 41]. Many endeavors leverage the intrinsic visuallinguistic alignment representation within these models and   integrate VLMs with explainability methods [37, 52] for   image localization tasks. This approach serves to alleviate   the significant time and cost required for dense manual annotation, particularly in tasks such as weakly supervised visual grounding [13, 20, 39, 40], weakly supervised segmentation [23, 25] and open-vocabulary segmentation [24, 46].   Visual grounding is a pivotal task in vision-language. We   adopt the widely used Grad-CAM [37] algorithm, integrating it into VLMs without introducing additional complexity. Our methodology unveils that VLMs can attain state-ofthe-art results in weakly supervised visual grounding tasks   through the application of explainability techniques.   However,   we find VLMs encounter challenges in   grounding while compositional reasoning is involved. As   shown in Figure 1, CLIP exhibits difficulty distinguishing   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   14141   between the \u201cbrown dog\u201d and the \u201cblack dog\u201d, and fails to   ground the \u201cpot behind dog\u201d while falsely activating the dog   area. To investigate this problem, we present ARPGrounding, a novel grounding benchmark designed to assess the   fine-grained visio-linguistic compositionality across three   dimensions: attribute, relation, and priority. We define two   objects of compositional ambiguity in a single image, each   corresponding to a semantic textual expression.   We test   models\u2019 compositional understanding by compelling models to pick the correct object based on the provided text. Using ARPGrounding benchmark, we observe that while all   of these VLMs can achieve performance levels comparable   to state-of-the-art results on conventional visual grounding   benchmarks, they struggle to perform beyond chance levels in relatively simple tasks that require compositional understanding. VLMs encounter difficulty differentiating between two objects with distinct attributes, falter in distinguishing relations, and fail to discern the primary and secondary targets within the text.   VLMs are pre-trained on extensive datasets featuring intricate scenes and detailed captions. These datasets are notable for their rich compositional structure. However, training on these datasets has been proven insufficient in addressing the dearth of compositional structure in VLMs.   Recent research has revealed that the challenges faced by   VLMs in compositional understanding stem from an artifact of their contrastive training objective, leading to shortcut learning while optimizing for the objectives [42, 49].   These shortcuts enable VLMs to get relatively good performance in downstream tasks but leave deficiencies in visiolinguistic tasks that demand a more nuanced understanding   of objects. In response, we propose a novel compositionaware fine-tuning pipeline to fine-tune VLMs without using   dense annotations. We first generate a text pair for each   image where each component in the pair describes distinct   objects using dependency parsing.   Then we introduce a   pretext task that is aimed at augmenting diversity within   the grounding heatmaps of these two texts. This approach   yields substantial improvements in the compositional understanding of VLMs.   Our main contributions are three-fold:   1. We employ explainability techniques to analyze the   VLMs and attain state-of-the-art results in weakly supervised visual grounding tasks.   2. We present ARPGrounding, a novel grounding dataset   designed to assess the fine-grained visio-linguistic compositionality.   Using ARPGrounding, we discover   VLMs\u2019 deficiencies in compositional reasoning.   3. We propose a composition-aware fine-tuning pipeline.   Empirical results show that this training pipeline effectively aids in enhancing compositional understanding of   VLMs.   2. Related Work   Evaluating vision-language compositionality. In recent   studies, researchers have introduced evaluation benchmarks   for assessing the compositional abilities of vision-language   models.   These studies have revealed that current models exhibit a limited understanding of compositionality   [28, 36, 43, 49, 51].   Specifically, VLMs exhibit limitations in accurately counting objects within an image [31],   lack proficiency in commonsense knowledge and reasoning   abilities [47, 48], falter in comprehending verbs [10, 30],   encounter difficulties in integrating objects with their attributes [36], face challenges in understanding spatial relations [14, 35], and demonstrate insensitivity to word order [42]. Our objective is also to evaluate vision-language   compositionality. However, we employ a more fine-grained   grounding task instead of a text-image matching task to explicitly assess whether the model correctly associates attributes and relations with the objects.   Weakly supervised visual grounding. Many prior methods [5, 9, 44] for weakly supervised visual grounding rely   on pre-trained object detectors for Region of Interest (ROI)   localization. These methods typically project text and ROIs   into a joint visual-textual embedding space, transforming   the grounding task into a retrieval task. However, detectorbased approaches neglect the holistic context of the entire   image and encounter challenges related to transfer and generalization when dealing with objects in distinct domains   or belonging to different classes.   In contrast, detectorfree methods address this issue by forgoing the use of object detectors. These methods perform dense localization   for the given query phrases, thereby generating grounding   heatmaps instead of ranking ROIs.   Some earlier works   [1, 3, 12, 45, 50] define and optimize auxiliary tasks using weakly supervised data.   While these auxiliary tasks   may not be identical to the grounding objective, optimizing them yields the desired results in visual grounding.   More recent research [38, 39] has leveraged VLMs trained   on large-scale visual-text alignment datasets.   These approaches use VLMs\u2019 explainability maps as pseudo-labels   to train grounding models, resulting in state-of-the-art performance in the field.   Gradient-based localization. The technique of pinpointing the most distinctive regions within an image for a   specific task serves as a prevalent approach to explain a   model\u2019s decision-making process visually.   Class activation maps (CAM) [52] was introduced as a technique to   yield weighted feature maps for various networks with minimal adjustments to the model. A subsequent enhancement,   Gradient-weighted Class Activation Mapping (Grad-CAM)   [37], further improves upon CAM by directly employing   gradients to produce weighted feature maps without necessitating model modifications or retraining.   The grounding heatmaps generated by these methods can be directly   14142   Figure 2. Examples from our ARPGrounding dataset, wherein   we primarily evaluate models\u2019 proficiency in comprehending three   types of compositionality. Each sample in ARPGrounding consists   of an image containing two intricately distinguishable objects. (a)   and (b) exemplify a sample of attribute composition, (c) and (d)   illustrate an example of relation composition, while (e) and (f)   demonstrate an example of priority composition. We test whether   the model can pick the correct object according to the text.   optimized to guide the model toward solutions that align   more closely with human-based annotations. Our proposed   method is also based on Grad-CAM heatmaps. We apply   the Grad-CAM algorithm to VLMs, enabling them to localize objects in images based on textual semantics. We   leverage Grad-CAM for fine-tuning VLMs to guide the generated heatmaps to be more distinguishable with regard to   samples that have different compositionality.   3. ARPGrounding Benchmark   In this section, we describe how ARPGrounding benchmark is constructed and how we use it to evaluate compositional grounding performance of VLMs.   3.1. Dataset   In order to construct a compositional grounding dataset,   we require images containing object bounding box annotations, along with corresponding attribute and relation annotations. Therefore we establish ARPGrounding benchmark upon Visual Genome (VG) [16], a large-scale dataset   comprising over 100,000 images, annotated with objects,   attributes, and relations. Additionally, we manually filter   the dataset when necessary to ensure data quality.   As shown in Figure 2, ARPGrounding contains three   types of compositionality: attributes, relations, and priorities. Each sample in ARPGrounding consists of an image containing two objects selected with bounding boxes.   These paired objects serve as mutual compositional distractors. Each sample includes distinct text corresponding to   these two objects, and these texts can be differentiated based   on attributes, relations, or priorities.   Attribute. Our primary goal is to discern two objects within   a single image. These objects belong to the same category   but possess distinct attributes. Our objective is to assess   the model\u2019s ability to accurately identify the correct object   based on the provided textual information. To locate such   object pairs, we initiate the process by traversing the scene   graph annotations of VG for a given image. We identify objects with identical class labels, excluding those occupying   less than 0.05% of the total image area, and then remove   object pairs that overlap. This procedure results in two nonoverlapping objects of the same class within the image, appropriately sized.   Subsequently, we aim to recognize attributes that can   effectively differentiate between these two objects.   A   straightforward approach assumes that distinct attribute   names lead to differentiation. However, we must exercise   caution as captions and scene graphs may not fully encompass all attributes [28]. Therefore, we need to carefully ascertain these attributes. For instance, if both objects are   \u201dfairy\u201d and \u201dwhite\u201d dogs, and one object is labeled solely   as \u201dfairy\u201d while the other one is only as \u201dwhite,\u201d these attributes, although different, may not effectively distinguish   between the objects. To address this concern, we leverage   WordNet [29] to confirm that the attributes of both objects   share the same grand hypernym while being distinct from   one another. Finally, we concatenate the attribute and object name to form a concise and accurate text description   for the respective object. For quantitative analysis, we only   use one attribute for each object.   Relation. The text pair for relation compositionality probing has to meet the requirement: the two texts should be   distinguishable from each other solely based on their relation. Firstly, the two texts need to differentiate from each   other through their relation to test the model\u2019s understanding of relation compositionality. Additionally, for quantitative analysis, the text should be identical in all aspects except for the relation. In this context, we primarily explore   the simplest case of text, specifically the object1-relationobject2 triplet scenario. To meet the requirements for quantitative testing, no attribute should be included in the text   as it could be utilized to distinguish between the two objects. Moreover, object1 in both texts should possess the   same class label but correspond to different objects in the   14143   image, thereby eliminating the influence of the object class.   Furthermore, object2 should be identical in both texts.   To identify suitable triplet pairs in the image, we traverse   all triplet pairs in the scene graph, ensuring that object1 in   two texts belong to the same category and that their bounding boxes do not overlap. We also verify that the relations   have distinct names and that object2 are the same. Simultaneously, we filter out bounding boxes with an area less than   0.05% of the total image area. However, we find that relations with distinct names can still have similar semantics,   such as \u201dcomputer above table\u201d and \u201dcomputer on top of   table,\u201d as well as \u201dgirl wearing jacket\u201d and \u201dgirl in jacket.\u201d   Therefore, we manually filter the relation data, ensuring that   objects in the pair can be distinguished based on text.   Priority. While testing models with the relation dataset, we   observed that models were not only influenced by the relation but also affected by another object present in the text.   When instructing the model to ground a triplet: object1relation-object2, both instances of object1 are highlighted,   demonstrating the model\u2019s inability to reason about the relation. Furthermore, object2 is also highlighted, indicating the model\u2019s deficiency in reasoning about the subject   among nouns. To investigate this issue, we propose the priority dataset. By utilizing the scene graph of an image,   we can easily obtain object1-relation-object2 triplet pairs   that reverse the positions of object1 and object2. However,   we have observed that the relations in these pairs may not   necessarily align with each other. On one hand, enforcing strict relation consistency could lead to violations of   visual semantics, as exemplified by the contradicting meanings in texts like \u201dslow cooker on top of microwave\u201d and   \u201dmicrowave on top of slow cooker.\u201d On the other hand, the   relationship between objects plays a crucial role in distinguishing the subject among nouns within a given text.   We utilize VG to construct our ARPGrounding datasets.   Within the 108,249 images from VG, we meticulously gathered a total of 6,632 samples for attribute, 370 samples for   relation, and 4,423 samples for priority. Each sample consists of two object-text pairs, which pertain to distinct object   instances within an image, yet exhibit ambiguity in terms   of compositionality. The relatively lower number of relation samples compared to the other two categories can be   attributed to the fact that in cases involving two ambiguous objects within an image, it is considerably more likely   for them to possess distinguishable attributes but less likely   for them to exhibit a distinguishable relation with another   object. More detailed statistics of ARPGrounding can be   found in the Appendix.   3.2. Metric   The performance on ARPGrounding is evaluated based   on the mean activation values inside the bounding boxes of   the heatmaps. Each sample in ARPGrounding comprises   two objects, each represented by a bounding box and the   associated text. Correct association between the texts and   their respective bounding boxes defines a sample\u2019s correctness. Given M_0 , M_1 represent binary masks derived from   object bounding boxes and H_0 , H_1 denote the generated   heatmaps, where masks and heatmaps match the image size,   the mean activation of M_i and H_j is defined as:     act(M _i,     H   _ j)       = \\ frac {1}{\\sum {M_i}}\\sum {M_i \\odot H_j}    (1)   The \\odot is element-wise multiplication. The score of a sample is computed based on mean activation combinations:     \\be gin  {a lig n   e   d   }       f   ( M_ 0,H_0,M _1, H _1) = \\ beg   in {cases}  1 \\ text { if    }  act(M_0, H_0) > act(M_1, H_0) \\\\ \\ \\ \\ \\ \\text {and } act(M_0, H_1) < act(M_1, H_1) \\\\ 0\\text { otherwise} \\end {cases} \\end {aligned}    (2)   This metric tests whether the ground truth object for a given   text is scored higher than the alternative object in the image   and whether this holds for the other object-text pair in the   sample as well.   Finally, we compute the mean accuracy across the entire   dataset:     a c c       =    \\fra c { 1}{ N} \\sum {f(M_0,H_0,M_1,H_1)}    (3)   3.3. Gradient-based Localization   To enable VLMs to discern objects in the image, we   employ explainability techniques to analyze the VLMs.   Our approach adopts the widely used Grad-CAM algorithm   [37], seamlessly integrating it into VLMs without introducing additional complexity. While Grad-CAM was originally   tailored for single-modal Convolutional Neural Networks   [17], we guide the model to focus on locating the targets described by semantically intricate textual expressions by utilizing the gradient flow from texts to the intermediate layer   of vision transformer.   To compute the grounding heatmap, we first extract an   intermediate attention map A_z in the multimodal transformer \\phi _f and denote this function as f_z, the input image   and text are symbolized as \\protect \\boldsymbol  {v} and \\protect \\boldsymbol  {t} respectively:     A _z = f_z (\\phi _f(\\boldsymbol {v}, \\boldsymbol {t}))    (4)   Then, we calculate the gradient of A_z with respect to the   score of image-text pair. The score denotes the image-text   similarity in the case of being trained with image-text contrastive loss or signifies the image-text matching score when   trained with image-text matching loss, depending on the   specific model. We represent the score uniformly as y and   the gradient is calculated as:     G _z =  \\   nabla A_z = \\frac {\\partial {y}}{\\partial {A_z}}    (5)   14144   Figure 3. Examples of dependency parsing using spaCy [11] (acl:   clausal modifier of a noun, dobj: direct object of a verb, det: determiner, amod: adjectival modifier of a noun). Each arc connects a   parent node to a child node. We utilize the root of the dependency   tree, namely, \u201dman\u201d and \u201dclock,\u201d to ascertain each element in the   pair describes different objects. Bounding boxes serve solely for   visualization purposes.   Next, we calculate the heatmap H using A_z and G_z as follows:     H = \\ma t hrm {ReLU} (A_z \\odot G_z)    (6)   where \\odot is an element-wise multiplicaiton. This heatmap   is resized to the resolution of input images and identifies   which area in the image explains the model decision for its   matching score.   4. Composition-aware Fine-tuning   Regarding the identified challenges in VLMs when it   comes to the task of compositional grounding, we suggest a direct solution involving weak supervision.   Our   composition-aware fine-tuning pipeline first generates a text   pair for each image where each component in the pair describes distinct objects. Then we introduce a pretext task   that is aimed at augmenting diversity within the grounding heatmaps of these two texts. This pipeline significantly   aids in enhancing the VLMs\u2019 understanding of compositional semantics. In the following section, we delineate the   methodologies for text pair generation and elaborate on the   loss functions utilized during the training process.   Text pair generation. Existing image-text datasets often   feature multiple descriptions for the same image. We generate text pairs by sampling different texts for each image.   Incorporating dependency parsing into the text sampling   phase, we analyze the grammatical structure of a sentence to   identify associated words and ascertain their relationships.   An illustration of dependency parsing with spaCy [11] is   depicted in Figure 3. For a given image, all the texts of the   image are represented in a tree structure, with directed arcs   representing the connections between each word in the sentence. We leverage the root of the dependency tree to formulate text pairs that describe distinct objects. Those pairs   contain different objects that are associated with various attributes, relations, and priorities, and provide rich compositional information for the subsequent training phase.   Loss.   To harness the rich compositional information in   the text pair, we propose a pretext task to induce variability in the generated heatmaps. Considering a training   batch crafted through the aforementioned procedure \\ p   rotec t    \\m at   hcal     {B} = \\{(\\boldsymbol {v}^i, \\boldsymbol {t}^i_0, \\boldsymbol {t}^i_1)\\}_{i=1}^n, where \\protect \\boldsymbol  {v}^i represents the image and \\p   ro te   ct \\boldsymbol  {t}^i_0, \\boldsymbol {t}^i_1   denote the text pair, we proceed with fine-tuning VLMs utilizing the following loss functions:     \\mathcal {L}    =    E _   {   (\\ b oldsymbol {v}, \\boldsymbol {t}_0, \\boldsymbol {t}_1) \\sim \\mathcal {B}}[\\frac {1}{w \\cdot h} \\sum {H_0 \\odot H_1}]    (7)   Where H_0 , H_1 are the heatmaps generated corresponding   to \\protect \\boldsymbol  {t}_0 and \\protect \\boldsymbol  {t}_1 respectively.   w and h stand for the width   and height of H_0 , H_1. This novel loss leverages coarsegrained text descriptions instead of dense bounding box annotations, thereby applying only weakly supervised guidance to the heatmap. This loss function directs the model   to generate distinct heatmaps for diverse textual inputs, encompassing various objects, attributes, relations, and priorities. Consequently, it encourages the model\u2019s outputs to be   more discriminative and alleviates noise arising from compositional ambiguity within the generated heatmaps.   5. Experiments   In this section, we report the results of four state-of-theart VLMs on conventional visual grounding datasets and   ARPGrounding dataset. Our findings indicate that VLMs   demonstrate proficiency on conventional visual grounding   datasets but reveal limitations on ARPGrounding. We also   explore our fine-tuning strategy on both CLIP and ALBEF   to demonstrate its effectiveness.   5.1. Datasets and Models   Datasets. In addition to ARPGrounding, we employ three   datasets: VG, Flickr30k entities [33], and ReferIt [15] following Akbari et al. [1]. We follow Akbari et al. [1] to   report the pointing game [50] accuracy.   These datasets   serve as the foundation for demonstrating VLMs\u2019 performance on weakly supervised visual grounding tasks. As for   composition-aware fine-tuning, we use region descriptions   in VG to construct the text pair. We apply a filtering process   to the images within VG to ensure that there is no overlap   between the training data and the test splits of any other   datasets. Ultimately, we collect a total of 83,517 images for   fine-tuning.   Models.   We evaluate four VLMs.   Specifically, we assess OpenAI\u2019s CLIP [34]\u2014a dual-stream model pre-trained   on a dataset comprising 400 million image-text pairs with   contrastive objectives. Additionally, we examine ALBEF   14145   Table 1. Results on visual grounding and ARPGrounding datasets.   Method   Visual Grounding   ARPGrounding   VG   Flickr   ReferIt   Mean   Attribute   Relation   Priority   Mean   random   11.15   27.24   24.30   20.90   25.00   25.00   25.00   25.00   WWbL [39]   62.31   75.63   65.95   67.96   WWbL^{++} [38]   66.63   79.95   70.25   72.28   CLIP   57.46   75.26   56.77   63.16   42.78   9.19   11.24   21.07   ALBEF   75.04   84.49   69.26   76.26   61.25   29.19   14.94   35.13   METER   60.52   75.28   66.41   67.40   43.70   26.49   38.39   36.19   BLIP2   69.50   84.96   68.71   74.39   23.46   31.35   15.28   23.36   [18], which introduces contextualized multimodal fusion   with a co-attention mechanism and is additionally trained   with masked language modeling and image-text matching   losses. Furthermore, we evaluate METER [6], characterized by stacked transformer encoding layers on both visual and text encoders. Lastly, our analysis includes BLIP2   [21], where a Q-Former is connected to a frozen image encoder, aiming to achieve improved image-text alignment   while preserving robust visual representation.   5.2. Evaluation of VLMs   Models perform well on conventional visual grounding   datasets. In Table 1, we present models\u2019 performance on   VG, Flickr, and ReferIt to demonstrate their performance on   weakly supervised visual grounding datasets. We also calculate the mean score for convenient comparison. We adopt   the same pointing game accuracy metric and compare our   results against previous state-of-the-art methods WWbL^{++}   [38]. We find that all four VLMs can achieve performance   on par with or surpass WWbL^{++} without any training.   Both ALBEF and BLIP2 outperform WWbL^{++}, setting   new state-of-the-art benchmarks. Additionally, CLIP and   METER exhibit performance levels that are comparable to   WWbL^{++} according to the mean score.   Models exhibit deficiencies in compositional grounding   datasets. Models struggle across the board on ARPGrounding, often performing close to or below random chance. As   shown in Table 1, models exhibit suboptimal performance   when assessed in terms of their average performance across   three compositional categories. From a model-specific perspective, we observe a meager absolute improvement of approximately 10% compared to the random chance from the   two top-performing models, namely ALBEF and METER,   while the remaining models perform even worse than random chance. Models also demonstrate varying reasoning   capabilities when confronted with distinct forms of compositionality. Notably, models perform relatively better in the   attribute category with a mean score of 42.80, but their performance is comparatively poorer in relation and priority,   scoring mean values of 24.06 and 19.96, respectively, indicating that detecting relation and priority is a more challenging problem than attribute recognition. Overall, models   exhibit deficiencies in ARPGrounding.   5.3. Composition-aware Fine-tuning   Training details.   We employ composition-aware finetuning on CLIP and ALBEF to showcase its efficacy. In   particular, we initiate the fine-tuning of CLIP and ALBEF   using identical pre-trained weights as used in section 5.2.   Each image in the training dataset is associated with approximately 50 textual descriptions. We select two descriptions   per image, each delineating distinct objects. We implement   our framework on PyTorch [32] and train it for 10 epochs on   an NVIDIA A100 GPU with 80GB of memory. For CLIP,   we resize the input images to 224 \\times 224 and cap the maximum length of each text to 77 as suggested by Radford et al.   [34]. We use a batch size of 256 and a learning rate of 1e7. For ALBEF, we resize the input images to 384 \\times 384   and set the maximum length of each text to 30 as Li et al.   [18]. We use a batch size of 54 and a learning rate of 2e7. Both models are optimized using AdamW [27] optimizer   with 50 steps of warmup and a cosine-annealing learning   rate scheduler.   Table 2. Grounding results comparison of fine-tuning two stateof-the-art VLMs.   Method   Visual Grounding   ARPGrounding   VG   Flickr   ReferIt   Attribute   Relation   Priority   CLIP   57.46   75.26   56.77   42.78   9.19   11.24   TSVLC   57.57   75.11   56.78   42.85   9.46   11.15   DAC   58.50   76.91   58.33   42.63   11.35   8.70   CLIP caft. (Ours)   60.43   78.07   63.75   44.56   13.24   21.30   ALBEF   75.04   84.49   69.26   61.25   29.19   14.94   ALBEF caft. (Ours)   74.80   85.93   74.67   66.34   38.65   24.21   Results. In Table 2, we present a comparative analysis between CLIP and CLIP caft., as well as ALBEF and ALBEF caft., where \u201ccaft.\u201d denotes composition-aware finetuning.   Additionally, we include the results of TSVLC   [7] and DAC [8], both of which are based on CLIP and   have demonstrated promising outcomes in addressing com14146   positional challenges in image-text matching tasks.   For   CLIP, composition-aware fine-tuning enhances CLIP\u2019s performance on Attribute from 42.78 to 44.56, on Relation   from 9.19 to 13.24, and on Priority from 11.24 to 21.30.   Conversely, no significant improvement is observed for   TSVLC and DAC. For ALBEF, this methodology elevates   performance on Attribute from 61.25 to 66.34, on Relation   from 29.19 to 38.65, and on Priority from 14.94 to 24.21.   Notably, it substantially augments the efficacy of both models in conventional visual grounding datasets as well, only   ALBEF slightly decreases in VG by 0.24%. Overall, ALBEF caft. becomes the best model, in comparison to all   other models, and in ARPGrounding-Priority, it lags behind only METER. Figure 4 illustrates some visualization   results, more visualization results are shown in Appendix.   In general, our proposed composition-aware fine-tuning   yields significant improvements in ARPGrounding and also   enhances performance in conventional visual grounding   datasets.   Our results underscore the efficacy of extracting rich information from diverse textual sources related to   an image, even in the absence of dense annotations, leading to substantial advancements in compositional understanding. While diverse vision-language pre-training objectives contribute significantly to representation learning,   Ma et al. [28] suggests that merely expanding the scale   of pre-training datasets is ineffective in capturing compositional structures.   Hence, we furnish evidence supporting the notion that pursuing algorithmic enhancements can   augment model capacity through the utilization of costeffective image-text annotations.   5.4. Analysis of Compositional Grounding   In this section, we report the fine-grained score of attribute and relation of ARPGrounding, we also conduct a   case study on priority since categorizing priorities is not   straightforward.   We categorize attributes into action, color, material, size,   and state, and divide relations into action and spatial following Zhao et al. [51]. We use text that falls into these   categories to distinguish two objects, therefore the random   performance is 50%. As shown in Figure 3, for attributes,   models get relatively high sores on color and material, low   on action, size, and state, and perform worst on size. This   might stem from the need for scene-based reasoning when   comprehending size. Size-related terms (e.g., small, large,   giant) may lack consistency in visual size due to influences   such as camera angles and the distance between the object   and the camera. For relations, CLIP and METER excel in   spatial relations, while ALBEF and BLIP2 demonstrate superior performance in action relations. This suggests no   uniform tendencies in relation to grounding performance.   For priority, we plot the ten most frequent object pairs and   count the frequency of each object being chosen, as shown   Figure 4. Visualization of grounding results of ALBEF and ALBEF caft. From left to right, the three columns depict the original image, the heatmap generated by ALBEF, and the heatmap   generated by ALBEF caft. We demonstrate that our compositionaware fine-tuning pipeline effectively enhances compositional understanding.   Figure 5. Analysis of compositional grounding on attribute and   relation splits in ARPGrounding. We further categorize attributes   into action, color, material, size, and state. These are represented   in the figure with X-axis labels starting with \u2019A\u2019. Relations are   divided into action and spatial, denoted in the table with labels   starting with \u2019R\u2019. A horizontal dashed line signifies chance performance.   in Figure 6. We observe that models exhibit a bias toward   specific objects rather than adhering to priority considerations.   5.5. Ablation Studies   In this section, we present ablations to demonstrate   the choices in the proposed composition-aware fine-tuning   pipeline. Particularly we investigate how much does the   text pair generation process and the new pretext task benefit fine-tuning.   As described in section 4, the generation of a text pair involves utilizing a dependency parsing tree to sample a pair   14147   Figure 6. Analysis of compositional grounding on priority. We   plot the ten most frequent object pairs and count the frequency of   each object being chosen by CLIP. The more frequent object in   each pair was manually selected for leftward plotting.   Table 3. Ablation studies of text pair generation process. \\dagger , \\ddagger    represents fine-tuned with text pair (1), (2) respectively. We use   the same loss function with caft. in these ablation studies.   Method   Visual Grounding   ARPGrounding   VG   Flickr   ReferIt   Attribute   Relation   Priority   CLIP   57.46   75.26   56.77   42.78   9.19   11.24   CLIP \\dagger    55.67   75.11   53.05   45.67   9.46   5.63   CLIP \\ddagger    60.52   78.47   63.61   44.26   11.89   13.90   CLIP caft. (Ours)   60.43   78.07   63.75   44.56   13.24   21.30   ALBEF   75.04   84.49   69.26   61.25   29.19   14.94   ALBEF \\dagger    57.36   67.93   56.46   55.71   17.84   8.61   ALBEF \\ddagger    72.71   83.46   73.32   65.65   35.95   18.80   ALBEF caft. (Ours)   74.80   85.93   74.67   66.34   38.65   24.21   Table 4. Ablation studies of the new pretext task. \u201cft.\u201d represents fine-tuning CLIP with contrastive loss, and fine-tuning ALBEF with image-text matching loss. \u201ccaft.\u201d represents trained with   the new pretext task. We use the same training data in these ablation studies.   Method   Visual Grounding   ARPGrounding   VG   Flickr   ReferIt   Attribute   Relation   Priority   CLIP   57.46   75.26   56.77   42.78   9.19   11.24   CLIP ft.   57.53   76.35   60.20   43.15   11.35   6.67   CLIP caft. (Ours)   60.43   78.07   63.75   44.56   13.24   21.30   ALBEF   75.04   84.49   69.26   61.25   29.19   14.94   ALBEF ft.   65.77   73.58   63.89   58.46   30.54   8.82   ALBEF caft. (Ours)   74.80   85.93   74.67   66.34   38.65   24.21   of text that pertain to distinct objects. We design two text   pair variants including (1) text pair that refers to the same   object and (2) text pair that is randomly sampled. We follow the composition-aware fine-tuning pipeline but use different training data. Results of fine-tuning CLIP and ALBEF with these two variants are shown in Table 3. It can   be observed that fine-tuning with text pair (1) adversely affects grounding performance. Conversely, fine-tuning with   text pair (2) contributes to improved performance. This is   attributed to the fact that, on average, images in the VG   dataset contain approximately 35 objects, making random   sampling likely to yield pairs describing different objects.   However, the optimal results are achieved through sampling   via the dependency parsing tree. Ablation studies of the proposed pretext task are shown in Table 4. We provide CLIP   fine-tuned with contrastive learning loss and ALBEF finetuned with image-text matching loss on our generated text   pairs. The results indicate that both contrastive learning loss   and image-text matching loss diminish the grounding performance of both CLIP and ALBEF across nearly all test   datasets. In contrast, when CLIP and ALBEF are fine-tuned   on the novel pretext task, there is a noticeable enhancement   in performance across all test datasets. This proves that the   generated text pairs offer valuable compositional information. Moreover, the new pretext task effectively extracts this   information compared to contrastive learning and imagetext matching.   5.6. Fully Supervised Methods on ARPGrounding   Table 5. ARPGrounding results of GLIP and Grounding DINO.   Method   ARPGrounding   Attribute   Relation   Priority   GLIP-T   34.91   12.43   11.76   GLIP-L   37.15   9.46   12.34   Grounding-DINO-T   37.24   6.76   15.42   Grounding-DINO-B   47.03   13.78   25.68   To explore the performance of fully supervised methods on compositional grounding, we test GLIP [22] and   Grounding DINO [26] on ARPGrounding as shown in Table 5. We use models to generate the highest score bounding box of the text and compare the Intersection over Union   (IOU) between the positive bounding box and the negative   bounding box with regard to the text. We find these two   fully supervised visual grounding models do no better than   weakly supervised VLMs. The best performing fully supervised method, Grounding-DINO-B, falls short in attribute   against ALBEF, lacks competitiveness with ALBEF, METER, and BLIP2 in relation, and demonstrates inferior performance compared to METER in priority.   6. Conclusion   In this paper, we first employ explainability techniques   to achieve state-of-the-art results in weakly supervised visual grounding tasks using VLMs.   We then introduce   ARPGrounding, a novel benchmark, and reveal the limitations of VLMs in fine-grained visio-linguistic compositionality.   To address these challenges, we propose a   composition-aware fine-tuning pipeline that significantly   enhances VLMs\u2019 compositional understanding without relying on dense annotations.   Acknowledgment.   This   work   was   jointly   supported   by   the   National   Key   R&D   Program   of   China   (2022ZD0116309)   and   the   National   Natural   Science   Foundation   of   China   (62236010   and   62276261)   14148", "conf": "CVPR", "year": "2024", "index": 719}, {"title": "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers   Chongyan Chen1, Samreen Anjum2, Danna Gurari1,2   1 University of Texas at Austin   2 University of Colorado Boulder", "abstract": "Visual question answering is a task of predicting the answer to a question about an image. Given that different   people can provide different answers to a visual question,   we aim to better understand why with answer groundings.   We introduce the first dataset that visually grounds each   unique answer to each visual question, which we call VQAAnswerTherapy. We then propose two novel problems of   predicting whether a visual question has a single answer   grounding and localizing all answer groundings. We benchmark modern algorithms for these novel problems to show   where they succeed and struggle. The dataset and evaluation server can be found publicly at https://vizwiz.org/tasksand-datasets/vqa-answer-therapy/.   1.", "content": "Visual question answering (VQA) is the task of predicting the answer to a question about an image. A fundamental   challenge is how to account for when a visual question has   multiple natural language answers, a scenario shown to be   common [10]. Prior work [2] revealed reasons for these differences, including due to subjective or ambiguous visual   questions. However, it remains unclear to what extent answer differences arise because different visual content in an   image is described versus because the same visual content   is described differently (e.g., using different language).   Our work is designed to disentangle the vision problem   from other possible reasons that could lead to answer differences. To do so, we introduce the first dataset where all   valid answers to each visual question are grounded, meaning we segment for each answer the visual evidence in the   image needed to arrive at that answer. This new dataset,   which we call VQA-AnswerTherapy, consists of 5,825 visual questions from the popular VQAv2 [9] and VizWiz [11]   datasets. We find that 16% of the visual questions have multiple answer groundings, and provide fine-grained analysis   to better elucidate when and why this arises.   We also introduce two novel algorithmic challenges,   which are exemplified in Figure 1. First, we introduce the   Single Answer Grounding Challenge, which entails preFigure 1: Examples from our VQA-AnswerTherapy dataset   showing that visual questions with different natural language answers can have multiple answer groundings (first   row) or all share the same answer grounding (second row).   dicting for a visual question whether all valid answers will   describe the same grounding or not. Next, the Grounding   Answer(s) Challenge entails localizing the answer groundings for all valid answers to a visual question. We benchmarked models for these novel tasks to demonstrate the   baseline performance for modern architectures and to highlight where they are succeeding and struggling.   We offer this work as a valuable foundation for improving our understanding and handling of annotator differences. Success on our Single Answer Grounding ChalThis ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   15315   lenge will enable solutions to notify users when there is uncertainty which visual evidence to consider, enabling users   to clarify a visually ambiguous visual question. This can   immediately benefit visually impaired individuals since a   portion of our dataset comes from this population (i.e.,   VizWiz [11]). Success on the Answer(s) Grounding Challenge can enable users of VQA solutions to better understand the varied reasoning processes that can lead to different natural language answers, while also contributing to   enhanced model explainability and trustworthiness. More   generally, this work can inform how to account for annotator differences for other related tasks such as image captioning, visual dialog, and open-domain VQA (e.g., VQAs   found on Yahoo!Answers and Stack Exchange). This work   also contributes to ethical AI by enabling revisiting how   VQA models are developed and evaluated to consider the   diversity of plausible answer groundings rather than a single (typically majority) one. To facilitate future extensions,   we publicly-share our dataset and a public evaluation server   with leaderboard for our dataset challenges at the following link: https://vizwiz.org/tasks-and-datasets/vqa-answertherapy/.   2. Related Work   Answer Differences in VQA Datasets.   While many   datasets have been created to support the development of   VQA algorithms [22, 9, 11], a long-standing challenge has   been how to account for the common situation that, for   many visual questions, different answers are observed from   different people [10]. Prior work has offered initial steps.   For example, prior work characterized when [10], to what   extent [28], and why answers differ in mainstream VQA   datasets (e.g., for visual questions that are difficult, ambiguous, or subjective as well as answers that are synonymous) [2].   Other work introduced ways to evaluate   VQA models that acknowledge there can be multiple valid   answers, whether provided explicitly from different people [1, 17] or augmented automatically from NLP tools to   capture plausible, semantically related answers [17]. Another work focused on rewriting visual questions to remove   ambiguity regarding what are valid answers [23]. Complementing prior work, we explore answer differences in the   VQA task from the perspective of grounding, specifically   exploring whether different answers arise because different   visual content in an image is being described.   Answer Grounding Datasets.   Numerous datasets have   been proposed to support developing models that locate   the visual evidence humans rely on to answer visual questions. This has been motivated by observations that answer   groundings can serve as a valuable foundation for debugging VQA models, providing explanations for VQA model   predictions, protecting user privacy by enabling obfuscation   of irrelevant content in images, and facilitating search behaviors by highlighting relevant visual content in images.   A commonality of prior work [8, 19, 13, 7, 4, 34, 16, 12,   13, 19, 3, 5, 26] is that only one answer grounding for   one selected answer is provided for each visual question.   Our work, in contrast, acknowledges that a visual question   can have multiple valid answers and so multiple valid answer groundings. We introduce the first dataset where all   valid answers to each visual question are grounded. This   new dataset, which we call VQA-AnswerTherapy, enables   us to introduce two novel tasks of predicting for a given   visual question whether all answers will be based on the   same visual evidence and predicting for a visual question   the groundings for all valid answers.   Automated VQA Methods.   Modern automated VQA   models typically only return a single answer; e.g., the predicted answer with the highest probability from a softmax   output layer of a neural network. Yet, people often ask visual questions that lead to multiple valid answers [10]. To   account for this practical reality, we propose novel tasks   and introduce the first models for sharing richer information with end users by (1) indicating when there are multiple plausible answer groundings to a visual question and (2)   locating those grounded regions in images.   3. VQA-AnswerTherapy Dataset   3.1. Dataset Creation   VQA Source.   Our work builds upon two popular VQA   datasets that reflect two distinct scenarios:   VizWizVQA [11] and VQAv2 [9]. The images and questions of   the VizWiz-VQA dataset come from visually impaired people who shared them in authentic use cases where they were   trying to obtain assistance in understanding their visual surroundings.   In contrast, the images and questions of the   VQAv2 dataset come from different sources: while the images come from the MS COCO dataset [6] (and so were   collected from the Internet), the questions were generated   by crowd workers. Despite these differences, these datasets   have in common that they both include for each imagequestion pair 10 crowdsourced answers, each of which was   curated based on the same crowdsourcing interface.   VQA Filtering.   Our goal is to unambiguously ground   each answer for visual questions that have more than one   valid answer. To focus on these visual questions of interest,   we applied filters to the original VQA sources, which consist of 32,842 image-question pairs for VizWiz-VQA and   443,757 for VQAv2 training dataset.   First, we removed   answers indicating a visual question is unanswerable (i.e.,   \u201cunsuitable\u201d or \u201cunanswerable\u201d). Then, we only focused on   the remaining visual questions that have two or more valid   natural language answers, where we define valid answers   15316   as those for which at least two out of the ten crowdworkers   gave the exact same answer (i.e., using string matching).   1 Similar to prior work [3], we also filter visual questions   that embed multiple sub-questions. An example is \u201cHow   big is my TV and what is on the screen, and what is the   model number, and what brand is it?\u201d Following [3], we   removed visual questions with more than five words and   the word \u201cand\u201d, trimmed visual questions containing a repeated question down to a single question ( e.g., from \u201cwhat   is this? what is this?\u201d to \u201cwhat is this?\u201d), and filtered visual   questions flagged as \u201ccontaining more than one question\u201d   in metadata provided by [3].   We then selected 27,741 visual questions with 60,526   unique answers as candidates for our dataset.   Included   are all visual questions from VizWiz-VQA that met the   aforementioned criteria (i.e., 9,528 visual questions with   20,930 unique answers) and a similar amount sampled from   VQAv2\u2019s training set (i.e., 18,213 visual questions with   39,596 unique answers). We included all visual questions   used in [2], which indicates why answers to each visual   question differ, to support downstream analysis.   Answer Grounding Task Design.   We designed a user interface to ground the different answers for each visual question. It presents the image-question pair alongside one of its   associated answers at a time.   For each answer, two questions were asked to ensure the   answer could be unambiguously grounded to one region.   First, a worker had to indicate if a given answer is correct   or not. If correct, then the worker had to specify how many   polygons must be drawn to ground the answer from the following options: zero, one, or more than one. To simplify the   task, we only instructed the worker to ground the answer   when exactly one polygon was needed to ground answer.   We leave future work to explore when there are multiple   polygons (e.g., \u201cHow much money is there?\u201d for an image   showing multiple coins).   To ground an answer, a worker was instructed to click   a series of points on the image to create a connected polygon. After one answer grounding was generated for a visual   question, the annotator could then choose for a new answer   to select a previously drawn polygon or draw a new polygon. Instructions were provided for how to complete the   task, including for many challenging annotation scenarios   (e.g., objects with holes or complex boundaries).   Answer Grounding Annotation Collection.   We hired   crowd workers from Amazon Mechanical Turk to generate answer groundings, given their on-demand availability.   Like prior work [3], we only accepted workers from the   1We follow the status quo established by prior work [3, 10] to obtain   valid answers by using exact string matching (ESM) to provide an upper bound for expected differences). Around 36% of visual questions in   VizWiz and VQAv2 datasets have more than one valid answer.   United States who had completed at least 500 Human Intelligence Tasks (HITs) with over a 95% acceptance rate.   For each candidate worker, we provided a one-on-one zoom   training on our task. We then provided a qualifying annotation test to verify workers understood the instructions, and   only accepted workers who passed this test.   For annotation of our VQAs, we collected two answer   groundings per image-question-answer triplet to enable examination of whether the annotations match and so are   likely unambiguous, high-quality results. To support the ongoing collection of high-quality results, we also conducted   both manual and automated quality control mechanisms.   Ground Truth Generation.   We analyzed the two sets   of annotations collected for each of the 27,741 visual questions to establish ground truths. We did this after removing answers that at least one person flagged as \u201cincorrect\u201d   and visual questions with answers referring to no polygon   or multiple polygons. This left 12,290 visual questions and   26,682 unique image-question-answer triplets. For each answer, we calculated the intersection-over-union (IoU) between the two answer groundings. If the IoU was large   (i.e., equal to or larger than 75%), we used the larger of the   two groundings as ground truth since often the smaller one   is contained in the larger one. Otherwise, we deemed that   answer has an ambiguous grounding and so removed the   answer from our dataset. Examples of high-quality answer   grounding results are shown in Figure 2, where answers to   a visual question can either have multiple groundings (e.g.,   \u201cWhat is over the elephant\u201d and \u201cWhat does this logo say\u201d)   or a single grounding (e.g., \u201cshirt\u2019s color\u201d).   Figure 2: High-quality annotations from our dataset. These   also illustrate a trend that visual questions related to text   recognition often have multiple answer groundings while   recognizing color often have a single grounding.   15317   All   VQAv2   VizWiz-VQA   Multiple   Top-1   What is this?   What is the man wearing?   What is this?   Top-2   What is in this box?   What is on the table?   What is in this box?   Top-3   What does this say?   Where is the pizza?   What does this say?   Top-4   What is it?   What does the street sign say?   What is it?   Top-5   What kind of coffee is this?   What does the sign say?   What kind of coffee is this?   Single   Top-1   What is this?   What color is the train?   What is this?   Top-2   What color is this?   What color is the cat?   What color is this?   Top-3   What is it?   What is the man holding?   What is it?   Top-4   What\u2019s this?   What room is this?   What color is this shirt?   Top-5   What color is this shirt?   What color is the bus?   What color is this shirt?   Table 1: The five most common questions that lead to multiple answer groundings and a single answer grounding for all   visual questions as well as for VQAv2 and VizWiz-VQA independently. Of note, the overall frequency is dominated by   VizWiz-VQA\u2019s frequency since the most common questions is far larger for this dataset than observed for VQAv2.   3.2. Dataset Analysis   We now analyze our final dataset, which includes   5,825 visual questions with 12,511 unique visual-questionanswer-grounding sets. This includes 7,426 answer groundings for 3,442 visual questions from VizWiz-VQA dataset   and 5,085 answer groundings for 2,383 visual questions   from VQAv2 dataset. This final dataset excludes all visual   questions with less than two unique answers.   Prevalence of Single Versus Multiple Groundings.   We   first explore how often visual questions have different   answers describing the same visual evidence (a single   grounding) versus different visual evidence (multiple answer groundings). We flag a visual question as having different answers describing the same grounding if an answer   grounding pair has an IoU score larger than 0.9.   We found 15.7% (i.e., 916/5,825) of visual questions   with answers leading to mulitiple answer groundings. Yet,   the status quo for VQA research neglects this reality that   different answers can refer to different visual evidence [8,   19, 13, 7, 4, 34, 16, 12, 8, 13, 19, 3, 5, 26]. We suspect existing models would struggle with these 15.7% questions,   both for VQA and answer grounding, due to their visual   ambiguity.   We next identify the most common questions for visual questions that have multiple as well as a single answer grounding. To do so, we tally how often each question   leads to different answer groundings as well as to a single   answer grounding respectively. Results are shown in Table 1. We observe questions about recognizing objects is   common for both scenarios. In contrast, questions about   recognizing text is more prevalent when there are multiple   answer groundings while questions about recognizing color   is more prevalent for visual questions with a single answer   grounding. We also observe that questions related to a location often leads to multiple answer groundings, as shown   in Table 1 (Top-3 \u201cWhere is the pizza\u201d) and exemplified in   Figure 1 (\u201cwhere is the man\u201d) and Figure 2 (\u201cWhat is over   the elephant\u201d). These finding suggests that a valuable predictive cue for AI models to predict whether there is a single   grounding or multiple groundings for all answers are identifying the vision skills needed to answer a visual question.   When comparing the trend for visual questions to have   multiple answer groundings across both data sources, we   observe it is more prevalent for visual questions coming   from VizWiz-VQA than VQAv2; i.e., it accounts for 22%   (i.e., 761/3,442) versus 7% (i.e., 155/2,383) of visual questions respectively. Consequently, multiple answer groundings are more common for an authentic VQA use case than   is captured in the most popular, yet contrived VQA dataset.   Reasons   Visual   Questions   Have   Multiple   Answer   Groundings.   We next analyze the 916 visual questions   that have more than one answer grounding. For each visual   question, we flag which relationship types arise between every possible answer grounding pair from the following options: disjoint, equal, contained, and intersected. We categorize an answer pair as disjoint when IoU equals 0, equal   when the value is larger than 0.9, contained when one region is part of the other region, such that the size of their   intersection is equal to the minimum of their sizes and the   size of their union is equal to the maximum of their sizes,   and intersected when 0.9\u2265IoU >0 and they do not have a   contained relationship.   We first tally how many relationship types each visual   question exhibits between its different answer grounding   pairs, overall as well as with respect to each VQA source.   Results are shown in Table 2. We find that most visual questions (i.e., 89%) have just one relationship type between   their answer groundings. We suspect it is because most of   the visual questions only have two valid answers, two answer groundings, and thus one kind of relationship. When   comparing results from the two VQA sources, we observe   VQAv2 has slightly more relationships than VizWiz-VQA   15318   All   VQAv2   VizWiz-VQA   1   89% (812)   86% (133)   89% (679)   2   11% (103)   14% (22)   11% (81)   3   0% (1)   0% (0)   0% (1)   Table 2: Number of different kinds of relationships that a   visual question\u2019s answers have, overall and per data source.   All   VQAv2   VizWiz-VQA   Disjoint   10% (99)   16% (28)   8% (71)   Intersected   67% (685)   60% (107)   68% (578)   Contained   15% (151)   12% (21)   15% (130)   Equal   8% (86)   12% (21)   8% (65)   Table 3: Percentage of visual questions with multiple answer groundings having each relationship type between its   answer groundings, overall and for each data source.   dataset. We suspect this is due to a more even percentage   distribution across the four types of relationships we analyzed, as shown in Table 3.   We next tally how many visual questions have each type   of relationship, overall as well as with respect to each VQA   source. Results are shown in Table 3. Overall, we find   VizWiz-VQA and VQAv2 have a similar distribution of   answer grounding relationships. The most common relationship between answer groundings for a visual question   is intersection, with this occurring for over half of the visual questions. This finding has important implications for   both human visual perception and model development. We   suspect that when multiple individuals provide different answers based on distinct visual evidence, they may be focusing on the same object while paying attention to distinct   details, resulting in an intersection of visual evidence.   Relationship Between Why Answers Differ and Number   of Answer Groundings.   We next analyze the tendency   for visual questions that lead to a single versus multiple answer groundings to be associated with various reasons why   natural language answers can differ. For each visual question, we obtain the reasons why answers can differ using   the following seven labels provided in the VQA-AnswerDifference dataset [2]: low-quality image (LQI), insufficient visual evidence (INV), difficult questions (DFF), ambiguous questions (AMB), subjective questions (SBJ), synonymous answers (SYN), and varying levels of answer   granularity (GRN).2,3 Results are shown in a bar chart in   Figure 3, with the left part showing percentages for visual questions that have multiple answer groundings and the   right part showing percentages for visual questions with a   2We exclude the reasons \u201cSpam answer\u201d and \u201cInvalid question\u201d because, by definition, they cannot have grounded answers.   3As done in [2], we assign labels using a 2-person threshold.   Figure 3: Relationship of whether a visual question has a   single grounding for all answers and reasons for different   answers for the VQAv2 and VizWiz dataset sources.   Figure 4:   Visual questions with one answer grounding   alongside annotations indicating why answers differ [2].   single answer grounding.   Overall, visual questions with multiple answer groundings commonly are associated with varying levels of answer   granularity (GRN), ambiguous questions (AMB), and synonymous answers (SYN). The nearly identical results for   AMB and SYN are not surprising since over 85% of VQAs   labeled as SYN also occur with AMB in both the VizWizVQA and VQAv2 datasets.   Visual questions labeled with difficult (DFF) tend to   share a single grounding. Intuitively, this makes sense as   there is consensus around what the question is asking about   but people simply struggle to know what is the correct answer. An example of this scenario is shown in Figure 4,   with the question \u201cwhat kind of bird is this?\u201d   When comparing results from the two VQA sources,   15319   Figure 5: Amount of multiple answer groundings per visual   question for four vision skills, overall and per dataset source   (VQAv2 and VizWiz-VQA).   we observe that VQAv2 and VizWiz-VQA have large differences (larger than 10%) for four reasons: GRN, SYN,   AMB, and LQI. Examples of visual questions that are labeled as AMB, SYN, GRN are exemplified in Figure 4 (col   1, 3, and 4).   Relationships Between Vision Skills Needed to Answer   a Visual Question and Number of Answer Groundings.   We next evaluate how the vision skills needed to answer   a visual question relate to whether a visual question has   a single grounding. The following labels for the four vision skills are provided in the VizWiz-VQA-Skills dataset   [31]: object recognition (OBJ), text recognition (TXT),   color recognition (COL), and counting (CNT). Following   [31] we use majority vote from the 5 annotations to determine the vision skill labels. We perform our analysis over   all visual questions as well as with respect to each VQA   source independently. Results are shown in Figure 5, based   on observed percentages for each VQA source.   Overall, we found that visual questions trying to read text   tend to have multiple answer groundings. One common example is visual questions about products, as exemplified in   Figure 1 (e.g., chips product). In contrast, questions related   to recognizing color tend to have a single answer grounding. We suspect people might express \u2018color\u2019 in different   ways because of individual or cultural differences, despite   often looking at the same region. For example, a question   asking \u201cWhat is the color of this cloth?\u201d might get different   of answers \u201ckhaki\u201d, \u201ctan\u201d, and \u201cbrown\u201d despite all referring   to the same region (i.e., the cloth).   We also evaluate relationships between visual questions   that result in multiple answer groundings with respect to   each of the four vision skills overall as well as with respect to each VQA source independently. We determine if   Figure 6: The heatmap table shows the percentage and the   number of relationships between answer groundings with   respect to each of the four vision skills for our dataset (overall) and for each image source (VQAv2 and VizWiz-VQA).   a visual question has a single grounding and what skills are   needed following the same process of the previous analysis.   Results are shown in Figure 6.4 Overall, we observe that   visual questions related to \u201ctext recognition\u201d and \u201cobject   recognition\u201d are more likely to have a \u201cdisjoint\u201d relationship compared to \u201ccolor recognition\u201d and \u201ccounting\u201d skills.   Examples are shown in Figure 1 (\u201ccabinets\u201d and \u201chood\u201d are   disjoint) and Figure 2 (\u201cblanket\u201d and \u201cumbrella\u201d are disjoint; \u201crsb\u201d and \u201croyal society for blind\u201d are disjoint).   4. Algorithm Benchmarking   Using the VQA-AnswerTherapy dataset, we now quantify how well modern architectures support two novel tasks   of (1) predicting if a visual question shares the same   grounding for all answers and (2) localizing all groundings   for all answers to a visual question.   Dataset Splits.   Our VQA-AnswerTherapy dataset contains 3,794, 646, and 1,385 for train/val/test sets, respectively. The visual questions from the VizWiz-VQA dataset   are split to match the train/val/test splits of the original   VizWiz-VQA dataset [11]. Our dataset also has visual questions originating from the training set of the VQAv2 dataset   [9], which is split into train/val/test splits using 70%, 10%,   and 20% of the data respectively.   4.1. Single Answer Grounding Challenge   The task is to predict if a visual question will result in answers that all share the same grounding. For completeness,   we also explore the predicting if a visual question will result   in answers that multiple groundings. We evaluate methods   4Results for VQAv2 may not be representative since only a small   amount of our dataset\u2019s visual questions have vision skill labels.   15320   Precision   Recall   Model Type:   ViLT   mPLUG-Owl   Na\u00a8\u0131ve (M)   Na\u00a8\u0131ve (S)   ViLT   mPLUG-Owl   Na\u00a8\u0131ve (M)   Na\u00a8\u0131ve (S)   All:S   0.86   0.80   0.80   0.94   0.82   0.0   1.0   VQAv2:S   0.93   0.93   0.92   0.97   0.81   0.0   1.0   VizWiz-VQA:S   0.82   0.74   0.74   0.92   0.83   0.0   1.0   All:M   0.59   0.20   0.20   0.37   0.20   1.0   0.0   VQAv2:M   0.24   0.11   0.08   0.11   0.26   1.0   0.0   VizWiz-VQA:M   0.63   0.25   0.26   0.42   0.17   1.0   0.0   Table 4: Performance of methods at predicting whether a visual question will result in answers that all share single (i.e., \u2018S\u2019)   or multiple (i.e., \u2018M\u2019) groundings respectively, overall as well as with respect to each data source. Of note, no values (\u2018-\u2019)   are entered for some models because they do not yield valid scores. This includes the Na\u00a8\u0131ve (M) for task \u2018S\u2019 with respect to   precision and Na\u00a8\u0131ve (S) for task \u2018M\u2019 with respect to precision, because no positives are predicted, making the denominator   zero (i.e., precision =  \\te xt {True Posit ive} / (\\text {True Positive} + \\text {False Positive}) . This also includes the Na\u00a8\u0131ve (M) model for task \u2018S\u2019 with   respect to recall and Na\u00a8\u0131ve (S) model for task \u2018M\u2019 with respect to recall, because there are no positives and so the numerator   is again zero (i.e., recall = True Positive / (True Positive + False Negative).   using two standard metrics for binary classification tasks:   precision and recall.   Models.   We benchmark four models. We fine-tune a top   performing algorithm for the VQA task, ViLT [15], on the   training set on of our entire dataset. To do so, we modified   the output layer of the architecture with a two-class softmax   activation to support binary classification. We also benchmark the state-of-the-art vision and language foundation   model which was the first to achieve human parity on the   VQA Challenge, mPLUG-Owl [29], in a zero-shot setting   with the prompt \u201cDo all plausible answers to the question   indicate the same visual content in this image?\u201d This zeroshot setting is a useful baseline because of the imbalanced   and relatively small size of our dataset to support training.   We finally benchmark two na\u00a8\u0131ve baselines that each only   predict one label, i.e., all samples share the same answer   grounding or all samples have multiple answer groundings.   Results.   Results are reported in Table 4 for predicting   whether a visual question has answers that all share single   grounding (\u2018S\u2019) and predicting whether a visual question   has answers with multiple groundings (\u2018M\u2019). Testing results   are reported for the entire dataset, as well as on each VQA   source (VQAv2 and VizWiz-VQA) independently.   We observe that it is much more difficult to predict when   answers have multiple groundings compared to a single   grounding, i.e., both ViLT and mPLUG-Owl receive much   lower precision and recall when predicting whether there   are multiple answers grounding compared to predicting if   there is a single answer grounding. This is true both for   the fine-tuned ViLT model, which is susceptible to failing   from the data imbalance (i.e., only 15.7% of visual questions have multiple answers grounding), as well as the the   zero-shot mPLUG-Owl solution.   Figure 7: Qualitative results for the fine-tuned ViLT model   on the Single Answer Grounding Challenge, alongside the   question-answer pair and ground truth answer groundings.   We next analyze overall performance for the models. While ViLT is an inferior VQA model compared to   mPLUG-Owl, it achieves better performance once finetuned on our dataset for our novel task.   This enhancement is striking, considering the limited size of our dataset.   This observation underscores the value of our dataset, illustrating how even a modest number of samples can bolster   the robustness of current models.   In contrast, the foundation model, mPLUG-Owl, achieves inferior or comparable performance to a na\u00a8\u0131ve baseline. We manually inspected all examples where the top-performing fine-tuned   ViLT struggles, and show examples in Figure 7. For instances where there is a single answer grounding and ViLT   predicts multiple answer groundings, across both VizWizVQA and VQAv2 sources, often images show text or multi15321   ple objects while the question typically", "conf": "ICCV", "year": "2023", "index": 235}, {"title": "Language Models can Solve Computer Tasks   Geunwoo Kim   University of California, Irvine   kgw@uci.edu   Pierre Baldi   University of California, Irvine   pfbaldi@ics.uci.edu   Stephen McAleer\u2217   Carnegie Mellon University   smcaleer@cs.cmu.edu", "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency   and productivity by automating repetitive tasks and assisting in complex problemsolving. Ideally, such agents should be able to solve new computer tasks presented   to them through natural language commands. However, previous approaches to   this problem require large amounts of expert demonstrations and task-specific   reward functions, both of which are impractical for new tasks. In this work, we   show that a pre-trained large language model (LLM) agent can execute computer   tasks guided by natural language using a simple prompting scheme where the   agent Recursively Criticizes and Improves its output (RCI). The RCI approach   significantly outperforms existing LLM methods for automating computer tasks and   surpasses supervised learning (SL) and reinforcement learning (RL) approaches on   the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the   InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful   of demonstrations per task rather than tens of thousands, and without a task-specific   reward function. Furthermore, we demonstrate RCI prompting\u2019s effectiveness in   enhancing LLMs\u2019 reasoning abilities on a suite of natural language reasoning tasks,   outperforming chain of thought (CoT) prompting with external feedback. We find   that RCI combined with CoT performs better than either separately. Our code can   be found here: https://github.com/posgnu/rci-agent.   1", "content": "A long-standing goal in artificial intelligence has been to create generally-intelligent agents that can   accomplish cognitive tasks as well as humans. Such agents should be able to solve any computer task   a human can by communicating via natural language. By automating repetitive tasks and providing   assistance in complex problem-solving, generally-intelligent virtual agents may radically increase   productivity.   Recently, large language models (LLMs) have shown remarkable in-context learning capabilities   across a variety of domains and tasks [12, 69, 5, 17, 26, 64, 8, 46, 6]. Although LLMs can impressively   manipulate text and can use high-level API tools [59, 48, 41], previous approaches to using LLMs   that directly take keyboard and mouse actions on computers have had difficulty compared to imitation   learning and reinforcement learning approaches [24]. LLMs that take keyboard and mouse actions on   computers face a number of obstacles, such as ensuring that generated actions are task-appropriate   (task grounding), feasible in the agent\u2019s current state (state grounding), and admissible to be executed   (agent grounding).   \u2217Corresponding author.   37th Conference on Neural Information Processing Systems (NeurIPS 2023).   Figure 1: MiniWoB++ environment. Every task contains a natural language prompt in yellow. The   agent then uses keyboard strokes and mouse clicks to accomplish the task.   The previous best-performing approaches for taking actions on computers have not used LLMs.   Instead, they have trained networks from scratch to predict actions given prompts and screenshots or   DOM information, either via supervised learning (SL) from expert demonstrations, reinforcement   learning (RL) on a handcrafted reward signal, or both (SL+RL) [30]. Although SL+RL works well   on a number of individual computer tasks, since it requires expert data and a reward function for   every task, it has not been shown to generalize to novel tasks in a few-shot setting.   In this work, we show that a pre-trained LLM agent can successfully execute computer tasks   guided by natural language. Our method employs a simple prompting scheme, which we call   Recursive Criticism and Improvement (RCI), that significantly outperforms existing LLM methods   for automating computer tasks. RCI works by first having the LLM generate an output based on   zero-shot prompting. Then, RCI prompts the LLM to identify problems with the given output. After   the LLM has identified problems with the output, RCI prompts the LLM to generate an updated   output.   When applying RCI to computer tasks, we improve task grounding, state grounding, and agent   grounding sequentially. Firstly, task grounding prompts the LLM with the task text, instructing it to   generate a high-level plan. Secondly, state grounding connects high-level concepts derived from the   task grounding step with actual HTML elements present in the current state, subsequently outputting   the appropriate action. Finally, agent grounding ensures the correct formatting of the action output   obtained from the state grounding step. RCI is applied to each of these three steps; however, we find   that critiquing the state-grounding step is only necessary once.   We evaluate the RCI approach on the MiniWoB++ benchmark [61], and show it surpasses existing   SL, RL, and LLM approaches. Furthermore, it proves itself to state-of-the-art compared to existing   methods, using only a small number of demonstrations per task instead of tens of thousands, and   without relying on a task-specific reward function. This significant reduction in required demonstrations and the elimination of task-specific reward functions make our method more practical and   accessible for new tasks. Furthermore, as the capabilities of LLMs continue to improve, one can   expect the performance of our method to improve as well.   In addition to its success in automating computer tasks, we also showcase the effectiveness of RCI   prompting in enhancing the reasoning abilities of LLMs on a suite of natural language reasoning tasks.   When external feedback is given, our method achieves a significant performance increase over zeroshot prompting and slightly improves upon chain-of-thought [73] (CoT) prompting. Interestingly,   RCI and CoT have a synergistic effect, and their combination outperforms all other methods.   In summary, our work presents a new powerful and practical approach to enabling LLM agents to   execute computer tasks guided by natural language. The RCI prompting scheme not only outperforms   2   A:   Valerie's brother earns $10,000 per month, while Valerie herself   earns half of that amount. Together, they earn a total of $15,000 per   month. Their mother's monthly earnings amount to $30,000, which   is twice the combined salary of Valerie and her brother. Thus, the   total amount of money they all have together is $30,000. (Wrong)   <Few shot examples>   A:   Valerie earns $5000 per month, half of her brother's earnings,   making his monthly income $10000. Combined, Valerie and her   brother earn $15000 per month. Their mother earns double their   combined salary, amounting to $30000 per month. In total, they   have $30000 collectively. (Wrong)   A: Let's think step by step. First, Valerie's brother earns 2x and   Valerie earns x (half of her brother's earnings). Next, their mother's   income is twice the combined earnings of Valerie and her brother. In   this case, their total income amounts to $30,000. (Wrong)   A: Valerie's brother earns $10,000 per month, while Valerie herself   earns half of that amount. Together, they earn a total of $15,000 per   month. Their mother's monthly earnings amount to $30,000, which   is twice the combined salary of Valerie and her brother. Thus, the   total amount of money they all have together is $30,000.   Review your previous answer and find problems with your answer.   All individuals, including Valerie, her brother, and mother, should be   taken into account when calculating the overall sum of money.   Based on the problems you found, improve your answer.   Valerie's brother earns $10,000 per month, while Valerie herself   earns $5,000. Together, they earn a total of $15,000 per month. Their   mother earns double their combined salary, which amounts to   $30,000 per month. In total, they have $45,000 collectively. (Correct)   Few-Shot-CoT   Zero-Shot-CoT   RCI   Zero-Shot   Q: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of   money they all have together?   <Initial output generation>   <Critique>   <Improve>   Figure 2: Illustrative examples of explicit RCI prompting and baseline prompting approaches on   the GSM8K dataset. RCI prompting effectively addresses logical errors that arise in the baseline   prompting approaches. Prompts text is displayed in violet color.   previous methods in computer tasks, but also improves reasoning abilities for LLMs more broadly,   making it a significant contribution in the development of intelligent agents.   2   Methods   2.1   RCI Prompting   The self-critiquing ability of LLMs has demonstrated that LLMs can find errors in their own output   by themselves [58, 20, 3]. In light of this, we introduce a simple reasoning architecture called RCI   prompting, where we prompt LLMs to find problems in their output and improve the output based   on what they find. This architecture is designed to further enhance the reasoning ability of LLMs   by inserting a critique step before generating the final answer. Figure 2 compares example traces of   RCI prompting and baseline prompting methods on GSM8K dataset where language models should   answer grade school math problems. While baselines elicit answers with a single step of prompting,   RCI consists of two steps: criticize the previous answer (e.g., \"Review your previous answer and   find problems with your answer\") and improve the answer based on the critique (e.g., \"Based on   the problems you found, improve your answer\"). In this way, RCI prompting finds errors (e.g., the   overall sum of money only considered Valerie and her brother) in the previous answer and generates   an improved answer (e.g., money from Valerie\u2019s mother is included in the total) conditioned on   the critique. The iterative process of RCI can be continued until specific conditions are satisfied,   which could include receiving feedback from the environment, reaching the maximum predetermined   number of iterations, or adhering to certain heuristics. We define two approaches for achieving RCI:   explicit RCI and implicit RCI. Explicit RCI includes the critique in the prompt to generate improved   output and implicit RCI updates the previous output directly without sampling a critique explicitly.   Examples of explicit RCI and implicit RCI applied to computer tasks are illustrated in Figure 3 where   the action plan is improved based on an explicit critique while actions are updated without an explicit   critique. This process is described in more detail in the following section.   2.2   RCI for Computer Tasks   In this section we describe the application of RCI to computer tasks via a decomposition of action   selection into three reasoning steps: task grounding, state grounding, and agent grounding. The first   step, task grounding, involves generating a plan for task-solving and conditioning actions on this plan,   with RCI being used to improve the plan\u2019s success rate. The state grounding subsection discusses the   importance of grounding actions in the environment for language-based agents and how implicit RCI   is used to refine task-grounded actions to be feasible in the current state. Lastly, the agent grounding   3   Explicit RCI   Here is a plan to solve the task using   the instructions specified:   1. Type \"rm [filename]\" command to   delete the file ending with \".rb\".   2. Press \"enter\" key   Find problems with this plan.   This plan does not specify the filename   of the file ending with the extension   .rb. Without knowing the filename, the   agent cannot delete the file   Based on this, the improved plan for the agent to   complete the task are as follows.   1. Type \"ls\" command to list all files in the terminal.   2. Type \"rm [filename]\" command to delete the file   ending with \".rb\".   3. Press \"enter\" key after typing each command to   execute the command.   Plan   Critique   Improve   According to the current   plan,   the   next   proper   instruction should be `type   \"rm [filename]\"`.   Considering the output on the webpage,   the specific instruction for solving the   task should be `type` instruction with   \"rm code.rb\" command.   The task is to use the terminal below to delete a file ending with the extension .rb.   Task   Therefore, the single instruction   that matches one of the regular   expressions is `type \"rm code.rb\"`   3rd step   2nd step   1st step   Initial state   Current state   Task grounding   State grounding   Agent grounding   Implicit RCI   Figure 3: An illustrative execution trace of the agent for terminal tasks with RCI prompting. The   language model generates a step-by-step plan for the high-level task described in natural language,   which in this case involves using the terminal to delete a file ending with \".rb\". We then run an   explicit RCI on this plan, where we sample an improved plan based on the critique and the previous   plan, resulting in an improvement in the task-grounding of the plan. For each step, we first sample   the task-grounded action that follows the improved plan, and then the implicit RCI updates the   task-grounded actions sequentially to provide state-grounding and agent-grounding. Finally, the   agent-grounded action is executed by the instruction-following agent on the environment. The   prompts are highlighted, and the remaining text shows the outputs generated by the language model.   step focuses on ensuring that actions are admissible for the computer agent by employing implicit   RCI and conditioning agent-grounded actions on the current state, task, and other grounded actions,   with a loop count set to optimize performance.   2.2.1   Problem Setting   We assume that we are given an instruction-following computer agent that can execute a set of   admissible actions given some natural language instructions. An instruction that is not part of the   admissible actions will be ignored. At every step, we receive a high-level natural language task   prompt and a state of the environment. Given the current state and task, we sample the most probable   action from LLMs. The generated natural language action is then fed into the computer agent.   Sampling the actions in a fully generative manner presents a challenge, as the actions must consider   the given task, feasibility in the current state, and admissibility for the computer agent simultaneously.   Therefore, we propose decomposing this action sampling into three reasoning steps each of which   considers task grounding, state grounding, and agent grounding. Task grounding improves actions to   be more effective in solving the given task, state grounding ensures the feasibility of actions in the   current state, and agent grounding considers the executability of actions given the specification of the   computer agent. We first sample a step-by-step plan to solve the given task which improves the task   grounding. Next, the task-grounded action is sampled conditioned on the current state, task, and the   generated plan. The state-grounded actions is generated conditioned on the task-grounded action.   If the task-grounded action is not executable by the computer agent, the agent-grounded action is   sampled. For each sampling of grounded action, we use RCI prompting to make LLM consider some   specific information for grounding.   2.2.2   Grounding Language Model in Computer Tasks   Task grounding.   In the action sampling process, the first step involves generating a plan of   actionable steps for task solving from LLMs. Subsequently, actions are sampled from the same   LLMs, taking into account the present state, task, and generated plan. The benefits of conditioning on   4   the plan for improved grounding of actions are twofold. First, it enables LLMs to identify the stage   of task solving at which the agent is located, serving as a memory module. Second, we can perform   explicit RCI on the generated plan to further improve the plan\u2019s success rate. Although the number of   explicit RCI loops can be arbitrary, we observe that a single pass of explicit RCI suffices for most of   MiniWoB++ tasks.   State grounding.   In language-based agents, grounding actions in the environment is a crucial step   to enable real-world task performance. The aim of this phase is to enhance the task-grounded actions   to be feasible in the current state. Although the actions generated in the preceding phase may align   with the task, they may lack the specificity required to be executed in the current context. For example,   if the assigned task is to forward an email from Bob to Alice and the action obtained from the task   grounding phase is to click on an email from Bob in the email inbox, it is necessary to establish a   connection between the abstract concept of \"email from Bob\" and the concrete element, such as the   email heading, in the current webpage state represented by HTML. To achieve this goal, we perform   the implicit RCI and prompt the LLMs to consider the current state, which subsequently outputs   refined state-grounded actions. Moreover, the state-grounded action is additionally conditioned on   the task-grounded action. We avoid repeating the implicit RCI cycle more than once as it does not   impact the success rate based on our observations.   Agent grounding.   To ensure the successful integration of language-based methodologies in   decision-making processes, it is imperative to establish a scalable framework that guarantees the   admissibility of actions derived from the language model. While the preceding steps of sampling   produce a state-grounded action that is both feasible and grounded in the task, it may not be executable   by the agent due to issues such as improper formatting. To address this, Implicit RCI is employed,   whereby an agent-grounded action is sampled conditioned on the current state, task, task-grounded   action, and state-grounded action. The LLMs are prompted to consider specifications of the computer   agent. The implicit RCI is repeatedly run until the resulting action is executable, with a maximum   loop count set to limit the number of iterations. Empirical analysis on MiniWoB++ tasks suggests   that setting the loop count to 3 yields optimal performance.   3   Evaluation   3.1   Reasoning tasks   In our grounding enhancement process, RCI prompts the LLM to criticize its prior output, considering   the given context (e.g., current task, state, and agent), which ultimately leads to improved output. We   first demonstrate the effectiveness of RCI prompts in augmenting the reasoning capabilities of LLMs   across a range of reasoning benchmarks. We compare RCI to Chain-of-Thought (CoT) prompting, a   state-of-the-art method recognized for its effectiveness in reasoning tasks.   Specifically, we compare our approach with Few-Shot-CoT [73] where a few chain-of-thought   demonstrations are given as examples in prompting, and Zero-Shot-CoT [33] that elicit multiple   reasoning steps by simply adding \"Let\u2019s think step by step\" to the prompt. Following Kojima et   al. [33], our evaluation is conducted with 8 datasets from two categories of reasoning: arithmetic and   commonsense. Please refer to Appendix C.2 for a comprehensive depiction of the datasets. We use   the same experimental setting with their answer extraction method except that we use InstructGPT-3 +   RLHF (gpt-3.5-turbo) as the underlying language model. We use the same prompts that CoT uses and   we also use the answer cleansing approach used in CoT, but we only used answer extraction prompting   in zero-shot CoT experiments. We also use the same few-shot examples that were introduced in   [73] to evaluate Few-Shot CoT\u2019s performance on five arithmetic reasoning tasks. A threshold is   established by setting the maximum number of RCI loops to two, terminating the loop once the   output aligns with the ground-truth data. We observed that in the absence of this external feedback   mechanism, the RCI process is prone to false negative critics, subsequently leading to a decrease in   performance. Experimental results indicate that RCI without external feedback achieves zero-shot   performance in half of the benchmark tests, but underperforms in others, as shown in Appendix 17.   Comparison with Zero-Shot.   RCI prompting is better at solving reasoning tasks compared to zeroshot prompting. Table 1 summarizes the accuracy of our approach (Zero-Shot + RCI) and standard   zero-shot prompting for each reasoning benchmark. Zero-Shot + RCI substantially outperforms the   5   standard prompting in all benchmarks including arithmetic (GSM8K, MultiArith, AddSub, AQUA,   SVAMP, SingleEq) and common sense (CommonSenseQA, StrategyQA) tasks. RCI prompting   even achieves score gains from two arithmetic reasoning tasks (SingleEq and AddSub), which   do not require multi-step reasoning. This distinguishes our RCI prompting from the previous   CoT prompting methods [73, 33] that are not useful in simple reasoning tasks. It is also worth   noting that RCI prompting achieves a significant performance gain in commonsense reasoning tasks   (CommonSenseQA and StrategyQA). While Wei et al. [73] reported that only a substantially large   PaLM (540B) model can benefit from Few-Shot-CoT, RCI prompting can provide performance gain   even with a smaller InstructGPT-3 + RLHF (175B) model.   Arithmetic   Common Sense   GSM8K   MultiArith   AddSub   SVAMP   SingleEq   AQuA   CommonSenseQA   StrategyQA   Zero-Shot   77.95   94.48   88.58   80.70   86.61   60.23   64.56   48.81   Zero-Shot + RCI   85.43   97.64   89.76   84.65   94.49   67.32   68.11   61.81   Table 1: RCI prompting increases the reasoning capability of LLMs on all of eight reasoning   benchmarks.   Comparison with Chain-of-Thought.   The performance results of RCI and CoT baselines on   arithmetic reasoning tasks are summarized in Table 2. Notably, Zero-Shot + RCI outperforms   Zero-Shot CoT and Few-Shot CoT without any CoT prompting in four tasks except MultiArith.   In MultiArith tasks, where most of the standard prompting\u2019s answers are correct (96.06%), RCI   prompting does not yield significant performance gains. RCI prompting has a synergistic collaborative   impact on the two CoT baselines. Namely, Zero-Shot CoT + RCI and Few-Shot CoT + RCI attain the   highest scores on four out of the five tasks. These findings suggest a promising avenue for future   research: combining RCI with other prompting methods for CoT, such as self-consistency [58].   GSM8K   MultiArith   AddSub   SVAMP   SingleEq   Zero-Shot   78.35   96.06   85.83   78.35   91.34   Zero-Shot + RCI   85.43   97.64   89.76   84.65   94.49   Zero-Shot CoT   82.28   96.85   83.86   79.92   89.37   Zero-Shot CoT + RCI   86.22   97.24   89.88   85.83   90.94   Few-Shot CoT   80.31   98.82   89.37   83.46   91.73   Few-Shot CoT + RCI   84.25   99.21   90.55   87.40   93.70   Table 2: Chain-of-Thought prompting exhibits a synergistic effect when coupled with RCI prompting   in arithmetic reasoning tasks.   3.2   Computer tasks   3.2.1   Setup   MiniWoB++ benchmark suite.   The miniwob++ task suite is selected as the main benchmark to   evaluate our computer agent. MiniWoB++ [36], an extension of MiniWoB [61], is a web-based   simulation environment that offers a diverse range of computer tasks, from simple button-clicking to   complex compositional tasks requiring advanced reasoning, such as solving math problems. Its shared   action space, including keyboard and mouse, and a common state space centered around HTML code   enables our proposed agent to be thoroughly evaluated in ample tasks. Additionally, the varying levels   of complexity between tasks enable a systematic evaluation of our work. The action space consists of   two operations each of which controls the keyboard and mouse. The first action enables typing of   arbitrary characters or special keys such as Backspace and Enter. The second action involves moving   and clicking the mouse, allowing the agent to interact with visible HTML elements on a webpage.   All actions can be executed through natural language instructions defined by regular expressions that   are presented within the initial prompts provided to the LLMs. The regular expressions employed in   our evaluation are presented in Appendix D. Our action space definition is similar to previous works,   6   CC-Net   WebN-T5-3B   (no history)   WebN-T5-3B    CC-Net   Others   CC-Net   (no dictionary actions)     CC-Net   Ours   SL SotA   RL SotA   SL + RL SotA   Model   0.0   0.2   0.4   0.6   0.8   Average Success Rate   0.32   0.45   0.50   0.57   0.76   0.85   0.94   0.94   0.57   0.67   0.94   MiniWoB++ Average Performance   Few-Shot In-Context Learning (Ours)   RL + SL   SL   RL   (a)   10   1   10   2   10   3   10   4   Amount of Demos per Task   0.0   0.2   0.4   0.6   0.8   1.0   Average Success Rate   CC-Net (SL)   WebN-T5-3B   WebN-T5-3B   (no action history)   CC-Net (SL + RL)   World of bits   (SL + RL)   Workflow guided exploration   (SL + RL)   Ours   MiniWoB++ Average Performance    vs. Amount of Data   (b)   Figure 4: (a) Average performance comparison with baselines. Our agent with RCI prompting   achieves state-of-the-art performance in MiniWoB++ environment. The tasks that were included in   the averaging process are indicated in Table 18. (b) Relationship between performance and amount   of expert training data. Our agent displays comparable performance to the current state-of-the-art   scores on the MiniWoB++ benchmark, despite using the least amount of data.   such as [25, 32, 36], in which clicking actions directly interact with HTML elements. However, for   typing actions, we extend beyond simple form-filling by using keyboard-based typing actions. Instead   of relying on dictionary-based typing actions [30], where the agent simply chooses from a predefined   dictionary of texts, our approach requires the agent to predict the proper text input. Our approach,   therefore, has a better generalization capability for diverse computer tasks. The state space of our   agent consists solely of HTML code.   Model choices.   For the purpose of evaluating the effectiveness of RCI prompting, multiple language models are used in our experiments. Specifically, we employ three models, namely, GPT-3   (davinci) [5], InstructGPT-3 (text-davinci-002) [47, 72, 57], and InstructGPT-3 + RLHF (gpt-3.5turbo, gpt-4) [47]. Unless otherwise specified, we primarily evaluate our computer agent with the   InstructGPT-3 + RLHF models (gpt-3.5-turbo, gpt-4). Additionally, we use GPT-3 and InstructGPT-3   models for ablation studies. All the models were obtained through the OpenAI API, and further   details can be found in Appendix C.1.   Evaluated tasks.   We employ a set of 55 tasks to enable fair comparisons with baselines, as previous   works are only evaluated on a subset of tasks consistently. Furthermore, to assess the performance of   models on challenging tasks, we have selected tasks that involve free-form language typing actions,   which have been reported to have an almost-zero success rate in previous works (e.g., terminal).   Notably, certain commonly evaluated tasks in prior works are excluded due to the excessive length of   HTML code for some UI components, which are described in Appendix C.3.   Metrics   Consistent with prior studies, our main evaluation criterion is the success rate, which   measures the ability of our agent to actually complete the assigned task. This rate is calculated as the   proportion of successful episodes, which are defined as those in which the agent receives a positive   reward. We identified two modes of failure: the production of unexecutable actions and task failure.   When the agent generates an unexecutable action following the implicit RCI step, it fails immediately.   Moreover, an episode is considered unsuccessful when the agent, despite effectively executing the   plan generated, is unable to accomplish the task and thus receives no reward.   3.2.2   Outperforming baselines on MiniWoB++ task suite   We present Figure 4a which summarizes the average success rate of our agent and baseline models   over the MiniWoB++ benchmark. The results demonstrate significant outperformance of our approach   over supervised learning models. Specifically, we observe a 41% higher score than the WebN-T53B, which employs a finetuned large language model with 12K expert demonstration data. Our   approach also outperforms reinforcement learning approaches that require an order of magnitude   7   more interactions with the environment. Among all the baselines, our approach achieves the second   highest score. The sole model that surpasses our agent is the CC-Net, which involves co-training   of reinforcement learning and imitation learning. However, a direct comparison with CC-Net is   not possible since it uses dictionary-based typing actions. In other words, CC-Net selects text from   a predefined list for typing actions in some tasks, while our approach is fully generative. Thus,   CC-Net (without dictionary-based action) in Figure 4a serves as our appropriate comparison and we   outperform it by 6%. The performance data for CC-Net (with no dictionary-based action) is obtained   from the ablation study section in their paper [30].   Another comparative analysis is performed to evaluate the performance of our agent in contrast to the   state-of-the-art agents in three categories, namely supervised learning, reinforcement learning, and   a combination of both. To facilitate a fair comparison, we specifically isolate LLM-based state-ofthe-art approaches, which share similarities with our approach to solving computer tasks. The best   per-task performance achieved by each category is then aggregated, and the outcomes are presented   as SotA in Figure 4a. The result shows that our agent surpasses SotA by 37 percentage points in   supervised learning and by 27 percentage points in reinforcement learning. Notably, our proposed   RCI prompting method outperforms the SotA LLM approach [24], even when the latter employs   both finetuning and few-shot examples in prompts. This outcome highlights the effectiveness of our   approach in extracting vital knowledge for computer tasks from language models. Our agent even   achieves a slight edge over SotA (less than 1 percentage point) in the combined use of supervised and   reinforcement learning, which employs significantly more expert data and online interactions. We   also provide task-level performance comparisons in Figure 10, where tasks are arranged in ascending   order based on the difference between our agent\u2019s performance and the baseline. We observed three   main failure modes of our agent: (i) underperformance in tasks that require long-horizon planning   (e.g., guess-number, search-engine, use-spinner), (ii) difficulty in selecting appropriate actions for   tasks that require multi-step reasoning (e.g., tic-tac-toe, use-autocomplete), and (iii) lower scores in   tasks that rely on visual rendering of HTML code to solve the task (e.g., count-shape). These failures   are explained in more detail in Appendix F.   3.2.3   Lowest sample complexity   Figure 4b provides a comparative analysis of the total number of samples used in several models and   their mean performance. We begin by discussing CC-Net [30] model, which employs 2.4 million   expert demonstrations (equivalent to 6,300 hours) collected from 77 human participants across 104   tasks for behavior cloning. This amounts to an average of 23,076 demonstrations per task. In contrast,   the WebN-T5-3B [24] model uses 12,000 expert demonstrations to fine-tune its pre-trained T5 model.   Rather than directly updating model parameters with demonstration data, our approach involves   integrating two to three demonstrations into the prompt for in-context learning, which biases the   model output without any parameter updates. This approach allows our agent to generalize to unseen   tasks with only a handful of demonstrations. Our results show that our agent achieved a higher   success rate than all baselines, requiring 120x fewer samples than WebN-T5-3B and 11,000x fewer   samples than CC-Net. Given the challenges of obtaining expert demonstrations for computer tasks,   our findings demonstrate the practicality of our approach in automating such tasks.   3.2.4   Ablating the groundings   This section examines the impact of grounding improvement on task success rates. We conduct   ablations to isolate the contributions of task, state, and agent grounding improvements by eliminating   RCI prompting at each stage. We categorize tasks by three different difficulty levels to provide a   more detailed understanding of the effects of grounding improvements across a diverse range of tasks.   We conducted a task grounding ablation by eliminating the plan sampling stage. This modification   entails generating actions directly from the state, without the need for conditioning on a step-by-step   plan. State grounding is evaluated by directly applying the agent-grounding update to task-grounded   actions. Lastly, we ablate the implicit RCI of the agent grounding by letting the state-grounded   action be the final output of the agent. Figure 5 illustrates the performance degradation resulting   from each ablation of grounding. Our results indicate that each grounding contribution is essential   to solving computer tasks, with each contributing almost equally to the overall success rate. The   reason for this is partially due to the fact that the three methods of improving grounding are not   mutually exclusive, but rather complementary, with one enhancement in grounding contributing to   multiple action groundings. Examples of cross-grounding improvement are provided in Appendix E.   8   Moreover, it has been observed that state grounding plays a crucial role in enabling an agent to use   relevant information during episodes, particularly in scenarios where the initial state does not offer   sufficient information to accomplish the task, such as terminal task. Interestingly, task grounding   significantly improves the success rate when a task requires a long-horizon action plan, such as   the click checkboxes large task. We also observe that agent grounding significantly enhances the   feasibility of actions. Notably, in simpler tasks, the success rate decreases by 60% in contrast to the   baseline without the agent grounding. This finding is of particular significance as it distinguishes   our work from prior investigations [1, 28], which employ additional trained model components. In   contrast, our study solely relies on the reasoning ability of language models.   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Easy   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Medium   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Hard   Figure 5: Ablation analysis on the different types of grounding across tasks with varying degrees of   difficulty. The experimental design employs the use of InstructGPT-3 + RLHF model (gpt-3.5-turbo).   3.2.5   Ablating the language model   The performance of our agent is contingent on the quality of the underlying pre-trained language   models used, so enhancing language models can lead to an improvement in the agent\u2019s performance.   In this section, we present a comparison of the agent\u2019s performance using three distinct language   models: GPT-3, InstructGPT-3, and InstructGPT-3 + RLHF (gpt-3.5-turbo). Our objective is to   investigate the relationship between LLMs\u2019 capability and their ability to solve MiniWoB++ tasks.   The experimental setting employed in Section 3.2.4 is replicated in this study. Figure 6 depicts   the average success rate of three language models on tasks of varying difficulty levels. Our results   reveal that LLMs struggle to effectively complete tasks without instruction fine-tuning. This may   be attributed to the absence of intricate prompt engineering, as our observations have indicated   that GPT-3 displays sufficient competence in comprehending HTML code, regular expressions, and   engaging in reasoning.   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Easy   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Medium   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Hard   Figure 6: Ablation study on different language models across tasks of varying degrees of difficulty.   4   Limitations   In the course of our work, several limitations became apparent that may serve as potential avenues   for further research. One central concern is our primary focus on the InstructGPT-3 + RLHF models   (gpt-3.5-turbo, gpt-4), leaving the generalization ability of RCI to other models unexplored. The   versatility of RCI across diverse models remains a pertinent question, suggesting that future studies   should expand their scope to determine the robustness and adaptability of RCI. Handling lengthy   HTML presents another challenge. The current model grapples with extensive HTML states. While it   has been suggested that efficiency might be bolstered by pruning HTML states to exclude non-critical   9   elements, the task itself is non-trivial. A fundamental constraint of LLMs is the limited context   length, which can hamper handling extensive HTML states effectively. Addressing this may require   architectural adjustments or novel parsing methods. Our agent\u2019s action space, mainly restricted to   clicks and typing, limits its web navigation capabilities. There\u2019s a need to diversify its actions for a   more seamless experience. Furthermore, The agent\u2019s focus on short-term decisions overlooks the   necessity for long-term strategy, especially in tasks requiring coordinated sequences. Broadening   this focus is essential for versatile applications. Lastly, the intricate UI components populating   contemporary websites present a challenge for LLMs to fully understand the HTML states. The   subtle nuances of such components, which may not be discernible through HTML alone, underscore   the need for adding more modalities to the state definition. Addressing these issues is crucial to   enhance the RCI agent, making it more adaptable and efficient in practical applications.   5   Discussion   This work is part of a growing literature showing that LLMs might be all you need for hard decisionmaking problems [76]. In contrast to imitation learning and reinforcement learning approaches,   LLMs can solve novel tasks in a zero-shot or few-shot manner, and don\u2019t require task-dependent   expert data or a reward function. Furthermore, we expect that as the capabilities of LLMs and   foundation models increase, our method will naturally improve as well. However, we find that   current capabilities of LLMs aren\u2019t as powerful as task-dependent SL+RL approaches on some   computer tasks. Also, RCI is more expensive to run compared to approaches that just sample once   from the LLM. There are many avenues for future research in increasing the capacity of LLMs in   decision-making tasks. First, our experiments use LLMs on HTML code, but ideally methods based   on multimodal foundation models [16, 55, 2, 46] will be able to take actions based on text, images,   audio, and video as input [4, 18, 44, 71]. Second, the results presented in this paper all use pre-trained   LLMs. We expect the performance of our method to increase when using LLMs fine-tuned to solve   computer tasks.   Importantly, current LLMs are poor at reasoning tasks, such as playing tic-tac-toe, because they do   not think ahead. Although RCI improves reasoning capabilities in LLMs, there exists much work   to be done on increasing the reasoning capabilities in LLMs. This will be crucial to accomplish   hard cognitive tasks on computers that require thinking ahead. Similar to other prompting-based   approaches for reasoning in LLMs, RCI can be viewed as using the LLM\u2019s output to write to an   external memory, which is later retrieved to choose an action. LLMs with memory have been   demonstrated to be computationally universal [60], meaning that in principle all that is needed to   run arbitrary programs is the right prompt. Since RCI represents a basic version of this powerful   framework, we anticipate the development of more advanced RCI variations in the future. There   is a vast array of potential methods that repeatedly feed the output of particular prompts into the   LLM. For example, multiple different LLMs can simulate the information exchange between team   members in an organization. This would enable the merging of diverse perspectives to tackle complex   problems. In such a context, incorporating game theory and multi-agent systems research could   significantly enhance the overall performance. Reinforcement learning could be used to discover   effective structures involving loops and prompts [81], either through human feedback or a given   reward function. This optimization process can be further refined by exploring the space of potential   loop and prompt structures, identifying those that yield the best results, and fine-tuning the model   accordingly [75].   Acknowledgement   This material is based upon work supported by the National Science Foundation under Grant #2127309   to the Computing Research Association for the CIFellows 2021 Project.", "conf": "IJCAI", "year": "2023", "index": 466}, {"title": "Language Models can Solve Computer Tasks   Geunwoo Kim   University of California, Irvine   kgw@uci.edu   Pierre Baldi   University of California, Irvine   pfbaldi@ics.uci.edu   Stephen McAleer\u2217   Carnegie Mellon University   smcaleer@cs.cmu.edu", "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency   and productivity by automating repetitive tasks and assisting in complex problemsolving. Ideally, such agents should be able to solve new computer tasks presented   to them through natural language commands. However, previous approaches to   this problem require large amounts of expert demonstrations and task-specific   reward functions, both of which are impractical for new tasks. In this work, we   show that a pre-trained large language model (LLM) agent can execute computer   tasks guided by natural language using a simple prompting scheme where the   agent Recursively Criticizes and Improves its output (RCI). The RCI approach   significantly outperforms existing LLM methods for automating computer tasks and   surpasses supervised learning (SL) and reinforcement learning (RL) approaches on   the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the   InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful   of demonstrations per task rather than tens of thousands, and without a task-specific   reward function. Furthermore, we demonstrate RCI prompting\u2019s effectiveness in   enhancing LLMs\u2019 reasoning abilities on a suite of natural language reasoning tasks,   outperforming chain of thought (CoT) prompting with external feedback. We find   that RCI combined with CoT performs better than either separately. Our code can   be found here: https://github.com/posgnu/rci-agent.   1", "content": "A long-standing goal in artificial intelligence has been to create generally-intelligent agents that can   accomplish cognitive tasks as well as humans. Such agents should be able to solve any computer task   a human can by communicating via natural language. By automating repetitive tasks and providing   assistance in complex problem-solving, generally-intelligent virtual agents may radically increase   productivity.   Recently, large language models (LLMs) have shown remarkable in-context learning capabilities   across a variety of domains and tasks [12, 69, 5, 17, 26, 64, 8, 46, 6]. Although LLMs can impressively   manipulate text and can use high-level API tools [59, 48, 41], previous approaches to using LLMs   that directly take keyboard and mouse actions on computers have had difficulty compared to imitation   learning and reinforcement learning approaches [24]. LLMs that take keyboard and mouse actions on   computers face a number of obstacles, such as ensuring that generated actions are task-appropriate   (task grounding), feasible in the agent\u2019s current state (state grounding), and admissible to be executed   (agent grounding).   \u2217Corresponding author.   37th Conference on Neural Information Processing Systems (NeurIPS 2023).   Figure 1: MiniWoB++ environment. Every task contains a natural language prompt in yellow. The   agent then uses keyboard strokes and mouse clicks to accomplish the task.   The previous best-performing approaches for taking actions on computers have not used LLMs.   Instead, they have trained networks from scratch to predict actions given prompts and screenshots or   DOM information, either via supervised learning (SL) from expert demonstrations, reinforcement   learning (RL) on a handcrafted reward signal, or both (SL+RL) [30]. Although SL+RL works well   on a number of individual computer tasks, since it requires expert data and a reward function for   every task, it has not been shown to generalize to novel tasks in a few-shot setting.   In this work, we show that a pre-trained LLM agent can successfully execute computer tasks   guided by natural language. Our method employs a simple prompting scheme, which we call   Recursive Criticism and Improvement (RCI), that significantly outperforms existing LLM methods   for automating computer tasks. RCI works by first having the LLM generate an output based on   zero-shot prompting. Then, RCI prompts the LLM to identify problems with the given output. After   the LLM has identified problems with the output, RCI prompts the LLM to generate an updated   output.   When applying RCI to computer tasks, we improve task grounding, state grounding, and agent   grounding sequentially. Firstly, task grounding prompts the LLM with the task text, instructing it to   generate a high-level plan. Secondly, state grounding connects high-level concepts derived from the   task grounding step with actual HTML elements present in the current state, subsequently outputting   the appropriate action. Finally, agent grounding ensures the correct formatting of the action output   obtained from the state grounding step. RCI is applied to each of these three steps; however, we find   that critiquing the state-grounding step is only necessary once.   We evaluate the RCI approach on the MiniWoB++ benchmark [61], and show it surpasses existing   SL, RL, and LLM approaches. Furthermore, it proves itself to state-of-the-art compared to existing   methods, using only a small number of demonstrations per task instead of tens of thousands, and   without relying on a task-specific reward function. This significant reduction in required demonstrations and the elimination of task-specific reward functions make our method more practical and   accessible for new tasks. Furthermore, as the capabilities of LLMs continue to improve, one can   expect the performance of our method to improve as well.   In addition to its success in automating computer tasks, we also showcase the effectiveness of RCI   prompting in enhancing the reasoning abilities of LLMs on a suite of natural language reasoning tasks.   When external feedback is given, our method achieves a significant performance increase over zeroshot prompting and slightly improves upon chain-of-thought [73] (CoT) prompting. Interestingly,   RCI and CoT have a synergistic effect, and their combination outperforms all other methods.   In summary, our work presents a new powerful and practical approach to enabling LLM agents to   execute computer tasks guided by natural language. The RCI prompting scheme not only outperforms   2   A:   Valerie's brother earns $10,000 per month, while Valerie herself   earns half of that amount. Together, they earn a total of $15,000 per   month. Their mother's monthly earnings amount to $30,000, which   is twice the combined salary of Valerie and her brother. Thus, the   total amount of money they all have together is $30,000. (Wrong)   <Few shot examples>   A:   Valerie earns $5000 per month, half of her brother's earnings,   making his monthly income $10000. Combined, Valerie and her   brother earn $15000 per month. Their mother earns double their   combined salary, amounting to $30000 per month. In total, they   have $30000 collectively. (Wrong)   A: Let's think step by step. First, Valerie's brother earns 2x and   Valerie earns x (half of her brother's earnings). Next, their mother's   income is twice the combined earnings of Valerie and her brother. In   this case, their total income amounts to $30,000. (Wrong)   A: Valerie's brother earns $10,000 per month, while Valerie herself   earns half of that amount. Together, they earn a total of $15,000 per   month. Their mother's monthly earnings amount to $30,000, which   is twice the combined salary of Valerie and her brother. Thus, the   total amount of money they all have together is $30,000.   Review your previous answer and find problems with your answer.   All individuals, including Valerie, her brother, and mother, should be   taken into account when calculating the overall sum of money.   Based on the problems you found, improve your answer.   Valerie's brother earns $10,000 per month, while Valerie herself   earns $5,000. Together, they earn a total of $15,000 per month. Their   mother earns double their combined salary, which amounts to   $30,000 per month. In total, they have $45,000 collectively. (Correct)   Few-Shot-CoT   Zero-Shot-CoT   RCI   Zero-Shot   Q: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of   money they all have together?   <Initial output generation>   <Critique>   <Improve>   Figure 2: Illustrative examples of explicit RCI prompting and baseline prompting approaches on   the GSM8K dataset. RCI prompting effectively addresses logical errors that arise in the baseline   prompting approaches. Prompts text is displayed in violet color.   previous methods in computer tasks, but also improves reasoning abilities for LLMs more broadly,   making it a significant contribution in the development of intelligent agents.   2   Methods   2.1   RCI Prompting   The self-critiquing ability of LLMs has demonstrated that LLMs can find errors in their own output   by themselves [58, 20, 3]. In light of this, we introduce a simple reasoning architecture called RCI   prompting, where we prompt LLMs to find problems in their output and improve the output based   on what they find. This architecture is designed to further enhance the reasoning ability of LLMs   by inserting a critique step before generating the final answer. Figure 2 compares example traces of   RCI prompting and baseline prompting methods on GSM8K dataset where language models should   answer grade school math problems. While baselines elicit answers with a single step of prompting,   RCI consists of two steps: criticize the previous answer (e.g., \"Review your previous answer and   find problems with your answer\") and improve the answer based on the critique (e.g., \"Based on   the problems you found, improve your answer\"). In this way, RCI prompting finds errors (e.g., the   overall sum of money only considered Valerie and her brother) in the previous answer and generates   an improved answer (e.g., money from Valerie\u2019s mother is included in the total) conditioned on   the critique. The iterative process of RCI can be continued until specific conditions are satisfied,   which could include receiving feedback from the environment, reaching the maximum predetermined   number of iterations, or adhering to certain heuristics. We define two approaches for achieving RCI:   explicit RCI and implicit RCI. Explicit RCI includes the critique in the prompt to generate improved   output and implicit RCI updates the previous output directly without sampling a critique explicitly.   Examples of explicit RCI and implicit RCI applied to computer tasks are illustrated in Figure 3 where   the action plan is improved based on an explicit critique while actions are updated without an explicit   critique. This process is described in more detail in the following section.   2.2   RCI for Computer Tasks   In this section we describe the application of RCI to computer tasks via a decomposition of action   selection into three reasoning steps: task grounding, state grounding, and agent grounding. The first   step, task grounding, involves generating a plan for task-solving and conditioning actions on this plan,   with RCI being used to improve the plan\u2019s success rate. The state grounding subsection discusses the   importance of grounding actions in the environment for language-based agents and how implicit RCI   is used to refine task-grounded actions to be feasible in the current state. Lastly, the agent grounding   3   Explicit RCI   Here is a plan to solve the task using   the instructions specified:   1. Type \"rm [filename]\" command to   delete the file ending with \".rb\".   2. Press \"enter\" key   Find problems with this plan.   This plan does not specify the filename   of the file ending with the extension   .rb. Without knowing the filename, the   agent cannot delete the file   Based on this, the improved plan for the agent to   complete the task are as follows.   1. Type \"ls\" command to list all files in the terminal.   2. Type \"rm [filename]\" command to delete the file   ending with \".rb\".   3. Press \"enter\" key after typing each command to   execute the command.   Plan   Critique   Improve   According to the current   plan,   the   next   proper   instruction should be `type   \"rm [filename]\"`.   Considering the output on the webpage,   the specific instruction for solving the   task should be `type` instruction with   \"rm code.rb\" command.   The task is to use the terminal below to delete a file ending with the extension .rb.   Task   Therefore, the single instruction   that matches one of the regular   expressions is `type \"rm code.rb\"`   3rd step   2nd step   1st step   Initial state   Current state   Task grounding   State grounding   Agent grounding   Implicit RCI   Figure 3: An illustrative execution trace of the agent for terminal tasks with RCI prompting. The   language model generates a step-by-step plan for the high-level task described in natural language,   which in this case involves using the terminal to delete a file ending with \".rb\". We then run an   explicit RCI on this plan, where we sample an improved plan based on the critique and the previous   plan, resulting in an improvement in the task-grounding of the plan. For each step, we first sample   the task-grounded action that follows the improved plan, and then the implicit RCI updates the   task-grounded actions sequentially to provide state-grounding and agent-grounding. Finally, the   agent-grounded action is executed by the instruction-following agent on the environment. The   prompts are highlighted, and the remaining text shows the outputs generated by the language model.   step focuses on ensuring that actions are admissible for the computer agent by employing implicit   RCI and conditioning agent-grounded actions on the current state, task, and other grounded actions,   with a loop count set to optimize performance.   2.2.1   Problem Setting   We assume that we are given an instruction-following computer agent that can execute a set of   admissible actions given some natural language instructions. An instruction that is not part of the   admissible actions will be ignored. At every step, we receive a high-level natural language task   prompt and a state of the environment. Given the current state and task, we sample the most probable   action from LLMs. The generated natural language action is then fed into the computer agent.   Sampling the actions in a fully generative manner presents a challenge, as the actions must consider   the given task, feasibility in the current state, and admissibility for the computer agent simultaneously.   Therefore, we propose decomposing this action sampling into three reasoning steps each of which   considers task grounding, state grounding, and agent grounding. Task grounding improves actions to   be more effective in solving the given task, state grounding ensures the feasibility of actions in the   current state, and agent grounding considers the executability of actions given the specification of the   computer agent. We first sample a step-by-step plan to solve the given task which improves the task   grounding. Next, the task-grounded action is sampled conditioned on the current state, task, and the   generated plan. The state-grounded actions is generated conditioned on the task-grounded action.   If the task-grounded action is not executable by the computer agent, the agent-grounded action is   sampled. For each sampling of grounded action, we use RCI prompting to make LLM consider some   specific information for grounding.   2.2.2   Grounding Language Model in Computer Tasks   Task grounding.   In the action sampling process, the first step involves generating a plan of   actionable steps for task solving from LLMs. Subsequently, actions are sampled from the same   LLMs, taking into account the present state, task, and generated plan. The benefits of conditioning on   4   the plan for improved grounding of actions are twofold. First, it enables LLMs to identify the stage   of task solving at which the agent is located, serving as a memory module. Second, we can perform   explicit RCI on the generated plan to further improve the plan\u2019s success rate. Although the number of   explicit RCI loops can be arbitrary, we observe that a single pass of explicit RCI suffices for most of   MiniWoB++ tasks.   State grounding.   In language-based agents, grounding actions in the environment is a crucial step   to enable real-world task performance. The aim of this phase is to enhance the task-grounded actions   to be feasible in the current state. Although the actions generated in the preceding phase may align   with the task, they may lack the specificity required to be executed in the current context. For example,   if the assigned task is to forward an email from Bob to Alice and the action obtained from the task   grounding phase is to click on an email from Bob in the email inbox, it is necessary to establish a   connection between the abstract concept of \"email from Bob\" and the concrete element, such as the   email heading, in the current webpage state represented by HTML. To achieve this goal, we perform   the implicit RCI and prompt the LLMs to consider the current state, which subsequently outputs   refined state-grounded actions. Moreover, the state-grounded action is additionally conditioned on   the task-grounded action. We avoid repeating the implicit RCI cycle more than once as it does not   impact the success rate based on our observations.   Agent grounding.   To ensure the successful integration of language-based methodologies in   decision-making processes, it is imperative to establish a scalable framework that guarantees the   admissibility of actions derived from the language model. While the preceding steps of sampling   produce a state-grounded action that is both feasible and grounded in the task, it may not be executable   by the agent due to issues such as improper formatting. To address this, Implicit RCI is employed,   whereby an agent-grounded action is sampled conditioned on the current state, task, task-grounded   action, and state-grounded action. The LLMs are prompted to consider specifications of the computer   agent. The implicit RCI is repeatedly run until the resulting action is executable, with a maximum   loop count set to limit the number of iterations. Empirical analysis on MiniWoB++ tasks suggests   that setting the loop count to 3 yields optimal performance.   3   Evaluation   3.1   Reasoning tasks   In our grounding enhancement process, RCI prompts the LLM to criticize its prior output, considering   the given context (e.g., current task, state, and agent), which ultimately leads to improved output. We   first demonstrate the effectiveness of RCI prompts in augmenting the reasoning capabilities of LLMs   across a range of reasoning benchmarks. We compare RCI to Chain-of-Thought (CoT) prompting, a   state-of-the-art method recognized for its effectiveness in reasoning tasks.   Specifically, we compare our approach with Few-Shot-CoT [73] where a few chain-of-thought   demonstrations are given as examples in prompting, and Zero-Shot-CoT [33] that elicit multiple   reasoning steps by simply adding \"Let\u2019s think step by step\" to the prompt. Following Kojima et   al. [33], our evaluation is conducted with 8 datasets from two categories of reasoning: arithmetic and   commonsense. Please refer to Appendix C.2 for a comprehensive depiction of the datasets. We use   the same experimental setting with their answer extraction method except that we use InstructGPT-3 +   RLHF (gpt-3.5-turbo) as the underlying language model. We use the same prompts that CoT uses and   we also use the answer cleansing approach used in CoT, but we only used answer extraction prompting   in zero-shot CoT experiments. We also use the same few-shot examples that were introduced in   [73] to evaluate Few-Shot CoT\u2019s performance on five arithmetic reasoning tasks. A threshold is   established by setting the maximum number of RCI loops to two, terminating the loop once the   output aligns with the ground-truth data. We observed that in the absence of this external feedback   mechanism, the RCI process is prone to false negative critics, subsequently leading to a decrease in   performance. Experimental results indicate that RCI without external feedback achieves zero-shot   performance in half of the benchmark tests, but underperforms in others, as shown in Appendix 17.   Comparison with Zero-Shot.   RCI prompting is better at solving reasoning tasks compared to zeroshot prompting. Table 1 summarizes the accuracy of our approach (Zero-Shot + RCI) and standard   zero-shot prompting for each reasoning benchmark. Zero-Shot + RCI substantially outperforms the   5   standard prompting in all benchmarks including arithmetic (GSM8K, MultiArith, AddSub, AQUA,   SVAMP, SingleEq) and common sense (CommonSenseQA, StrategyQA) tasks. RCI prompting   even achieves score gains from two arithmetic reasoning tasks (SingleEq and AddSub), which   do not require multi-step reasoning. This distinguishes our RCI prompting from the previous   CoT prompting methods [73, 33] that are not useful in simple reasoning tasks. It is also worth   noting that RCI prompting achieves a significant performance gain in commonsense reasoning tasks   (CommonSenseQA and StrategyQA). While Wei et al. [73] reported that only a substantially large   PaLM (540B) model can benefit from Few-Shot-CoT, RCI prompting can provide performance gain   even with a smaller InstructGPT-3 + RLHF (175B) model.   Arithmetic   Common Sense   GSM8K   MultiArith   AddSub   SVAMP   SingleEq   AQuA   CommonSenseQA   StrategyQA   Zero-Shot   77.95   94.48   88.58   80.70   86.61   60.23   64.56   48.81   Zero-Shot + RCI   85.43   97.64   89.76   84.65   94.49   67.32   68.11   61.81   Table 1: RCI prompting increases the reasoning capability of LLMs on all of eight reasoning   benchmarks.   Comparison with Chain-of-Thought.   The performance results of RCI and CoT baselines on   arithmetic reasoning tasks are summarized in Table 2. Notably, Zero-Shot + RCI outperforms   Zero-Shot CoT and Few-Shot CoT without any CoT prompting in four tasks except MultiArith.   In MultiArith tasks, where most of the standard prompting\u2019s answers are correct (96.06%), RCI   prompting does not yield significant performance gains. RCI prompting has a synergistic collaborative   impact on the two CoT baselines. Namely, Zero-Shot CoT + RCI and Few-Shot CoT + RCI attain the   highest scores on four out of the five tasks. These findings suggest a promising avenue for future   research: combining RCI with other prompting methods for CoT, such as self-consistency [58].   GSM8K   MultiArith   AddSub   SVAMP   SingleEq   Zero-Shot   78.35   96.06   85.83   78.35   91.34   Zero-Shot + RCI   85.43   97.64   89.76   84.65   94.49   Zero-Shot CoT   82.28   96.85   83.86   79.92   89.37   Zero-Shot CoT + RCI   86.22   97.24   89.88   85.83   90.94   Few-Shot CoT   80.31   98.82   89.37   83.46   91.73   Few-Shot CoT + RCI   84.25   99.21   90.55   87.40   93.70   Table 2: Chain-of-Thought prompting exhibits a synergistic effect when coupled with RCI prompting   in arithmetic reasoning tasks.   3.2   Computer tasks   3.2.1   Setup   MiniWoB++ benchmark suite.   The miniwob++ task suite is selected as the main benchmark to   evaluate our computer agent. MiniWoB++ [36], an extension of MiniWoB [61], is a web-based   simulation environment that offers a diverse range of computer tasks, from simple button-clicking to   complex compositional tasks requiring advanced reasoning, such as solving math problems. Its shared   action space, including keyboard and mouse, and a common state space centered around HTML code   enables our proposed agent to be thoroughly evaluated in ample tasks. Additionally, the varying levels   of complexity between tasks enable a systematic evaluation of our work. The action space consists of   two operations each of which controls the keyboard and mouse. The first action enables typing of   arbitrary characters or special keys such as Backspace and Enter. The second action involves moving   and clicking the mouse, allowing the agent to interact with visible HTML elements on a webpage.   All actions can be executed through natural language instructions defined by regular expressions that   are presented within the initial prompts provided to the LLMs. The regular expressions employed in   our evaluation are presented in Appendix D. Our action space definition is similar to previous works,   6   CC-Net   WebN-T5-3B   (no history)   WebN-T5-3B    CC-Net   Others   CC-Net   (no dictionary actions)     CC-Net   Ours   SL SotA   RL SotA   SL + RL SotA   Model   0.0   0.2   0.4   0.6   0.8   Average Success Rate   0.32   0.45   0.50   0.57   0.76   0.85   0.94   0.94   0.57   0.67   0.94   MiniWoB++ Average Performance   Few-Shot In-Context Learning (Ours)   RL + SL   SL   RL   (a)   10   1   10   2   10   3   10   4   Amount of Demos per Task   0.0   0.2   0.4   0.6   0.8   1.0   Average Success Rate   CC-Net (SL)   WebN-T5-3B   WebN-T5-3B   (no action history)   CC-Net (SL + RL)   World of bits   (SL + RL)   Workflow guided exploration   (SL + RL)   Ours   MiniWoB++ Average Performance    vs. Amount of Data   (b)   Figure 4: (a) Average performance comparison with baselines. Our agent with RCI prompting   achieves state-of-the-art performance in MiniWoB++ environment. The tasks that were included in   the averaging process are indicated in Table 18. (b) Relationship between performance and amount   of expert training data. Our agent displays comparable performance to the current state-of-the-art   scores on the MiniWoB++ benchmark, despite using the least amount of data.   such as [25, 32, 36], in which clicking actions directly interact with HTML elements. However, for   typing actions, we extend beyond simple form-filling by using keyboard-based typing actions. Instead   of relying on dictionary-based typing actions [30], where the agent simply chooses from a predefined   dictionary of texts, our approach requires the agent to predict the proper text input. Our approach,   therefore, has a better generalization capability for diverse computer tasks. The state space of our   agent consists solely of HTML code.   Model choices.   For the purpose of evaluating the effectiveness of RCI prompting, multiple language models are used in our experiments. Specifically, we employ three models, namely, GPT-3   (davinci) [5], InstructGPT-3 (text-davinci-002) [47, 72, 57], and InstructGPT-3 + RLHF (gpt-3.5turbo, gpt-4) [47]. Unless otherwise specified, we primarily evaluate our computer agent with the   InstructGPT-3 + RLHF models (gpt-3.5-turbo, gpt-4). Additionally, we use GPT-3 and InstructGPT-3   models for ablation studies. All the models were obtained through the OpenAI API, and further   details can be found in Appendix C.1.   Evaluated tasks.   We employ a set of 55 tasks to enable fair comparisons with baselines, as previous   works are only evaluated on a subset of tasks consistently. Furthermore, to assess the performance of   models on challenging tasks, we have selected tasks that involve free-form language typing actions,   which have been reported to have an almost-zero success rate in previous works (e.g., terminal).   Notably, certain commonly evaluated tasks in prior works are excluded due to the excessive length of   HTML code for some UI components, which are described in Appendix C.3.   Metrics   Consistent with prior studies, our main evaluation criterion is the success rate, which   measures the ability of our agent to actually complete the assigned task. This rate is calculated as the   proportion of successful episodes, which are defined as those in which the agent receives a positive   reward. We identified two modes of failure: the production of unexecutable actions and task failure.   When the agent generates an unexecutable action following the implicit RCI step, it fails immediately.   Moreover, an episode is considered unsuccessful when the agent, despite effectively executing the   plan generated, is unable to accomplish the task and thus receives no reward.   3.2.2   Outperforming baselines on MiniWoB++ task suite   We present Figure 4a which summarizes the average success rate of our agent and baseline models   over the MiniWoB++ benchmark. The results demonstrate significant outperformance of our approach   over supervised learning models. Specifically, we observe a 41% higher score than the WebN-T53B, which employs a finetuned large language model with 12K expert demonstration data. Our   approach also outperforms reinforcement learning approaches that require an order of magnitude   7   more interactions with the environment. Among all the baselines, our approach achieves the second   highest score. The sole model that surpasses our agent is the CC-Net, which involves co-training   of reinforcement learning and imitation learning. However, a direct comparison with CC-Net is   not possible since it uses dictionary-based typing actions. In other words, CC-Net selects text from   a predefined list for typing actions in some tasks, while our approach is fully generative. Thus,   CC-Net (without dictionary-based action) in Figure 4a serves as our appropriate comparison and we   outperform it by 6%. The performance data for CC-Net (with no dictionary-based action) is obtained   from the ablation study section in their paper [30].   Another comparative analysis is performed to evaluate the performance of our agent in contrast to the   state-of-the-art agents in three categories, namely supervised learning, reinforcement learning, and   a combination of both. To facilitate a fair comparison, we specifically isolate LLM-based state-ofthe-art approaches, which share similarities with our approach to solving computer tasks. The best   per-task performance achieved by each category is then aggregated, and the outcomes are presented   as SotA in Figure 4a. The result shows that our agent surpasses SotA by 37 percentage points in   supervised learning and by 27 percentage points in reinforcement learning. Notably, our proposed   RCI prompting method outperforms the SotA LLM approach [24], even when the latter employs   both finetuning and few-shot examples in prompts. This outcome highlights the effectiveness of our   approach in extracting vital knowledge for computer tasks from language models. Our agent even   achieves a slight edge over SotA (less than 1 percentage point) in the combined use of supervised and   reinforcement learning, which employs significantly more expert data and online interactions. We   also provide task-level performance comparisons in Figure 10, where tasks are arranged in ascending   order based on the difference between our agent\u2019s performance and the baseline. We observed three   main failure modes of our agent: (i) underperformance in tasks that require long-horizon planning   (e.g., guess-number, search-engine, use-spinner), (ii) difficulty in selecting appropriate actions for   tasks that require multi-step reasoning (e.g., tic-tac-toe, use-autocomplete), and (iii) lower scores in   tasks that rely on visual rendering of HTML code to solve the task (e.g., count-shape). These failures   are explained in more detail in Appendix F.   3.2.3   Lowest sample complexity   Figure 4b provides a comparative analysis of the total number of samples used in several models and   their mean performance. We begin by discussing CC-Net [30] model, which employs 2.4 million   expert demonstrations (equivalent to 6,300 hours) collected from 77 human participants across 104   tasks for behavior cloning. This amounts to an average of 23,076 demonstrations per task. In contrast,   the WebN-T5-3B [24] model uses 12,000 expert demonstrations to fine-tune its pre-trained T5 model.   Rather than directly updating model parameters with demonstration data, our approach involves   integrating two to three demonstrations into the prompt for in-context learning, which biases the   model output without any parameter updates. This approach allows our agent to generalize to unseen   tasks with only a handful of demonstrations. Our results show that our agent achieved a higher   success rate than all baselines, requiring 120x fewer samples than WebN-T5-3B and 11,000x fewer   samples than CC-Net. Given the challenges of obtaining expert demonstrations for computer tasks,   our findings demonstrate the practicality of our approach in automating such tasks.   3.2.4   Ablating the groundings   This section examines the impact of grounding improvement on task success rates. We conduct   ablations to isolate the contributions of task, state, and agent grounding improvements by eliminating   RCI prompting at each stage. We categorize tasks by three different difficulty levels to provide a   more detailed understanding of the effects of grounding improvements across a diverse range of tasks.   We conducted a task grounding ablation by eliminating the plan sampling stage. This modification   entails generating actions directly from the state, without the need for conditioning on a step-by-step   plan. State grounding is evaluated by directly applying the agent-grounding update to task-grounded   actions. Lastly, we ablate the implicit RCI of the agent grounding by letting the state-grounded   action be the final output of the agent. Figure 5 illustrates the performance degradation resulting   from each ablation of grounding. Our results indicate that each grounding contribution is essential   to solving computer tasks, with each contributing almost equally to the overall success rate. The   reason for this is partially due to the fact that the three methods of improving grounding are not   mutually exclusive, but rather complementary, with one enhancement in grounding contributing to   multiple action groundings. Examples of cross-grounding improvement are provided in Appendix E.   8   Moreover, it has been observed that state grounding plays a crucial role in enabling an agent to use   relevant information during episodes, particularly in scenarios where the initial state does not offer   sufficient information to accomplish the task, such as terminal task. Interestingly, task grounding   significantly improves the success rate when a task requires a long-horizon action plan, such as   the click checkboxes large task. We also observe that agent grounding significantly enhances the   feasibility of actions. Notably, in simpler tasks, the success rate decreases by 60% in contrast to the   baseline without the agent grounding. This finding is of particular significance as it distinguishes   our work from prior investigations [1, 28], which employ additional trained model components. In   contrast, our study solely relies on the reasoning ability of language models.   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Easy   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Medium   Baseline   Without   task   grounding   Without   state   grounding   Without   agent   grounding   0.0   0.2   0.4   0.6   0.8   1.0   Success rate   Hard   Figure 5: Ablation analysis on the different types of grounding across tasks with varying degrees of   difficulty. The experimental design employs the use of InstructGPT-3 + RLHF model (gpt-3.5-turbo).   3.2.5   Ablating the language model   The performance of our agent is contingent on the quality of the underlying pre-trained language   models used, so enhancing language models can lead to an improvement in the agent\u2019s performance.   In this section, we present a comparison of the agent\u2019s performance using three distinct language   models: GPT-3, InstructGPT-3, and InstructGPT-3 + RLHF (gpt-3.5-turbo). Our objective is to   investigate the relationship between LLMs\u2019 capability and their ability to solve MiniWoB++ tasks.   The experimental setting employed in Section 3.2.4 is replicated in this study. Figure 6 depicts   the average success rate of three language models on tasks of varying difficulty levels. Our results   reveal that LLMs struggle to effectively complete tasks without instruction fine-tuning. This may   be attributed to the absence of intricate prompt engineering, as our observations have indicated   that GPT-3 displays sufficient competence in comprehending HTML code, regular expressions, and   engaging in reasoning.   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Easy   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Medium   InstructGPT-3   +RLHF   (Baseline)   InstructGPT-3   GPT-3   0.00   0.25   0.50   0.75   1.00   Success rate   Hard   Figure 6: Ablation study on different language models across tasks of varying degrees of difficulty.   4   Limitations   In the course of our work, several limitations became apparent that may serve as potential avenues   for further research. One central concern is our primary focus on the InstructGPT-3 + RLHF models   (gpt-3.5-turbo, gpt-4), leaving the generalization ability of RCI to other models unexplored. The   versatility of RCI across diverse models remains a pertinent question, suggesting that future studies   should expand their scope to determine the robustness and adaptability of RCI. Handling lengthy   HTML presents another challenge. The current model grapples with extensive HTML states. While it   has been suggested that efficiency might be bolstered by pruning HTML states to exclude non-critical   9   elements, the task itself is non-trivial. A fundamental constraint of LLMs is the limited context   length, which can hamper handling extensive HTML states effectively. Addressing this may require   architectural adjustments or novel parsing methods. Our agent\u2019s action space, mainly restricted to   clicks and typing, limits its web navigation capabilities. There\u2019s a need to diversify its actions for a   more seamless experience. Furthermore, The agent\u2019s focus on short-term decisions overlooks the   necessity for long-term strategy, especially in tasks requiring coordinated sequences. Broadening   this focus is essential for versatile applications. Lastly, the intricate UI components populating   contemporary websites present a challenge for LLMs to fully understand the HTML states. The   subtle nuances of such components, which may not be discernible through HTML alone, underscore   the need for adding more modalities to the state definition. Addressing these issues is crucial to   enhance the RCI agent, making it more adaptable and efficient in practical applications.   5   Discussion   This work is part of a growing literature showing that LLMs might be all you need for hard decisionmaking problems [76]. In contrast to imitation learning and reinforcement learning approaches,   LLMs can solve novel tasks in a zero-shot or few-shot manner, and don\u2019t require task-dependent   expert data or a reward function. Furthermore, we expect that as the capabilities of LLMs and   foundation models increase, our method will naturally improve as well. However, we find that   current capabilities of LLMs aren\u2019t as powerful as task-dependent SL+RL approaches on some   computer tasks. Also, RCI is more expensive to run compared to approaches that just sample once   from the LLM. There are many avenues for future research in increasing the capacity of LLMs in   decision-making tasks. First, our experiments use LLMs on HTML code, but ideally methods based   on multimodal foundation models [16, 55, 2, 46] will be able to take actions based on text, images,   audio, and video as input [4, 18, 44, 71]. Second, the results presented in this paper all use pre-trained   LLMs. We expect the performance of our method to increase when using LLMs fine-tuned to solve   computer tasks.   Importantly, current LLMs are poor at reasoning tasks, such as playing tic-tac-toe, because they do   not think ahead. Although RCI improves reasoning capabilities in LLMs, there exists much work   to be done on increasing the reasoning capabilities in LLMs. This will be crucial to accomplish   hard cognitive tasks on computers that require thinking ahead. Similar to other prompting-based   approaches for reasoning in LLMs, RCI can be viewed as using the LLM\u2019s output to write to an   external memory, which is later retrieved to choose an action. LLMs with memory have been   demonstrated to be computationally universal [60], meaning that in principle all that is needed to   run arbitrary programs is the right prompt. Since RCI represents a basic version of this powerful   framework, we anticipate the development of more advanced RCI variations in the future. There   is a vast array of potential methods that repeatedly feed the output of particular prompts into the   LLM. For example, multiple different LLMs can simulate the information exchange between team   members in an organization. This would enable the merging of diverse perspectives to tackle complex   problems. In such a context, incorporating game theory and multi-agent systems research could   significantly enhance the overall performance. Reinforcement learning could be used to discover   effective structures involving loops and prompts [81], either through human feedback or a given   reward function. This optimization process can be further refined by exploring the space of potential   loop and prompt structures, identifying those that yield the best results, and fine-tuning the model   accordingly [75].   Acknowledgement   This material is based upon work supported by the National Science Foundation under Grant #2127309   to the Computing Research Association for the CIFellows 2021 Project.", "conf": "NIPS", "year": "2023", "index": 466}, {"title": "Entity Divider with Language Grounding in Multi-Agent Reinforcement Learning   Ziluo Ding * 1 Wanpeng Zhang * 1 Junpeng Yue 1 Xiangjun Wang 2 Tiejun Huang 1 3 Zongqing Lu 1 3", "abstract": "We investigate the use of natural language to drive   the generalization of policies in multi-agent settings. Unlike single-agent settings, the generalization of policies should also consider the influence of other agents. Besides, with the increasing   number of entities in multi-agent settings, more   agent-entity interactions are needed for language   grounding, and the enormous search space could   impede the learning process. Moreover, given a   simple general instruction, e.g., beating all enemies, agents are required to decompose it into   multiple subgoals and figure out the right one to   focus on. Inspired by previous work, we try to   address these issues at the entity level and propose a novel framework for language grounding   in multi-agent reinforcement learning, entity divider (EnDi). EnDi enables agents to independently learn subgoal division at the entity level   and act in the environment based on the associated   entities. The subgoal division is regularized by   agent modeling to avoid subgoal conflicts and promote coordinated strategies. Empirically, EnDi   demonstrates the strong generalization ability to   unseen games with new dynamics and expresses   the superiority over existing methods. The code   is available at https://github.com/PKU-RL/EnDi.   1.", "content": "The generalization of reinforcement learning (RL) agents   to new environments is challenging, even to environments   slightly different from those seen during training (Finn et al.,   2017). Recently, language grounding has been proven to be   an effective way to grant RL agents the generalization ability   (Zhong et al., 2019; Hanjie et al., 2021). By relating the dynamics of the environment with the text manual specifying   *Equal contribution 1School of Computer Science, Peking University 2inspir.ai 3Beijing Academy of Artificial Intelligence. Correspondence to: Zongqing Lu <zongqing.lu@pku.edu.cn>.   Proceedings of the 40 th International Conference on Machine   Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright   2023 by the author(s).   the environment dynamics at the entity level, the languagebased agent can adapt to new settings with unseen entities   or dynamics. In addition, language-based RL provides a   framework for enabling agents to reach user-specified goal   states described by natural language (K\u00a8uttler et al., 2020;   Tellex et al., 2020; Branavan et al., 2012). Language description can express abstract goals as sets of constraints on   the states and drive generalization.   However, in multi-agent settings, things could be different. First, the policies of others also affect the dynamics   of the environment, while the text manual does not provide   such information. Therefore, the generalization of policies   needs to consider the influence of others apart from how   entities behave in a new environment. Second, with the   increasing number of entities in multi-agent settings, so is   the number of agent-entity interactions needed for language   grounding. The enormous search space could impede the   learning process. Third, sometimes it is unrealistic to give   detailed instructions to tell exactly what to do for each agent.   On the contrary, a simple goal instruction, e.g., beating all   enemies or collecting all the treasuries, is more convenient   and effective. Therefore, learning subgoal division and cultivating coordinated strategies based on one single general   instruction is required.   The key to generalization in previous works (Zhong et al.,   2019; Hanjie et al., 2021) is grounding language to dynamics at the entity level. By doing so, agents can reason over   the dynamic rules of all the entities in the environment.   Since the dynamic of the entity is the basic component of   the dynamics of the environment, such language grounding is invariant to a new distribution of dynamics or tasks,   making the generalization more reliable. Inspired by this,   in multi-agent settings, the influence of policies of others   should also be reflected at the entity level for better generalization. In addition, after jointly grounding the text manual   and the language-based goal (goal instruction) to environment entities, each entity has been associated with explicit   dynamic rules and relationships with the goal state. Thus,   the entities with language grounding can also be utilized to   form a sub-goal division strategy.   We present two goal-based multi-agent environments based   on two previous single-agent settings, i.e., MESSENGER   (Hanjie et al., 2021) and RTFM (Zhong et al., 2019), which   1   Entity Divider with Language Grounding in Multi-Agent Reinforcement Learning   require generalization to new dynamics (i.e., how entities   behave), entity", "conf": "ICML", "year": "2023", "index": 149}, {"title": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics   Volume 1: Long Papers, pages 524\u2013544   July 9-14, 2023 \u00a92023 Association for Computational Linguistics   World-to-Words: Grounded Open Vocabulary Acquisition   through Fast Mapping in Vision-Language Models   Ziqiao Ma\u2217   Jiayi Pan\u2217   Joyce Chai   Computer Science and Engineering Division, University of Michigan   {marstin,jiayipan,chaijy}@umich.edu", "abstract": "The ability to connect language units to their   referents in the physical world, referred to as   grounding, is crucial to learning and understanding grounded meanings of words. While   humans demonstrate fast mapping in new word   learning, it remains unclear whether modern   vision-language models can truly represent language with their grounded meanings, and how   grounding may further bootstrap new word   learning. To this end, we introduce Grounded   Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in openworld language learning.   As an initial attempt, we propose World-to-Words (W2W), a   novel visually-grounded language model by   pre-training on image-text pairs highlighting   grounding as an objective. Through extensive   experiments and analysis, we demonstrate that   W2W is a more coherent and fast grounded word   learner, and that the grounding ability acquired   during pre-training helps the model to learn   unseen words more rapidly and robustly.1   1", "content": "Language is learned through sensorimotor experience in the physical world (Bisk et al., 2020). The   ability to connect language units to their referents   in the physical world, referred to as grounding,   plays an important role in learning and understanding grounded meanings of words (Harnad, 1990).   As shown in Figure 1, a human reader would easily ground noun phrases to the corresponding entities captured in the image. Even when the term   \u201cincinerator\u201d is new to human learners, they can   still locate the object of interest through the language and visual context, and acquire its meaning.   In fact, this ability to bootstrap new word learning with only minimal information, known as fast   mapping, is demonstrated abundantly in cognitive   \u2217Equal contribution.   1Code available at https://github.com/sled-group/   world-to-words.   A lady wearing a navy blue    stripe tank top is getting    ready to burn glass in    front of an incinerator.   Figure 1: Even when the term \u201cincinerator\u201d (highlighted yellow) is new to human learners, they can still   locate the most likely referent (indicated by the yellow   bounding box) in the perceived world by grounding.   literature on human language acquisition (Carey   and Bartlett, 1978; Carey, 1978; Golinkoff et al.,   2000; Smith and Yu, 2008).   Recently, there has been a substantial effort on   pre-training vision-language models (VLMs) (Du   et al., 2022a). Despite the exciting performance of   these models on a variety of downstream vision and   language tasks, it remains unclear whether these   models can truly understand or produce language   with their grounded meanings in the perceived   world, and how grounding may further bootstrap   new word learning. These questions are of interest from both a scientific and an engineering point   of view. From a scientific perspective, grounding is crucial to language learners, as children attend to intended objects in the environment when   producing (Tanenhaus et al., 1995; Meyer et al.,   1998) and comprehending (Smith et al., 2007) utterances. From an engineering perspective, even   with the availability of grounded vision language   datasets (image-text pairs with fine-grained wordobject mappings) (Plummer et al., 2015), the costly   grounding annotation can hardly cover the whole   vocabulary space during the training time. Building upon the pre-trained models, it\u2019s important for   the agent to have the ability to learn grounded new   words in a few shots of raw image-text pairs without word-object mappings.   To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA), a scalable formulation   to examine grounding and bootstrapping in openworld language learning. In this formulation, lan524   guage learning is a combination of learning to predict a word in a linguistic context as well as learning to ground the word in the physical world. Under this formulation, we explore the framework in   which the model first acquires the grounding ability   during pre-training, and then transfers this ability to   learn unseen words without grounding supervision.   As an initial step, we developed World-to-Words   (W2W), a novel visually grounded language model   motivated by recent advances in detection transformers (DETR) (Carion et al., 2020; Kamath et al.,   2021). Compared to many existing VLMs, W2W   performs language modeling upon explicit object   representations. The model first acquires the ability   to ground during pre-training, and then transfers   this intrinsic ability to learn unseen words when   grounded supervision is no longer available.   Our empirical results show that learning to map   words to their referents plays a significant role in   grounded word acquisition. By pre-training with   fine-grained word-object mappings, W2W demonstrates stronger performance in learning grounded   meanings of words, both seen and unseen, yet with   orders of magnitude fewer data compared to other   competitive VLM baselines. The pre-trained model   can further provide a foundation for efficient learning of new grounded words with a few examples.   We further present an in-depth analysis to understand potential predictors of W2W in word learning,   which demonstrates intriguing behaviors in comparison to human language learning. Our findings   will provide a stepping stone for future work on   grounded language learning in an open world.   2   Grounded Open Vocabulary   Acquisition (GOVA)   We start by introducing the settings of grounded   word acquisition and few-shot learning of new   words tasks, which are two key components of   the Grounded Open Vocabulary Acquisition (GOVA)   task formulation. We further present a unified evaluation protocol and introduce the dataset we curated for this problem.   2.1   Grounded Word Acquisition   Many vision-language tasks have been developed   in the past, e.g., visual question answering, visual commonsense reasoning, etc. However, these   tasks are mainly focused on the end task performance without scrutinizing whether words are   grounded to their corresponding visual entities. We   Two boats of people, a    smaller yellow <mask> with    two people and a larger    white boat with six people.   Two boats of people, a    smaller yellow boat with two    people and a larger white    boat with six people.   Input   Output   Figure 2: An instance of the word grounding task. Models are tasked to predict the missing word boat and   localize the corresponding smaller yellow boat in the   image coherently.   consider a formulation that directly examines if   vision-language models have the ability to acquire   grounded meanings of words, specifically, through   both language modeling and object localization.   Figure 2 shows an instance of the word acquisition task. A model is presented with an image   ximg \u2208I and an incomplete caption xcap \u2208T with   one of its groundable words w (e.g., nouns and adjectives) replaced by a MASK. The model is tasked to   predict this missing word w \u2208V based on all available context and localize the corresponding objects   Ow = {o1, o2, \u00b7 \u00b7 \u00b7 , on} in the image by proposing   the bounding boxes of them. Overall, a model capable of solving the grounded word acquisition task   is a function f : I \u00d7 T \u2192V \u00d7 R4n.   The language modeling part takes the form of a   cloze test, which predicts an open vocabulary word   and is widely adopted to evaluate pre-trained language models (Paperno et al., 2016; Petroni et al.,   2019; Jin et al., 2020). However, language modeling alone fails to provide a comprehensive evaluation of language grounding. For example in Figure 2, a model may correctly produce the word   \u201cboat,\u201d but mistakenly attributes the evidence to   the larger white boat in the image. To address   this limitation, we require models to localize the   corresponding object in the image. This design   is motivated by the disentanglement of object detection into object localization and class recognition (Singh et al., 2018; Zareian et al., 2021; Zhong   et al., 2022). It enables vision models to develop a   sense of objectness without relying on a predefined   set of object classes, thereby potentially allowing   them to generalize to unseen objects. Further comparison with related task setups is discussed in Section 5 and illustrated in Figure 8 in the Appendix.   2.2   Evaluation Metric   In language model evaluation, the commonly used   measures for assessing performance are the stan525   dard hit-rate-at-k (HR@k) measure and perplexity (Salazar et al., 2020; Jin et al., 2020). In masked   language modeling, the log perplexity of a word w   is defined as the log pseudo-perplexity:   log PPL(w) = \u2212log P(w|ximg, xcap)   (1)   In object detection evaluation, especially for   phrase grounding where multiple referents are   possible (Kamath et al., 2021), Any-Protocol   and All-Protocol are commonly adopted.   Assuming n ground truth bounding boxes B   =   {b1, b2, \u00b7 \u00b7 \u00b7 , bn} and m predicted bounding boxes   eB = { eb1, eb2, \u00b7 \u00b7 \u00b7 , f   bm}, the intersection-over-union   (IoU) in both protocols is defined as:   IoUany = 1   n   X   i\u2208{1,2,\u00b7\u00b7\u00b7 ,n}   max   j\u2208{1,2,\u00b7\u00b7\u00b7 ,m} IoU(bi, ebj)   (2)   IoUall = IoU(\u222aB, \u222aeB)   (3)   However, these metrics only capture unimodal   performance without concerning the correctness of   cross-modal mapping. We design two new metrics   to combine language and vision performance:   \u2022 Grounded hit-rate (G-HR@k), the proportion   of tests with the masked word appearing in the   top-k candidates and a localization IoU over 0.5.   \u2022 Grounded perplexity (G-PPL) as follows:   log G-PPL(w) =   (   \u221e   if IoU = 0   log PPL(w) \u2212log IoU   else   (4)   2.3   Few-Shot Learning of New Words   Although there are grounding datasets available,   i.e., image-text pairs with word-object mapping annotation (Plummer et al., 2015), it is impractical   to obtain such fine-grained annotation on a large   scale and to cover the whole vocabulary space V.   We therefore explore grounded new word learning   as a few-shot learning problem, especially under   the setting of incremental class learning (Mandziuk and Shastri, 1999; Kemker et al., 2018). An   intuitive illustration of the few-shot new word learning framework is provided in Figure 3. Under   this framework, a computational model is developed in two stages. During the pre-training stage,   the model receives image-caption pairs, with finegrained word-object annotation for a set of base   words Vseen \u2286V. After pre-training, the model is   provided with few samples of raw text-image pairs,   each containing a set of unseen words Vunseen \u2286V   that the model has to acquire.   Someone is slicing a loaf    of bread using a knife on    a wooden cutting board.   I am slicing the pizza with    a knife and stacking the    pieces onto the plate.   Few-shot Learning Vunseen   Pre-training Vseen   test   test   Figure 3: An illustration of the few-shot new word learning paradigm. The model first pre-trains on a grounding   dataset with a set of base words (Vseen), and then attempts to acquire a set of unseen words (Vunseen) in a   small number of raw text-image pairs. Tests are performed after each training session.   Tests are performed after each training stage.   It\u2019s important to note that the unseen words may   not be completely new, e.g., the models may have   encountered these words in its language encoder   initialized with pre-trained language models. We   consider them \u201cunseen\u201d because the model never   sees these words paired with their referent, i.e., the   grounded meanings of the words are unknown.   2.4   Dataset Curation   We build our dataset based on the Flickr30K Entities dataset (Plummer et al., 2015), which contains   image-text pairs with dense annotations between   groundable phrases and bounding boxes of objects.   The groundable phrases and regions are defined by   the dataset, as chunks of text that refer to object   bounding boxes. To construct word grounding instances, we use Stanza (Qi et al., 2020) to parse the   caption, enumerate every word in the groundable   phrase, and identify those with a POS tag of NOUN   or ADJ. These groundable words are replaced by   MASK one at a time and matched to their corresponding bounding boxes.   The dataset is divided into 4 splits: pre-training   set, unseen words training set, seen words test set,   and unseen words test set. We start by selecting 31   unseen words and holding out all text-image pairs   containing these words from the training split of   Flickr30K Entities. The hold-out text-image pairs   are further divided into the training and test sets   for unseen words. The remaining training split   of Flickr30K Entities is used for the pre-training   set. To prevent frequent words (e.g., \u201cman\u201d) from   dominating the test results of the seen words, we   choose 60 seen words and sample an equal number   of test instances for each word from the test split   of Flickr30K Entities. More details and statistics   of the dataset are available in Appendix A.   526   RoBERTa   Linear   ResNet   Linear   2D    Positional    Embedding   ~~~~~~   \u2295   Concat   Multimodal Encoder                       Object Queries   MLM   Grounding   OL   Two boats of people, a    smaller <mask> boat with two    people and a <mask> white    boat with six people.   Two boats of people, a    smaller yellow boat with two    people and a larger white    boat with six people.\u00d8   QL   QO   KO   VO   Text Decoder   Object Decoder   KL   VL   Figure 4: An overview of the W2W architecture, a visually grounded language model pre-trained with three objectives:   masked language modeling (MLM), object localization (OL), and grounding through word-region alignment (WRA).   3   Computational Models   3.1   The World-to-Words (W2W) Model   Humans demonstrate fast mapping, the ability   to learn new words with only minimal information (Carey and Bartlett, 1978; Carey, 1978;   Golinkoff et al., 2000).   Motivated by how visual grounding helps humans in bootstrapping new   words, we propose a computational framework   that first acquires the ability to ground during pretraining, and then transfers this intrinsic ability to   learn unseen words when grounded supervision is   no longer available. We introduce World-to-Words   (W2W), a novel visually-grounded language model   with an end-to-end design as illustrated in Figure 4.   Model Architecture.   Similarly to dual-stream   vision-language models, W2W encodes the textual   input with a pre-trained language model (Liu et al.,   2019), and encodes image input with convolutional   backbone (He et al., 2016) with 2D positional encoding added. The text and image representations   are linearly projected onto a joint semantic space   and concatenated. The multimodal representation   is then forwarded into a cross-encoder with selfattention layers. The cross-encoded representations   in the final layer are sent into an object decoder,   together with a set of learnable object queries. The   object decoder produces an object embedding for   each input object query, which can be considered   as a representation of the proposed object. The   object representations are further forwarded to the   text decoder, which allows language modeling to   explicitly attend to the perceived objects. We discuss the pre-training objectives, especially how the   model acquires grounding in the following paragraphs. Other details are available in Appendix B.   Masked Language Modeling (MLM).   As an intrinsic task, we follow the majority of existing pretrained vision-language models to perform masked   language modeling with a two-layer MLP. Words in   input text are randomly masked out, and the model   predicts the masked words conditioned on the corrupted sentence and image. Words in groundable   phrases are masked with a probability of 0.4 and   those in non-groundable regions are masked with a   lower probability of 0.1.   Object Localization (OL).   Each object representation will be decoded by a shared three-layer MLP   to produce a bounding box. We follow prior detection transformers (DETR) (Carion et al., 2020;   Kamath et al., 2021) to perform bipartite matching   between proposed boxes and ground truth boxes   with a Hungarian loss (Kuhn, 1955).   The predicted boxes are optimized towards ground truth using the generalized intersection-over-union (GIoU)   loss (Rezatofighi et al., 2019) and the L1 loss.   Grounding.   The notion of Grounding is realized by grounded pre-training through word-region   alignment (WRA) which enables fine-grained   cross-modal mapping between words and objects.   It consists of two levels of alignment: positional   alignment and semantic alignment. In positional   alignment, the model learns to map each object representation to words in the sentence, which could   possibly be a MASK or an additional no-object label   \u2205(Yu and Siskind, 2013; Kamath et al., 2021). We   use a fully-connected layer to predict the distribution over token positions with cross-entropy loss.   In semantic alignment, the model learns to bring   word representations closer to the object representations that they ground to, and push the unrelated   pairs farther. We use a contrastive loss over the   final layers of the object and text decoders.   3.2   Baselines   Groundless   Baseline.   A   baseline   with   no   grounding ability is developed by pre-training W2W   in the same condition but removing the grounding   527   Models   Seen (|Vseen| = 60)   Unseen (|Vunseen| = 31)   G-HR@1 (\u2191)   log G-PPL (\u2193)   HR@1 (\u2191)   log PPL (\u2193)   Acc (\u2191)   IoU (\u2191)   G-HR@1 (\u2191)   log G-PPL (\u2193)   HR@1 (\u2191)   log PPL (\u2193)   Acc (\u2191)   IoU (\u2191)   RoBERTa   38.0   2.75   23.1   4.96   RoBERTa (FT)   47.9   1.99   24.3   4.38   ViLT   64.7   1.27   32.7   3.68   MDETR   27.8 / 27.0   25.3 / 28.0   26.3 / 20.2   23.9 / 21.7   ViLT+MDETR   19.8 / 19.3   2.53 / 2.43   64.7   1.27   31.1 / 30.4   28.5 / 31.2   8.6 / 8.1   5.07 / 5.12   32.7   3.68   27.3 / 23.3   25.0 / 23.8   VisualBERT (FT)   28.5 / 2.96 / 42.3   2.33   68.1 / 53.3 / 10.2 / 5.60 / 20.7   4.81   50.6 / 45.2 / W2Ww/o G (FT)   28.9 / 27.8   2.33 / 2.38   63.9   1.41   44.0 / 43.0   40.0 / 38.2   1.1 / 1.1   11.89 / 12.04   3.7   10.87   38.7 / 31.9   36.2 / 31.0   W2W   47.0 / 46.3   1.79 / 1.81   66.9   1.26   66.8 / 66.3   58.8 / 57.6   2.3 / 2.3   11.58 / 11.74   4.2   11.01   61.3 / 53.1   56.3 / 48.0   Table 1: Test results on the seen and unseen words, obtained immediately after pre-training. Unless noted explicitly   as fine-tuned (FT), all results reflect the performance of models without fine-tuning. Evaluations under both All and   Any-protocols are provided in the table as (All/Any) pairs. For models depending on a frozen pre-trained object   detector, we can only provide evaluation under All-Protocol. We note that the unseen words are only unseen to W2W   models, as pre-trained baselines have encountered them all during development. We report the results for reference.   objectives in the loss function. We refer to this   groundless model as W2Ww/o G. Like a typical pretrained VLM, e.g., VisualBERT (Li et al., 2019),   W2Ww/o G performs language modeling based on the   object features, without explicit cross-modal referential grounding. We apply W2Ww/o G on GOVA task   by fine-tuning the model on the pre-training dataset   with grounding objective until convergence.   Pre-trained Baselines.   For the majority of the   pre-trained VLMs, the unseen words are known   during pre-training. Also, the primary focus of   this work is to understand grounding and bootstrapping in grounded word acquisition. It\u2019s not   our goal to scale up or re-train all variants of pretraining frameworks. Therefore, we compare our   model to the pre-trained VLMs with equal or reasonably larger scales for only reference and analysis purposes. We choose representative baselines   in phrase grounding, as presented in Table 1:   \u2022 \u201cDetect-and-Recognize\u201d Baseline: Models under this framework rely on a pre-trained frozen   object detector, and then learn to predict words   from proposed objects. We choose the fine-tuned   VisualBERT (Li et al., 2019) for this type.   \u2022 \u201cProduce-and-Localize\u201d Baseline: Models under this framework rely on a pre-trained visionlanguage model to predict the missing word,   and then perform referring expression comprehension and propose objects.   We combine   ViLT (Kim et al., 2021) and MDETR (Kamath   et al., 2021) for their competitive performance   in vision-conditioned language modeling and   phrase grounding individually.   4   Empirical Findings   4.1   Grounded Pre-training   The results of this section are obtained from the   test immediately following pre-training.   Models   # Param   # Imgs   # Caps   Objectives   RoBERTa   120M   MLM   VisualBERT   180M   200K   567K   MLM, ITM   ViLT   110M   4.0M   10M   WRA*, MLM, ITM   MDETR   200M   200K   1.3M   WRA, OL   W2W   200M   30K   150K   WRA, MLM, OL   W2Ww/o G   200M   30K   150K   MLM, OL   *WRA is formulated as word-patch alignment in ViLT, thus it   cannot perform object localization without major modifications.   Table 2: The baselines for comparisons and", "conf": "ACL", "year": "2023", "index": 375}, {"title": "Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding   Haojun Jiang1* Yuanze Lin3*\u2020 Dongchen Han1 Shiji Song1 Gao Huang1,2\u2021   1Tsinghua University, BNRist   2BAAI   3University of Washington   {jhj20, hdc19}@mails.tsinghua.edu.cn, yuanze@uw.edu,   {shijis, gaohuang}@tsinghua.edu.cn", "abstract": "Visual grounding, i.e., localizing objects in images according to natural language queries, is an important topic   in visual language understanding. The most effective approaches for this task are based on deep learning, which   generally require expensive manually labeled image-query   or patch-query pairs.   To eliminate the heavy dependence on human annotations, we present a novel method,   named Pseudo-Q, to automatically generate pseudo language queries for supervised training. Our method leverages an off-the-shelf object detector to identify visual objects from unlabeled images, and then language queries   for these objects are obtained in an unsupervised fashion   with a pseudo-query generation module. Then, we design   a task-related query prompt module to speci\ufb01cally tailor   generated pseudo language queries for visual grounding   tasks. Further, in order to fully capture the contextual relationships between images and language queries, we develop a visual-language model equipped with multi-level   cross-modality attention mechanism. Extensive experimental results demonstrate that our method has two notable   bene\ufb01ts: (1) it can reduce human annotation costs signi\ufb01cantly, e.g., 31% on RefCOCO [65] without degrading original model\u2019s performance under the fully supervised setting, and (2) without bells and whistles, it achieves superior or comparable performance compared to state-of-theart weakly-supervised visual grounding methods on all the   \ufb01ve datasets we have experimented. Code is available at   https://github.com/LeapLabTHU/Pseudo-Q.   1.", "content": "Visual grounding (VG) task [13,24,40,65] has achieved   great progress in recent years, with the advances in both   computer vision [16,20,21,25,26,46,56,57,59] and natural   language processing [4,14,41,50,53]. It aims to localize the   *Equal contribution.   \u2020This work was done during an internship at Tsinghua.   \u2021Corresponding author.   Query: horse on the right   (a) Fully-supervised VG   (b) Weakly-supervised VG   Object Proposal   Pseduo-query   Query: horse on the right   Horses   Query: black horse on the left   Image with Pseudo-label   Query: right brown horse   Unlabeled Image   Object Detector   Image Region   Image Region   Image Region   Figure 1. Comparison with fully and weakly-supervised visual grounding method. (a) Fully-supervised VG utilizes image   region-query pairs as supervision signals. (b) Weakly-supervised   VG adopts only language queries. (c) Our Pseudo-Q method is   free of any task-related annotations.   objects referred by natural language queries, which is essential for various vision-language tasks, e.g., visual question   answering [2] and visual commonsense reasoning [67].   Most existing visual grounding methods can be categorized into two types: fully-supervised [8, 13, 22, 23, 33, 35]   and weakly-supervised [6, 10, 19, 36, 38, 49, 55, 58]. Although these two lines of works have made remarkable successes, they rely heavily on manually annotated datasets.   However, obtaining a large quantity of manual annotations,   especially natural language queries, is expensive and timeconsuming.   To annotate queries, humans need to \ufb01rstly   recognize the visual objects and identify their attributes,   and then determine diverse relationships between them on   a case-by-case basis, such as spatial (e.g., left and right),   preposition (e.g., in and with), action (e.g., throwing some15513   thing), and comparative (e.g., smaller and bigger). Among   them, spatial relation is the most frequently queried one.   To reduce the burden of human annotation, we propose   a pseudo language query based approach (Pseudo-Q) for   visual grounding.   Our inspiration comes from previous   works [17, 31] that address the high annotation cost issue   in image captioning task, by leveraging an unlabelled image set, a sentence corpus, and an off-the-shelf object detector. However, the visual grounding task is more complicated and challenging, as it involves the modelling of relations between objects.   To accurately ground objects by language queries, it\u2019s   fundamental to recognize their categories, attributes, and   relationships. Thus, when it comes to generating pseudo   region-query pairs for an unlabelled image set, we need   to focus on three key components:   (1) salient objects   (nouns) which are most likely to be queried, (2) intrinsic   attributes possessed by the queried objects, and (3) the important spatial relationships between the objects. Motivated by [17, 42], we leverage an off-the-shelf object detector [1] to locate the most notable candidates with high   con\ufb01dence, and an attribute classi\ufb01er [1] to recognize common attributes. However, these detectors are unable to distinguish the spatial relations between objects.   Thus, we   present a heuristic algorithm to determine the spatial relationships between the objects of the same class by comparing their areas and relative coordinates. With these three   essential components, pseudo-queries with respect to spatial relations between objects can be generated.   To further improve the performance of our method, we   also propose a query prompt module which attentively tailors generated pseudo queries into task-related query templates for visual grounding. For the visual-language model,   we put forward a multi-level cross-modality attention mechanism in the fusion module to encourage a deeper fusion   between visual and language features.   Extensive experiments have veri\ufb01ed the effectiveness of   our method. First, in fully supervised manner, it can reduce   human annotation costs by 31% without sacri\ufb01cing original model\u2019s performance on RefCOCO [65]. Second, without bells and whistles, it can obtain superior or comparable   performance even compared with state-of-the-art weaklysupervised visual grounding methods on \ufb01ve datasets, including RefCOCO [65], RefCOCO+ [65], RefCOCOg [40],   ReferItGame [28] and Flickr30K Entities [44].   In summary, this paper makes three-fold contributions:   (1) We introduce the \ufb01rst pseudo-query based visual   grounding method that deals with the most dominant   spatial relationships among objects.   (2) We propose a query prompt module to speci\ufb01cally   tailor pseudo-queries for visual grounding task, and   a visual-language model equipped with multi-level   cross-modality attention is put forward to fully capture   the contextual relationships of different modalities.   (3) Extensive experiments demonstrate that our approach   can not only dramatically reduce the manual labelling   costs without performance sacri\ufb01ce under the fully   supervised condition, but also surpass or achieve   comparable performance with state-of-the-art weaklysupervised visual grounding methods.   2. Related Work   2.1. Natural Language Visual Grounding   Visual grounding is a crucial component in vision and   language, and it serves as the fundamental of other tasks,   such as VQA. Recent visual grounding methods can be   summarized into three categories: fully-supervised [8, 13,   22, 23, 33, 35], weakly-supervised [6, 10, 19, 36, 38, 49, 55,   58], and unsupervised [54, 63].   Fully-supervised methods rely heavily on the manual labeled patch-query pairs.   Unfortunately, obtaining such sophisticated annotations is   expensive and time-consuming.   Consequently, weaklysupervised approaches attempt to alleviate the issue by utilizing only image-query pairs. These methods [6, 38] usually leverage a mature object detector to compensate the   missing bounding box labels for training.   However, annotating the language queries for salient objects in an image is the most laborious part. Thus, unsupervised methods [54, 63] attempt to train a model or directly detect   queried objects without any annotations. Our work is also   an unsupervised method. However, unlike previous methods, we present a novel method, named Pseudo-Q, to automatically generate pseudo-queries for supervised learning.   2.2. Vision-Language Transformer   Transformer [53] has been \ufb01rstly proposed to address   natural language processing (NLP) tasks. ViT [16] makes   the \ufb01rst attempt to apply a transformer for image classi\ufb01cation task [12]. Motivated by the success of ViT, DETR [5]   and Segmenter [48] further extend the transformer for object detection and segmentation tasks respectively.   There are also many efforts [9, 13, 32, 39, 45, 51],   which try to handle visual-language tasks by transformer.   TransVG [13] proposes a novel framework with transformer   structure for visual grounding task. CLIP [45] and UNITER   [9] leverage transformers for jointly learning text and image   representations.   LXMERT [51] establishes a large-scale   transformer to learn cross-modality representation. In this   work, we propose a novel multi-level cross-modality attention on the top of the TransVG for cross-modality learning.   2.3. Visual Recognition without Annotation   There have been several works [3, 7, 11, 15, 18, 27, 42,   52, 69] for zero-shot visual tasks. Zero-shot object detection tasks [3, 18] are designed for detecting unseen object   15514   Visual Language Model   Object Detecor   Pseudo-Query   Generation   Module   Object Proposals   \u00b7\u00b7\u00b7   Query Prompt Module         <Relationship>         <Relationship>   <Noun>   (a) Training Stage   (b) Inference Stage   Unlabeled Image   Query Prompt   Module   Test Query   Prompt: find the region that   corresponds to the description   {pseudo-query}\u00a0   Final query: find the region that   corresponds to the description\u00a0   left standing\u00a0man   Query1: right\u00a0building   Query2: building on the left   \u00b7\u00b7\u00b7   person on the right   man in the middle   Visual Language Model   Test Image   Visual Grounding Results   Output: (x, y, w, h)   (x, y)   w   h   Query3: left\u00a0standing\u00a0man   left man   Query4: red\u00a0jacket man on the left   Query5: man in the center/behind   Query6: man on the right   <Attribute>   Figure 2. Overview of our Pseudo-Q method. Better view in color and zoom in. The proposed approach consists of a pseudo-query   generation module, a query prompt module, and a visual-language model. (a) During the training stage, pseudo image region-query pairs   are generated to train visual language model. (b) During the inference stage, the test query is \ufb01lled into the prompt template, and the target   object is located by the trained model.   classes whose labels are missing. While zero-shot action   recognition task [11, 15, 27] recognizes pre-de\ufb01ned action   categories without using action labels. Our work\u2019s emphasis lies in locating object regions without using any taskrelated annotations, e.g., image regions and queries.   As for zero-shot visual grounding, the pioneering work   ZSGNet [47] focuses on query phrases which may contain   unseen nouns or object categories. It consists of a language   module to encode query features, a visual module to extract   image features, and an anchor generator to produce anchors.   However, note that the focus of our work is different from   ZSGNet, which is proposed for recognizing unseen classes,   In addition, ZSGNet utilizes manual annotations while we   do not rely on any task-related labels.   3. Method   In this section, we explain our Pseudo-Q method in detail. In Sec. 3.1, we introduce the overview of Pseudo-Q.   In Sec. 3.2, we elaborate the pseudo-query generation module. In Sec. 3.3, the details of the task-related query prompt   module are shown. Finally, we illustrate the mechanism of   our multi-level cross-modality attention in Sec. 3.4.   3.1. Overview   Previous visual grounding methods rely on expensive   human annotations, i.e., image region-query pairs for fullysupervised approaches [13,22,35] or image-query pairs for   weakly-supervised approaches [36, 37, 49]. We \ufb01rstly propose a pseudo language query based method without using   any task-related annotations at training.   Speci\ufb01cally, the Pseudo-Q approach consists of three   components, including: (1) pseudo-query generation module, (2) query prompt module, and (3) visual-language   model. The illustration of Pseudo-Q is shown in Figure 2.   Taking an unlabeled image as an explanation, the detector   can produce several object proposals. Then, these proposals are fed into pseudo-query generation module, which can   automatically generate nouns, attributes, and relationships   for these proposals. Together with these elements, we can   easily create pseudo language queries.   Subsequently, the query prompt module re\ufb01nes created   pseudo language queries for visual grounding task. Finally,   we propose a visual-language model to fully capture the   contextual relationship between the image regions and corresponding pseudo language queries.   3.2. Pseudo-Query Generation   In general, the \ufb01rst step for visual grounding is recognizing the categories of queried objects. However, such a   simple grounding strategy leads to ambiguities in complex   scenes, e.g., \u201ca talk person on the left\u201d or \u201ca short person   on the right\u201d, without understanding their spatial relations   or attributes. Thus, to accurately locate visual objects by   language queries, a visual grounding model needs to understand queried objects\u2019 categories, attributes, and their relationships. Based on the above analysis, generating pseudo   language queries for candidate objects involving three key   components: nouns, attributes and relationships.   Nouns. Inspired by works [17,31,42], we adopt an offthe-shelf detector [1] to obtain the object proposals. Unlike   image classi\ufb01cation task where each image contains only   one major object, scenes in visual grounding task are more   complex due to plenty of candidate objects. While it is natural to select the most salient objects as candidates, such a   process requires intensive manual labor which is not available in our setup. Instead, we use detection con\ufb01dence as   a criterion. Concretely, the top-N objects with highest con\ufb01dence are kept as our proposals. Furthermore, we empirically discover that the detector will focus on a large quantity   of tiny objects which are less likely to be queried. Thus, we   propose to remove tiny objects before generating proposals.   15515   (a) Pseudo-Query Generation Module   man in the behind   <Spatial Relationship>   Generated Pseudo-Queries   (b) Visual Language Model   Query Prompt Module   Final query: find the region that   corresponds to the description   \u00a0left\u00a0standing\u00a0man   ConvNet   Layer1   Language   Encoder   Cross-Modality Fusion Module   Regression Head   Pred   (x,y,w,h)   Layer2   Layer6   \u00b7\u00b7\u00b7   Layer1   Visual Encoder   Attention Layer   Attention\u00a0Layer   Attention\u00a0Layer   \u00b7\u00b7\u00b7   Feature concatenation operation   \u00b7\u00b7\u00b7   (x1,y1)   (x2,y2)   (x3,y3)   x1<x2<x3   area1/area2>\u03b1   left\u00a0man; center\u00a0man; right man         <Relationship>         Query1: right wooden\u00a0building   Query2: building on the left   Query3: left\u00a0standing\u00a0man   Query4: red\u00a0jacket man on the left   Query5: man in the center/behind   Query6: man on the right   <Common Attribute>   classifer   <Attribute>   standing   red   building   wooden   IoU(\u00a0 \u00a0,\u00a0 \u00a0)>\u03b2   jacket man\u00a0   <Noun>   man   jacket   ...   ...   1.Select top-6 objects based on confidence.   2.Uniformly sample 6 pseudo-queries.   <Noun><Attribute><Relationship>   <Clothes Attribute>   Figure 3. (a) The pseudo-query generation module produces spatial relationships and attributes for corresponding objects. (b) The   visual-language model consists of a visual encoder, a language encoder, and a cross-modality fusion module.   Attributes. They are important semantic cues that help   models understand scenes better. We investigate that, in   existing datasets [28, 40, 65], common attributes including   color, size (tall), material (wooden) and human state of motion (e.g. standing and walking), etc. Similar to obtaining   the nouns, we take advantage of an off-the-shelf attribute   classi\ufb01er [1] to predict the above common attributes of corresponding objects. In general, one object may have several attributes, such as \u201ca tall person is walking\u201d, and it is   ideal to recognize as many attributes as possible. However,   limited by the capability of the model, we only keep the attribute with the highest con\ufb01dence and exceeding a prede\ufb01ned threshold as the \ufb01nal proposal. Furthermore, clothes   are also important attributes for a person which can be determined by calculating the IoU value between clothes and   person, as shown in Figure 3(a).   Relationships. We observe that spatial relationship is   one of the most frequently used relations in most existing   datasets [40,65] to distinguish different objects. In order to   excavate latent spatial relationships, we propose a heuristic   algorithm as shown in Figure 3(a).   In general, spatial relationship can be divided into three   dimensions: horizontal (i.e., left, middle, and right), vertical (i.e., top and bottom) and depth (i.e., front and behind). Note that each previously generated object proposal   is represented by a set of coordinates which naturally embrace spatial information.   We can obtain the horizontal   and vertical spatial relationships by comparing the center   coordinates of objects along with these two dimensions.   Meanwhile, to increase the robustness of the algorithm,   the numerical difference of two objects\u2019 coordinates in the   same dimension is required to be greater than a pre-de\ufb01ned   threshold. Finally, we can determine the spatial relations,   such as left, right and center, for different visual objects of   the same category.   In the depth dimension, we assume that, for the same   kind of object, the closer the object is to a camera the larger   the object region. Concretely, we calculate the ratio of the   area of the largest object region to the smallest object region   and set a threshold to determine whether there is a front   and behind relationship. If the ratio exceeds the threshold,   we assign front and behind relationships to the largest and   smallest objects respectively.   Pseudo-queries. After obtaining three key elements, we   can generate all possible pseudo-queries for an image following the templates in Appendix. Finally, we sample up to   M pseudo image region-query pairs if the number of candidates is greater than M, otherwise, we sample all.   3.3. Query Prompt Module   With the advances of pre-trained language models [4,   14], prompt engineering is proposed to better utilize their   learned knowledge at pre-training stage. Inspired by the   recent success of prompt engineering in visual-language   tasks, e.g., image-language pre-training [45], we propose a   query prompt module to excavate the hidden knowledge of   pre-trained language model (Sec. 3.4) by re\ufb01ning generated   pseudo language queries for visual grounding task.   While the prompt templates proposed in CLIP [45]   works well for the image classi\ufb01cation task, we empirically \ufb01nd that they are ineffective for the challenging visual grounding task. Consequently, in this work, we explore   prompt templates exclusively for visual grounding. Our introduced query prompt module follows certain templates,   e.g., \u201c\ufb01nd the region that corresponds to the description   {pseudo-query}\u201d or \u201cwhich region does the text {pseudoquery} describe?\u201d. Such design is speci\ufb01cally tailored for   visual grounding task, since the focus of this task lies on   locating the regions of referred objects.   3.4. Visual-Language Model   Our visual-language model consists of a visual encoder,   a language encoder and a cross-modality fusion module   to fuse information from two modalities. The designs of   15516   the visual encoder and the language encoder are following   TransVG [13]. We elaborate them in Appendix.   Cross-modality fusion module. Previous method [13]   naively utilizes \ufb01nal features of visual and language encoders to acquire cross-modality information.   However,   such a simple approach is suboptimal, since each level of   visual feature possesses valuable semantic information [21,   34]. To be more speci\ufb01c, low-level features usually denote   coarse information, e.g., shape and edge, while high-level   features can represent \ufb01ner information, e.g., intrinsic object properties. Thus, we further propose multi-level crossmodality attention (ML-CMA) to thoroughly fuse textual   embedding with multi-level visual features.   The mechanism of ML-CMA is shown in Figure 3(b).   Features of each visual transformer layer are passed into   a cross-modality fusion module with the extracted textual   embedding to calculate cross-modality self-attention. Then,   we concatenate all updated visual or textual features from   different levels respectively, and utilize a fully connected   layer to map them into the original dimension. Finally, all   features are concatenated and fed into a regression head to   predict referred object regions. The regression head composes of three fully connected layers.   4. Experiments   Dataset and setups.   Following previous visual   grounding methods [13, 60], we evaluate our method   on \ufb01ve datasets: RefCOCO [65], RefCOCO+ [65], RefCOCOg [40], ReferItGame [28], and Flickr30K Entities [44]. We follow the same train/val/test splits from [13]   for all datasets. The number of training images in these \ufb01ve   datasets are 16994, 16992, 24698, 8994, and 29779. Note   that we don\u2019t use any manual annotations during the training stage, they are only leveraged for evaluation.   Implementation details. We choose a pre-trained detector [1] and attribute classi\ufb01er [1] on Visual Genome   dataset [30], which contains 1600 object and 400 attribute   categories. As we mentioned in Sec. 3.2, we select top-N   and sample up to M pseudo-queries for each image. Specifically, on RefCOCO, we select top-3 objects according to   the detection con\ufb01dence and uniformly sample 6 pseudoqueries from all possible candidates. As for RefCOCO+,   RefCOCOg, ReferItGame, and Flickr30K Entities, we use   top-3 objects/12 pseudo-queries, top-2 objects/4 pseudoqueries, top-6 objects/15 pseudo-queries, and top-7 objects/28 pseudo-queries, respectively.   Training details.   All our experiments are conducted   under Pytorch framework [43] with 8 RTX3090 GPUs.   Our visual-language model is end-to-end optimized with   AdamW. The initial learning rate is set to 2.5\u00d710\u22125 for the   visual and language encoder and 2.5 \u00d7 10\u22124 for the crossmodality fusion module. The batch size is 256. All the   datasets use cosine learning rate schedule except Flickr30K   Entities which adopts exponential decay schedule with 0.85   decay rate. Our model is trained with 10 epochs on RefCOCO, RefCOCOg, and ReferItGame, 20 epochs on RefCOCO+ and Flickr30K Entities. The data augmentations   that we utilize are following TransVG [13], e.g., RandomResizeCrop, RandomHorizontalFlip and ColorJitter.   4.1. Comparison with State-of-the-art Methods   We report comparison results with existing unsupervised [54,62,63] and weakly-supervised [38,49,55] methods. Note that the weakly-supervised methods are trained   with expensive annotated queries. As", "conf": "CVPR", "year": "2022", "index": 184}, {"title": "RGNet: A Unified Clip Retrieval and Grounding   Network for Long Videos   Tanveer Hannan1,2\u22c6   Md Mohaiminul Islam3   Thomas Seidl1,2   Gedas   Bertasius3   1 LMU Munich   2 MCML   3 UNC Chapel Hill", "abstract": ". Locating specific moments within long videos (20\u2013120 minutes) presents a significant challenge, akin to finding a needle in a haystack.   Adapting existing short video (5\u201330 seconds) grounding methods to this   problem yields poor performance. Since most real-life videos, such as   those on YouTube and AR/VR, are lengthy, addressing this issue is crucial. Existing methods typically operate in two stages: clip retrieval and   grounding. However, this disjoint process limits the retrieval module\u2019s   fine-grained event understanding, crucial for specific moment detection.   We propose RGNet which deeply integrates clip retrieval and grounding into a single network capable of processing long videos into multiple   granular levels, e.g., clips and frames. Its core component is a novel transformer encoder, RG-Encoder, that unifies the two stages through shared   features and mutual optimization. The encoder incorporates a sparse   attention mechanism and an attention loss to model both granularity   jointly. Moreover, we introduce a contrastive clip sampling technique to   mimic the long video paradigm closely during training. RGNet surpasses   prior methods, showcasing state-of-the-art performance on long video   temporal grounding (LVTG) datasets MAD and Ego4D. The code is   released at https://github.com/Tanveer81/RGNet.   Keywords: Long Video Temporal Grounding \u00b7 Moment Localization   1", "content": "The exponential rise in online videos has created a demand for effective content retrieval systems. The retrieval results of user-defined queries, particularly   in long videos (20-120 minutes), often require locating specific moments. Most   existing solutions [5,41,53,55], tailored for short videos (5-30 seconds), struggle   when applied to hour-long videos, slowing down the retrieval and impacting the   quality of the results [40,54]. This challenge emerges because it is very difficult   to locate short moments in a long video based on text queries, a task known as   Long Video Temporal Grounding (LVTG) [14,40].   \u22c6Corresponding author: hannan@dbs.ifi.lmu.de   2   Hannan et al.   \u00a0Long\u00a0   Video   38:36   38:42   Time   00:00   90:00   Query: Mum hangs   up the laundry outside   of the\u00a0farmhouse.   36:00   39:00   \u00a0Retrieved\u00a0   Clip Feature   Predicted   Moment   RG-Encoder   Decoder   RGNet   Fig. 1: Overview of RGNet. It predicts the moment boundary specified by textual   queries from an hour-long video. First, our proposed RG-Encoder maps the video and   text features to a joint space and retrieves the relevant clip feature. The subsequent   grounding decoder processes the retrieved features to predict the beginning and end   times of the moment. The encoder parallelly operates at multiple levels of granularity   (e.g., clip and frame) to achieve an end-to-end-solution.   A straightforward solution [16,37] for the LVTG task is to divide the video   into shorter clips, retrieve the most relevant one, and apply a grounding network   to predict the moment. However, the grounding network cannot rectify failure   in the clip retrieval phase.   Empirically, we evaluate the impact of the two stages in Sec. 5.3 and identify   clip retrieval as the primary factor for poor performance.   The existing methods have a separate module for clip retrieval, which is disjoint from their grounding network (see Fig. 2). They typically follow a text-video   retrieval [13,23,29] technique to select the relevant clip. However, video retrieval   only requires a high-level understanding of video topics. For example, a typical   video retrieval query is \u201cA movie about a family surviving in a farmhouse.\u201d. In   contrast, an example clip retrieval query in Fig. 1 is \u201cFind the moment when the   mum hangs up laundry outside the farmhouse\u201d. These specific moments from a   long video require fine-grained event understanding. Thus, video retrieval models   are suboptimal for these fine-grained moment localization tasks.   We attribute this disjoint retrieval to the poor performance of these models. However, unifying clip selection and grounding is challenging due to their   distinct setups. The former is a retrieval task, whereas the latter is a regression   task. Moreover, it requires modeling two levels of video granularity: clip and   frame. To address these technical challenges, we propose RGNet, a unified clip   Retrieval and Grounding Network (see Fig. 1). The single network enables endto-end training of both stages. This improves the retrieval module\u2019s fine-grained   event understanding by directly optimizing it with the moment annotations. Parallelly, the grounding network also benefits from the strong multimodal features   produced by our encoder, further enhancing the localization capability.   To achieve this unification, we propose a novel transformer encoder, RGEncoder, which enables the grounding network to perform clip retrieval. This   RGNet   3   Long Video   Grounding   Encoder   Moment   Retrieval\u00a0   Network   Text   Text   Long Video   Moment   Text   Existing Disjoint Network   Proposed Unified RGNet   Grounding   Decoder   Grounding   Decoder   Retrieval &\u00a0   Grounding   Encoder   Video Clip   Video Clip   Fig. 2: Unified Solution. (left) Existing methods involve a separate retrieval and   grounding network. The disjoint retrieval lacks fine-grained event understanding, which   is crucial for moment localization. (right) Our unified network architecture overcomes   it by deeply integrating the retrieval module with the grounding objective.   eliminates the suboptimal video retrieval network and effectively models long   video clip retrieval. To enable the retrieval module to understand fine-grained   events, we design a sparse-attention mechanism in the encoder. The sparse attention enables the retrieval module to focus on specific events in the video,   which is more synergistic with the grounding task. Thus RG-Encoder is capable   of operating both at clip and frame level granularity. We propose an Intra-Clip   Attention Loss, which motivates the sparse attention to focus more on the video   frames aligned with the specified event. Furthermore, we propose a negative clip   mining technique to simulate clip retrieval from long videos during training.   This negative mining enables our network to train on a large batch size with an   inter-clip contrastive loss. The large number of negative clips in the batch closely   mimics the long video paradigm and reduces the gap between the training and   test phases.   Together, these components of RGNet bridge the gap between the two stages   of LVTG, enhancing the fine-grained event understanding in hour-long videos.   In summary, our contributions are fourfold:   \u2013 We systematically deconstruct existing LVTG methods into clip retrieval and   grounding stages. Through empirical evaluations, we discern that disjoint   retrieval is the primary factor contributing to poor performance.   \u2013 Based on our observations, we introduce RGNet, which integrates clip retrieval with grounding through parallel clip and frame-level modeling. This   obviates the necessity for a separate video retrieval network, replaced instead   by an end-to-end clip retrieval module tailored specifically for long videos.   \u2013 We introduce sparse attention to the retriever and a corresponding loss to   model fine-grained event understanding in long-range video. We propose a   contrastive negative clip-mining strategy to simulate clip retrieval from a   long video during training.   \u2013 RGNet achieves state-of-the-art performance across both LVTG datasets,   Ego4D [14] and MAD [40]. For instance, RGNet outperforms the previous   best Ego4D method [39] by a substantial margin (9.7%).   4   Hannan et al.   2   Related Literature   Short Video Temporal Grounding. The recent grounding methods mainly   focus on short videos. The best-performing ones utilize transformer variants   [3, 27, 34, 56]. For example, Moment-DETR [22] adopted transformers for combined video and text features. Subsequent works, such as UMT [28] and QDDETR [35], split the cross-modal and unimodal modeling of video and text   features for enhanced performance. EATR [18] improves its decoder with videospecific moment queries. These models were initially designed for shorter videos.   However, when applied to hour-long videos, the moments become extremely tiny,   posing a challenge akin to finding a needle in a haystack. Hence, these methods   perform poorly on long videos, as noted by the authors of the MAD dataset [40].   In contrast, we effectively address the task by simultaneously processing long   videos at two granular levels (video clips and frames) within a single network.   Long Video Temporal Grounding. Recently, video temporal grounding has   been adapted for long videos with MAD [40] and Ego4D [14] datasets. Typically,   the methods designed for long videos involve two stages. Proposal-free methods [2,26,41,53,55] segment lengthy videos into smaller parts to predict candidate moments and then rank them to obtain the final predictions. Proposal-based   methods [16,37] generate proposal clips or anchors and subsequently apply their   grounding model to the retrieved proposals. Some methods, like CONE [16], and   M-Guidance [2], utilize the detection transformer [3] as the grounding network   to improve the grounding phase. These proposal-based models are more efficient   and perform better than their counterparts. However, they need a separate retrieval module to select the proposal clips. This disjoint two-stage architecture is   what we find to be the main reason for the poor performance of these models. In   contrast, we propose a novel transformer encoder that solves both clip retrieval   and grounding in a unified way, enabling end-to-end optimization.   Video-text Retrieval. The early retrieval methods [4, 7, 21, 49, 50] designed   multimodal fusion techniques to align pre-trained video and text models. To   bridge the gap between pre-training and downstream tasks, large-scale pretrained video-language representations [1, 6, 9, 11, 31, 42, 45\u201347] have been introduced in various works. Several studies [8, 10, 19, 20, 32, 43, 44, 48, 51] have   transferred the knowledge from CLIP [38] to text-video retrieval tasks. In recent studies, various sampling strategies [13,23,29] have been explored, enabling   the model to selectively concentrate on pertinent video frames based on provided text inputs. For instance, Clip-BERT [23] introduces a sparse sampling   strategy, X-Pool [13] utilizes a cross-modal attention model, and TS2-Net [29]   adapts CLIP to capture temporal information and eliminate unimportant tokens. While existing methods aim to retrieve high-level video topics from a pool   of video collections, moment localization demands a fine grained understanding   to identify specific events within a video. Despite the similar setup to video retrieval, retrieving the corresponding clip from a long video necessitates deeper   RGNet   5   event comprehension. Therefore, we tailored our encoder to enhance event understanding and simulated precise clip retrieval conditions during training.   3   RGNet   This section presents a detailed description of our proposed unified LVTG method.   As illustrated in Fig. 1, RGNet takes a long video and a text query as input and   predicts the precise moment semantically correlated with the query in an end-toend manner. The whole network is divided into the proposed RG-Encoder (Fig.   3) and a decoder (Sec. 3.3). The encoder retrieves the most relevant clip feature   from the input long video. The decoder then processes the retrieved clip feature   to predict the precise moment boundary. Additionally, an Intra-Clip Attention   Loss and an Inter-Clip Contrastive Loss are incorporated to facilitate low-level   event understanding in long-range videos.   Cross-Attn.   q   k,v   +   xL   Content   Sparsifier   xL   Context   R   Retrieved Clip Feature   Retrieval-Attn.   C   Fc   P   Video Clips   Gumbel-softmax   Linear Proj.   Clip Frames (F)   Attention\u00a0   Mask (M)   S   Query   Frame Relevancy (G)   R   F   F   CC   Fig. 3: Overview of RG-Encoder. It takes video clips and textual query as input   and retrieves the relevant clip features. First, a cross-attention fuses the clips with text,   and the sparsifier masks the out-of-moment frames. Based on the mask, the retrieval   attention focuses on in-moment frames (colored red) and generates clip-level context   and frame-level content features. We combine the context and content to generate the   retrieved clip feature.   3.1   Feature Extraction   Let us assume we have an untrimmed long video V consisting of T frames and   a query sentence S with N words corresponding to a target moment\u2019s center   \u03c4c and width \u03c4w. Following previous methods [16,37], we use pre-trained frozen   models to extract visual features V = {f 1, f 2, ..., f T } \u2208RT \u00d7Df as well as textual   features S = {w1, w2, ..., wN} \u2208RN\u00d7Dw, where Df and Dw represent the feature   dimensions of video frames and query words. We slice the long video V into clips   of length Lc and feed them into our model. We employ a sliding window of length   6   Hannan et al.   Lc and stride Lc/2 to obtain a collection of Tc clips C = {C1, C2, ..., CTc} \u2208   RTc\u00d7Lc\u00d7Df . Here, the ith clip, Ci = {f si+1, f si+2, ..., f si+Lc} \u2208RLc\u00d7Df , where   si is the start index of the clip.   3.2   RG-Encoder   All the clips C from a single video comprise samples in a batch. The RG-Encoder   (Fig. 3) processes them with the query S to retrieve the relevant clip feature P.   First, the cross-attention generates query-conditioned frame features F. Then,   based on the frame-query correlation calculated from F, the sparsifier determines   the relevant frames inside the clip to produce a mask M for retrieval attention.   We incorporate a learnable token R to capture the context of the clip. Additionally, the frame features F undergo retrieval attention based on the predicted   mask M to generate the content feature Fc. The combination of context R and   content Fc forms the retrieved clip feature P.   Cross Attention: To evaluate the relevance of each frame within the clip Ci,   we utilize cross-attention, where frame features act as queries and text features   serve as keys and values. Cross-attention represents the frame features as a   weighted average of the textual features, scaled by their mutual correlation. This   process results in text-conditioned frame features F i, establishing a fine-grained   correspondence between the modalities. The query, key and values are calculated   as Qi = lQ(Ci) \u2208RLc\u00d7Df , K = lK(S) \u2208RN\u00d7Dq and V = lV (S) \u2208RN\u00d7Dq   respectively. Here, lQ(\u00b7), lK(\u00b7), and lV (\u00b7) denote the projection layers for the   query, key, and value. Then, we apply the cross-attention layer as follows:     \\ begin {gathe re d } F^{i} = \\operatorname {softmax}({{Q}}^i{K}^T){V} + {Q}^{i} \\\\ \\end {gathered}    (1)   Sparsifier: To retrieve the clip, we need to assess the relevance of each clip based   on the text query. Clip relevance is determined by aggregating the relevancy of   its frames. However, given the small moment duration, most frames are unrelated   to the query. Hence, we classify frames into relevant and non-relevant categories   using the sparsifier to assist retrieval attention in focusing on these fine-grained   events. We calculate the relevancy Gj \u2208[0, 1] of the jth frame with Eq. 2. We   use a differentiable Gumbel-softmax function [15, 17] for enabling end-to-end   training. The detailed calculation is in the supplementary materials.     { G}^{j} = \\operatorname {Gumbel-softmax}(\\text {linear}({f}^{j})) \\label {eqn:sampler}    (2)   The intra-clip attention loss optimizes this classification using the ground   truth frame relevancy information described in Sec. 3.4. To distinguish irrelevant frames from their counterparts, we constrain their updates in the retrieval   attention. This restriction is implemented with an attention mask computed   from the learned frame relevancy Gj. The attention mask M between jth and   kth frame is calculated with Eq. 3. Here, the jth and kth frames are the attention query and key, respectively. This leads to a reduced set of relevant and   informative frames for subsequent attention.   RGNet   7     \\l ab e   l    {e q: m ask } { M }   (j , k)=\\left \\{\\begin {array}{ll} 0 & \\text { if } {G}^{j}>0.5 \\hspace {5pt} \\text {or} \\hspace {5pt} j = k\\\\ -\\infty & \\text { otherwise } \\end {array} \\right .    (3)   Retrieval Attention: We aim for a learnable clip retrieval function to train   end-to-end with the grounding objective. Thus, we devise the retrieval attention   for aggregating the text-conditioned frame features based on their relevancy.   To condense the ith clip into a contextual feature embedding, we introduce a   randomly initialized learnable vector Ri. We denote this as retrieval token, which   is trained with the inter-clip contrastive loss.   First, we concatenate the retrieval token with the text-conditioned frame features to create the queries Qi = [Ri, F i] for retrieval attention. We also generate   an attention mask containing all zeros for the retrieval token and concatenate it   with M. We set the key K = Qi, and the value V = Qi to calculate the retrieval   attention according to Eq. 4. We get the clip-level context feature, Ri = \u02dcQi,1   and frame-level content features, Fc   i,j = [ \u02dcQi,2, \u02dcQi,3, ..., \u02dcQi,Lc+1]. Here, Fc   i,j is   the jth frame feature of the ith clip.     \\ b egin {gather e d} { \\Tilde {Q}}^{i} = \\operatorname {softmax}({Q}^{i}{K}^{T}+ {M}){V} + {Q}^{i} \\\\ \\end {gathered} \\label {eq:ret_attn}    (4)   Further, the context feature R is utilized to retrieve the clip. It undergoes a   linear projection layer, ls(\u00b7), to obtain context scores Sr = sigmoid(ls(R)). We   retrieve the most relevant clip based on the context score Sr. Importantly, the   context R represents the queried moment by attending to the reduced set of relevant frames. So, we fuse it with the frame features with Eq. 5 to produce stronger   multimodal inputs for the decoder. It enhances the decoder\u2019s localization capability significantly. The decoder only processes features from the retrieved clip   to predict the moment.     {P } ^{   i,j }  =  {F_c}^{i,j} + {R}^i \\times {G}^{j}, \\hspace {1pt} i \\in P, \\forall j\\\\ \\label {eq:fuse} i   (5)   3.3   Grounding Decoder   We process the retrieved clip features with a transformer decoder to predict   precise moment boundaries. Previous methods [16,22] feed both modalities into   the decoder, introducing multiple positional information (e.g., time in the video   and word order in the text) that complicates the detection task. In contrast, our   encoder injects text information directly into the clip, eliminating the requirement to feed both modalities into the decoder. We incorporate learnable anchor   queries [27] to represent the moment\u2019s center and width as (\u03c4c, \u03c4w). The decoder   has 2 cross-attention and 2 self-attention layers.   3.4   Loss Functions   Intra-Clip Attention Loss guides our sparsifier to distinguish frames within   and outside of the ground truth moment. This enables the retrieval attention   to focus on relevant video regions and achieve fine-grained event understanding.   8   Hannan et al.   Specifically, the in-moment frames are required to maintain higher relevancy   scores than those outside the ground truth moment. The loss is defined as:     \\be g in {sp l i t} \\m athca l  {L}_ {\\text {attn}} = & \\operatorname {max}(0, \\Delta + S_c(i,j_{\\text {out}}) - S_c(i,j_{\\text {in}})) \\end {split}   (6)   In the equation, Sc is the relevancy score, \u2206represents the margin, and anchor   i is an in-moment frame. We randomly chose in-moment and out-of-moment   frames jin and jout from the same clip. To calculate the relevancy score, the   context features Ri, and encoded clip P i,j undergoes linear layers , lr(\u00b7) and   lc(\u00b7), to obtain projected context Ri   proj = lr(Ri) \u2208RDl and content features,   P i,j   proj = lc(P i,j) \u2208RLc\u00d7Dl. Then, we calculate the frame-level relevancy score:     S_c (i , j)    = { R } ^i_   {{proj}} \\cdot P^{i,j}_{{proj}} \\label {eq:samp_loss}    (7)   Inter-Clip Contrastive loss. A long video contains many clips with similar   environments and scenes. However, most of these clips do not contain the specific   moment we are searching for. So, high-level video understanding is not enough   to distinguish precise moments. During the test phase, we need to handle many   such negative clips. To tackle this issue, unlike previous approaches [16], we   sample a large number of negative clips and train our retriever on a large batch   size. Hence, this negative clip sampling reduces the disparity between the train   and test phases.   For this section, we denote the context feature, Ri,j for ith proposal and   jth text query. We train our model contrastively with positive text-clip pairs   [Ri,j]i=j and negative pairs [Ri,j]i\u0338=j. We use the InfoNCE loss [36] to identify   the positive text-clip pair amongst a set of unrelated negative samples. Here,   lcont(\u00b7) is a linear projection layer that transforms the Df dimensional context   feature into a one-dimensional logit.     \\ma t h   c   a   l {   L}_{\\text {cont}   }    = - \\sum _{i} log \\frac {\\exp (l_{\\text {cont}}({R^{i,i}}))}{\\sum _{j}\\exp (l_{\\text {cont}}({R^{i,j}}))} \\label {equ: adapt}    (8)   Grounding loss. The objective functions for grounding, which aim to locate   desired moments, are adopted from the baseline approach [22]. The grounding   loss Lg measures the discrepancy between the ground truth (GT) and predicted   moments. A one-to-one correspondence between GT and predicted moments is   established using the Hungarian algorithm. This loss includes both an L1 loss   and a generalized IoU loss (LgIoU). Additionally, a cross-entropy loss LCE is   employed to classify predicted moments as either foreground or background.   Thus, Lg is defined as follows:   Lg = \u03bbL1||\u03c4 \u2212\u02c6\u03c4|| + \u03bbgIoULgIoU(\u03c4, \u02c6\u03c4) + \u03bbCELCE,   (9)   where \u03c4 and \u02c6\u03c4 represent the ground-truth moment and its corresponding prediction, containing center coordinates \u03c4c and width \u03c4w. The hyper-parameters   \u03bb\u2217are used for balancing the losses. Finally, with the attention and contrastive   loss, the total loss Ltotal is defined as follows:   Ltotal = \u03bbattnLattn + \u03bbcontLcont + Lg   (10)   RGNet   9   4   Experimental Setup   4.1   Datasets and Evaluation Metric   MAD [40] is an extensive dataset comprising 1.2K hours of full-length movies   and 384K natural language queries, each associated with specific moments in   the videos. The videos are orders of magnitude longer than previous datasets,   with an average duration of 110 minutes, while the specified text moments are   on average 4.1 seconds. This small moment-to-video ratio poses a significant   challenge for the grounding task.   Ego4D-NLQ [14] is a large-scale egocentric video data set with multiple challenges. Specifically, we use the episodic memory benchmark Ego4D-NLQ, which   requires localizing where the answer to a natural language query can be seen. It   contains around 13 template questions, amounting to around 74K queries. The   train, val, and test sets contain 11.3K, 3.9K, and 4.0K queries. The video length   ranges from 8 to 20 minutes, with an average of 8.25 minutes, and the average   moment duration is 8.3 seconds. This means the moments constitute only 2% of   the input video on average.   Grounding Metric: For grounding, we follow previous methods [16, 40], and   adopt the standard metric Recall@k at IoU=\u03b8 (Rk\u03b8). This metric represents   the percentage of testing samples with at least one grounding prediction whose   intersection over union (IoU) with the ground truth (GT) is larger than \u03b8 among   the top-k predictions.   Retrieval Metric: To assess our proposal retrieval stage independently of the   grounding, we utilize the standard retrieval metric [13], Recall at Rank k (R@k).   This metric calculates the percentage of GT moments present in the top-k retrieved proposals.   4.2   Implementation Details:   We use pre-trained CLIP [38] and EgoVLP [24] models to extract video frames   from MAD and Ego4D. Text features for both datasets are extracted using   CLIP. The feature extractors are frozen, and their pre-trained weights remain   unchanged during training. For Ego4D, we pre-train our model with NaQ annotations from previous work [39]. The proposal length, Wc is set to 180s for MAD   and 48s for Ego4D, with only the top-30 and top-5 proposals retrieved, respectively. Training on Ego4D and MAD is conducted on four Nvidia-RTX-A6000   GPUs while fine-tuning on Ego4D is performed on a single GPU. Considering   the longer proposals in MAD, we utilize a batch size of 32 for Ego4D and 8   for MAD. The number of moment queries is set to 5. For loss computation, we   set hyperparameters as \u03bbL1 = 10, \u03bbgIoU = 1, \u03bbCE = 4, \u03bbsamp = 1, \u03bbcont = 10,   and \u2206= 0.2. We use Xavier initialization [12] and employ AdamW [30] with   an initial learning rate of 1 \u00d7 10\u22124. RGNet is trained for 35 and 200 epochs on   10   Hannan et al.   MAD and Ego4D datasets. The initial learning rate is reduced by one order of   magnitude at epochs 25 for MAD and 120 for Ego4D.   5   Results and Analysis   We first compare our performance with previous state-of-the-art models. Then,   we report detailed ablation studies of our proposed method and visualize the   qualitative results compared to the disjoint baseline.   Model   NaQ   Ego4D-NLQ [14]   MAD [40]   Avg   R1.3   R5.3   R1.5   R5.5   Avg   R1.1   R5.1 R1.3 R5.3 R1.5 R5.5   Avg   2D-TAN [55]   \u2717   5.04   12.89   2.02   5.88   6.46   3.22   11.90 2.52   9.25   1.58   5.69   5.69   6.07   UniVTG [25]   \u2717   11.74   7.54   3.25   7.88   7.60   VSLNet [53]   \u2713   10.26 19.01   5.81   12.67 11.93   VLG-Net [41]   \u2717   3.64   11.66 2.76   9.31   1.65   5.99   5.84   M-DETR [22]   \u2717   8.23   23.23   5.01   13.37 12.46   3.60   12.98 2.81   9.86   1.67   5.58   6.08   9.27   M-Guide [2]   \u2717   9.30   18.96 4.65 13.06 2.16   7.40   9.26   SOONet [37]   \u2717   8.00   22.40   3.76   11.09 11.31   11.26 23.21 9.00 19.64 5.32 13.14 13.59 12.45   H-Hands [52]   \u2717   13.20 23.30   7.90   15.60 15.00   CONE [16]   \u2717   14.15 30.33   8.18   18.02 17.67   8.90   20.51 6.87 16.11 4.10   9.59   11.01 14.34   EgoVLP   \u2713   15.90 26.38   9.46   17.80 17.38   ReLeR [39]   \u2713   19.31 23.62 11.59 15.75 17.57   Ours (Default)   \u2717   18.28 34.02 12.04 22.89 21.81 12.43 25.12 9.48 18.72 5.61 10.86 13.70 17.80   Ours   \u2713   20.63 41.67 12.47 25.08 24.96 12.43 25.12 9.48 18.72 5.61 10.86 13.70 19.33   Table 1: Main results on Ego4D-NLQ and MAD. RGNet achieves state-ofthe-art performance on both datasets. Our default network is trained without NaQ   annotations [39] on Ego4D. Even without NaQ annotations, our default network shows   a larger improvement, underscoring the effectiveness of our solution with limited data.   5.1   Result on the Ego4D-NLQ Dataset   RGNet demonstrates state-of-the-art performance on all metrics for the Ego4DNLQ benchmark. As shown in Table 1, RGNet improves its primary evaluation   metric, the R1.3 and R5.3 score average by 9.7%. Separately, we outperform the   previous best methods on R1.3 and R5.3 by 1.32% and 18.1%, respectively.   This improvement is consistent across other metrics as well. The unified clip and   frame-level modeling in RGNet parallelly capture long- and short-range temporal   dependencies, contributing to its superior performance. By modeling the entire   video as a whole, RGNet learns better cross-modal alignment by contrasting   visual scenarios from various events within the same video. Notably, compared   to SooNet [37] and CONE [16], two prominent proposal-based methods, RGNet   achieves an average score improvement of 13.7% and 7.3%. This substantial   improvement is attributed to our unified modeling of clip retrieval, whereas they   separately employ a retrieval similarity search between the video and the text.   5.2   Result on the MAD Dataset   We compare our model\u2019s performance on the MAD dataset in Tab. 1. RGNet   achieves state-of-the-art performance on both the R1.1 and R5.1 scores by outRGNet   11   performing the previous best SooNet [37] by 1.2% and 1.9%. Together with its   competitive performance on other metrics, RGNet demonstrates the importance   of end-to-end modeling of long videos. Specifically, our performance of R1 is superior at all IoU thresholds, which testifies to the effectiveness of the proposed   retrieval method. Notably, prior models rely on heuristic dot product similarity search applied to CLIP [38] features for the retrieval phase. In contrast, our   shared retrieval feature and its unified optimization with the grounding objective   drastically enhance the first stage retrieval, as seen in Tab. 2. It further helps   the network capture more discriminative events crucial for detecting extremely   tiny moments in hour-long videos.   5.3   Disjoint vs. Unified Architecture   Since the long video temporal grounding (LVTG) performance relies on the first   stage of clip retrieval, it does not perfectly reflect the actual moment localization capability of its grounding network. To assess the standalone grounding   performance, we designed an oracle experiment operating exclusively on the clip   where the ground truth moment is present. Compared to the oracle grounding,   the LVTG R1.3 drops by 15.7% and 23.9% for Ego4D and MAD in Tab. 2.   The poor grounding in LVTG emerges from the suboptimal retrieval accuracy   of the disjoint network. Hence, retrieving clips where the ground truth moment   exists is crucial, independent of the subsequent grounding accuracy. Regardless   of the effectiveness of moment boundary detection, if the clip containing the   moment boundary cannot be accurately retrieved. Our unified approach consisData Model   Clip Retrieval Oracle Grounding   LVTG   R@1   R@5   R1.3   R5.3   R1.3   R5.3   Ego4d   Baseline 31.71   64.63   29.84   54.01   14.15 30.33   Ours   42.08   76.28   36.53   63.64   20.63 41.67   MAD   Baseline 12.41   24.50   29.49   53.02   6.87   16.11   Ours   25.01   50.02   33.42   63.43   9.48 18.72   Table 2: Empirical impact of two stages. To asses standalone grounding capability,   we run an oracle experiment on the clip where the ground truth moment is present.   The grounding capability degrades in LVTG evaluation because of incorrect selection   by the disjoint clip retrieval network. Our unified model improves retrieval significantly,   which leads to more effective temporal grounding in long videos.   tently improves all retrieval metrics compared to the disjoint baseline. Notably,   we achieve a staggering 10.4% and 12.6% improved R@1 score for Ego4D and   MAD. This improvement in clip retrieval leads to state-of-the-art LVTG performance. Moreover, the grounding also improves because of the stronger clip   features generated by our encoder. Compared to the disjoint baseline, the oracle   R1.3 scores improved by about 6.7% and 4.0% for Ego4D and MAD, respectively. Hence, the unified end-to-end architecture proves more effective at LVTG   by mutually improving both stages.   12   Hannan et al.   5.4   Ablation Studies   This section reports detailed ablation studies on the proposed modules and loss   functions. We also ablate with various clip lengths and retrieved clip numbers.   Unless stated, the experiments are done with the Default network w/o NaQ annotations [39] for the Ego4D-NLQ dataset and CONE [16] as the baseline.   Model   R1.3   R5.3   R1.5   R5.5   RGNet (Default)   18.28   34.02   12.04   22.89   w/o Retrieval Token   17.80   33.99   11.25   22.32   w/o Sparsifier   16.12   31.57   9.91   20.47   w/o RG-Encoder\u2217   14.15   30.33   8.18   18.02   w/o Contrastive Loss   17.41   32.12   10.79   22.80   w/o Attention Loss   16.21   31.59   9.91   20.53   Table 3: Cumulative ablation study for the proposed modules and losses of RGNet.   The experiment is conducted w/o NaQ augmentation on the Ego4D dataset. In each   step, we remove one of our proposed modules or losses to evaluate their individual   impact. \u2217denotes the baseline model.   Modules: In our initial experiments, we validate the effectiveness of each proposed module with a cumulative ablation in Tab. 3. The unified network demonstrates superior performance across all evaluation metrics, notably achieving an   R1.3 score of 18.28%. Note that the disjoint baseline performs 4.2% worse than   our unified network. This performance difference can be attributed to our models   unified architecture.   With the removal of the retrieval token for generating the clip feature in   Eq. 5, considerably drops the score by about 0.5%. This proves that modeling   both clip level context and frame level content are necessary to produces strong   multimodal features for the grounding decoder. Subsequently, eliminating the   sparsifier, there is a decline in R1.3 performance of approximately 1.7%. This   verifies the impact of the sparsifier in modeling the fine-grained event boundaries. Finally, removing the RG-Encoder yields a notable degradation of approximately 2.0% in the R1.3 score, which substantiates its capability to model   clip and frame level granularity jointly.   Loss Functions: Training RGNet without our negative clip sampling strategy for the contrastive loss (Tab. 3) results in an almost 0.9% decline in the   R1.3 score. Our sampling strategy mimics long-video setup closely during training, which improves event discrimination capability inside the same scene and   surroundings.   By eliminating the attention loss, we observe a further reduction of 1.2%   in grounding performance. The attention loss enables our encoder to model finegrained events in long videos which is crucial for specific moment detection.   RGNet   13   Number of Clips: The number of retrieved proposal clips significantly influences the Long Video Temporal Grounding (LVTG) performance. Our model   consistently outperforms the baseline across various choices of top-k retrieved   clips, as shown in Fig. 4. Even with only a single retrieved clip, our performance surpasses the baseline\u2019s optimal result for both datasets. Moreover, our   model exhibits a less steep performance decline when fewer clips are retrieved.   Importantly, with fewer clips, the grounding network runs fewer times, leading   to an overall speedup. Therefore, our model strikes a favorable balance between   efficiency and performance, showcasing superior performance than the baseline.   Number of Clips   4.0   5.5   7.0   8.5   10.0   0   5   10   15   20   Disjoint   Unified (Ours)   R1@.3              MAD   Number of Clips   10.00   12.25   14.50   16.75   19.00   0   5   10   15   20   Disjoint   Unified (Ours)   R1@.3              Ego-4d   Fig. 4: Impact of number of retrieved clips. Reducing the number of clips speeds   up the network execution time. While the baseline model experiences a significant   drop in performance with this reduction, RGNet shows a noticeably smaller decline in   performance under the same conditions.   Clip Length: Varying clip length impacts both retrieval and grounding performance. The number of clip decreases with longer clips, making the retrieval   task easier. Fig. 5 reports a monotonically improving retrieval with longer clips.   However, after a certain length, moment localization quality starts to deteriorate. With longer clips, the task becomes extremely difficult. We see the decline   after the 180s and 48s for the MAD and Ego4D datasets. Hence, we set the default clip length of our method to these optimal lengths. Importantly, the unified   network helps outscore the disjoint baseline on all tested clip lengths (inversely   proportional to the clip numbers in Fig. 4).   Fig. 5: Impact of retrieved clip length. Longer clips result in improved retrieval   due to fewer candidates. However, grounding becomes exceedingly difficult with longer   clips. For example, the grounding performance drops after 180 seconds and 48 seconds   for the MAD and Ego4D datasets.   14   Hannan et al.   5.5   Qualitative Analysis   We visualize the qualitative predictions of RGNet compared to the disjoint baseline in Fig. 6. For the first and second queries, the baseline struggles to retrieve   the correct clip where the scissor appears. RGNet accurately retrieves the clip   with the searched object and precisely localizes the moment it appeared in the   video. Disjoint retrieval often leads to incorrect clip selection, which the final   grounding network cannot recover from. For the third query, the baseline selects the correct clip but fails to localize the precise moment the man closes the   car door, indicating a gap between the two stages. RGNet mitigates this gap   by leveraging features from the retrieved clip to improve moment localization.   More visual outputs are included the supplementary materials.   Query 1:\u00a0 Where is the scissor ?\u00a0   Query 3:\u00a0\u00a0Did I leave the car door open?   Query 2:\u00a0\u00a0How many stainless bowls did I pick?   331.7s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0GT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0335.4s   331.6s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RGNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0335.7s   420.8s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Baseline\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0428.2s   63.7s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Baseline\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 68.3s   0s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0GT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a012.5s   0.3s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0RGNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a010.2s   212.9s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Baseline\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0216.7s   3.4s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0GT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a08.7s   3.9s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0RGNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a09s   Fig. 6: Qualitative Results. The baseline fails to retrieve the correct clip in the first   two queries. Since they primarily depict the same indoor room throughout the whole   video, precise event discrimination is vital for accurate clip retrieval. In the third query,   the baseline cannot identify the moment within the correctly retrieved clip, detecting   it only after it has concluded. Precise event localization demands improved alignment   between visual events and the queried text. RGNet correctly localizes all the moments.   6   Conclusion   We introduce an end-to-end model for long video temporal grounding, unifying the two stages of prevailing methods with shared features and mutual optimization. We conduct independent analyses of the two stages, shaping our   solution based on their distinct impacts. This leads to a better understanding of   fine-grained events in long videos. Our approach demonstrates state-of-the-art   performance on challenging long video grounding datasets, validating its effectiveness. Like all LVTG methods, we rely on a pre-trained image encoder to   extract the visual features. A promising future direction includes eliminating   the need for an image encoder and training directly on the raw video data.   RGNet   15", "conf": "ECCV", "year": "2024", "index": 449}, {"title": "Character Grounding and Re-Identi\ufb01cation in   Story of Videos and Text Descriptions   Youngjae Yu1,2, Jongseok Kim1, Heeseung Yun1,   Jiwan Chung1, and Gunhee Kim1,2   1 Seoul National University, Seoul, Korea   2 Ripple AI, Seoul, Korea   {yj.yu,js.kim,heeseung.yun,jiwanchung}@vision.snu.ac.kr,   {gunhee}@snu.ac.kr   http://vision.snu.ac.kr/projects/CharacterReid/", "abstract": ". We address character grounding and re-identi\ufb01cation in multiple story-based videos like movies and associated text descriptions. In   order to solve these related tasks in a mutually rewarding way, we propose a model named Character in Story Identi\ufb01cation Network (CiSIN).   Our method builds two semantically informative representations via joint   training of multiple objectives for character grounding, video/text reidenti\ufb01cation and gender prediction: Visual Track Embedding from videos   and Textual Character Embedding from text context. These two representations are learned to retain rich semantic multimodal information   that enables even simple MLPs to achieve the state-of-the-art performance on the target tasks. More speci\ufb01cally, our CiSIN model achieves   the best performance in the Fill-in the Characters task of LSMDC 2019   challenges [35]. Moreover, it outperforms previous state-of-the-art models   in M-VAD Names dataset [30] as a benchmark of multimodal character   grounding and re-identi\ufb01cation.   1", "content": "Searching persons in videos accompanying with free-form natural language descriptions is a challenging problem in computer vision and natural language   research [30,32,34,35]. For example, in the story-driven videos such as movies   and TV series, distinguishing who is who is a prerequisite to understanding the   relationships between characters in the storyline. Thanks to the recent rapid   progress of deep neural network models for joint visual-language representation   [46,42,28,19], it has begun to be an achievable goal to understand interactions of   characters that reside in the complicated storyline of videos and associate text.   In this work, we tackle the problem of character grounding and re-identi\ufb01cation   in consecutive pairs of movie video clips and corresponding language descriptions. The character grounding indicates the task of locating the character mentioned in the text within videos. The re-identi\ufb01cation can be done in both text   and image domain; it groups the tracks of the same person across video clips   or identi\ufb01es tokens of the identical person across story sentences. As the main   2   Y. Yu et al.   [SOMEONE]    approaches    [SOMEONE], who    leans against ~   [SOMEONE]    heads off.   ! = {$%, $' \u22ef, $)}   His brow furrowed,    [SOMEONE]    looks down   at the ground   [SOMEONE] eyes    him angrily, her    jaw  clenched.   [SOMEONE] folds    her arms.   (1) Video Re-Id   (3) Text Re-Id   (2) Character Grounding   [PERSON1]    [PERSON2]    + = {,%, ,' \u22ef, ,)}   Fig. 1. The problem statement. Given consecutive C pairs of video clips and corresponding language descriptions, our CiSIN model aims at solving three multimodal   tasks in a mutually rewarding way. Character grounding is the identity matching between person tracks and [SOMEONE] tokens. Video/text re-identi\ufb01cation is the identity   matching between person tracks in videos and [SOMEONE] tokens in text, respectively.   testbed of our research, we choose the recently proposed Fill-in the Characters   task of the Large Scale Movie Description Challenge (LSMDC) 2019 [35], since   it is one of the most large-scale and challenging datasets for character matching   in videos and associated text. Its problem statement is as follows. Given \ufb01ve   pairs of video clips and text descriptions that include the [SOMEONE] tokens for   characters, the goal is to identify which [SOMEONE] tokens are identical to one   another. This task can be tackled minimally with text re-identi\ufb01cation but can   be synergic to jointly solve with video re-identi\ufb01cation and character grounding.   We propose a new model named Character-in-Story Identi\ufb01cation Network   (CiSIN) to jointly solve the character grounding and video/text re-identi\ufb01cation   in a mutually rewarding way, as shown in Figure 1. The character grounding,   which connects the characters between di\ufb00erent modalities, complements both   visual and linguistic domains to improve re-identi\ufb01cation performance. In addition, each character\u2019s grounding can be better solved by closing the loop between   both video/text re-identi\ufb01cation and neighboring character groundings.   Our method proposes two semantically informative representations. First, Visual Track Embedding (VTE) involves motion, face and body-part information   of the tracks from videos. Second, Textual Character Embedding (TCE) learns   rich information of characters and their actions from text using BERT [6]. They   are trained together to share various multimodal information via multiple objectives, including character grounding, video/text re-identi\ufb01cation and attribute   prediction. The two representations are powerful enough for simple MLPs to   achieve state-of-the-art performance on the target tasks.   We summarize the contributions of this work as follows.   1. We propose the CiSIN model that can jointly tackle character grounding and   re-identi\ufb01cation in both video and text narratives. To the best of our knowledge, our work is the \ufb01rst to jointly solve these three tasks, each of which has   been addressed separately in previous research. Our model is jointly trained   Character Grounding and Re-Id in Story of Videos and Text Descriptions   3   via multi-task objectives of these complementary tasks in order to create   synergetic e\ufb00ects from one domain to another and vice versa.   2. Our CiSIN model achieves the best performance so far in two benchmarks   datasets: LSMDC 2019 challenge [35] and M-VAD Names [30] dataset. The   CiSIN model attains the best accuracy in the Fill-in the Characters task   of LSMDC 2019 challenges. Moreover, it achieves the new state-of-the-art   results on grounding and re-identi\ufb01cation tasks in M-VAD Names.   2   Related Work   Linking characters with visual tracks. There has been a long line of work   that aims to link character names in movie or TV scripts with their corresponding visual tracks [7,38,2,32,29,40,27,14]. However, this line of research deals with   more constrained problems than ours; for example, having the templates of main   characters or knowing which characters appear in a clip. On the other hand, our   task requires person grounding and re-identi\ufb01cation in more free-formed multiple   sentences, without ever seeing characters before.   Human retrieval with natural language. Visual content retrieval with   natural language queries has been mainly addressed by joint visual-language   embedding models [42,18,12,24,43,13,26,22,21], and extended to the video domain [46,42,28]. Such methodologies have also been applied to movie description   datasets such as MPII-MD [34] and M-VAD Names [30]. Pini et al. [30] propose   a neural network that learns a joint multimodal embedding for human tracking   in videos and verb embedding in text. Rohrbach et al. [34] develop an attention   model that aligns human face regions and mentioned characters in the description. However, most previous approaches tend to focus on the retrieval of human   bounding boxes with sentence queries in a single clip while ignoring story coherence. On the other hand, our model understands the context within consecutive   videos and sentences; thereby can achieve the best result reported so far in the   human retrieval in the video story.   Person re-identi\ufb01cation in multiple videos. The goal of person reidenti\ufb01cation (re-id) is to associate with identical individuals across multiple   camera views [9,10,8,23,51,1,4,52,20,47,15]. Some methods exploit part information for better re-identi\ufb01cation against occlusion and pose variation [37,39,44].   However, this line of work often has dealt with visual information only and   exploited the videos with less diverse human appearances (e.g. standing persons taken from CCTV) than movies taken by various camerawork (e.g. head   shots, half body shots and two shots). Moreover, our work takes a step further   by considering the video description context of story-coherent multiple clips for   character search and matching.   Visual co-reference resolution. Visual co-reference resolves pronouns and   linked character mentions in text with the character appearances in video clips.   Ramanathan et al. [32] address the co-reference resolution in TV show description with the aid of character visual models and linguistic co-reference resolution   features. Rohrbach et al. [34] aim generate video descriptions with grounded   4   Y. Yu et al.   and co-referenced characters. In the visual dialogue domain [36,19], the visual   co-reference problem is addressed to identify the same entity/object instances in   an image for answering questions with pronouns. On the other hand, we focus   on identifying all character mentions blanked as [SOMEONE] tokens in multiple   descriptions. This is inherently di\ufb00erent from the co-reference resolution that   can use pronouns or explicit gender information (e.g. she, him) as clues.   3   Approach   Problem Statement. We address the problem of character identity matching   in the movie clips and associated text descriptions. To be speci\ufb01c, we follow   the Fill-in the Characters task of the Large Scale Movie Description Challenge   (LSMDC) 20193. As shown in Figure 1, the input is a sequence of C pairs of   video clips and associated descriptions that may include the [SOMEONE] tokens   for characters (i.e. C = 5 in LSMDC 2019). The [SOMEONE] tokens correspond   to proper nouns or pronouns of characters in the original description. The goal   of the task is to replace the [SOMEONE] tokens with local character IDs that are   consistent within the input set of clips, not globally in the whole movie.   We decompose the Fill-in the Characters task into three subtasks: (i) character grounding \ufb01nds the character tracks of each [SOMEONE] token in the video,   (ii) video re-identi\ufb01cation that groups visual tracks of the identical character in   videos and (iii) text re-identi\ufb01cation that connects between the [SOMEONE] tokens of the same person. We de\ufb01ne these three subtasks because each of them is   an important research problem with its own applications, and jointly solving the   problems is mutually rewarding. Thereby we can achieve the best result reported   so far on LSMDC 2019 and M-VAD Names [30] (See the details in section 4).   In the following, we \ufb01rst present how to de\ufb01ne person tracks from videos   (section 3.1). We then discuss two key representations of our model: Visual   Track Embedding (VTE) for person tracks (section 3.2) and Textual Character   Embedding (TCE) for [SOMEONE] tokens (section 3.3). We then present the   details of our approach to character grounding (section 3.4) and video/text reidenti\ufb01cation (section 3.5). Finally, we discuss how to train all the components of   our model via multiple objectives so that the solution to each subtask helps one   another (section 3.6). Figure 2 illustrates the overall architecture of our model.   3.1   Video Preprocessing   For each of C video clips, we \ufb01rst resize it to 224\u00d7224 and uniformly sample   24 frames per second. We then detect multiple person tracks as basic character   instances in videos for grounding and re-identi\ufb01cation tasks. That is, the tasks   reduce to the matching problems between person tracks and [SOMEONE] tokens.   Person tracks. We obtain person tracks {tm}M   m=1 as follows. M denotes the   number of person tracks throughout C video clips. First, we detect bounding   3 https://sites.google.com/site/describingmovies/lsmdc-2019.   Character Grounding and Re-Id in Story of Videos and Text Descriptions   5   boxes (bbox) of human bodies using the rotation robust CenterNet [53] and   group them across consecutive frames using the DeepSORT tracker [45]. The   largest bbox in each track is regarded as its representative image {hm}M   m=1,   which can be used as a simple representation of the person track.   Face regions. Since person tracks are not enough to distinguish \u201cwho is   who\u201d, we detect the faces for better identi\ufb01cation. In every frame, we obtain   face regions using MTCNN [49] and resize them to 112\u00d7112. We then associate   each face detection with the person track that has the highest IoU score.   Track metadata. We extract track metadata {mm}M   m=1, which include the   (x, y) coordinates, the track length and the size of the representative image hm.   All of them are normalized with respect to the original clip size and duration.   3.2   Visual Track Embedding   For better video re-identi\ufb01cation, it is critical to correctly measure the similarity   between track i and track j. As rich semantic representation of tracks, we build   Visual Track Embedding (VTE) as a set of motion, face and body-part features.   Motion embedding. We apply the I3D network [3] to each video clip and   obtain the last CONV feature of the \ufb01nal Inception module, whose dimension is   (time, width, height, feature) = (\u230at   8\u230b, 7, 7, 1024). That is, each set of 8 frames is   represented by a 7 \u00d7 7 feature map whose dimension is 1024. Then, we extract   motion embedding {im}M   m=1 of each track by mean-pooling over the cropped   spatio-temporal tensor of this I3D feature.   Face embedding. We obtain the face embedding {fm}M   m=1 of a track, by   applying ArcFace [5] to all face regions of the track and mean-pooling over them.   Body-part embedding. To more robustly represent visual tracks against   the ill-posed views or cameraworks (e.g. no face is shown), we also utilize the   body parts of a character in a track (e.g. limb and torso). We \ufb01rst extract the   pose keypoints using the pose detector of [53], and then obtain the body-part embedding {{pm,k}K   k=1}M   m=1 by selecting keypoint-corresponding coordinates from   the last CONV layer (7 \u00d7 7 \u00d7 2048) of ImageNet pretrained ResNet-50 [11]:   pm,k = ResNet(hm)[xk, yk],   (1)   where pm,k is the representation of the k-th keypoint in a pose, hm is the representative image of the track, and (xi, yi) is a relative position of the keypoint.   To ensure the quality of pose estimation, we only consider keypoints whose con\ufb01dences are above a certain threshold (\u03c4 = 0.3).   In summary, VTE of each track m includes three sets of embeddings for   motion, face and body parts: VTE = {im, fm, pm}.   3.3   Textual Character Embedding   Given C sentences, we make a uni\ufb01ed language representation of the characters   (i.e. [SOMEONE] tokens), named Textual Character Embedding (TCE) as follows.   6   Y. Yu et al.   [Someone]   him   chases    [Someone]   a   [Someone]   topples    over   BERT-Base   \ud835\udc46= {\ud835\udc601, \ud835\udc602 \u22ef, \ud835\udc60\ud835\udc41}   \ud835\udc36captions   (\ud835\udc36=5)   \ud835\udc36clips   \u22ee   \u22ee   throws   towel   \u22ee   \u22ee   (a) Textual Character Embedding   (b) Visual Track Embedding    Character Grounding   Motion   (I3D)   Face   (ArcFace)   Bodypart   (CenterNet)   Metadata   Size, Length    Coordinate    \u22ee   (f) Character Identity Graph   Feature set \ud835\udc61\ud835\udc5a   \u22ee   \ud835\udc47= {\ud835\udc611, \ud835\udc612 \u22ef, \ud835\udc61\ud835\udc40}   Characters   in videos   Mentioned    characters   Named entities    in descriptions   \ud835\udc73   \ud835\udc7d   \ud835\udc6e   \ud835\udc40vertices   \ud835\udc41vertices   (c) Bipartite    Grounding Graph   (e) Video   Re-Id Graph   \ud835\udc6e   \ud835\udc7d   (d) Text Re-Id   Graph \ud835\udc73   \ud835\udc6e\ud835\udc7d\ud835\udc6e\ud835\udc7b   \ud835\udc602   \ud835\udc601   \ud835\udc60\ud835\udc41   \ud835\udc60\u2217   \ud835\udc6a   Identity Matching   [Person1]   [Person2]   Fig. 2. Overview of the proposed Character-in-Story Identi\ufb01cation Network (CiSIN)   model. Using (a) Textual Character Embedding (TCE) (section 3.3) and (b) Visual   Track Embedding (VTE) (section 3.2), we obtain the (c) bipartite Character Grounding   Graph G (section 3.4). We build (d)\u2013(e) Video/Text Re-Id Graph V and L, from which   (f) Character Identity Graph C is created. Based on the graphs, we can perform the   three subtasks jointly (section 3.5).   Someone embedding. We use the BERT model [6] to embed the [SOMEONE]   tokens in the context of C consecutive sentences. We load pretrained BERTBase-Uncased with initial weights. We denote each sentence {wc   l }Wc   l=1 where Wc   is the number of words in the c-th sentence. As an input to the BERT model, we   concatenate the C sentences {{wc   l }Wc   l=1}C   c=1 by placing the [SEP] token in each   sentence boundary. We obtain the embedding of each [SOMEONE] token as   bn = BERT({{wc   l }Wc   l=1}C   c=1, k),   (2)   where k is the position of the [SOMEONE] token. We let this word representation {bn}N   n=1 (i.e. the BERT output at position k), where N is the number of   [SOMEONE] tokens in the C sentences.   Action embedding. For a better representation of characters, we also consider the actions of [SOMEONE] described in the sentences. We build a dependency   tree for each sentence with the Stanford Parser [31], and \ufb01nd the ROOT word associated with [SOMEONE], which is generally the verb of the sentence. We then   obtain the ROOT word representation {an}N   n=1 as done in Eq.(2).   In summary, TCE of each character n includes two sets of embeddings for   someone and action. Since they have the same dimension as the output of BERT,   we simply concatenate them as a single embedding: TCE = sn = [bn; an].   Character Grounding and Re-Id in Story of Videos and Text Descriptions   7   3.4   Character Grounding   For each clip c, we perform the character grounding using VTE = {im, fm, pm}   of person tracks tm (section 3.2) and TCE = sn = [bn; an] of characters (section   3.3). Due to the heterogeneous nature of our VTE (i.e. fm contains appearance   information such as facial expression while im does motion information), we   separately fuse VTE with TCE and then concatenate to form fused ground   representation gn,m:   gface   n,m = MLP(sn) \u2299MLP(fm), gmotion   n,m   = MLP(sn) \u2299MLP(im),   (3)   gn,m = [MLP(mm); gface   n,m; gmotion   n,m   ],   (4)   where \u2299is a Hadamard product, [;] is concatenation, mm is the track metadata,   and the MLP is made up of two FC layers with the identical output dimension.   Finally, we calculate the bipartite Grounding Graph G \u2208RN\u00d7M as   Gnm = g(sn, tm) = MLP(gn,m),   (5)   where the MLP consists of two FC layers with the scalar output.   Attributes. In addition to visual and textual embeddings, there are some   attributes of characters that may be helpful for grounding. For example, as in   MPII-MD Co-ref [34], gender information can be an essential factor for matching   characters. However, most [SOMEONE] tokens have vague context to hardly infer   gender information, although some of them may be clear like gendered pronouns   (e.g. she, he) or nouns (e.g. son, girl). Thus, we add an auxiliary attribute   module that predicts the gender of [SOMEONE] using bn as input:   attrlogit = \u03c3(MLP(bn)),   (6)   where the MLP has two FC layers with scalar output, and \u03c3 is a sigmoid function.   The predicted attribute is neither explicitly used for any grounding or reidenti\ufb01cation tasks. Instead, the module is jointly trained with other components   to encourage the shared character representation to learn the gender context   implicitly. Moreover, it is straightforward to extend this attribute module to   other information beyond gender like the characters\u2019 roles.   3.5   Re-Identi\ufb01cation   We present our method to solve video/text re-identi\ufb01cation tasks, and explain   how to aggregate all subtasks to achieve the Fill-in the Characters task.   Video Re-Id Graph. We calculate the Video Re-Id Graph V \u2208RM\u00d7M   that indicates the pairwise similarity scores between person tracks. We use the   face and body-part embeddings of VTE. We \ufb01rst calculate the face matching   score of track i and j, namely fi,j, by applying a 2-layer MLP to face embedding   fm followed by a dot product and a scaling function:   fi,j = wscale   f   f \u2032   i \u00b7 f \u2032   j + bscale   f   ,   f \u2032   m = MLP(fm),   (7)   8   Y. Yu et al.   ResNet   [7,7,2048]   Right shoulder   Left shoulder   Right elbow   Right wrist   (Dismiss due to low threshold <  !)   Front   Side   Back   ResNet   [7,7,2048]   ResNet   [7,7,2048]   Fig. 3. Pose-based body-part matching. We only consider keypoints that appear across   scenes with con\ufb01dences scores above a certain threshold \u03c4(= 0.3) such as right/left   shoulder and right elbow, while ignoring right wrist as it falls short of the threshold.   where wscale   f   , bscale   f   \u2208R are learnable scalar weights for scaling.   We next calculate the pose-guided body-part matching score qi,j. Unlike   conventional re-identi\ufb01cation tasks, movies may not contain full keypoints of   a character due to camerawork like upper body close up shots. Therefore, we   only consider the pairs of keypoints that are visible in both tracks (Figure 3):   qi,j = wscale   p   Zi,j   X   k   \u03b4i,k\u03b4j,kpi,k \u00b7 pj,k + bscale   p   ,   (8)   where pi,k is the body-part embedding of VTE for keypoint k of track i, \u03b4i,k   is its binary visibility value (1 if visible otherwise 0), Zi,j = P   k \u03b4i,k\u03b4j,k is a   normalizing constant and wscale   p   , bscale   p   are scalar weights.   Finally, we obtain Video Re-Id Graph V by summing both face matching   score and body-part matching score:   Vi,j = fi,j + qi,j.   (9)   Text Re-Id Graph. The Text Re-Id Graph L \u2208RN\u00d7N measure the similarity between every pair of [SOMEONE] token as   Li,j = \u03c3(MLP(bi \u2299bj)),   (10)   where \u2299is a Hadamard product, \u03c3 is a sigmoid and the MLP has two FC layers.   Solutions to three subtasks. Since we have computed all pairwise similarity between and within person tracks and [SOMEONE] tokens in Eq.(5 and   Eq.(9)\u2013(10), we can achieve the three tasks by thresholding. We perform the   character grounding by \ufb01nding the column with the maximum value for each   row in the bipartite Grounding Graph G \u2208RN\u00d7M. The video re-identi\ufb01cation   is carried out by \ufb01nding the pair whose score in Video Re-Id Graph V is positive.   Finally, the text re-identi\ufb01cation can be done as will be explained below for the   Fill-in the Characters task since they share the same problem setting.   Character Grounding and Re-Id in Story of Videos and Text Descriptions   9   Fill-in the characters task. We acquire the Character Identity Graph C:   C = avg(L, R)   where R = \u03c3(GVGT ), Gn = argmaxm\u2208Mc g(sn, tm),   (11)   where \u03c3 is a sigmoid, G \u2208RN\u00d7M is the bipartite Grounding Graph in Eq.(5),   V \u2208RM\u00d7M is the Video Re-Id Graph in Eq.(9), and L \u2208RN\u00d7N is the Text   Re-Id Graph in Eq.(10). We perform the Fill-in the Characters task by \ufb01nding   Cij \u22650.5, for which we decide [SOMEONE] token i and j as the same character.   Although we can solve the task using only the Text Re-Id graph L, the   key idea of Eq.(11) is that we also consider the other loop leveraging character   grounding G and the Video Re-Id graph V. That is, we \ufb01nd the best matching   track for each token in the same clip c, where Mc is the candidate tracks in the   clip c. We then use the Video Re-Id graph to match tracks and apply G again to   \ufb01nd their character tokens. Finally, we average the scores from these two loops   of similarity between [SOMEONE] tokens.   3.6   Joint Training   We perform joint training of all components in the CiSIN model so that both   VTE and TCE representation share rich multimodal semantic information, and   subsequently help solve character grounding and video/text re-identi\ufb01cation in   a mutually rewarding way. We \ufb01rst introduce the loss functions.   Losses. For character grounding, we use a triplet loss where a positive pair   maximizes the ground matching score g in Eq.(5) while a negative one minimizes   L(s, t, t\u2212) = max(0, \u03b1 \u2212g(s, t) + g(s, t\u2212)),   (12)   L(s, t, s\u2212) = max(0, \u03b1 \u2212g(s, t) + g(s\u2212, t)),   (13)   where \u03b1 is a margin, (s, t) is a positive pair, t\u2212is a negative track, and s\u2212is a   negative token. For video re-identi\ufb01cation, we also use a triplet loss:   L(t0, t+, t\u2212) = max(0, \u03b2 \u2212V0,+ + V0,\u2212)   (14)   where \u03b2 is a margin and V is the score in Video Re-Id Graph. (t0, t+) and (t0, t\u2212)   are positive and negative track pair, respectively. For text re-identi\ufb01cation, we   train parameters to make Character Identity Graph C in Eq.(11) closer to the   ground truth Cgt with a binary cross-entropy (BCE) loss. When computing C   in Eq.(11), we replace the argmax of Gn with the softmax for di\ufb00erentiability:   Gn = softmaxm\u2208Mg(sn, tm). Additionally, the attribute module is trained with   the binary cross-entropy loss for gender class (i.e. female, male).   Training. We use all losses to train the model jointly. While \ufb01xing the parameters in the motion embedding (I3D) and the face embedding (ArcFace),   we update all the other parameters in all MLPs, ResNets and BERTs during   training. Notably, BERT models in TCE are trained by multiple losses (i.e.   character grounding, text re-identi\ufb01cation and the attribute loss) to learn better   multimodal representation.   10   Y. Yu et al.   In Eq.(11), the Grounding Graph G is a bipartite graph for cross-domain   retrieval between the Text Re-Id Graph L and the Video Re-Id Graph V. The   bipartite graph G identi\ufb01es a subgraph of V for the characters mentioned in   the text such that the subgraph is topologically similar to L. By joint training   of multiple losses, the similarity metric between the visual and textual representation of the same character increases, consequently improving both character   grounding and re-identi\ufb01cation performance.   Details. We unify the hidden dimension size of all MLPs as 1024 and use   the leaky ReLU activation for every FC layer in our model. The whole model is   trained with the Adam optimizer [17] with a learning rate of 10\u22125 and a weight   decay of 10\u22128. We train for 86K iterations for 15 epochs. We set the margin   \u03b1 = 3.0 for the triplet loss in Eq.(12)\u2013(13) and \u03b2 = 2.0 for Eq.(14).   4   Experiments   We evaluate the proposed CiSIN model in two benchmarks of character grounding and re-identi\ufb01cation: M-VAD Names [30] dataset and LSMDC 2019 challenge   [35], on which we achieve new state-of-the-art performance.   4.1   Experimental Setup   M-VAD Names. We experiment three tasks with M-VAD Names [30] dataset:   character grounding and video/text re-identi\ufb01cation. We group \ufb01ve successive   clips into a single set, which is the same setting with LSMDC 2019, to evaluate   the model\u2019s capability to understand the story in multiple videos and text. MVAD Names dataset is annotated with persons\u2019 name and their face tracks, which   we use as ground truth for training.   For evaluation of character grounding, we measure the accuracy by checking   whether the positive track is correctly identi\ufb01ed. For evaluation of video reidenti\ufb01cation, we predict the Video Re-Id Graph and then calculate the accuracy   by comparing it with the ground truth. For evaluation of text re-identi\ufb01cation,   we calculate the accuracy with the ground truth character matching graph.   LSMDC 2019. We report experimental results for the Fill-in the Characters task of LSMDC 2019 [35], which is the superset of M-VAD dataset [41].   Contrary to M-VAD Names, LSMDC 2019 only provides local ID ground truths   of [SOMEONE] tokens with no other annotation (e.g. face bbox annotation and   characters\u2019 names). We exactly follow the evaluation protocol of the challenge.   4.2   Quantitative Results   M-VAD Names. Table 1 summarizes the quantitative results of video reidenti\ufb01cation tasks. We compare with state-of-the-art strong-ReID-baseline [25]   using the o\ufb03cial implementation4 which is pretrained on market-1501 [51]. For   4 https://github.com/michuanhaohao/reid-strong-baseline.   Character Grounding and Re-Id in Story of Videos and Text Descriptions   11   Table 1. Results of video re-identi\ufb01cation on the validation set of M-VAD Names [30].   Video Re-Id   Accuracy   ResNet-50 (ImageNet pretrained)   0.607   strong-ReID-baseline [25]   0.617   Face+fullbody+poselets [50]   0.776   Face+head+body+upperbody [16]   0.779   RANet visual context [15]   0.787   CiSIN Face only   0.783   + Human bbox   0.781   + Body parts   0.799   + Body parts + Text   0.806   Table 2. Results of character grounding (left) and text re-identi\ufb01cation (right) on   the validation set of M-VAD Names [30]. Attr, Text, Visual and Grounding means   joint training with the attribute module, Text Re-Id Graph, Video Re-Id Graph and   Character Grounding Graph, respectively.   Character Grounding   Accuracy   M-VAD Names [30]   0.621   MPII-MD Co-ref [33]   0.622   CiSIN w/o motion   0.606   CiSIN w/o Attr & Text   0.651   CiSIN w/o Text   0.673   CiSIN   0.684   Text Re-Id   Accuracy   BERT baseline [6]   0.734   JSFusion [48] with BERT   0.744   CiSIN w/o Grounding & Visual   0.743   CiSIN w/o Visual   0.754   CiSIN w/o Text   0.737   CiSIN   0.767   fair comparison, we choose the same ResNet-50 as the CNN backbone. We also   test other visual cue-based baselines [50,16,15] using the same face and body   features with CiSIN. We then \ufb01ne-tune the model with M-VAD Names dataset.   Despite its competence in person re-identi\ufb01cation research, the strong-ReIDbaseline signi\ufb01cantly underperforms our model by 18.9% of accuracy drop. MVAD Names dataset contains much more diverse person appearances in terms   of postures, sizes and ill-posedness (e.g. extremely wide shots, partial and back   views), which makes the existing re-identi\ufb01cation model hard to attain competitive scores. In ablation studies, our model with only face embedding achieves   a considerable gain of 16.6%, which implies that utilizing facial information is   crucial for re-identi\ufb01cation in movies. However, adding simple average-pooled   human bbox embedding (+ Human bbox in Table 1) does not improve the performance. Instead, combining with our body-part embedding (pm) yields better   accuracy, meaning that it can convey full-body information better than simple   bbox embedding especially when human appearances are highly diverse. The   variant (+ Text) in Table 1 uses the grounding and text matching to further improve video re-identi\ufb01cation accuracy, as done in Eq.(11). We update the Video   Re-Id Graph as V = V+\u03bbGT LG, where \u03bb is a trainable parameter, G is the bipartite Grounding Graph, and L is the Text Re-Id Graph. The result hints that   contextual information inside the text helps improve re-identi\ufb01cation in videos.   Table 2 presents the results of character grounding and text re-identi\ufb01cation   in M-VAD Names dataset. For character grounding, we report the results of two   12   Y. Yu et al.   Table 3. Quantitative results of the \u201cFill-in the Characters in Description\u201d task for   blinded test dataset in LSMDC 2019 challenge. * denotes the scores from the \ufb01nal   o\ufb03cial scores reported in LSMDC 2019 challenge slides.   Fill-in the Characters   Accuracy   Human median*   0.870   Human (w/o video) median*   0.700   O\ufb03cial baseline*   0.639   YASA*   0.648   CiSIN (Visual-only)   0.620   CiSIN (Text-only)   0.639   CiSIN (Separate training)   0.653   CiSIN   0.673   state-of-the-art models and di\ufb00erent variants of our CiSIN model. Originally,   the MPII-MD Co-ref [34] is designed to link the given character\u2019s name and gender to corresponding visual tracks. For fair comparison, we use the supervised   version in [33] that utilizes ground truth track information, but do not provide   exact character name nor gender information at test time. Our model outperforms existing models with large margins. Moreover, joint training enhances our   grounding performance by 3.3%p than na\u00a8\u0131vely trained CiSIN w/o Attr & Text.   For the text-re identi\ufb01cation task, we use JSFusion [48] and the BERT of   our model with no other component as the baselines. JSFusion is the model that   won the LSMDC 2017 and we modify its Fill-in-the blank model to be applicable   to the new task of Fill-in the Characters. The CiSIN w/o Grounding & Visual   indicates this BERT variant with additional joint training with the attribute   loss. It enhances accuracy by 0.9%p and additional training with the Character   Grounding graph further improves 1.1%p. Also, we report the score of using   only Video Re-Id Graph and bipartite Grounding Graph, which is 73.7%. As   expected, jointly training of the whole model increases the accuracy to 76.7%.   LSMDC 2019. Table 3 shows the results of the Fill-in the Characters task   in LSMDC 2019 challenge. The baseline and human performance are referred   to LSMDC 2019 o\ufb03cial website5. Our model trained with both textual and   visual context achieves 0.673, which is the best score for the benchmark. For the   variant of CiSIN (Separate training), each component is separately trained with   its own loss function. This model without joint training shows the performance   drop by 2%p. Obviously, our model that lacks the text Re-Id graph signi\ufb01cantly   underperforms, meaning the association with text is critical for the task.   4.3   Qualitative Results   Fill-in the Characters. Figure 4 illustrates the results of CiSIN model for the   Fill-in the Characters task with correct (left) and near-miss (right) examples.   Figure 4(a) is a correct example where our model identi\ufb01es characters in conversation. Most frames expose only the upper body of characters, where clothing   5 https://sites.google.com/site/describingmovies/lsmdc-2019.   Character Grounding and Re-Id in Story of Videos and Text Descriptions   13   [PERSON1] smiles    at him.   [PERSON1] walks in    with a sympathetic    smile.   [PERSON2] waves.   [PERSON1] perches    beside him on the edge    of the bed.   Her injured husband    regards her gravely.   [PERSON1]   [PERSON2]   [PERSON1]   [PERSON1]   (d)   [PERSON1] repeats    the process over and    over.   [PERSON2] kisses    her sweetly.   [PERSON3]   Smiles.   [PERSON1]    fingers the rosy    spot.   Screwing up her doll-like    features, [PERSON1]    hits herself again.   [PERSON1]   [PERSON1]   [PERSON1]   [PERSON1]   [PERSON1]   GT   GT   [PERSON1]    [PERSON2]    [PERSON3]    CiSIN Prediction :    Ground Truth (GT) : [PERSON #]    Prediction   (Correct/Wrong)   GT   (a)   (c)   On Saturday night,    [PERSON1] hurries    down \u2026   The room is    deserted.    [PERSON1] starts to    read.    Turning the page,    [PERSON1] finds    SOMEONE's article.    [PERSON1] crumples    up the newspaper and    throws it \u2026   [PERSON1]   [PERSON1]   [PERSON1]   [PERSON1]   (b)   A yellow butterfly   flutters above the   emerging snowdrops.   [PERSON1] hangs   back as [PERSON2]    walks \u2026   Falling snow   deadens her footsteps.    Kneeling beside her   friend, [PERSON2]    puts out a hand \u2026   [PERSON3] looks at   her miserably.    [PERSON1],    [PERSON2]   [PERSON1]   [PERSON3]   Prediction   (Correct/Wrong)   GT   Prediction   (Correct/Wrong)   GT   Prediction   (Correct/Wrong)   GT   Fig. 4. Qualitative results of our CiSIN model. (a)-(d) are the Fill-in the Characters   examples with median frames of the character grounding trajectories. Green examples   indicate correct inferences by our model, while red ones with underlines in (b) and (d)   are incorrect. Note that the bounding boxes are generated by our model.   and facial features become decisive clues. Figure 4(b) shows a failure case to   distinguish two main characters [PERSON1,2] in black coats since the characters   are seen in a distant side view. Figure 4(c) is another successful case; in the \ufb01rst   and fourth video clips, the Video Re-Id Graph alone hardly identi\ufb01es the character because he is too small (in the 1st clip) or only small parts (hands) appear   (in the 4th clip). Nonetheless, our model can distinguish character identity from   the story descriptions in text. In Figure 4(d), although the model can visually   identify the same characters in neighboring video clips, the character grounding   module repeatedly pays false attention to the other character. In such cases, our   model often selects the candidate that stands out the most.   Character Grounding. Figure 5 shows character grounding results with   top six visual tracks. Figure 5(a) depicts three character grounding examples   14   Y. Yu et al.   (a)   (b)   [PERSON1]    stops in the    lobby.    [PERSON1]    closes his    eyes.   [PERSON1]    shuts his    phone.    [PERSON1]    [PERSON1]    [PERSON1]    [PERSON2]   [PERSON1]    [PERSON2]   Ours CiSIN   Our w\\o    Text Re-Id   Now on a hotel    room, [...]    watches TV    while shaving.    [...] shows him    the photo again,    [...] cringes.    [...] climbs out.     Top 6 Visual Tracks   Groudning   Prediction   Fig. 5. Qualitative results of character grounding by the CiSIN model. (a) Examples   of character grounding where [...] denotes a [SOMEONE] token. (b) Examples of Fill-in   the Characters with and without the Text Re-Id Graph.   where our model uses action (e.g. shaving, showing a photo, climbing) and facial   (e.g. shaving cream, cringe) information to pick the right character. Figure 5(b)   shows the e\ufb00ect of the Text Re-Id Graph. In the \ufb01rst clip, two people are standing   still in front of the lobby, and our grounding module would select [PERSON2] as   an incorrect prediction without the Text Re-Id Graph. However, it can later   capture the coherence of the text that the mentioned person is the same.   5   Conclusion   We proposed the Character-in-Story Identi\ufb01cation Network (CiSIN) model for   character grounding and re-identi\ufb01cation in a sequence of videos and descriptive   sentences. The two key representations of the model, Visual Track Embedding   and Textualized Character Embedding, are easily adaptable in many video retrieval tasks, including person retrieval with free-formed language queries. We   demonstrated that our method signi\ufb01cantly improved the performance of video   character grounding and re-identi\ufb01cation in multiple clips; our method achieved   the best performance in a challenge track of LSMDC 2019 and outperformed   existing baselines for both tasks on M-VAD Names dataset.   Moving forward, there may be some interesting future works to expand the   applicability of the CiSIN model. First, we can explore human retrieval and   re-identi\ufb01cation tasks in other domains of videos. Second, as this work only improved the re-identi\ufb01cation of those mentioned in descriptions, we can integrate   with a language generation or summarization module to better understand the   details of a speci\ufb01c target person in the storyline.   Acknowledgement We thank SNUVL lab members for helpful comments. This   research was supported by Seoul National University, Brain Research Program   by National Research Foundation of Korea (NRF) (2017M3C7A1047860), and   AIR Lab (AI Research Lab) in Hyundai Motor Company through HMC-SNU   AI Consortium Fund. Gunhee Kim is the corresponding author.   Character Grounding and Re-Id in Story of Videos and Text Descriptions   15", "conf": "ECCV", "year": "2020", "index": 41}, {"title": "Counterfactual Contrastive Learning for   Weakly-Supervised Vision-Language Grounding   Zhu Zhang1,2 , Zhou Zhao1,2\u2217, Zhijie Lin1 , Jieming Zhu3 , and Xiuqiang He3   1Zhejiang University   2Key Laboratory Foundation of Information Perception and Systems for Public Security of MIIT,   Nanjing University of Science and Technology   3Huawei Noah\u2019s Ark Lab   {zhangzhu, zhaozhou, linzhijie}@zju.edu.cn   {jamie.zhu, hexiuqiang1}@huawei.com", "abstract": "Weakly-supervised vision-language grounding aims to localize a target moment   in a video or a speci\ufb01c region in an image according to the given sentence query,   where only video-level or image-level sentence annotations are provided during   training. Most existing approaches employ the MIL-based or reconstruction-based   paradigms for the WSVLG task, but the former heavily depends on the quality   of randomly-selected negative samples and the latter cannot directly optimize the   visual-textual alignment score. In this paper, we propose a novel Counterfactual   Contrastive Learning (CCL) to develop suf\ufb01cient contrastive training between counterfactual positive and negative results, which are based on robust and destructive   counterfactual transformations. Concretely, we design three counterfactual transformation strategies from the feature-, interaction- and relation-level, where the   feature-level method damages the visual features of selected proposals, interactionlevel approach confuses the vision-language interaction and relation-level strategy   destroys the context clues in proposal relationships. Extensive experiments on \ufb01ve   vision-language grounding datasets verify the effectiveness of our CCL paradigm.   1", "content": "Vision-language grounding is a fundamental and crucial problem in multi-modal understanding.   Video grounding [14, 17] aims to identify the temporal boundaries of the target moment according to   the given description. And image grounding [20, 28, 46] localizes a speci\ufb01c region described by a   referring expression. Recently, to avoid expensive manual annotations, researchers begin to explore   Weakly-Supervised Vision-Language Grounding (WSVLG), which only needs the video-sentence   or image-sentence pairs during training. Most existing approaches [33, 26, 11, 29, 15, 23, 8, 37]   employ the MIL-based or reconstruction-based paradigm to train weakly-supervised grounding   networks, but they both have some drawbacks. The MIL-based methods [11, 29, 15, 8] often de\ufb01ne   the original vision-language pairs as positive samples, construct the unmatched vision-language pairs   as negative samples, and directly learn the latent visual-textual alignment by an inter-sample loss.   However, these approaches heavily depend on the quality of randomly-selected negative samples,   which are often easy to distinguish and cannot provide strong supervision signals. On the other hand,   reconstruction-based methods [33, 26, 23, 37] attempt to reconstruct the sentence query from visual   contents during training and utilize intermediate results, such as attention weights, to localize the   target proposal (i.e., region or moment) during inference. But these methods cannot directly optimize   the visual-textual alignment scores which are applied for inference. Considering the proposals with   \u2217Zhou Zhao is the corresponding author.   34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.   Image Grounding   Video Grounding   Query: The buffalo standing between    two other buffalo.   Query: The woman then dances all around the room in her ballet shoes, showing off    jumps and spins she can do.   Figure 1: Examples of image grounding and video grounding.   higher weights are not necessarily more relevant to the sentence query [21], the indirect optimization   may restrict further performance improvement.   Recently, contrastive learning has greatly promoted unsupervised pretraining of visual representations [16, 7], which develops contrastive training between positive and negative samples to learn   expressive visual features. In this paper, we propose a novel Counterfactual Contrastive Learning (CCL) paradigm for the WSVLG task, which constructs \ufb01ne-grained supervision signals from   counterfactual results to directly optimize the visual-textual alignment. Speci\ufb01cally, we \ufb01rst employ a MIL-based pre-trained grounding network to estimate the given vision-language pairs to   produce original results. By the gradient-based selection method, we then build a critical proposal   set and an inessential proposal set. Next, we design Robust Counterfactual Transformations (RCT)   based on the inessential set and devise Destructive Counterfactual Transformations (DCT) according   to the critical set. After it, we apply the grounding network with constructed RCT and DCT to   generate counterfactual results of the vision-language pairs, including positive and negative results   corresponding to RCT and DCT, respectively. Finally, we develop a ranking loss to focus on the   score-based difference between positive and negative results, and further devise the consistency   loss to consider the distribution-based discrepancy between them. To make the contrastive training   effective, the network with DCT needs to generate plausible negative results, where crucial visual   contents corresponding to the sentence query are destroyed while unnecessary contents are retained.   On the contrary, the network with RCT should damage visual contents of inessential proposals and   produce robust positive results. Thus, we design the counterfactual transformations from three levels:   (1) the feature-level strategy damages the features (i.e. endogenous clues) of selected proposals   by the memory-based replacement; (2) the interaction-level strategy confuses the vision-language   interaction by destroying the multi-modal fusion; and (3) the relation-level strategy perturbs the   context relations (i.e. exogenous clues) of chosen proposals by counterfactual relation construction.   The three strategies are applied to the intermediate process of network inference rather than the raw   inputs, and produce ambiguous results for suf\ufb01cient contrastive learning.   The main contributions of this paper are summarized as follows:   \u2022 We propose a novel counterfactual contrastive learning for WSVLG, which develops suf\ufb01cient contrastive training between counterfactual positive and negative results.   \u2022 We design the feature-level, interaction-level and relation-level strategies for counterfactual   transformations, which are applied to the intermediate process of network inference.   \u2022 Our CCL not only focuses on the score-based difference between the positive and negative   results but also considers the distribution-based discrepancy between them.   \u2022 We conduct extensive experiments on \ufb01ve large-scale vision-language grounding datasets to   verify the effectiveness of our proposed CCL paradigm.   2   Related Work   Image Grounding. Image grounding aims to localize the object region corresponding to the given   referring expression. Early supervised approaches [20, 28, 47, 30, 48] directly learn the latent   visual-textual alignment. And further works explore the language expression decomposition [46, 19],   co-attention interaction [12, 55] and relation construction [44, 43]. Besides, Liu et al. [27] introduce   the adversarial erasing approaches [36, 40] into this \ufb01eld, which detect the crucial contents by the   Grad-CAM method [35] and hide these contents to make the network further focus on other relevant   contents. Under the weakly-supervised setting, researchers build the supervision signals by the   2   reconstruction-based paradigm [33, 4, 54, 26] and MIL-based paradigm [51, 54, 11]. Rohrbach et   al. [33] learn the intermediate attention weights to localize the region by language reconstruction, and   Chen et al. [4] design knowledge-aided consistency in both visual and language modalities. Further,   Liu et al. [26] devise the collaborative training of language reconstruction and attribute classi\ufb01cation.   From the MIL-based view, Zhang et al. [51] propose a variational Bayesian method to explore the   context modeling and Datta et al. [11] build caption-conditioned image encoding to infer the latent   region-phrase correspondences.   Video Grounding. Video grounding tries to determine the temporal boundaries of the video moment   corresponding to the given sentence. Existing supervised methods can be categorized into the   top-down and bottom-up frameworks. The top-down approaches pre-de\ufb01ne a series of moment   proposals and select the target one by multi-modal estimation, including explicit proposals by sliding   windows [14, 17, 24, 25] and implicit proposals by multi-granularity anchors after visual-textual   interaction [2, 50, 53, 42, 52, 49]. And the bottom-up framework [3, 5, 41] does not pre-de\ufb01ne   moment proposals and directly predict the probabilities of temporal boundaries across frames. Under   the weakly-supervised setting, the MIL-based methods [29, 15, 8] learn the visual-textual alignment   by the inter-sample loss. Among them, Mithun et al. [29] utilize text-guided attention to learn the   latent alignment between frames and texts. Gao et al. [15] apply an alignment module to learn the   visual-textual consistency and devise a detection module to rank moment proposals. And Chen et   al. [8] develop a coarse-to-\ufb01ne manner to detect the accurate moment. Under the reconstruction-based   paradigm, Lin et al. [23] rank candidate moment proposals by a language reconstruction reward. And   Song et al. [37] further leverage attentional re-construction to rank the proposals.   3   Counterfactual Contrastive Learning   3.1   The Formulation of Weakly-Supervised Grounding Networks under CCL   Given the sentence query Q and the instance C (i.e. video V or image I), we aim to train a network   GNet(C, Q) to detect the most relevant proposal p without any (p, Q) alignment annotations during   training, where a proposal means a moment from the video or a region from the image. To illustrate   the CCL paradigm clearly, we \ufb01rst formulate the weakly-supervised grounding network GNet(C, Q)   under CCL. Speci\ufb01cally, we decompose GNet(C, Q) into three important modules:   \u2022 Encoder Module EM(\u00b7): It learns the sentence feature q and word features S = {sn}N   n=1   from the query Q, where N is the word number. It also extracts T proposal features   H = {ht}T   t=1 from the instance C, i.e. moment or region features.   \u2022 Interaction Module IM(\u00b7): It develops the vision-language interaction and output multimodal proposal features L = {lt}T   t=1. The interaction methods include feature fusion [52],   attention-based aggregation [43] and so on.   \u2022 Relation Module RM(\u00b7): It builds relation reasoning between proposals and outputs the   features P = {pt}T   t=1 and corresponding proposal scores K = {kt}T   t=1. Finally, we can   obtain the alignment score Agg(K) for the query-instance sample, where Agg(\u00b7) is an   aggregation function based on proposal scores, e.g., averaging the proposal scores, selecting   the maximum of these scores or averaging the top-n scores.   3.2   The CCL Architecture   In this section, we introduce our CCL paradigm in detail. As shown in Figure 2, we \ufb01rst use the   network GNet(C, Q) to estimate the given query-instance sample. We then apply the gradient-based   method to select the critical and inessential proposals. We then construct robust counterfactual   transformations (RCT) based on the inessential proposal set and design destructive counterfactual   transformations (DCT) according to the critical proposal set. After it, the network GNet(C, Q) with   RCT and DCT generates counterfactual results for the query-instance sample, including positive and   negative results corresponding to RCT and DCT, respectively. Finally, we can develop suf\ufb01cient   counterfactual contrastive training with ranking and consistency losses.   Concretely, we \ufb01rst pre-train GNet(C, Q) with a conventional MIL-based paradigm. For a sample   (C, Q), we introduce the randomly-selected instance C and query Q to construct the negative samples   (C, Q) and (C, Q). We then calculate the alignment score Agg(K) for (C, Q), and compute Agg(Kc)   3   DCT   : Destructive Counterfactual Transformation   : Robust Counterfactual Transformation    (\ud835\udc36, \ud835\udc44)   Critical Proposal Set   Inessential Proposal Set   GNet   GNet   Consistency   Loss   Proposal Scores   {\ud835\udc58)})+,   Ranking Loss   {\ud835\udc58)})+,   \u00d7\ud835\udc3d    (\ud835\udc36, \ud835\udc44)   {\ud835\udc58)   01})+,   GNet    (\ud835\udc36, \ud835\udc44)   {\ud835\udc58)   21})+,   \u00d7\ud835\udc3d   RCT   RCT   DCT   Figure 2: The framework of counterfactual contrastive learning for WSVLG.   and Agg(Kq) for (C, Q) and (C, Q). The MIL-based triplet loss Ltri is given by   Ltri = max(0, \u2206mil \u2212Agg(K) + Agg(Kc)) + max(0, \u2206mil \u2212Agg(K) + Agg(Kq)),   (1)   where \u2206mil is a margin value which is set to 1.0. Besides it, we also add a diversity loss [9] Ldiv to   adjust the score distribution and stabilize the weakly-supervised training, give by   ekt =   exp(kt)   PT   t=1 exp(kt)   , Ldiv = \u2212   T   X   t=1   ektlog(ekt), Lmil = Ltri + \u03b2Ldiv,   (2)   where Lmil is the pre-trained loss and \u03b2 is set to 0.01 to balance two losses.   After MIL-based pretraining, we develop our CCL paradigm to train GNet(C, Q). The \ufb01rst step is to   build the critical proposal set P+ and inessential proposal set P\u2212. Speci\ufb01cally, we apply GNet(C, Q)   to generate the proposal features H, proposal scores K and alignment score Agg(K) for (C, Q). A   simple method can directly select the proposals with higher scores as the critical proposals. But we   further employ the gradient-based selection method and apply the modi\ufb01ed Grad-CAM [35] to derive   the contribution of t-th proposal features, given by   Contribution(ht) = (\u2207htAgg(K))T 1,   (3)   where ht is the t-th proposal features and 1 is an all-ones vector. Based on the contribution of each   proposal, we can select M most important proposals as P+. We then randomly choose M proposals   from the resting as P\u2212. With P+/P\u2212, we can construct robust and destructive counterfactual   transformations RCT/DCT and apply them to the intermediate process of network inference, which is   introduced in the next section. In brief, we apply GNet(C, Q) with DCT to generate proposal scores   Kdj = {kdj   t }T   t=1 as a counterfactual negative result, where we construct J destructive transformations   and Kdj is the j-th negative result. Likewise, we generate J counterfactual positive results by   GNet(C, Q) with RCT, where Krj = {krj   t }T   t=1 is the j-th positive result.   Based on the proposal scores K, {Krj}   J   j=1 and {Kdj}   J   j=1 of the original, positive and negative   results, we can devise the contrastive loss to learn the proposal-language alignment by the score-based   and distribution-based difference. On the one hand, the positive results should have higher alignment   score than negative results, thus we develop a margin-based ranking loss by   Lrank = max(0, \u2206rank \u22121   J   J   X   j=1   Agg(Krj) + 1   J   J   X   j=1   Agg(Kdj)),   (4)   where \u2206rank is a margin value which is set to 0.6. On the other hand, we consider the distributionbased consistency loss to maintain the consistency of the score distributions on the original and   positive results, and pull the distributions of original and negative results. Concretely, we conduct the   softmax with a low temperature on score distributions and then develop the consistency loss by   k   \u2217   t =   exp(k\u2217   t /\u03c4)   PT   t=1 exp(k\u2217   t /\u03c4)   , Lcons = 1   J   J   X   j=1   (\u2212   T   X   t=1   ktlog(k   rj   t ) +   T   X   t=1   ktlog(k   dj   t )),   (5)   where \u03c4 is the softmax temperature, which is set to 0.5 and produces a sharper score distribution over   proposals [18]. Here we regard the original normalized results {kt}T   t=1 as the given pseudo labels.   Training. We \ufb01nally combine the two losses to form the counterfactual contrastive loss by   Lccl = Lrank + \u03bbLcons,   (6)   4   where we set \u03bb to 0.2 for the balance of two losses. During training, we update the network parameters   \u0398 at every step, so the network GNet(C, Q) for critical/inessential proposal selection also has the   latest parameters instead of retaining the pre-trained parameters.   Inference. Our CCL paradigm is only applied to the training process, thus it will not affect the speed   of inference. During inference, we can directly input the given sample into the network GNet(C, Q)   and select the proposal p with the highest proposal score kt as the grounding result.   3.3   Counterfactual Transformation   In this section, we introduce three counterfactual transformation strategies that applied to the intermediate process of network inference. Note that because each strategy will affect the subsequent   inference, three strategies cannot be used together but can only be applied individually.   Feature-Level Strategy (FLS). Given the set P+ with M critical proposal features {ht}M   t=1, we   \ufb01rst design a feature-level DCT to generate counterfactual negative results. Concretely, we damage   the features of critical proposals by memory-based replacement. That is, we replace critical proposal   features from EM(\u00b7) and input them to IM(\u00b7). A simple method is to replace the features with the   all-zero mask vector, but it is easy to distinguish. Further, we maintain a proposal memory bank   with B untrainable vectors {mp   b}B   b=1 that have the same dimensionality with ht. After MIL-based   pre-training, we initialize these vectors by the proposal features from randomly-selected different   samples. Next, for each feature ht from P+, we calculate the L2-normalized dot-product with   {mp   b}B   b=1 and conduct the softmax over these scores as a probability distribution. We next sample a   memory vector mp   i according to this distribution and replace ht. After the replacement in a minibatch, we update these selected memory vectors by mp   i \u2190\u03b1mp   i + (1 \u2212\u03b1)   1   |Si|   P   t\u2208Si ht, where Si   is the set of proposals that are replaced with mp   i in the mini-batch and \u03b1 is the hyper-parameter to   control the momentum update. Likewise, the network GNet(C, Q) with RCT replaces the inessential   features in P\u2212with the selected memory vectors to produce counterfactual positive results.   Interaction-Level Strategy (ILS). The interaction-level strategy is applied to the multi-modal   interaction unit MMI(ht, at) in EM(\u00b7), where at is language feature corresponding to the t-th   proposal feature ht. Existing grounding approaches [52, 8] often regard the sentence feature q as   at. Further, some works [44, 43, 53, 49] apply an attention-based aggregation to dynamically extract   language feature at for each proposal from word features {sn}N   n=1. Similar to FLS, we apply a   memory-based replacement method. Speci\ufb01cally, we maintain another query memory bank with B   untrainable vectors {mq   b}B   b=1, which are initialized by the language features from random samples.   For the proposal ht in P+, GNet(C, Q) with DCT replaces the language features at by one selected   mq   i and generate the counterfactual negative result. The selection and update method is the same as   in FLS. On the contrary, we destroy the language features corresponding to the proposal in P\u2212while   producing counterfactual positive results.   Relation-Level Strategy (RLS). Recent grounding approaches [44, 43, 52, 50] often contain a   relation module to develop context relations between proposals and we design the RLS to perturb   the relation construction. Concretely, we \ufb01rst de\ufb01ne the relation edge between proposals as eij =   (i, j, tpij, wij), which represents an edge from the proposal j to the proposal i with the type tpij and   weight wij. This de\ufb01nition of edges can be widely applied to the directed graph [44], undirected   graph [5], pre-built static graph with multi-type edges [43] and learning-based dynamic graph with   edge weights [50]. For negative result generation, GNet(C, Q) with DCT transforms crucial edges   {(i, j, tpij, wij) | i \u2208P+} related to proposals in P+ as following rules: 20% probability of no   change, 60% probability of replacing the j with another randomly proposal and 20% probability of   changing the edge type tpij to another randomly type. If the graph only has one-type edges, we set   the probability of replacing j to 80%. Note that if an edge eij is undirected, the transformation of eij   will also affect the reversed eji. Thus, during positive result generation, we only transform the edges   {(i, j, tpij, wij) | i \u2208P\u2212and j /\u2208P+} if the relation graph is undirected.   3.4   Concrete Grounding Networks Under CCL   In this section, we introduce concrete grounding networks to verify our CCL paradigm, where we   adopt the simple and mature components rather than complex designs as in [44, 43, 50, 5]. We brie\ufb02y   describe these components and the details are introduced in Section 1 of the supplementary material.   5   Video Grounding Network (VGN). In EM(\u00b7), we apply a Bi-GRU [10] to model the pre-extracted   visual features and obtain contextual features {fi}F   i=1. We de\ufb01ne an anchor with the boundaries (s, e)   as a moment proposal, and its proposal feature is given by ht = MaxPooling({fi}e   i=s). We then learn   the word features {sn}N   n=1 by another Bi-GRU. In IM(\u00b7), we employ an attention method [3, 49] to   aggregate word features for each proposal, and use the gate-based fusion [53] to generate multi-modal   features {lt}T   t=1. In RM(\u00b7), we de\ufb01ne an undirected static graph G where an edge is built between   two proposals if their temporal IoU (i.e. Intersection-Over-Union) is larger than 0.3. Next, we apply   the two-layer GAT [39] to capture proposal-proposal relationships and produce the proposal scores   {kt}T   t=1 by a linear layer. Finally, we select a top-n aggregation function Agg(\u00b7) to compute the   alignment score by Agg({kt}T   t=1) =   1   |Stop|   P   t\u2208Stop kt, where Stop is the set of top-n scores.   Image Grounding Network (IGN). In EM(\u00b7), we use the pre-trained Faster R-CNN [32] to extract   visual features ht and spatial features hs   t = [xs   t, ys   t , ws   t , hs   t] for each proposal, where (xs   t, ys   t ) are   the normalized center coordinates of the proposal and (ws   t , hs   t) are the normalized width and height.   Likewise, we apply a Bi-GRU to learn the word features {sn}N   n=1. In IM(\u00b7), we adopt the proposalword attention [44, 43] to learn proposal-aware language feature ct for each ht, and simply fuse them   by lt = Wl[ht; ct] + bl. In RM(\u00b7), we de\ufb01ne a directed spatial graph [45, 44, 43] with multi-type   edges, where tpij is based on the spatial features hs   i and hs   j. The details are introduced in Section 1.2   of the supplementary material. Next, we apply a GCN with edge-wise gates [22, 45, 43] to model   the proposal relations and learn the features {xt}T   t=1. Finally, we fuse them with spatial features   by pt = [xt; Wphs   t] and calculate the proposal scores {kt}T   t=1 by a linear layer with the sigmoid   activation. The aggregation function Agg(\u00b7) is consistent with VGN.   4   Experiments   4.1   Datasets and Evaluation Metrics   We conduct experiments on two large-scale video grounding datasets ActivityCaption [1] and   Charades-STA [14], and three large-scale image grounding datasets RefCOCO [47], RefCOCO+ [47]   and RefCOCOg [28]. The dataset details are introduced in Section 2 of the supplementary material.   For a fair comparison, we follow previous works [14, 17] to employ the R@n,IoU=m as the evaluation   metrics for video grounding. Concretely, we \ufb01rst compute the IoU between the predicted moments   and ground truth, and R@n,IoU=m is the percentage of at least one of the top-n moments having the   IoU > m. As for image grounding, we calculate the Accuracy as the metric. If the IoU between the   selected region and ground truth is larger than 0.5, we regard it as a right grounding result.   4.2   Implementation Details   Following previous works [14, 53, 26], we extract C3D [38] features for videos in ActivityCaption and   Charades-STA, where the maximum length of feature sequences is 256 and longer sequences are downsampled. We pre-de\ufb01ne moment proposals by sliding windows with widths [16,32,64,96,128,160]   and the stride is 1/4 of the window width, where we generate about 140 proposals for each video. For   image grounding, we extract 2,048-d visual features and 4-d spatial features for objects by ResNet101 based Faster R-CNN [32], where we obtain 36 proposals for each image. As for sentences, we   extract the 300-d embedding for each word by the pre-trained Glove embedding [31]. As for the   hyper-parameters in CCL, we set the proposal number M in P+/P\u2212to 32 for video grounding and   12 for image grounding. In FLS and ILS, we set the number B of memory vectors to 100, where the   coef\ufb01cient \u03b1 of the momentum update is set to 0.9. To avoid time-consuming training, the number   J of RCT/DCT is set to 3, where each type of transformation strategies is only applied once and   produces a counterfactual result. That is, there are 3 counterfactual positive results corresponding to   the robust FLS, ILS and RLS, and 3 negative results corresponding to the destructive FLS, ILS and   RLS. During MIL-based pretraining, we use an Adam optimizer [13] with the initial learning rate   0.001. We then use another Adam optimizer with the initial learning rate 0.0005 for the CCL training.   4.3   Performance Evaluation on Video Grounding   Considering videos contain consecutive and intricate events, video grounding is relatively more   dif\ufb01cult than image grounding. So we conduct more experiments on video grounding.   6   Table 1: Performance comparisons for video grounding on Charades-STA and ActivityCaption. The best results are bold   and the results with underlines are the best in baselines. \u2020: MIL-based methods; *: reconstruction-based methods   Method   Training   Charades-STA   ActivityCaption   R@1   R@1   R@5   R@5   R@1   R@1   R@5   R@5   IoU=0.5   IoU=0.7   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   IoU=0.3   IoU=0.5   CTRL [14]   FS   23.63   8.89   58.92   29.52   TGN [2]   FS   43.81   27.93   54.56   44.20   2D-TAN [52]   FS   39.81   23.25   79.33   52.15   59.45   44.51   85.53   77.13   WSLLN\u2020 [15]   WS   42.80   22.70   TGA\u2020 [29]   WS   19.94   8.84   65.52   33.51   CTF\u2020 [8]   WS   27.30   12.90   44.30   23.60   SCN\u2217[23]   WS   23.58   9.97   71.80   38.87   47.23   29.22   71.45   55.69   MARN\u2217[37]   WS   31.94   14.81   70.00   37.40   47.01   29.95   72.02   57.49   VGN\u2020   WS   30.77   12.23   70.58   37.64   46.17   28.79   71.23   55.13   VGN+CCL   WS   33.21   15.68   73.50   41.87   50.12   31.07   77.36   61.29   Table 2: Ablation results for video grounding about the counterfactual transformations and contrastive loss.   Setting   Charades-STA   ActivityCaption   R@1   R@1   R@5   R@5   R@1   R@1   R@5   R@5   IoU=0.5   IoU=0.7   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   IoU=0.3   IoU=0.5   VGN (Base)   30.77   12.23   70.58   37.64   46.17   28.79   71.23   55.13   VGN+CCL (Full)   33.21   15.68   73.50   41.87   50.12   31.07   77.36   61.29   VGN+FLS   32.06   14.66   72.28   39.65   50.53   30.78   75.66   59.35   VGN+ILS   31.60   13.78   71.76   39.15   47.68   29.32   74.37   58.16   VGN+RLS   31.28   13.46   71.94   38.70   48.42   29.45   73.38   57.38   w/o. rank loss   31.16   13.22   71.56   38.29   47.19   29.83   73.14   57.20   w/o. cons loss   32.55   14.71   72.49   40.37   48.87   29.28   76.09   59.27   Baseline. We compare our method with supervised and weakly-supervised methods. Supervised approaches include the early method CTRL [14], TGN with attention-based interaction [2] and 2D-TAN   with proposal relation modeling [52]. Weakly-supervised works contain the MIL-based approaches   WSLLN [15], TGA [29], CTF [8] and reconstruction-based methods SCN [23], MARN [37].   Evaluation Results. Table 1 reports the performance comparison between our method and existing   baselines on Charades-STA and ActivityCaption datasets, where VGN is the basic model with only   MIL-based training and VGN+CCL is under our CCL paradigm. Overall, VGN+CCL achieves   the best weakly-supervised performance on all criteria of two datasets, while VGN has a close   performance to the state-of-the-art baseline MARN. This fact suggests our CCL paradigm can   develop suf\ufb01cient confrontment between counterfactual positive and negative results, and signi\ufb01cantly   improve the weakly-supervised accuracy. More speci\ufb01cally, reconstruction-based methods SCN and   MARN outperform MIL-based approaches WSLLN, TGA, CTF and VGN, which indicates the MILbased training cannot construct strong supervision signals from randomly-selected negative samples.   But our CCL paradigm provides \ufb01ne-grained contrastive training by counterfactual transformations   in the intermediate process of network inference. Moreover, VGN+CCL outperforms the supervised   approaches CTRL and TGN and its performance is further close to the state-of-the-art supervised   method 2D-TAN. This demonstrates our CCL framework can reduce the gap between supervised and   weakly-supervised video grounding methods.   0   20   40   60   Proposal Number in  P+/ P10   15   20   25   30   35   Recall(%)   R@1 IOU=0.5   R@1 IOU=0.7   (a) Charades-STA   0   20   40   60   Proposal Number in  P+/ P25   30   35   40   45   50   55   Recall(%)   R@1 IOU=0.3   R@1 IOU=0.5   (b) ActivityCaption   Figure 3: Effect of the number M in P+/P\u2212.   Ablation Study.   We next perform ablation   studies on three counterfactual transformations   and the contrastive loss. Concretely, we \ufb01rst   train VGN using the CCL paradigm with only   one transformation strategy and produce three   ablation model VGN+FLS, VGN+ILS and   VGN+RLS. For a fair comparison, we keep the   number J of positive and negative transformations unchanged, that is, repeat J times using   7   Table 4: Performance comparisons and ablation results for image grounding on RefCOCO, RefCOCO+   and RefCOCOg. The best results are bold and the results with underlines are the best in baselines.   Method   Settings   RefCOCO   RefCOCO+   RefCOCOg   Val   TestA   TestB   Val   TestA   TestB   Val   VC   w/o. reg   17.14   22.30   19.74   24.05   28.14   VC   20.91   21.77   25.79   25.54   33.66   VC   w/o. \u03b1   32.68   27.22   34.68   28.10   29.65   ARN   Llan + Ladp   31.58   35.50   28.32   31.73   34.23   29.35   32.60   ARN   Llan + Ladp + Latt   32.17   35.35   30.28   32.78   34.35   32.13   33.09   IGN   Base   31.05   34.39   28.16   31.13   34.44   29.59   32.17   IGN   CCL   34.78   37.64   32.59   34.29   36.91   33.56   34.92   IGN   FLS   33.15   36.23   31.07   32.90   35.28   32.42   33.88   IGN   ILS   32.28   35.27   30.50   32.13   35.74   31.74   33.23   IGN   RLS   32.77   35.54   29.56   31.99   34.83   31.28   32.86   IGN   w/o. rank loss   32.54   35.62   30.46   32.34   35.10   31.64   33.74   IGN   w/o. cons loss   33.17   36.29   31.18   33.28   35.63   32.35   33.44   one strategy. As shown in Table 2, VGN+CCL outperforms three ablation models on almost all   metrics of two datasets, but three ablation models achieve the obvious performance improvement than   the basic VGN, which illustrates each counterfactual transformation is helpful for weakly-supervised   training and the collaboration of three strategies can further enhance the contrastive learning. Next,   we discard one loss from the contrastive loss at a time to generate the ablation models VGN (w/o.   rank loss) and VGN (w/o. cons loss). From the results in Table 2, two ablation models have performance degradation than the full model, indicating each loss is necessary during CCL training. And   VGN (w/o. rank loss) achieves the worse accuracy than VGN (w/o. cons loss), demonstrating the   importance of the ranking loss.   Hyper-Parameters Analysis. We then explore the effect of two crucial hyper-parameter: the   proposal number M in P+/P\u2212and the number J of positive/negative transformations. We \ufb01rst   set M to [8, 16, 32, 48, 64] and display the results in Figure 3. We note the model has the best   performance on both two datasets while M is set to 32. Because the counterfactual transformation   cannot suf\ufb01ciently destroy crucial grounding clues when M is too small. And the negative results   may be easy to distinguish and lead to insuf\ufb01cient contrastive learning while M is too large.   Table 3: Effect of the number J of positive/negative counterfactual transformations.   Number   Charades-STA   ActivityCaption   R@1   R@1   R@1   R@1   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   3   33.21   15.68   50.12   31.07   6   33.65   15.52   49.32   31.16   Next, in order to verify whether the larger J   can improve model performance, we construct 6   positive/negative counterfactual transformations   during CCL training and report the results in   Table 3. We can \ufb01nd the model has close performance on both datasets when J is set to 3 and   6, suggesting our CCL paradigm is insensitive   to the transformation number.   4.4   Performance Evaluation on Image Grounding   Baseline. We compare our method with the MIL-based method VC [51] and reconstruction-based   approach ARN [26]. Considering the weakly-supervised setting, we use the detected object features   from the pre-trained Faster R-CNN rather than the features from ground truth box annotations.   Evaluation Results. Table 4 show the evaluation results of our method and existing baselines   on three large-scale datasets. The fundamental results are similar to video grounding, that is, the   performance of the basic IGN is slightly worse than the existing state-of-the-art method ARN and   our CCL paradigm can signi\ufb01cantly boost the model accuracy.   Ablation Study. We also conduct ablation studies for image grounding about the counterfactual   transformations and contrastive loss. From Table 4, we can \ufb01nd the IGN models with FLS, ILS   and RLS achieve better performance than the basic one, especially IGN+FLS. This veri\ufb01es the   8   Table 5: Performance comparisons with negative sample mining methods.   Method   Charades-STA   ActivityCaption   RefCOCO   RefCOCO+   R@1 IoU=0.5   R@1 IoU=0.7   R@1 IoU=0.3   R@1 IoU=0.5   TestB   TestB   Hard Negative Mining   31.19   13.38   46.92   29.41   28.77   30.72   Proposal Masking   31.54   14.15   47.29   29.33   30.22   31.95   CCL   33.21   15.68   50.12   31.07   32.59   33.56   Ground Truth   Query: Snowboarder in dark green jacket holding a snowboard.   IGN   ARN   IGN+CCL   (a) Image Grounding   Query: The person immediately opened a window.   GT   SCN   8.00s   15.20s   8.00s   15.78s   6.39   13.40   7.23s   16.33s   VGN   VGN+CCL   (b) Video Grounding   Figure 4: Typical examples of weakly-supervised image grounding and video grounding results.   effectiveness of three counterfactual transformations. And similar to video grounding, IGN+CCL   with hybrid counterfactual strategies still outperforms all ablation models. From ablation results of   the contrastive loss, we observe the performance of IGN (w/o. rank loss) and IGN (w/o. cons loss)   drops around 1%\u223c2% on each metric, and IGN (w/o. cons loss) outperforms IGN (w/o. rank loss)   on almost all metrics. This fact indicates our CCL training mainly depends on the ranking loss and   the consistency loss can further boost the contrastive learning.   4.5   Performance Comparison with Negative Sample Mining Approaches   Compared with previous MIL-based approaches [11, 29, 15, 8], our CCL paradigm develops suf\ufb01cient   contrastive training between counterfactual positive and negative results to address the problem that   negative samples in MIL-based methods are often easy to distinguish. However, there are other   approaches [34, 6] to solve the problem from the perspective of negative sample mining. To further   validate the effectiveness of our CCL paradigm, we compare it with two negative sample mining   approaches. Concretely, the Hard Negative Mining method [34] selects those unmatched visionlanguage pairs with high alignment scores during model inference as the negative samples for the   MIL-based training, which spends much time on negative sample selection. And the Proposal   Masking method [6] synthesizes hard negative samples by directly masking important proposals and   then trains the weakly-supervised model under the MIL-based paradigm. As shown in Table 5, our   CCL paradigm achieves better weakly-supervised performance than two stronger baselines from the   perspective of negative sample mining. This fact suggests our proposed counterfactual transformations   and contrastive training can provide more effective supervision signals for the WSVLG task.   4.6   Qualitative Analysis   As shown in Figure 4, we display two typical examples of weakly-supervised grounding results   to qualitatively verify the effectiveness of our CCL paradigm. By intuitive comparison, we can   \ufb01nd the CCL paradigm can improve the grounding accuracy of the basic IGN and VGN model   by distinguishing the target one from plausible proposals, which veri\ufb01es the effectiveness of our   contrastive training between counterfactual results. More examples are shown in Section 3 of the   supplementary material.   5   Conclusions   In this paper, we propose a novel CCL paradigm for weakly-supervised vision-language grounding.   We design the feature-, interaction- and relation-level counterfactual transformations and develop   suf\ufb01cient contrastive training between counterfactual positive and negative results. Extensive experiments on \ufb01ve grounding datasets verify the effectiveness of our CCL paradigm. For future work, we   will further explore weakly-supervised contrastive learning and improve training ef\ufb01ciency.   9   Broader Impact   This paper introduces a novel CCL paradigm for weakly-supervised vision-language grounding and   improves grounding performance. Vision-language grounding is a crucial technique in multi-modal   understanding and can be applied to the human-computer interaction \ufb01eld. So this research can   promote the development of a multi-modal interaction system and facilitate people\u2019s daily lives.   And the exploration of weakly-supervised training in this paper can save the labor cost for data   annotations. The failure of this technique may lead to an inaccurate multi-modal understanding and   cause the mistake of the system based on the grounding results. Moreover, we validate our method   on large-scale public vision-language datasets and do not leverage biases in the data.   Acknowledgments   This work is supported by the National Key R&D Program of China under Grant No.   2018AAA0100603, Zhejiang Natural Science Foundation LR19F020006 and the National Natural Science Foundation of China under Grant No.61836002, No.U1611461 and No.61751209. This   research is supported by the Key Laboratory Foundation of Information Perception and Systems for   Public Security of MIIT (Nanjing University of Science and Technology) under Grant 202001 and   the Huawei Noah\u2019s Ark Lab.", "conf": "IJCAI", "year": "2020", "index": 128}, {"title": "Counterfactual Contrastive Learning for   Weakly-Supervised Vision-Language Grounding   Zhu Zhang1,2 , Zhou Zhao1,2\u2217, Zhijie Lin1 , Jieming Zhu3 , and Xiuqiang He3   1Zhejiang University   2Key Laboratory Foundation of Information Perception and Systems for Public Security of MIIT,   Nanjing University of Science and Technology   3Huawei Noah\u2019s Ark Lab   {zhangzhu, zhaozhou, linzhijie}@zju.edu.cn   {jamie.zhu, hexiuqiang1}@huawei.com", "abstract": "Weakly-supervised vision-language grounding aims to localize a target moment   in a video or a speci\ufb01c region in an image according to the given sentence query,   where only video-level or image-level sentence annotations are provided during   training. Most existing approaches employ the MIL-based or reconstruction-based   paradigms for the WSVLG task, but the former heavily depends on the quality   of randomly-selected negative samples and the latter cannot directly optimize the   visual-textual alignment score. In this paper, we propose a novel Counterfactual   Contrastive Learning (CCL) to develop suf\ufb01cient contrastive training between counterfactual positive and negative results, which are based on robust and destructive   counterfactual transformations. Concretely, we design three counterfactual transformation strategies from the feature-, interaction- and relation-level, where the   feature-level method damages the visual features of selected proposals, interactionlevel approach confuses the vision-language interaction and relation-level strategy   destroys the context clues in proposal relationships. Extensive experiments on \ufb01ve   vision-language grounding datasets verify the effectiveness of our CCL paradigm.   1", "content": "Vision-language grounding is a fundamental and crucial problem in multi-modal understanding.   Video grounding [14, 17] aims to identify the temporal boundaries of the target moment according to   the given description. And image grounding [20, 28, 46] localizes a speci\ufb01c region described by a   referring expression. Recently, to avoid expensive manual annotations, researchers begin to explore   Weakly-Supervised Vision-Language Grounding (WSVLG), which only needs the video-sentence   or image-sentence pairs during training. Most existing approaches [33, 26, 11, 29, 15, 23, 8, 37]   employ the MIL-based or reconstruction-based paradigm to train weakly-supervised grounding   networks, but they both have some drawbacks. The MIL-based methods [11, 29, 15, 8] often de\ufb01ne   the original vision-language pairs as positive samples, construct the unmatched vision-language pairs   as negative samples, and directly learn the latent visual-textual alignment by an inter-sample loss.   However, these approaches heavily depend on the quality of randomly-selected negative samples,   which are often easy to distinguish and cannot provide strong supervision signals. On the other hand,   reconstruction-based methods [33, 26, 23, 37] attempt to reconstruct the sentence query from visual   contents during training and utilize intermediate results, such as attention weights, to localize the   target proposal (i.e., region or moment) during inference. But these methods cannot directly optimize   the visual-textual alignment scores which are applied for inference. Considering the proposals with   \u2217Zhou Zhao is the corresponding author.   34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.   Image Grounding   Video Grounding   Query: The buffalo standing between    two other buffalo.   Query: The woman then dances all around the room in her ballet shoes, showing off    jumps and spins she can do.   Figure 1: Examples of image grounding and video grounding.   higher weights are not necessarily more relevant to the sentence query [21], the indirect optimization   may restrict further performance improvement.   Recently, contrastive learning has greatly promoted unsupervised pretraining of visual representations [16, 7], which develops contrastive training between positive and negative samples to learn   expressive visual features. In this paper, we propose a novel Counterfactual Contrastive Learning (CCL) paradigm for the WSVLG task, which constructs \ufb01ne-grained supervision signals from   counterfactual results to directly optimize the visual-textual alignment. Speci\ufb01cally, we \ufb01rst employ a MIL-based pre-trained grounding network to estimate the given vision-language pairs to   produce original results. By the gradient-based selection method, we then build a critical proposal   set and an inessential proposal set. Next, we design Robust Counterfactual Transformations (RCT)   based on the inessential set and devise Destructive Counterfactual Transformations (DCT) according   to the critical set. After it, we apply the grounding network with constructed RCT and DCT to   generate counterfactual results of the vision-language pairs, including positive and negative results   corresponding to RCT and DCT, respectively. Finally, we develop a ranking loss to focus on the   score-based difference between positive and negative results, and further devise the consistency   loss to consider the distribution-based discrepancy between them. To make the contrastive training   effective, the network with DCT needs to generate plausible negative results, where crucial visual   contents corresponding to the sentence query are destroyed while unnecessary contents are retained.   On the contrary, the network with RCT should damage visual contents of inessential proposals and   produce robust positive results. Thus, we design the counterfactual transformations from three levels:   (1) the feature-level strategy damages the features (i.e. endogenous clues) of selected proposals   by the memory-based replacement; (2) the interaction-level strategy confuses the vision-language   interaction by destroying the multi-modal fusion; and (3) the relation-level strategy perturbs the   context relations (i.e. exogenous clues) of chosen proposals by counterfactual relation construction.   The three strategies are applied to the intermediate process of network inference rather than the raw   inputs, and produce ambiguous results for suf\ufb01cient contrastive learning.   The main contributions of this paper are summarized as follows:   \u2022 We propose a novel counterfactual contrastive learning for WSVLG, which develops suf\ufb01cient contrastive training between counterfactual positive and negative results.   \u2022 We design the feature-level, interaction-level and relation-level strategies for counterfactual   transformations, which are applied to the intermediate process of network inference.   \u2022 Our CCL not only focuses on the score-based difference between the positive and negative   results but also considers the distribution-based discrepancy between them.   \u2022 We conduct extensive experiments on \ufb01ve large-scale vision-language grounding datasets to   verify the effectiveness of our proposed CCL paradigm.   2   Related Work   Image Grounding. Image grounding aims to localize the object region corresponding to the given   referring expression. Early supervised approaches [20, 28, 47, 30, 48] directly learn the latent   visual-textual alignment. And further works explore the language expression decomposition [46, 19],   co-attention interaction [12, 55] and relation construction [44, 43]. Besides, Liu et al. [27] introduce   the adversarial erasing approaches [36, 40] into this \ufb01eld, which detect the crucial contents by the   Grad-CAM method [35] and hide these contents to make the network further focus on other relevant   contents. Under the weakly-supervised setting, researchers build the supervision signals by the   2   reconstruction-based paradigm [33, 4, 54, 26] and MIL-based paradigm [51, 54, 11]. Rohrbach et   al. [33] learn the intermediate attention weights to localize the region by language reconstruction, and   Chen et al. [4] design knowledge-aided consistency in both visual and language modalities. Further,   Liu et al. [26] devise the collaborative training of language reconstruction and attribute classi\ufb01cation.   From the MIL-based view, Zhang et al. [51] propose a variational Bayesian method to explore the   context modeling and Datta et al. [11] build caption-conditioned image encoding to infer the latent   region-phrase correspondences.   Video Grounding. Video grounding tries to determine the temporal boundaries of the video moment   corresponding to the given sentence. Existing supervised methods can be categorized into the   top-down and bottom-up frameworks. The top-down approaches pre-de\ufb01ne a series of moment   proposals and select the target one by multi-modal estimation, including explicit proposals by sliding   windows [14, 17, 24, 25] and implicit proposals by multi-granularity anchors after visual-textual   interaction [2, 50, 53, 42, 52, 49]. And the bottom-up framework [3, 5, 41] does not pre-de\ufb01ne   moment proposals and directly predict the probabilities of temporal boundaries across frames. Under   the weakly-supervised setting, the MIL-based methods [29, 15, 8] learn the visual-textual alignment   by the inter-sample loss. Among them, Mithun et al. [29] utilize text-guided attention to learn the   latent alignment between frames and texts. Gao et al. [15] apply an alignment module to learn the   visual-textual consistency and devise a detection module to rank moment proposals. And Chen et   al. [8] develop a coarse-to-\ufb01ne manner to detect the accurate moment. Under the reconstruction-based   paradigm, Lin et al. [23] rank candidate moment proposals by a language reconstruction reward. And   Song et al. [37] further leverage attentional re-construction to rank the proposals.   3   Counterfactual Contrastive Learning   3.1   The Formulation of Weakly-Supervised Grounding Networks under CCL   Given the sentence query Q and the instance C (i.e. video V or image I), we aim to train a network   GNet(C, Q) to detect the most relevant proposal p without any (p, Q) alignment annotations during   training, where a proposal means a moment from the video or a region from the image. To illustrate   the CCL paradigm clearly, we \ufb01rst formulate the weakly-supervised grounding network GNet(C, Q)   under CCL. Speci\ufb01cally, we decompose GNet(C, Q) into three important modules:   \u2022 Encoder Module EM(\u00b7): It learns the sentence feature q and word features S = {sn}N   n=1   from the query Q, where N is the word number. It also extracts T proposal features   H = {ht}T   t=1 from the instance C, i.e. moment or region features.   \u2022 Interaction Module IM(\u00b7): It develops the vision-language interaction and output multimodal proposal features L = {lt}T   t=1. The interaction methods include feature fusion [52],   attention-based aggregation [43] and so on.   \u2022 Relation Module RM(\u00b7): It builds relation reasoning between proposals and outputs the   features P = {pt}T   t=1 and corresponding proposal scores K = {kt}T   t=1. Finally, we can   obtain the alignment score Agg(K) for the query-instance sample, where Agg(\u00b7) is an   aggregation function based on proposal scores, e.g., averaging the proposal scores, selecting   the maximum of these scores or averaging the top-n scores.   3.2   The CCL Architecture   In this section, we introduce our CCL paradigm in detail. As shown in Figure 2, we \ufb01rst use the   network GNet(C, Q) to estimate the given query-instance sample. We then apply the gradient-based   method to select the critical and inessential proposals. We then construct robust counterfactual   transformations (RCT) based on the inessential proposal set and design destructive counterfactual   transformations (DCT) according to the critical proposal set. After it, the network GNet(C, Q) with   RCT and DCT generates counterfactual results for the query-instance sample, including positive and   negative results corresponding to RCT and DCT, respectively. Finally, we can develop suf\ufb01cient   counterfactual contrastive training with ranking and consistency losses.   Concretely, we \ufb01rst pre-train GNet(C, Q) with a conventional MIL-based paradigm. For a sample   (C, Q), we introduce the randomly-selected instance C and query Q to construct the negative samples   (C, Q) and (C, Q). We then calculate the alignment score Agg(K) for (C, Q), and compute Agg(Kc)   3   DCT   : Destructive Counterfactual Transformation   : Robust Counterfactual Transformation    (\ud835\udc36, \ud835\udc44)   Critical Proposal Set   Inessential Proposal Set   GNet   GNet   Consistency   Loss   Proposal Scores   {\ud835\udc58)})+,   Ranking Loss   {\ud835\udc58)})+,   \u00d7\ud835\udc3d    (\ud835\udc36, \ud835\udc44)   {\ud835\udc58)   01})+,   GNet    (\ud835\udc36, \ud835\udc44)   {\ud835\udc58)   21})+,   \u00d7\ud835\udc3d   RCT   RCT   DCT   Figure 2: The framework of counterfactual contrastive learning for WSVLG.   and Agg(Kq) for (C, Q) and (C, Q). The MIL-based triplet loss Ltri is given by   Ltri = max(0, \u2206mil \u2212Agg(K) + Agg(Kc)) + max(0, \u2206mil \u2212Agg(K) + Agg(Kq)),   (1)   where \u2206mil is a margin value which is set to 1.0. Besides it, we also add a diversity loss [9] Ldiv to   adjust the score distribution and stabilize the weakly-supervised training, give by   ekt =   exp(kt)   PT   t=1 exp(kt)   , Ldiv = \u2212   T   X   t=1   ektlog(ekt), Lmil = Ltri + \u03b2Ldiv,   (2)   where Lmil is the pre-trained loss and \u03b2 is set to 0.01 to balance two losses.   After MIL-based pretraining, we develop our CCL paradigm to train GNet(C, Q). The \ufb01rst step is to   build the critical proposal set P+ and inessential proposal set P\u2212. Speci\ufb01cally, we apply GNet(C, Q)   to generate the proposal features H, proposal scores K and alignment score Agg(K) for (C, Q). A   simple method can directly select the proposals with higher scores as the critical proposals. But we   further employ the gradient-based selection method and apply the modi\ufb01ed Grad-CAM [35] to derive   the contribution of t-th proposal features, given by   Contribution(ht) = (\u2207htAgg(K))T 1,   (3)   where ht is the t-th proposal features and 1 is an all-ones vector. Based on the contribution of each   proposal, we can select M most important proposals as P+. We then randomly choose M proposals   from the resting as P\u2212. With P+/P\u2212, we can construct robust and destructive counterfactual   transformations RCT/DCT and apply them to the intermediate process of network inference, which is   introduced in the next section. In brief, we apply GNet(C, Q) with DCT to generate proposal scores   Kdj = {kdj   t }T   t=1 as a counterfactual negative result, where we construct J destructive transformations   and Kdj is the j-th negative result. Likewise, we generate J counterfactual positive results by   GNet(C, Q) with RCT, where Krj = {krj   t }T   t=1 is the j-th positive result.   Based on the proposal scores K, {Krj}   J   j=1 and {Kdj}   J   j=1 of the original, positive and negative   results, we can devise the contrastive loss to learn the proposal-language alignment by the score-based   and distribution-based difference. On the one hand, the positive results should have higher alignment   score than negative results, thus we develop a margin-based ranking loss by   Lrank = max(0, \u2206rank \u22121   J   J   X   j=1   Agg(Krj) + 1   J   J   X   j=1   Agg(Kdj)),   (4)   where \u2206rank is a margin value which is set to 0.6. On the other hand, we consider the distributionbased consistency loss to maintain the consistency of the score distributions on the original and   positive results, and pull the distributions of original and negative results. Concretely, we conduct the   softmax with a low temperature on score distributions and then develop the consistency loss by   k   \u2217   t =   exp(k\u2217   t /\u03c4)   PT   t=1 exp(k\u2217   t /\u03c4)   , Lcons = 1   J   J   X   j=1   (\u2212   T   X   t=1   ktlog(k   rj   t ) +   T   X   t=1   ktlog(k   dj   t )),   (5)   where \u03c4 is the softmax temperature, which is set to 0.5 and produces a sharper score distribution over   proposals [18]. Here we regard the original normalized results {kt}T   t=1 as the given pseudo labels.   Training. We \ufb01nally combine the two losses to form the counterfactual contrastive loss by   Lccl = Lrank + \u03bbLcons,   (6)   4   where we set \u03bb to 0.2 for the balance of two losses. During training, we update the network parameters   \u0398 at every step, so the network GNet(C, Q) for critical/inessential proposal selection also has the   latest parameters instead of retaining the pre-trained parameters.   Inference. Our CCL paradigm is only applied to the training process, thus it will not affect the speed   of inference. During inference, we can directly input the given sample into the network GNet(C, Q)   and select the proposal p with the highest proposal score kt as the grounding result.   3.3   Counterfactual Transformation   In this section, we introduce three counterfactual transformation strategies that applied to the intermediate process of network inference. Note that because each strategy will affect the subsequent   inference, three strategies cannot be used together but can only be applied individually.   Feature-Level Strategy (FLS). Given the set P+ with M critical proposal features {ht}M   t=1, we   \ufb01rst design a feature-level DCT to generate counterfactual negative results. Concretely, we damage   the features of critical proposals by memory-based replacement. That is, we replace critical proposal   features from EM(\u00b7) and input them to IM(\u00b7). A simple method is to replace the features with the   all-zero mask vector, but it is easy to distinguish. Further, we maintain a proposal memory bank   with B untrainable vectors {mp   b}B   b=1 that have the same dimensionality with ht. After MIL-based   pre-training, we initialize these vectors by the proposal features from randomly-selected different   samples. Next, for each feature ht from P+, we calculate the L2-normalized dot-product with   {mp   b}B   b=1 and conduct the softmax over these scores as a probability distribution. We next sample a   memory vector mp   i according to this distribution and replace ht. After the replacement in a minibatch, we update these selected memory vectors by mp   i \u2190\u03b1mp   i + (1 \u2212\u03b1)   1   |Si|   P   t\u2208Si ht, where Si   is the set of proposals that are replaced with mp   i in the mini-batch and \u03b1 is the hyper-parameter to   control the momentum update. Likewise, the network GNet(C, Q) with RCT replaces the inessential   features in P\u2212with the selected memory vectors to produce counterfactual positive results.   Interaction-Level Strategy (ILS). The interaction-level strategy is applied to the multi-modal   interaction unit MMI(ht, at) in EM(\u00b7), where at is language feature corresponding to the t-th   proposal feature ht. Existing grounding approaches [52, 8] often regard the sentence feature q as   at. Further, some works [44, 43, 53, 49] apply an attention-based aggregation to dynamically extract   language feature at for each proposal from word features {sn}N   n=1. Similar to FLS, we apply a   memory-based replacement method. Speci\ufb01cally, we maintain another query memory bank with B   untrainable vectors {mq   b}B   b=1, which are initialized by the language features from random samples.   For the proposal ht in P+, GNet(C, Q) with DCT replaces the language features at by one selected   mq   i and generate the counterfactual negative result. The selection and update method is the same as   in FLS. On the contrary, we destroy the language features corresponding to the proposal in P\u2212while   producing counterfactual positive results.   Relation-Level Strategy (RLS). Recent grounding approaches [44, 43, 52, 50] often contain a   relation module to develop context relations between proposals and we design the RLS to perturb   the relation construction. Concretely, we \ufb01rst de\ufb01ne the relation edge between proposals as eij =   (i, j, tpij, wij), which represents an edge from the proposal j to the proposal i with the type tpij and   weight wij. This de\ufb01nition of edges can be widely applied to the directed graph [44], undirected   graph [5], pre-built static graph with multi-type edges [43] and learning-based dynamic graph with   edge weights [50]. For negative result generation, GNet(C, Q) with DCT transforms crucial edges   {(i, j, tpij, wij) | i \u2208P+} related to proposals in P+ as following rules: 20% probability of no   change, 60% probability of replacing the j with another randomly proposal and 20% probability of   changing the edge type tpij to another randomly type. If the graph only has one-type edges, we set   the probability of replacing j to 80%. Note that if an edge eij is undirected, the transformation of eij   will also affect the reversed eji. Thus, during positive result generation, we only transform the edges   {(i, j, tpij, wij) | i \u2208P\u2212and j /\u2208P+} if the relation graph is undirected.   3.4   Concrete Grounding Networks Under CCL   In this section, we introduce concrete grounding networks to verify our CCL paradigm, where we   adopt the simple and mature components rather than complex designs as in [44, 43, 50, 5]. We brie\ufb02y   describe these components and the details are introduced in Section 1 of the supplementary material.   5   Video Grounding Network (VGN). In EM(\u00b7), we apply a Bi-GRU [10] to model the pre-extracted   visual features and obtain contextual features {fi}F   i=1. We de\ufb01ne an anchor with the boundaries (s, e)   as a moment proposal, and its proposal feature is given by ht = MaxPooling({fi}e   i=s). We then learn   the word features {sn}N   n=1 by another Bi-GRU. In IM(\u00b7), we employ an attention method [3, 49] to   aggregate word features for each proposal, and use the gate-based fusion [53] to generate multi-modal   features {lt}T   t=1. In RM(\u00b7), we de\ufb01ne an undirected static graph G where an edge is built between   two proposals if their temporal IoU (i.e. Intersection-Over-Union) is larger than 0.3. Next, we apply   the two-layer GAT [39] to capture proposal-proposal relationships and produce the proposal scores   {kt}T   t=1 by a linear layer. Finally, we select a top-n aggregation function Agg(\u00b7) to compute the   alignment score by Agg({kt}T   t=1) =   1   |Stop|   P   t\u2208Stop kt, where Stop is the set of top-n scores.   Image Grounding Network (IGN). In EM(\u00b7), we use the pre-trained Faster R-CNN [32] to extract   visual features ht and spatial features hs   t = [xs   t, ys   t , ws   t , hs   t] for each proposal, where (xs   t, ys   t ) are   the normalized center coordinates of the proposal and (ws   t , hs   t) are the normalized width and height.   Likewise, we apply a Bi-GRU to learn the word features {sn}N   n=1. In IM(\u00b7), we adopt the proposalword attention [44, 43] to learn proposal-aware language feature ct for each ht, and simply fuse them   by lt = Wl[ht; ct] + bl. In RM(\u00b7), we de\ufb01ne a directed spatial graph [45, 44, 43] with multi-type   edges, where tpij is based on the spatial features hs   i and hs   j. The details are introduced in Section 1.2   of the supplementary material. Next, we apply a GCN with edge-wise gates [22, 45, 43] to model   the proposal relations and learn the features {xt}T   t=1. Finally, we fuse them with spatial features   by pt = [xt; Wphs   t] and calculate the proposal scores {kt}T   t=1 by a linear layer with the sigmoid   activation. The aggregation function Agg(\u00b7) is consistent with VGN.   4   Experiments   4.1   Datasets and Evaluation Metrics   We conduct experiments on two large-scale video grounding datasets ActivityCaption [1] and   Charades-STA [14], and three large-scale image grounding datasets RefCOCO [47], RefCOCO+ [47]   and RefCOCOg [28]. The dataset details are introduced in Section 2 of the supplementary material.   For a fair comparison, we follow previous works [14, 17] to employ the R@n,IoU=m as the evaluation   metrics for video grounding. Concretely, we \ufb01rst compute the IoU between the predicted moments   and ground truth, and R@n,IoU=m is the percentage of at least one of the top-n moments having the   IoU > m. As for image grounding, we calculate the Accuracy as the metric. If the IoU between the   selected region and ground truth is larger than 0.5, we regard it as a right grounding result.   4.2   Implementation Details   Following previous works [14, 53, 26], we extract C3D [38] features for videos in ActivityCaption and   Charades-STA, where the maximum length of feature sequences is 256 and longer sequences are downsampled. We pre-de\ufb01ne moment proposals by sliding windows with widths [16,32,64,96,128,160]   and the stride is 1/4 of the window width, where we generate about 140 proposals for each video. For   image grounding, we extract 2,048-d visual features and 4-d spatial features for objects by ResNet101 based Faster R-CNN [32], where we obtain 36 proposals for each image. As for sentences, we   extract the 300-d embedding for each word by the pre-trained Glove embedding [31]. As for the   hyper-parameters in CCL, we set the proposal number M in P+/P\u2212to 32 for video grounding and   12 for image grounding. In FLS and ILS, we set the number B of memory vectors to 100, where the   coef\ufb01cient \u03b1 of the momentum update is set to 0.9. To avoid time-consuming training, the number   J of RCT/DCT is set to 3, where each type of transformation strategies is only applied once and   produces a counterfactual result. That is, there are 3 counterfactual positive results corresponding to   the robust FLS, ILS and RLS, and 3 negative results corresponding to the destructive FLS, ILS and   RLS. During MIL-based pretraining, we use an Adam optimizer [13] with the initial learning rate   0.001. We then use another Adam optimizer with the initial learning rate 0.0005 for the CCL training.   4.3   Performance Evaluation on Video Grounding   Considering videos contain consecutive and intricate events, video grounding is relatively more   dif\ufb01cult than image grounding. So we conduct more experiments on video grounding.   6   Table 1: Performance comparisons for video grounding on Charades-STA and ActivityCaption. The best results are bold   and the results with underlines are the best in baselines. \u2020: MIL-based methods; *: reconstruction-based methods   Method   Training   Charades-STA   ActivityCaption   R@1   R@1   R@5   R@5   R@1   R@1   R@5   R@5   IoU=0.5   IoU=0.7   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   IoU=0.3   IoU=0.5   CTRL [14]   FS   23.63   8.89   58.92   29.52   TGN [2]   FS   43.81   27.93   54.56   44.20   2D-TAN [52]   FS   39.81   23.25   79.33   52.15   59.45   44.51   85.53   77.13   WSLLN\u2020 [15]   WS   42.80   22.70   TGA\u2020 [29]   WS   19.94   8.84   65.52   33.51   CTF\u2020 [8]   WS   27.30   12.90   44.30   23.60   SCN\u2217[23]   WS   23.58   9.97   71.80   38.87   47.23   29.22   71.45   55.69   MARN\u2217[37]   WS   31.94   14.81   70.00   37.40   47.01   29.95   72.02   57.49   VGN\u2020   WS   30.77   12.23   70.58   37.64   46.17   28.79   71.23   55.13   VGN+CCL   WS   33.21   15.68   73.50   41.87   50.12   31.07   77.36   61.29   Table 2: Ablation results for video grounding about the counterfactual transformations and contrastive loss.   Setting   Charades-STA   ActivityCaption   R@1   R@1   R@5   R@5   R@1   R@1   R@5   R@5   IoU=0.5   IoU=0.7   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   IoU=0.3   IoU=0.5   VGN (Base)   30.77   12.23   70.58   37.64   46.17   28.79   71.23   55.13   VGN+CCL (Full)   33.21   15.68   73.50   41.87   50.12   31.07   77.36   61.29   VGN+FLS   32.06   14.66   72.28   39.65   50.53   30.78   75.66   59.35   VGN+ILS   31.60   13.78   71.76   39.15   47.68   29.32   74.37   58.16   VGN+RLS   31.28   13.46   71.94   38.70   48.42   29.45   73.38   57.38   w/o. rank loss   31.16   13.22   71.56   38.29   47.19   29.83   73.14   57.20   w/o. cons loss   32.55   14.71   72.49   40.37   48.87   29.28   76.09   59.27   Baseline. We compare our method with supervised and weakly-supervised methods. Supervised approaches include the early method CTRL [14], TGN with attention-based interaction [2] and 2D-TAN   with proposal relation modeling [52]. Weakly-supervised works contain the MIL-based approaches   WSLLN [15], TGA [29], CTF [8] and reconstruction-based methods SCN [23], MARN [37].   Evaluation Results. Table 1 reports the performance comparison between our method and existing   baselines on Charades-STA and ActivityCaption datasets, where VGN is the basic model with only   MIL-based training and VGN+CCL is under our CCL paradigm. Overall, VGN+CCL achieves   the best weakly-supervised performance on all criteria of two datasets, while VGN has a close   performance to the state-of-the-art baseline MARN. This fact suggests our CCL paradigm can   develop suf\ufb01cient confrontment between counterfactual positive and negative results, and signi\ufb01cantly   improve the weakly-supervised accuracy. More speci\ufb01cally, reconstruction-based methods SCN and   MARN outperform MIL-based approaches WSLLN, TGA, CTF and VGN, which indicates the MILbased training cannot construct strong supervision signals from randomly-selected negative samples.   But our CCL paradigm provides \ufb01ne-grained contrastive training by counterfactual transformations   in the intermediate process of network inference. Moreover, VGN+CCL outperforms the supervised   approaches CTRL and TGN and its performance is further close to the state-of-the-art supervised   method 2D-TAN. This demonstrates our CCL framework can reduce the gap between supervised and   weakly-supervised video grounding methods.   0   20   40   60   Proposal Number in  P+/ P10   15   20   25   30   35   Recall(%)   R@1 IOU=0.5   R@1 IOU=0.7   (a) Charades-STA   0   20   40   60   Proposal Number in  P+/ P25   30   35   40   45   50   55   Recall(%)   R@1 IOU=0.3   R@1 IOU=0.5   (b) ActivityCaption   Figure 3: Effect of the number M in P+/P\u2212.   Ablation Study.   We next perform ablation   studies on three counterfactual transformations   and the contrastive loss. Concretely, we \ufb01rst   train VGN using the CCL paradigm with only   one transformation strategy and produce three   ablation model VGN+FLS, VGN+ILS and   VGN+RLS. For a fair comparison, we keep the   number J of positive and negative transformations unchanged, that is, repeat J times using   7   Table 4: Performance comparisons and ablation results for image grounding on RefCOCO, RefCOCO+   and RefCOCOg. The best results are bold and the results with underlines are the best in baselines.   Method   Settings   RefCOCO   RefCOCO+   RefCOCOg   Val   TestA   TestB   Val   TestA   TestB   Val   VC   w/o. reg   17.14   22.30   19.74   24.05   28.14   VC   20.91   21.77   25.79   25.54   33.66   VC   w/o. \u03b1   32.68   27.22   34.68   28.10   29.65   ARN   Llan + Ladp   31.58   35.50   28.32   31.73   34.23   29.35   32.60   ARN   Llan + Ladp + Latt   32.17   35.35   30.28   32.78   34.35   32.13   33.09   IGN   Base   31.05   34.39   28.16   31.13   34.44   29.59   32.17   IGN   CCL   34.78   37.64   32.59   34.29   36.91   33.56   34.92   IGN   FLS   33.15   36.23   31.07   32.90   35.28   32.42   33.88   IGN   ILS   32.28   35.27   30.50   32.13   35.74   31.74   33.23   IGN   RLS   32.77   35.54   29.56   31.99   34.83   31.28   32.86   IGN   w/o. rank loss   32.54   35.62   30.46   32.34   35.10   31.64   33.74   IGN   w/o. cons loss   33.17   36.29   31.18   33.28   35.63   32.35   33.44   one strategy. As shown in Table 2, VGN+CCL outperforms three ablation models on almost all   metrics of two datasets, but three ablation models achieve the obvious performance improvement than   the basic VGN, which illustrates each counterfactual transformation is helpful for weakly-supervised   training and the collaboration of three strategies can further enhance the contrastive learning. Next,   we discard one loss from the contrastive loss at a time to generate the ablation models VGN (w/o.   rank loss) and VGN (w/o. cons loss). From the results in Table 2, two ablation models have performance degradation than the full model, indicating each loss is necessary during CCL training. And   VGN (w/o. rank loss) achieves the worse accuracy than VGN (w/o. cons loss), demonstrating the   importance of the ranking loss.   Hyper-Parameters Analysis. We then explore the effect of two crucial hyper-parameter: the   proposal number M in P+/P\u2212and the number J of positive/negative transformations. We \ufb01rst   set M to [8, 16, 32, 48, 64] and display the results in Figure 3. We note the model has the best   performance on both two datasets while M is set to 32. Because the counterfactual transformation   cannot suf\ufb01ciently destroy crucial grounding clues when M is too small. And the negative results   may be easy to distinguish and lead to insuf\ufb01cient contrastive learning while M is too large.   Table 3: Effect of the number J of positive/negative counterfactual transformations.   Number   Charades-STA   ActivityCaption   R@1   R@1   R@1   R@1   IoU=0.5   IoU=0.7   IoU=0.3   IoU=0.5   3   33.21   15.68   50.12   31.07   6   33.65   15.52   49.32   31.16   Next, in order to verify whether the larger J   can improve model performance, we construct 6   positive/negative counterfactual transformations   during CCL training and report the results in   Table 3. We can \ufb01nd the model has close performance on both datasets when J is set to 3 and   6, suggesting our CCL paradigm is insensitive   to the transformation number.   4.4   Performance Evaluation on Image Grounding   Baseline. We compare our method with the MIL-based method VC [51] and reconstruction-based   approach ARN [26]. Considering the weakly-supervised setting, we use the detected object features   from the pre-trained Faster R-CNN rather than the features from ground truth box annotations.   Evaluation Results. Table 4 show the evaluation results of our method and existing baselines   on three large-scale datasets. The fundamental results are similar to video grounding, that is, the   performance of the basic IGN is slightly worse than the existing state-of-the-art method ARN and   our CCL paradigm can signi\ufb01cantly boost the model accuracy.   Ablation Study. We also conduct ablation studies for image grounding about the counterfactual   transformations and contrastive loss. From Table 4, we can \ufb01nd the IGN models with FLS, ILS   and RLS achieve better performance than the basic one, especially IGN+FLS. This veri\ufb01es the   8   Table 5: Performance comparisons with negative sample mining methods.   Method   Charades-STA   ActivityCaption   RefCOCO   RefCOCO+   R@1 IoU=0.5   R@1 IoU=0.7   R@1 IoU=0.3   R@1 IoU=0.5   TestB   TestB   Hard Negative Mining   31.19   13.38   46.92   29.41   28.77   30.72   Proposal Masking   31.54   14.15   47.29   29.33   30.22   31.95   CCL   33.21   15.68   50.12   31.07   32.59   33.56   Ground Truth   Query: Snowboarder in dark green jacket holding a snowboard.   IGN   ARN   IGN+CCL   (a) Image Grounding   Query: The person immediately opened a window.   GT   SCN   8.00s   15.20s   8.00s   15.78s   6.39   13.40   7.23s   16.33s   VGN   VGN+CCL   (b) Video Grounding   Figure 4: Typical examples of weakly-supervised image grounding and video grounding results.   effectiveness of three counterfactual transformations. And similar to video grounding, IGN+CCL   with hybrid counterfactual strategies still outperforms all ablation models. From ablation results of   the contrastive loss, we observe the performance of IGN (w/o. rank loss) and IGN (w/o. cons loss)   drops around 1%\u223c2% on each metric, and IGN (w/o. cons loss) outperforms IGN (w/o. rank loss)   on almost all metrics. This fact indicates our CCL training mainly depends on the ranking loss and   the consistency loss can further boost the contrastive learning.   4.5   Performance Comparison with Negative Sample Mining Approaches   Compared with previous MIL-based approaches [11, 29, 15, 8], our CCL paradigm develops suf\ufb01cient   contrastive training between counterfactual positive and negative results to address the problem that   negative samples in MIL-based methods are often easy to distinguish. However, there are other   approaches [34, 6] to solve the problem from the perspective of negative sample mining. To further   validate the effectiveness of our CCL paradigm, we compare it with two negative sample mining   approaches. Concretely, the Hard Negative Mining method [34] selects those unmatched visionlanguage pairs with high alignment scores during model inference as the negative samples for the   MIL-based training, which spends much time on negative sample selection. And the Proposal   Masking method [6] synthesizes hard negative samples by directly masking important proposals and   then trains the weakly-supervised model under the MIL-based paradigm. As shown in Table 5, our   CCL paradigm achieves better weakly-supervised performance than two stronger baselines from the   perspective of negative sample mining. This fact suggests our proposed counterfactual transformations   and contrastive training can provide more effective supervision signals for the WSVLG task.   4.6   Qualitative Analysis   As shown in Figure 4, we display two typical examples of weakly-supervised grounding results   to qualitatively verify the effectiveness of our CCL paradigm. By intuitive comparison, we can   \ufb01nd the CCL paradigm can improve the grounding accuracy of the basic IGN and VGN model   by distinguishing the target one from plausible proposals, which veri\ufb01es the effectiveness of our   contrastive training between counterfactual results. More examples are shown in Section 3 of the   supplementary material.   5   Conclusions   In this paper, we propose a novel CCL paradigm for weakly-supervised vision-language grounding.   We design the feature-, interaction- and relation-level counterfactual transformations and develop   suf\ufb01cient contrastive training between counterfactual positive and negative results. Extensive experiments on \ufb01ve grounding datasets verify the effectiveness of our CCL paradigm. For future work, we   will further explore weakly-supervised contrastive learning and improve training ef\ufb01ciency.   9   Broader Impact   This paper introduces a novel CCL paradigm for weakly-supervised vision-language grounding and   improves grounding performance. Vision-language grounding is a crucial technique in multi-modal   understanding and can be applied to the human-computer interaction \ufb01eld. So this research can   promote the development of a multi-modal interaction system and facilitate people\u2019s daily lives.   And the exploration of weakly-supervised training in this paper can save the labor cost for data   annotations. The failure of this technique may lead to an inaccurate multi-modal understanding and   cause the mistake of the system based on the grounding results. Moreover, we validate our method   on large-scale public vision-language datasets and do not leverage biases in the data.   Acknowledgments   This work is supported by the National Key R&D Program of China under Grant No.   2018AAA0100603, Zhejiang Natural Science Foundation LR19F020006 and the National Natural Science Foundation of China under Grant No.61836002, No.U1611461 and No.61751209. This   research is supported by the Key Laboratory Foundation of Information Perception and Systems for   Public Security of MIIT (Nanjing University of Science and Technology) under Grant 202001 and   the Huawei Noah\u2019s Ark Lab.", "conf": "NIPS", "year": "2020", "index": 128}, {"title": "Found a Reason for me? Weakly-supervised Grounded Visual Question   Answering using Capsules   Aisha Urooj Khan1,2, Hilde Kuehne2, Kevin Duarte1, Chuang Gan2, Niels Lobo1, Mubarak Shah1   1 CRCV, University of Central Florida , 2 MIT-IBM Watson AI Lab", "abstract": "The problem of grounding VQA tasks has seen an increased attention in the research community recently, with   most attempts usually focusing on solving this task by using   pretrained object detectors.   However, pre-trained object   detectors require bounding box annotations for detecting   relevant objects in the vocabulary, which may not always   be feasible for real-life large-scale applications. In this paper, we focus on a more relaxed setting: the grounding of   relevant visual entities in a weakly supervised manner by   training on the VQA task alone. To address this problem, we   propose a visual capsule module with a query-based selection mechanism of capsule features, that allows the model   to focus on relevant regions based on the textual cues about   visual information in the question. We show that integrating the proposed capsule module in existing VQA systems   signi\ufb01cantly improves their performance on the weakly supervised grounding task. Overall, we demonstrate the effectiveness of our approach on two state-of-the-art VQA   systems, stacked NMN and MAC, on the CLEVR-Answers   benchmark, our new evaluation set based on CLEVR scenes   with groundtruth bounding boxes for objects that are relevant for the correct answer, as well as on GQA, a real world   VQA dataset with compositional questions. We show that   the systems with the proposed capsule module consistently   outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance   on VQA task.1   1.", "content": "VQA systems have now matured to the point where their usage is increasing in real life applications such as answering   questions based on radiology images [1], helping visually   impaired people [10], and human-robot interactions [38].   However, with the increasing maturity of such systems, it   also becomes important to know how the answer is actually   generated in order to assess if it is based on the right cues   1Code will be available at https://github.com/aurooj/   WeakGroundedVQA_Capsules.git   Figure 1. Problem de\ufb01nition: Given an input image and a question, we want to answer the question as well as localize the evidence (shown in green boxes) with VQA supervision alone. Best   viewed in color.   or not. If the question is \u201cAre there black horses to the right   of the vehicle?\u201d (see \ufb01gure 1), it may be important to know   if the answer is generated because the network found black   horses at the right place in the image or not. This allows to   judge the overall correctness beyond simply evaluating the   textual answer. Recent works [17, 36, 4, 20] try to address   this problem by starting to evaluate not only the VQA accuracy, but also the accuracy of grounding that the answer is   based on. The grounding of an answer is usually assessed   by considering the respective attention map of the image   for the given answer, and by evaluating if the objects that   are relevant for the right answer are attended to or not.   To achieve good grounding accuracy, most approaches   in this \ufb01eld rely on input feature maps from object detection   models that are pretrained with the relevant object classes.   This restricts the scope to known object classes such as MS   COCO [31], or require to annotate the regions of relevant   objects, and to pretrain an object detector for them[17].   Only few attempts have been made so far to address this   problem to train both, the VQA as well as the grounding,   without pretrained object detection based on the information of the VQA task alone as e.g. in context of the GQA   dataset by only using spatial (appearance) features [17].   This paper focuses on exactly this scenario: weakly supervised visual grounding based on VQA supervision. The   idea here is that both tasks, the visual question-answering   as well as the correct visual grounding, should be learned   8465   from the VQA task alone. Hence, we do not use any objectlevel information as an input or in supervision.   The correct grounding in this case is usually based on   two major tasks, \ufb01nding the relevant visual instances and,   usually, modeling the relation between those instances as   seen in \ufb01gure 1. To address this problem, we propose extending current VQA frameworks with capsules. Capsule   networks were introduced by Sabour et al. [39], and have   shown promising results for image interpretability [25] and   segmentation in various \ufb01elds such as 3D point clouds [48],   videos [7] and medical images [28]. This is the result of   capsule layers\u2019 ability to learn part-to-whole relationships   for object entities through routing-by-agreement. We believe this capability to model objects and their relations   quali\ufb01es capsules as a good choice for addressing the problem of weakly-supervised grounding in VQA.   Current capsule-based methods follow the practice of   adding capsule layers on top of convolutional features, and   training them with object class supervision. A discrete and   supervised masking operation, i.e. masking all capsules except the ground-truth class capsule, is often applied to reconstruct or segment the object corresponding to the given   class. In case of weak VQA grounding, no class or object based supervision is available; only an embedding of   a natural language question is given. Therefore, we propose a \u201csoft-masking\u201d procedure which selects the capsule(s) based on the input question. For example, if the reasoning operation is Find(\u201cblue spheres\u201d), the soft-masking   operation will mask all capsules not representing the \u201cblue   spheres\u201d. Once the irrelevant capsules are masked, the capsule representations are passed to future reasoning operations to complete the VQA task.   To evaluate VQA systems for their answer grounding   ability, we consider two datasets, the recently proposed   GQA dataset [17] as well as the CLEVR dataset [23]. To   allow the evaluation of grounding accuracy on CLEVR,   we propose a new CLEVR validation set, named CLEVRAnswers. CLEVR-Answers provides VQA pairs with the   respective ground truth bounding boxes for all objects that   the answer is based on. Note that, as we are not interested   in using any object annotations during training, we only   need ground truth bounding boxes during evaluation, but   not during training. The idea is, thus, to train on the standard CLEVR training set and to learn visual representations   of objects during this training without further annotation.   We use this new evaluation set to test current state-of-theart frameworks, MAC [16] and Stacked NMN [13] with respect to their grounding abilities. We show that, although all   frameworks perform at the same level with respect to VQA   accuracy, there are major differences with respect to their   grounding abilities. We show that using capsules with soft   query-based masking signi\ufb01cantly improves existing methods\u2019 grounding abilities.   2. Related Work   VQA and visual grounding   Recent approaches for VQA   task rely on object level features as input to improve the   VQA accuracy   [2, 14, 26, 45, 21, 40, 8, 15, 44]. Those   features are extracted from pretrained object detectors. This   makes the VQA task easier and usually performs better than   spatial or appearance features, but it also adds an additional   preprocessing step (detecting objects) to the pipeline. Additionally, since the pretraining relies on the object classes   in the training set, it limits the extension of such methods   to datasets with object-level annotation. Basic appearance   or grid-based features, e.g., based on a backbone pretrained   on ImageNet, are easier to generate and have recently been   shown to work as well as object level features [19] for the   VQA task. All these approaches usually only focus on the   accuracy of the VQA task, and do not evaluate the respective grounding of their answers.   Focusing on this capability, several VQA datasets now   provide grounding labels such as GQA [17], VCR [46],   VQS [8], CLEVRER [42, 5] and TVQA+ [29]. Here, object   annotations are either provided for all objects in the visual   input, or only for the objects relevant to both question and   answer. Out of those, GQA speci\ufb01cally focuses on the evaluation of grounding accuracy with and without object detection supervision and attempts to evaluate MAC [16] and   BottomUp [2] for their grounding ability in natural images.   We, therefore, choose GQA to evaluate capsule-augmented   systems in real world for weakly supervised grounding. Additionally, we compute the answer grounding in terms of   overlap and IOU to measure how precise this grounding is   in correlation to the answer.   VQA and visual reasoning on CLEVR   CLEVR [23] is   a diagnostic visual reasoning dataset with compositional   questions to test performance of VQA systems on a variety of reasoning skills. Since the introduction of CLEVR, a   large number of VQA systems [13, 24, 43, 16, 33, 34, 41]   have surfaced to solve VQA task on this benchmark achieving near perfect VQA accuracy [22]. One line of works   [24, 13, 34] uses reasoning layouts supervision provided   with the image-QA pairs. Additionally, neuro-symbolic approaches over object level features are proposed, e.g., in   [43, 33]. Many of those ideas implicitly or explicitly include the concept of grounding on this dataset, but usually   rely on a pretrained object detector to generate initial object   attention maps. Several other variants of CLEVR have also   emerged trying to solve various problems such as CLEVRCoGent [23], CLEVR-Dialog [27], CLOSURE [3], simplyCLEVR [36], and CLEVR-Ref+ [32], with CLEVR-Ref+   focusing on grounding based on referential expressions, but   not VQA, and simply-CLEVR providing only grounding labels for one or all objects in the image. We, on the other   hand, provide bounding box labels for all question types   8466   Figure 2. Overview of our pipeline: given the question-image pair, we obtain image features X using an image encoder \u03c8 and question   features (both sentence fs and word embeddings fw) using an RNN \u03c1. These question-based features are then input to a multi-hop   reasoning module which generates T textual queries q0, q1, ..., qT . The Primary Capsules layer then transforms the convolutional image   features into capsules (each capsule has a k \u00d7 k pose matrix and an activation weight). The primary capsules use a routing-by-agreement   algorithm to vote for higher level capsules at each spatial location. These capsules are used as visual representation inside the reasoning   process. At each timestep t \u2208{1, 2, ..., T}, a soft masking module \ufb01rst masks irrelevant capsules using textual query qt to select a subset   of capsules at each spatial location denoted as Vmct. Then, the reasoning operation is performed over selected capsules (using attention   combined with other reasoning). Output of the reasoning operation is a vector. Output Module then aggregates these outputs and predicts   the answer. We train the system with VQA supervision only. At inference time, we post-process the attention maps produced by the   reasoning modules to obtain grounding predictions.   without imposing any constraint on the number of objects   relevant to answer grounding. Thus, CLEVR-Answers enables us to evaluate grounding abilities of current state-ofthe-art methods without any constraints.   Capsule networks   Hinton et al. [11] \ufb01rst proposed capsule networks to learn vectors of view equivariant features   from images. More recently, Sabour et al. [39] extended   capsule networks with an iterative routing-by-agreement algorithm to classify and segment multiple digits within an   image.   Several works have proposed improved methods   for routing [12, 47, 30, 18] as well as have applied capsule networks to different tasks and domains [6, 28, 48, 35].   While most previous works tend to be supervised by calculating a loss over a set of \u201cclass capsules\u201d, our proposed   approach does not have this capsule-to-object supervision;   rather, capsules are incorporated in our system as intermediate layers and are learned by using weak supervision from   question answers. Several capsule networks which perform   classi\ufb01cation tasks [39, 6, 37] tend to use a masking operation to ensure capsules learn class-speci\ufb01c representations.   This operation masks all capsule pose values to 0 except for   the selected (i.e. ground-truth or predicted) class capsule,   and uses this masked representation to reconstruct or segment the input image or video. Since there are no groundtruth class annotations, we propose a novel soft-masking   operation which effectively selects the capsule(s) relevant   to the input query and masks irrelevant capsules.   3. Proposed Approach   3.1. Problem Formulation   Given an input image I and a question Q, our goal is   to output the correct answer a \u2208A, where A denotes the   answer vocabulary, and B bounding box predictions for the   objects which led to the answer a. Figure 1 illustrates the   problem. Our pipeline is explained in the following sections. An overview of the framework is given in \ufb01gure 2.   3.2. Input Embeddings   Question embedding We are given a question Q of words   w1, w2, ..., wl, where l is the length of Q in words. Let V   be the vocabulary for question words in the training set with   a lookup embedding E \u2208R|V |\u00d7de. Each word in Q is represented by a de-dimensional initial embedding vector. Let   \u03c6(Q, [w1, w2, ..., wl]) be a sentence encoder which outputs   both sentence level embedding fs for Q as well as wordlevel features fw. These sentence-level and word-level embeddings are then input to our system. Following previous   works [13, 16], we choose \u03c6 to be a BiLSTM. Output dimensions for sentence embedding fs and word embeddings   fw are, therefore, fs \u2208Rdq, fw \u2208Rdq, where dq = 2 \u00d7 d,   and d is the dimension of sentence encoder.   Image embedding Given an input image I, we compute   8467   a feature map X = \u03c8(I), where \u03c8 is a pretrained image   encoder and X \u2208RH\u00d7W \u00d7df denotes the features extracted   for I (df is the feature dimension).   3.3. Textual query generator   To answer a question based on an image, a VQA system performs attentional parsing of the question, i.e. attends to selected words from the question iteratively depending on the   reasoning required to answer the question. This approach   of splitting the question into subqueries is often termed as   multi-hop or recurrent reasoning, where a query is generated at each reasoning step to attend to the image to collect answer-relevant knowledge. Let \u03c1 be our query generator which takes sentence embedding, fs, and word embeddings, fw, as an input at each time step t (t = 1, 2, .., T),   and outputs query qt as an output.   qt = \u03c1(fs, fw), \u2200t \u2208{1, 2, ..., T}.   (1)   More details are discussed in the supplementary.   3.4. Capsules with soft masking   A capsule is a group of neurons representing an entity or a   part of an entity. In this work, we use matrix capsules [12],   which are composed of a logistic unit (called the activation)   and a 4x4 pose matrix (called the pose). The activation indicates the presence of a speci\ufb01c entity, whereas the pose   represents the entity\u2019s properties. A capsule layer consists   of many capsules, which use a routing-by-agreement algorithm to vote for capsules in the following layer in order   to model part-to-whole relationships. Matrix capsules use   EM-Routing algorithm for capsule routing. We integrate   them into the process as follows.   Visual capsules: From the image embedding, X, the primary capsules are obtained by using a learned convolution operation resulting in C1 capsule types each with a   4x4 pose matrix and an activation for each spatial position. The output dimension of the primary capsule layer   is RH\u00d7W \u00d7C1\u00d74\u00d74 and RH\u00d7W \u00d7C1\u00d71 for poses and activations respectively. To obtain a higher-level capsule representation, we perform EM-routing over primary capsules to   obtain a set of C2 capsules at each spatial position. These   capsules model different objects within the scene (including   the background). Output dimensions for poses and activations are RH\u00d7W \u00d7C2\u00d74\u00d74 and RH\u00d7W \u00d7C2\u00d71 respectively.   They are used as the visual representation of the input image in future steps.   Soft masking:   The trivial approach of leveraging this   capsule representation for VQA would be to group   the poses and activations to form a tensor of shape   RH\u00d7W \u00d7C2\u00d7(4\u00d74+1), and use them as a single feature map   like standard convolution-based methods. Although, this   performs decently well, it is not an ideal solution since it   treats each dimension of the capsule poses as independent   features and disregards the fact that all dimensions in the   capsule pose represent a single object or entity.   Instead of this independent feature selection, we propose the selection of individual capsules based on the question. This is achieved by masking capsules which are irrelevant to the reasoning operation. Previous capsule methods use masking for image reconstruction [12] or segmentation [7], however they require ground-truth class labels to   select the single capsule type which is not masked. Since   no object/class-level supervision is present for this task, we   propose learning which capsules should be masked in an   end-to-end manner. For each reasoning step, a fully connected layer generates a set of C2 logits denoting each capsule types\u2019 relevance for the given query. Mathematically,   this can be de\ufb01ned as:   mtlogits = \u03b7(qt),   (2)   where qt is the textual query at reasoning step t, and \u03b7 is   the fully connected layer. Then, a one-hot mask, mt \u2208RC2   is generated where mi = 1 for i = argmax(mtlogits). This   mask is then applied to the visual capsule layer:   Vmct = mt \u2299Yc2,   (3)   where, Yc2 is the output of visual capsules layer, and Vmct   are the masked visual capsules corresponding to textual   query qt. We call this operation hard masking.   We \ufb01nd that hard masking operation leads to sub-optimal   performance, since the lack of supervision leads to some   capsule never being selected, resulting in poor representations. To remedy this, we present a novel soft masking   method as visualized in the green box in \ufb01gure 2, which   allows gradients to \ufb02ow through all capsules. Instead of   creating a one-hot mask, a softmax operation is used on the   logits to create a set of soft weights, which then mask the   visual capsules, as follows:   Vmct = softmax(\u03b7(qt)) \u2299Yc2.   (4)   These masked visual capsules are then used for reasoning   operations as de\ufb01ned by their respective modules. We show   that incorporating capsules and soft masking within an attentional VQA system can boost its grounding ability signi\ufb01cantly without compromising VQA accuracy, therefore,   reducing the performance-explainability trade-off.   3.5. Output module   The reasoning modules output features which are aggregated over reasoning steps and sent to an output module i.e.,   a classi\ufb01er which outputs answer scores. For grounding predictions, we consider the spatial attention maps produced   by reasoning modules and post-process them to obtain the   object detections. The post-processing is described in the   following section.   8468   Figure 3. Sample images with QA pair and generated answer   grounding labels for CLEVR-Answers dataset.   4. Implementation details   We integrate capsules into two baseline VQA systems:   Stacked Neural Module Networks [13] and MAC [16]. We   make the following architectural changes to these systems.   Capsules with MAC.   MAC [16] is a recurrent reasoning architecture which performs T reasoning steps to answer the question. Each reasoning step involves generating a question-based control signal (textual query), using   this control signal to read from image features (using attention), and writing memory. The \ufb01nal output after T reasoning steps is then combined with the question and goes   into the answer classi\ufb01er. MAC also produces interpretable   attention maps for explaining the reasoning process behind   VQA. For capsule integration into the MAC cell, we make   the following changes: First, capsule layers are added on   top of the convolutional layer to obtain visual capsules from   image features. The read module is responsible for attending to spatial image features and retrieving query-relevant   image features, based on the previous output and the current   control signal (question based feature at timestep t). Inside   the read module, we \ufb01rst map the control signal to a feature   vector of dimensions C2 \u00d7 (4 \u00d7 4 + 1) using a trainable   linear layer. This feature vector is then used to generate a   soft mask to obtain only query-related capsules for further   reasoning. Weights for the masking layer are shared among   MAC cells. These masked capsules are then used for further   reasoning inside the read module.   Capsules   with   Stacked   Neural   Module   Network   (SNMN).   Stacked neural module network is an attentional VQA method following the same reasoning pipeline   as explained above. SNMN produces human interpretable   attention maps. SNMN trains convolutional layers on pretrained image features. The output of these convolutional   layers then goes into the reasoning modules and uses textual query to perform the reasoning operation producing an   attention map as output. To integrate capsules into SNMN,   we append our capsule module on top of image features to   obtain C2 visual capsules. Instead of convolutional image   features, reasoning modules now perform their reasoning   operation on capsules. For query-based soft masking, each   neural module has a fully connected layer which takes   textual query qt \u2208Rd as input and outputs a feature vector   of dimension C2 \u00d7 (4 \u00d7 4 + 1). This feature vector is then   used to generate capsule mask of size C2, and for further   interaction between query and masked capsules.   Each   reasoning module in SNMN has its own masking layer   except Scene, And, and Or, since these modules do not   use the textual parameter in their computations.   Generation of attention maps.   During training, capsule   layers learn to attend to different visual cues in the image,   including background regions when no grounding evidence   is available for the answer. In order to give more weight to   high attention regions and suppress attention on the background, we introduce an opacity parameter \u03b1. For uniform   attention regions, opacity is scaled up by \u03b1. After postprocessing spatial attention using \u03b1, an attention threshold   of 0.5 is applied to get a binary mask with high attention   regions. Each connected component in this binary mask is   considered an object detection. See supplementary for results w.r.t. variations in \u03b1.   5. Datasets   We perform our evaluation on two datasets: GQA and   CLEVR. GQA, as real world dataset for visual reasoning   and compositional question answering, combines the two   aspects of the proposed idea by providing an evaluation of   grounding VQA tasks. It also provides a baseline for the   weakly supervised grounding task by using only spatial features that are not pretrained on object annotations. CLEVR,   as opposed to GQA, provides a sandbox for visual reasoning VQA tasks with synthetic images only, no visual overlap to any ImageNet categories, and a challenging grounding setting with objects in various combinations of color,   shape, size, and material. Recent work on attentional VQA   systems (SNMN [13] and MAC [16]) show high VQA accuracy, making this dataset a good candidate to explore the   relationship of grounding and VQA accuracy.   GQA.   GQA is a real world visual reasoning dataset with   multi-hop reasoning questions.   GQA provides compositional questions for challenging real world images. Questions in GQA are more diverse than VQA 2.0 [9] in several   ways with more coverage to relation, spatial, and compositional questions [17]. This dataset consists of 22M QA   pairs for more than 113K images. GQA provides grounding labels for objects referenced in the question and answer   which makes it a suitable test bed for our task. We use the   balanced version of this dataset with the standard split provided by the authors for our experiments.   CLEVR-Answers for Visual Grounding.   In this paper,   we extend CLEVR dataset to CLEVR-Answers for visual   grounding of answers. The CLEVR dataset is a synthetically rendered dataset for the evaluation of visual reasoning   8469   Overlap   IOU   Method   T   #param   Acc.   P   R   F1   P   R   F1   MAC [16]   4   12.20M   97.70   24.92   56.27   34.55   13.99   33.50   19.73   MAC-Caps   12.92M   96.79   47.04   73.06   57.23   23.97   39.06   29.71   MAC [16]   6   12.72M   98.00   30.10   52.41   38.24   12.59   23.62   16.42   MAC-Caps   12.76M   98.02   48.49   79.75   60.31   29.03   47.63   36.07   MAC [16]   12   14.30M   98.54   28.66   53.27   37.27   8.50   18.11   11.57   MAC-Caps   15.02M   97.88   50.90   94.61   66.19   27.72   49.84   35.62   SNMN [13]   9   7.32M   96.18   52.87   67.03   59.12   37.81   47.50   42.11   SNMN-Caps   6.94M   96.66   73.81   78.13   75.91   50.58   51.80   51.18   Table 1. Comparison with baseline systems on CLEVR-Answers   validation set. MAC-Caps and SNMN-Caps are the variants with   the proposed soft masked capsules. For MAC, results are shown   with varying reasoning steps, T (column 2). SNMN uses T=9. See   section 6.1 for details. Numbers are reported in percentages.   and complex VQA tasks. It consists of a train set with 70K   images and approximately 700K question-answer pairs and   a validation set of 15K images with about 150K questionanswer pairs. To allow for an evaluation of visual grounding on this task, we use the framework provided by [23],   and generate new question-answer pairs with the bounding   box labels for the answers as shown in \ufb01gure 3. We use the   same training and validation scenes (images) and generate   10 new QA pairs for each image. To get localization labels   for each answer, we follow a two step process: First, we   obtain the set of object ids which leads to the answer. Each   question in CLEVR dataset is accompanied with a question graph, a stepwise reasoning layout with the information   required to solve the question [23]. We traverse question   graph in a backward direction starting from the last node   and do breadth-\ufb01rst-search (BFS) till we traverse all nodes   which are at breadth level=1. This gives us the list of objects which were used in the \ufb01nal reasoning step and generated an answer. Please note that not every answer will   have grounding labels. For instance, if the question is \u201chow   many blue rubber blocks are behind red cylinder?\u201d and the   answer is 0, then there will be no bounding box labels. Second, to get bounding boxes for this set of objects, we need   scene information. For each question and its corresponding   answer grounding objects, we use the center pixel coordinate information (available with each scene object) to locate   each object in the scene. Then, based on the object size and   shape, we use a few heuristics to get a rough estimate of the   bounding box around each object of interest.   This two-step process results in 901K bounding boxes (for   about 700K QA pairs) for training set and 193K boxes (for   about 150K QA pairs) for validation set i.e.   more than   1M bounding boxes labels. Note that we do not use those   bounding boxes for training, but we will provide them as   well to spur further research. To have a standard train-valtest setup for our experiments, we separate 1K training images with 10K QA pairs for validation of hyper parameters.   The original CLEVR validation set is used as test set and is   never seen during training or validation.   Overlap   IOU   Method   Grd. GT Acc.   P   R   F1   P   R   F1   MAC   Q   57.09 19.75 30.69 24.04   2.88   4.36   3.46   MAC-Caps   55.13 37.77 63.65 47.41   5.39   8.65   6.64   MAC   FA   57.09 22.43 31.35 26.15   3.30   4.48   3.80   MAC-Caps   55.13 41.53 63.00 50.06   6.14   8.85   7.25   MAC   A   57.09   5.61   27.36   9.31   0.92   4.46   1.52   MAC-Caps   55.13 11.95 62.56 20.07   2.32 11.91 3.88   MAC   All   57.09 25.01 30.48 27.47   3.66   4.28   3.95   MAC-Caps   55.13 46.06 62.30 52.96   7.03   8.72   7.79   Table 2. Results on GQA validation set for MAC with T=4. Results are based on grounding of objects referenced in the question (Q), full answer (FA), short answer (A), as well as combined   grounding of question and answer (All). We consistently outperform MAC in all metrics. When evaluating for a certain grounding label type, other detected objects are treated as false positives.   Numbers are reported in percentages.   6. Experiments and Results   Evaluation Metrics.   To evaluate the correct answer localization (grounding), we report precision, recall, and   F1-score based on two criteria: intersection over detection (Overlap), and intersection over union (IOU). Bounding boxes for the object detections are compared with the   ground truth bounding boxes to evaluate how close they   are to the ground truth labels in terms of overlap and IOU.   Predicted regions are considered true positives if the spatial   overlap of predicted bounding box and ground truth bounding box is greater than a certain threshold. The detection   threshold is 0.5. The baseline systems use a multi-hop reasoning process producing attention maps for each reasoning step. Since the reasoning process is divided into suboperations resulting in each operation producing a separate   attention map, it is possible that evidence for the correct   answer was attended at some intermediate step and not necessarily at the last step. To give advantage to the baseline   methods, we consider the best attention map in the reasoning process with respect to F1 score.   6.1. Comparison to baseline method   We \ufb01rst compare the impact of the proposed capsule   module on the two baseline systems, MAC and SNMN, on   the CLEVR-Answers dataset as well as MAC on GQA. See   tables 1, 2 and 3. We use SNMN and MAC as our baselines.   These VQA systems take a question with imagebased holistic features as input and generate answers with   interpretable attention maps. Visual capsules module has   the same number of capsules in both layers i.e., we set   C1 = C2 = C in all our experiments (C = no. of capsules).   CLEVR-Answers:   We \ufb01rst evaluate the performance of   both systems on the CLEVR-Answer benchmark.   We   extract 14x14x1024 dimensional features from the conv4   layer of a ResNet-101 backbone pretrained on ImageNet,   8470   Figure 4. Attention visualizations for MAC on GQA dataset. Column 1 shows results for MAC, column 2 shows results for MAC-Caps   and the same order is followed onwards. Row 1 shows input image, rows 2-5 are attention visualizations for each reasoning step (T=4)   with ground truth (green boxes) and detected grounding objects (red boxes), followed by attention on question words for each step. See   how MAC-Caps attends to the correct boxes. The attention on question words is also improved. Refer to section 6.3 for further details and   discussion. Best viewed in color.   which are referred to as spatial features in [17]. We pass   them through conv layers by MAC and SNMN to generate   14x14x512 dimensional features. We train the models for   25 epochs, using the model with best VQA accuracy for   grounding evaluation. The original MAC baseline reports   their best VQA accuracy with T=12 system [16]. However,   it is recommended to use four to six reasoning steps to get   interpretable attention maps. Thus, we train both, MAC and   MAC-Caps for T=4, 6, and 12 (\u03b1 is set to 1 for MAC).   Table 1 shows the difference of both systems, the MAC   and the MAC-Caps with respect to visual grounding. The   MAC baseline achieves best IOU F1-score of 19.73 with   T=4 whereas MAC-Caps achieves the best IOU F1-score   of 36.07 (19.41% \u2191) using T=6 without hurting VQA accuracy. Note that for MAC-Caps, the best Overlap F1-score   is reached at T=12, which is an indicator that larger attention maps are produced, which are not rated by the Overlap   measurement. Overall, we see a signi\ufb01cant and constant   increase across all evaluated scores of the proposed MACCaps compared to the MAC baseline. For the evaluation of   SNMN and SNMN-Caps, we train both systems with input   features as described above and choose the hyperparameters   as mentioned in [13]. Note that SNMN, opposed to MAC,   uses an expert layout setting, i.e, question graph layouts are   used and learned during training. We get our best results   with 24 capsules for SNMN-Caps as further evaluated in   Method   Validity Plausibility Consistency Distribution   Grounding   MAC   95.14   91.34   84.90   6.44   41.68 (30.34)   MAC-Caps   95.17   91.48   80.90   5.67   45.54 (38.82)   Table 3. Results on GQA validation set for other evaluation metrics. Grounding results are shown for attention maps from the last   (mean) reasoning step(s). Numbers are reported in percentages.   Overlap   IOU   Method   Acc.   P   R   F1   P   R   F1   (1) masked conv.   95.69   59.71   71.13   64.92   42.28   49.24   45.49   (2) hard masking   88.48   63.24   72.76   67.67   43.35   48.29   45.69   (3) shared mask layer   95.76   70.40   76.13   73.15   48.01   49.77   48.87   (4) w/mask (C=8)   95.34   63.12   74.38   68.29   42.08   47.61   44.67   (5) w/mask (C=16)   95.79   73.72   76.27   74.97   50.82   50.02   50.42   (6) w/mask (C=24)   96.66   73.81   78.13   75.91   50.58   51.80   51.18   Table 4. Ablations over the design choices for the proposed architecture on CLEVR-Answers val set with SNMN as base architecture.   Rows 1-3 show the in\ufb02uence of masking (with 16   capsules), where, masked conv.= masking of convolutional layer,   hard masking=one hot masking, shared mask layer=weights for   masking layer are shared among reasoning modules; w/mask=soft   weights are used to mask the capsules (rows 4-6). The lower part   shows the impact of number of capsules: C = no. of capsules.   section 6.2. Using \u03b1 = 7 gives us best grounding results for   SNMN. Overall, we see a similar increase in performance   as for MAC and MAC-Caps, with an IOU F1-score of 42.11   for SNMN and an IOU F1-score of 51.18 for SNMN-Caps.   GQA:   To assess the performance on real world data, we   evaluate our system in context of MAC on the GQA dataset.   8471   GQA provides grounding labels for question, single word   answer and sentence-based answer. We compare both setups, MAC and MAC-Caps, using the proposed grounding   score based on overlap and IOU (see table 2), as well as   the metrics proposed by Hudson et al. [17] where, for each   question-image pair, the grounding score is the sum of attention over ground truth region(s) r, averaged over all data   samples (see table 3). We use T=4 for both the MAC baseline and MAC-Caps, showing the best performance on this   dataset. We report results on the GQA validation set.   We again observe that MAC-Caps consistently outperforms the MAC baseline on all metrics in table 2.   We   notice signi\ufb01cant improvement (23.91% \u2191on F1-score) in   terms of Overlap with 3.45% improvement in F1-score in   terms of IOU for full answer grounding.   Note that the   scores, especially in context of IOU, are much lower on this   dataset compared to the CLEVR benchmark, which can be   attributed to the complexity of the natural images in this   context. Regarding the comparison to the metrics proposed   by [17] shown in table 2, we see the increase with respect to   the grounding abilities of the MAC-Caps compared to the   MAC baseline as well as compared to the reported spatial   feature baseline of 43% in [17]. Overall, both evaluations   show that the proposed capsule module allows for a better   learning of visual grounding from weak VQA supervision   even in a challenging real world setting given with GQA.   6.2. Ablations and Analysis   Convolutional layers vs. Capsules. To investigate how   much capsules contribute compared to convolutional layers,   we mask convolutional features instead of using capsules.   We add a convolutional layer on top of image features resulting in C \u00d7 (4 \u00d7 4 + 1) features to keep same number   of channels as in capsules (here, C=16). Similar to soft   masking in capsules, these convolutional features are also   masked before performing the reasoning operation. We \ufb01nd   that masked convolutional features perform 3.38% better   than the SNMN baseline in terms of IOU, but capsules still   outperform them with a large margin (45.49% vs. 50.42%)   for convolutional masking (see table 4 (1)=masked convolutions and (5)=baseline). This shows that query-based   masking of capsules performs superior when compared to   masked convolutional features .   Hard Masking vs. Soft Masking. There are two possible ways to mask capsules based on the query input. The   \ufb01rst is masking them using softmax scores which we call   soft masking; the second is keeping the capsule with highest probability and mask out the rest of the capsules (using   one hot vector as mask), which we call hard masking. We   \ufb01nd that using soft masking gives best results. When using   hard masking of capsules (C=16), it hurts VQA accuracy   (88.07%), although giving comparable results on grounding   metrics (see table 4 (2)=hard masking and (5)=baseline).   Therefore, we use soft masking for all our experiments.   Shared masking vs. separate masking. For SNMN, our   \ufb01nal architecture uses a separate masking layer for each reasoning module. We also experiment with using a single   masking layer with shared weights for all reasoning modules. While shared masking layer yields good results, we   get the best grounding scores using separate masking layer   (see table 4 (3)=shared mask layer and (5)=baseline).   Performance analysis w.r.t. no. of capsules. We \ufb01nally   analyze the system with varying number of capsules. We   train the SNMN-Caps model with C=8, 16, and 24. All   of them perform superior to the original SNMN in terms   of grounding while achieving comparable VQA accuracy.   With 24 capsules, SNMN-Caps outperforms the baseline   SNMN on both VQA and grounding task (table 4 (4-6)).   6.3. Qualitative Results   Figure 4 shows qualitative analysis on GQA dataset with   MAC-Caps. For samples, where both systems give the correct answer (columns 1-4), we observe that MAC often attends to corners in the image during the intermediate reasoning steps and attends to the region(s) of interest only   at the \ufb01nal stage. For instance, on \ufb01rst sample (columns   1-2), MAC never attended to the correct object yet somehow produces the correct answer. MAC-Caps, on the other   hand, pays attention correctly to relevant regions on earlier stages even for the case where the \ufb01nal answer is incorrect (columns 7-8). Additionally, MAC-Caps produces   more precise attention than the baseline system. Attention   on question words also seems to be improved for MACCaps (last row). Columns 5-6 show the case where, better   grounding leads the model to predict the answer correctly.   7. Conclusion   This work proposes a novel approach for the weakly supervised grounding of VQA tasks. The proposed capsulebased module can be integrated into current VQA systems.   To allow a combination of capsules with VQA based text   processing, we proposed a soft masking function that further improves weakly supervised answer grounding. We   show by evaluating the system on two challenging datasets,   GQA and CLEVR-Answers, the impact of the proposed   idea to learn a weakly supervised grounding in VQA tasks.   Acknowledgements. We thank reviewers for their helpful feedback.   We also thank Hui Wu for the helpful discussions in the initial phase of this   project. Aisha Urooj is supported by the ARO grant W911NF-19-1-0356   and Hilde Kuehne is supported by IARPA via DOI/IBC contract number   D17PC00341 for this work. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding   any copyright annotation thereon. Disclaimer: The views and conclusions   contained herein are those of the authors and should not be interpreted as   necessarily representing the of\ufb01cial policies or endorsements, either expressed or implied, of ARO, IARPA, DOI/IBC, or the U.S. Government.   8472", "conf": "CVPR", "year": "2021", "index": 103}, {"title": "The Thirty-Third AAAI Conference on Arti\ufb01cial Intelligence (AAAI-19)   Read, Watch, and Move: Reinforcement Learning for   Temporally Grounding Natural Language Descriptions in Videos   Dongliang He,\u2020 Xiang Zhao,\u2020 Jizhou Huang,\u2021,\u2020 Fu Li,\u2020 Xiao Liu,\u2020 Shilei Wen\u2020   \u2020Baidu Inc., Beijing, China   \u2021Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China   {hedongliang01, zhaoxiang05, huangjizhou01, lifu, liuxiao12, wenshilei}@baidu.com", "abstract": "The task of video grounding, which temporally localizes a   natural language description in a video, plays an important   role in understanding videos. Existing studies have adopted   strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a presegmented video, which inevitably suffer from exhaustively   enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by   learning an agent which regulates the temporal grounding   boundaries progressively based on its policy. Speci\ufb01cally, we   propose a reinforcement learning based framework improved   by multi-task learning and it shows steady performance gains   by considering additional supervised boundary information   during training. Our proposed framework achieves state-ofthe-art performance on ActivityNet\u201918 DenseCaption dataset   (Krishna et al. 2017) and Charades-STA dataset (Sigurdsson   et al. 2016; Gao et al. 2017) while observing only 10 or less   clips per video.   1", "content": "The task of grounding natural language in image/video,   which aims to localize a short phrase (Rohrbach et al. 2016;   Endo et al. 2017; Tellex and Roy 2009; Regneri et al.   2013) or a referring expression (Bojanowski et al. 2015;   Gao, Sun, and Nevatia 2016) in a given image/video, is essential for understanding the way human perceive images   and scenes. In the past few years, studies on detecting a   particular object in image (Pirinen and Sminchisescu 2018)   or activity in video (Yeung et al. 2016; Singh et al. 2016;   Zhao et al. 2017; Shou et al. 2017) have been extensively   exploited. However, they are restricted to a prede\ufb01ned close   set of object/activity space. In this paper, we take a further step by focusing on the problem of temporally grounding an open-ended natural language description in a long,   untrimmed video.   Speci\ufb01cally, given an untrimmed video and a natural language description, our task is to determine the start and end   time stamps of a clip in this video that corresponds to the   given description. Exploration on natural language enables   us to deal with not only an open set of objects/activities, but   also the correlations and interactions between the involved   Copyright c\u20dd2019, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   Video   Read   Watch   Agent   Description:   The parent watches the boy play drums   Move   Reward   Figure 1: Illustration of the proposed formulation of read,   watch, and move with reinforcement learning. At each time   step, the agent makes an observation, takes an action, and receives a reward evaluating that action. The policy is updated   according to this reward.   entities. This is a challenging task as it requires both vision   and language understanding, but it has important applications in video understanding tasks such as video retrieval   and text-oriented video highlight detection. Despite the fact   that both action localization and video grounding tasks share   the same goal of extracting a well-matched video clip w.r.t. a   given query, the methods for action localization are not readily applicable to video grounding. The main reason is that it   requires to map natural language descriptions to a close set   of labels before applying these methods, which inevitably   causes information loss. For example, if the sentence \u201cThe   parent watches the boy play drums\u201d is mapped to an action   label \u201cplay drums\u201d, action detectors can hardly tell the start   point of the presence of the parent.   Although there is a growing interest in grounding natural language in videos, e.g., (Gao et al. 2017; Hendricks et   al. 2017; Liu et al. 2018), these methods have a limitation   that they can be applied only when all candidates are exhaustively enumerated, as they need to slide over the entire   video or rank all possible clip-sentence pairs to be successful in generating the \ufb01nal grounding result. Moreover, ranking strategy (Hendricks et al. 2017) is specially designed for   pre-segmented videos and is not applicable to general cases.   To effectively detect the temporal boundaries of a given de8393   scription from a video, especially from a long, untrimmed   video, it is important to make judgments only by several   glimpses. To the best of our knowledge, the problem of   grounding natural language in videos without sliding over   the entire video has not been well addressed.   In this paper, we propose an end-to-end reinforcement   learning (RL) based framework for grounding natural language descriptions in videos. As shown in Figure 1, the   agent iteratively reads the description and watches the entire video as well as the temporary grounding clip and its   boundaries, and then it determines where to move the temporal grounding boundaries based on the policy. At each   time step, the agent makes an observation that details the   currently selected video clip, and takes an action according   to its policy. The environment is updated accordingly and   offers the agent a reward that represents how well the chosen action performs. Our key intuition is that given a description and the current grounding start and end points, human can make hypotheses on how these points should be   moved after observing a clip, and then iteratively adjust the   grounding boundaries step by step towards \ufb01nding the best   matching clip. Therefore, we naturally formulate this task as   learning an agent for sequential decision making to regulate   the temporal boundaries progressively based on the learned   policy. In addition, we introduce two distinctive characteristics into the RL framework. First, we improve the framework with multi-task learning and show steady performance   gains by considering additional supervised boundary information during training. Second, we introduce a location feature, which enables the agent to explicitly access where the   current grounding boundaries are.   Speci\ufb01cally, we design an observation network that takes   visual and textual information as well as current temporal   boundaries into consideration. Given the observation, the decision making policy is modeled by a policy network that   outputs the probability distribution over all possible actions,   and then the agent adjusts the grounding boundaries by taking an action according to the policy. The policy network   is implemented using a recurrent neural network (RNN).   In addition, in order to achieve better feature learning, we   propose to leverage the ground truth for supervised temporal IoU (tIoU) regression as well as boundary regression at each step. Our RL model can be trained end-toend for natural language description grounding within several steps of glimpses (observations) and temporal boundaries adjustments (actions). We evaluate our model on two   well-known datasets, i.e., ActivityNet DenseCaption (Krishna et al. 2017) and Charades-STA (Sigurdsson et al. 2016;   Gao et al. 2017). Experimental results show that our method   achieves state-of-the-art performance on these datasets and   validate the superiority of our method.   Here we summarize our main contributions:   \u2022 We propose a reinforcement learning based framework   for the task of grounding natural language descriptions in   videos.   \u2022 Supervised learning is combined with reinforcement   learning in a multi-task learning framework, which helps   the agent obtain more accurate information about the environment and better explore the state space to encounter   more reward, thus it can act more effectively towards task   accomplishment.   \u2022 Experiments conducted on two well-known datasets   demonstrate that the proposed method signi\ufb01cantly outperforms state-of-the-art method by a substantial margin,   which veri\ufb01es the effectiveness of our method.   2   Related Work   We \ufb01rst introduce the related work of grounding natural language in visual content. Then, we introduce the related work   on applying reinforcement learning for vision tasks.   The problem of grounding natural language descriptions   in videos has attracted an increasing interest more recently.   Previous studies typically treat it as a regression or ranking problem by adopting strategies of sliding window over   the entire video (Gao et al. 2017; Liu et al. 2018) or exhaustively ranking all possible clip-sentence pairs in a presegmented video (Hendricks et al. 2017). However, it is dif\ufb01cult to attain a reasonable compromise between accuracy   and computational ef\ufb01ciency due to they inevitably suffer   from exhaustively enumerated candidates and the false positive problem. To alleviate this problem, we formulate the   task as a sequential decision making problem, rather than   treat it as a regression or ranking problem. To this end, we   propose an effective method to address this problem with a   reinforcement learning based framework.   Our work is also related to the work on understanding   visual content. Here we brie\ufb02y review the closely related   work including image grounding, image/video captioning,   video retrieval, and temporal action localization in videos.   The task of image grounding, which aims to localize a   short phrase or an expression in an image, is a well-studied   \ufb01eld, e.g., (Plummer et al. 2015; Rohrbach et al. 2016;   Xiao, Sigal, and Jae Lee 2017; Endo et al. 2017). However, they have a different focus from ours. The task of image/video captioning, which aims to generate a description   in text from visual data, has been well studied in computer   vision, e.g., (Xu et al. 2015a; Pan et al. 2016; Gan et al. 2017;   Liang et al. 2017). The task of visual grounding can be regarded as the conjugate task of visual description generation.   Sentence based video retrieval methods (Xu et al. 2015b; Lin   et al. 2014; Bojanowski et al. 2015) use skip-thought (Kiros   et al. 2015) or bidirectional LSTM to encode text queries,   and incorporate video-language embedding to \ufb01nd the video   that is most relevant to the query. They mainly focus on retrieving an entire video for a given query, rather than \ufb01nding   a temporal segment that matches a given description from   a video. The task of temporal action localization is de\ufb01ned   as retrieving video segments relevant to a given action from   a prede\ufb01ned close set of action labels (Singh et al. 2016;   Lin, Zhao, and Shou 2017; Zhao et al. 2017; Yeung et al.   2016). There is also some work on retrieving temporal segments relevant to a sentence within a video in constrained   settings. For example, Tellex and Roy (2009) studied the   problem of retrieving video clips from home surveillance   videos using text queries from a \ufb01xed set of spatial prepositions. Bojanowski et al. (2015) proposed to align video seg8394   The parent watches    the boy play drums   GRU   FC   FC   Softmax   Policy: \ud835\udf0b\ud835\udc4e#   $  | \ud835\udc60($)   Take action \ud835\udc4e($),  which is sampled according to policy \ud835\udf0b\ud835\udc4e#   $  | \ud835\udc60($) , to adjust temporal boundaries   Skip-Thought   C3D   FC   FC   FC   FC   FC   FC   FC   Description Feature   Global Feature   Local Feature   Location Feature   Observation Network: \ud835\udc60($)   State-Value   tIoU   Regression   STOP   Action Space   Actor-Critic   Multi-Task   Environment   Location    Regression   Figure 2: Architecture of the RL-based framework. The action space consists of 7 different ways to adjust the temporal boundaries. State vector s(t), which fuses description embedding feature, global video feature, local video feature, and current location   embedding feature, is fed into the actor-critic (Sutton and Barto 2011) to learn both a policy and a state-value. Temporal IoU   between current video clip and the ground truth is regressed with supervised learning, so are the temporal localization boundaries.   ments with a set of sentences which is temporally ordered.   Our work is different in that we temporally ground openended, arbitrary natural language descriptions in videos.   The use of reinforcement learning (Williams 1992) in our   task is inspired by the recent remarkable success of applying   it in various vision tasks. The goal of reinforcement learning is to learn an agent to evaluate certain actions under   particular states or learn an action policy with respect to   current states. It is effective to optimize the sequential decision making problems. Recently, it has been successfully   applied to learn task-speci\ufb01c policies in various computer   vision problems. For example, spatial attention policies are   learned for image recognition in (Mnih et al. 2014) and (Liu   et al. 2017). RL framework is also leveraged for object detection in (Pirinen and Sminchisescu 2018). Li et al. (2018)   proposed an aesthetic aware reinforcement learning framework to learn the policy for image cropping and achieved   promising results. As for vision tasks in video, (Yeung et al.   2016) proposed to use RL for action localization in videos   and achieved a good trade-off between localization accuracy   and computation cost. Recently, video object segmentation   with deep reinforcement cutting-agent learning is explored   in (Han et al. 2018), and hierarchical reinforcement learning   (Sutton and Barto 2011) is incorporated for video description in (Wang et al. 2018). Our video grounding task can be   formulated as a sequential decision making problem, thus   it \ufb01ts naturally into the reinforcement learning paradigm.   While conceptually similar, our model is novel in that it successfully combines supervised learning with reinforcement   learning in a multi-task learning framework, which helps the   agent obtain more accurate information about the environment and better explore the state space to encounter more   reward, thus it can act more effectively towards task accomplishment.   3   Methodology   Given the insight that human usually localize a description in a video by following the iterative progress of observing a clip, making a hypothesis on how the boundaries   should be shifted, and verifying it, we formulate the task of   temporally grounding descriptions in videos as a sequential   decision-making problem. Therefore, it naturally \ufb01ts into the   RL framework. Figure 2 shows the architecture of the proposed model. The temporal grounding location is initially   set to [l(0)   s , l(0)   e ]. At each time step, the observation network   outputs the current state of the environment for the actorcritic module to generate an action policy. The policy de\ufb01nes   the probabilistic distribution of all the actions in the action   space and can be easily implemented by softmax operation.   The agent samples an action according to the policy and adjusts the temporal boundaries iteratively until encountering   a STOP action or reaching the maximum number of steps   (denoted by Tmax).   3.1   Environment and Action Space   In this task, the action space A consists of 7 prede\ufb01ned actions, namely, moving start/end point ahead by \u03b4 , moving   start/end point backward by \u03b4, shifting both start and end   point backward/forward by \u03b4, and a STOP action. In this paper, \u03b4 is set to N/10, where N is the video length. In fact, the   two shifting actions can be accomplished by two successive   moving actions, and we de\ufb01ne them mainly for the purpose   of enabling a faster temporal boundary adjustment.   Once an action is executed, the environment changes accordingly. Environment here is referred to as the combination of the video, the description, and the current temporal grounding boundaries. We use skip-thought (Kiros et al.   2015) to generate the sentence embedding E and feed the   8395   entire video into C3D (Tran et al. 2015) to obtain the global   feature vector VG. At each step, these features are retained.   To represent the status of the current temporal grounding   boundaries for the tth step of decision, we propose to explicitly involve the normalized start point and end point   L(t\u22121) = [l(t\u22121)   s   , l(t\u22121)   e   ], t = 1, 2, ..., Tmax as supplementary to the local C3D feature V (t\u22121)   L   of the current video   clip. In the observation network, sentence embedding, global   video feature, local video feature, and normalized temporal   boundaries are encoded using fully-connected (FC) layers   and their outputs are concatenated and fused by a FC layer   to generate the state vector s(t), which is calculated as:   s(t) = \u03a6(E, VG, V (t\u22121)   L   , L(t\u22121)).   (1)   3.2   Advantage Actor-Critic RL Algorithm   It has been proven that actor-critic algorithm (Sutton and   Barto 2011) is able to reduce the variance of the policy gradient. As shown in Figure 2, the sequential decision making process is modeled by an actor-critic module, which employs GRU (Cho et al. 2014) and FC layers to output the policy \u03c0(a(t)   i |s(t), \u03b8\u03c0) and a state-value v(s(t)|\u03b8v). The policy   determines the probabilistic distribution of actions and the   state-value serves as estimation of the value of the current   state (i.e., critic). \u03b8\u03c0 denotes parameters of the observation   network and the policy branch, \u03b8v denotes parameters of the   critic branch. The input of the GRU at each time step is the   observed state vector s(t). The hidden state of the GRU unit   is used for policy generation and state-value estimation.   Reward   Our goal is to learn a policy that can correctly   grounding descriptions in videos, so the reward for the reinforcement learning algorithm should encourage the agent to   \ufb01nd a better matching clip step by step. Intuitively, allowing   the agent to search within more steps could yield a more accurate result but it will introduce more computational cost.   To alleviate this issue, we reward the agent for taking better   actions: the agent will get a positive reward 1 if the temporal   IoU (tIoU) between L(t) = [l(t)   s , l(t)   e ] and the ground-truth   grounding location G = [gs, ge] gets higher after the tth step   of action. On the contrary, we punish the agent for taking   worse actions: the agent will get no reward if the tIoU gets   lower. To balance the number of actions and the accuracy,   we further punish the number of steps by a negative reward   of \u2212\u03c6\u2217t, where \u03c6 is the positive penalty factor ranging from   0 to 1. Moreover, there are chances that the start point gets   higher than the end point after taking several actions, which   is an undesirable state. To avoid such situations, the reward   function should punish the agent if the tIoU is a negative   value. To sum up, the reward of the tth step is formulated   as:   rt =   \uf8f1   \uf8f2   \uf8f3   1 \u2212\u03c6 \u2217t,   tIoU (t) > tIoU (t\u22121) \u22650   \u2212\u03c6 \u2217t,   0 \u2264tIoU (t) \u2264tIoU (t\u22121)   \u22121 \u2212\u03c6 \u2217t,   otherwise   ,   (2)   where tIoU is calculated as:   tIoU (t) = min(ge, l(t)   e ) \u2212max(gs, l(t)   s )   max(ge, l(t)   e ) \u2212min(gs, l(t)   s )   .   (3)   The task is modeled as a sequential decision making problem, whose goal is to \ufb01nally hit the ground truth, so the reward of following up steps should be traced back to the current step. Therefore, our \ufb01nal accumulated reward function   is designed as follows:   Rt =   \u001art + \u03b3 \u2217v(s(t)|\u03b8v),   t = Tmax   rt + \u03b3 \u2217Rt+1,   t = 1, 2, ..., Tmax \u22121 , (4)   where \u03b3 is a constant discount factor.   Objective   Instead of directly maximizing the expected reward, we introduce the advantage function (Sutton and Barto   2011), and our objective becomes maximizing the expected   advantage Ja(\u03b8):   Ja(\u03b8\u03c0) =   X   a\u2208A   {\u03c0(a|s, \u03b8\u03c0)(R \u2212v(s|\u03b8v))} .   (5)   The gradient of Ja is:   \u2207\u03b8\u03c0Ja(\u03b8\u03c0) =   X   a\u2208A   \u03c0(a|s, \u03b8\u03c0) (\u2207\u03b8\u03c0log \u03c0(a|s, \u03b8\u03c0)) (R \u2212v(s|\u03b8v)).   (6)   It is dif\ufb01cult to directly optimize this problem due to the   high dimension of the action sequence space. In RL, Mente   Carlo sampling is adopted and the policy gradient is approximated as:   \u2207\u03b8\u03c0Ja(\u03b8\u03c0) \u2248   X   t   \u2207\u03b8\u03c0log \u03c0(a(t)   i   |s(t), \u03b8\u03c0)(Rt \u2212v(s(t)|\u03b8v)). (7)   Therefore, the maximizing problem can be optimized by   minimizing the following loss function L\u2032   A using stochastic   gradient descent (SGD):   L\u2032   A(\u03b8\u03c0) = \u2212   X   t   \u0010   log \u03c0(a(t)   i |s(t), \u03b8\u03c0)   \u0011   (Rt \u2212v(s(t)|\u03b8v)).   (8)   We also follow the practice of introducing entropy of the   policy output H(\u03c0(a(t)   i |s(t), \u03b8\u03c0)) into the objective, since   maximizing the entropy can increase the diversity of actions (Li et al. 2018). Therefore, the overall loss of the actor   branch becomes:   LA(\u03b8\u03c0) = L\u2032   A(\u03b8\u03c0) \u2212   X   t   \u03bb0 \u2217H(\u03c0(a(t)   i |s(t), \u03b8\u03c0)),   (9)   where \u03bb0 is a constant scaling factor and it is set to 0.1 in   our experiments.   In the context of actor-critic algorithm, the objective of the   estimated state-value v(s(t)|\u03b8v) by critic branch is to minimize the mean squared error (MSE). Therefore, we introduce the MSE loss for the critic branch de\ufb01ned as follows:   LC(\u03b8v) =   X   t   \u0010   v(s(t)|\u03b8v) \u2212Rt   \u00112   .   (10)   Finally, the overall loss of the RL algorithm is a combination of the two losses:   Loss1 = LA(\u03b8\u03c0) + \u03bb1 \u2217LC(\u03b8v).   (11)   8396   3.3   Combination of Supervised Learning and RL   To learn more representative state vectors, we propose to   combine reinforcement learning and supervised learning   into a multi-task learning framework. The overall loss for   supervised learning is a combination of the tIoU regression   loss and the location regression loss:   Loss2 = LosstIoU + \u03bb2 \u2217LossLoc.   (12)   Here we de\ufb01ne the tIoU regression loss as the L1 distance   between the predicted tIoU and the ground-truth tIoU, i.e.,   LosstIoU =   X   t   |tIoU (t\u22121) \u2212P (t)   tIoU|,   (13)   where P (t)   IoU is the predicted tIoU at the tth step. Similarly,   the L1 location regression loss can be calculated as:   LossLoc =   X   t   y(t) \u0010   |gs \u2212P (t)   s | + |ge \u2212P (t)   e |   \u0011   /2, (14)   where P (t)   s   and P (t)   e   are the predicted start and end, respectively. y(t) is an indicator which equals 1 when tIoU (t\u22121) >   0.4 and 0 otherwise.   The proposed model can be trained end-to-end by minimizing the overall multi-task loss Loss1 + \u03bb3 \u2217Loss2. In   our experiments, \u03bb1, \u03bb2, and \u03bb3 are empirically set to 1. The   supervised loss enables the algorithm to learn to predict how   well the current state matches the ground truth and where the   temporal boundaries should be regressed while the RL loss   enables it to learn a policy for pushing the temporal boundaries to approach the ground truth step by step. In the training phase, parameters are updated with both supervised gradient and policy gradient. Therefore, the generalization of   the trained network can be improved by sharing the representations.   4   Experiments   4.1   Datasets and Evaluation Metric   The models are trained and evaluated on two well-known   datasets, i.e., ActivityNet DenseCaption (Krishna et al.   2017) and Charades-STA (Sigurdsson et al. 2016; Gao et   al. 2017). The DenseCaption dataset contains 37,421 and   17,505 natural language descriptions with their corresponding ground-truth temporal boundaries for training and test,   respectively. The Charades-STA dataset is originally collected for video classi\ufb01cation and video captioning. Gao et   al. (2017) processed the dataset to make it suitable for video   grounding task. In this dataset, there are 13,898 descriptionclip pairs in the training set, and 4,233 description-clip pairs   in the test set.   Following previous work (Yeung et al. 2016; Gao et al.   2017), we also evaluate the models according to the grounding accuracy Acc@0.5, which indicates whether the tIoU between the grounding result generated by a model and the   ground truth is higher than 0.5.   Method   DenseCaption   Charades-STA   Acc@0.5   Acc@0.5   RANDOM   18.1%   14.0%   FIXED   25.4%   23.1%   TALL (Gao et al.)   26.9%   25.8%   MCN (Hendricks et al.)   27.4%   30.8%   RL-Loc (Yeung et al.)   34.7%   32.5%   Ours   36.9%   36.7%   Table 1: The evaluation results of different methods.   4.2   Implementation Details   In the training phase, Adam with initial learning rate   of 1e-3 is used for optimization. In the test phase, an   action is greedily sampled at each time step: a(t)   =   arg maxa(t)   i   \u03c0(a(t)   i |s(t), \u03b8). The initial temporal grounding   location is set to the central half of the entire video, namely,   L(0) = [N/4, 3N/4], where N is the total length of the   video. Because no optical \ufb02ow frames are provided in the   DenseCaption dataset, we use only the RGB frames to train   our models. However, Charades-STA does not have such   limitation, thus all the models trained on it use both optical   \ufb02ow and RGB modalities.   The parameters used in the experiments are set as follows. Tmax is set to 10. The natural language description   is embedded into a 2400-dimension vector via skip-thought   (Kiros et al. 2015). In the observation network, the output   size of the FC layer for encoding the description feature is   set to 1024. Both global feature and local feature are encoded into 512-dimension vectors by FC layers. The normalized temporal boundary L(t\u22121) is embedded into a 128dimension vector with a FC layer. The dimension of the state   vector s(t) at the tth step is 1024. The GRU cell which is   used to model the sequential decision process gets hidden   size of 1024. \u03c6 is set to 0.001. \u03b3 is set to 0.3 on DenseCaption and 0.4 on Charades-STA.   4.3   Comparison with Baselines   Action localization methods can be adapted to this task by   feeding video and sentence features and then making 0 vs. 1   action localization. In fact, both Gao et al. (2017) (denoted   by TALL) and Hendricks et al. (2017) (denoted by MCN)   have adopted this idea. We use both methods as baselines   and implement them using the source codes released by their   authors. Yeung et al. (2016) (denoted by RL-Loc) uses RL   for action localization, we also adapt it for video grounding   and use it as a baseline. In addition, to better evaluate our   proposed model, we also use two hand-crafted approaches as   baselines. First, a random strategy (denoted by RANDOM)   is employed by randomly selecting a video clip with half of   the entire video length for arbitrary descriptions. Second, a   \ufb01xed strategy (denoted by FIXED) is employed by selecting   the central part with a \ufb01xed length (N/2 for DenseCaption   and N/3 for Charades-STA) for any descriptions.   Table 1 shows the evaluation results. We observe that:   1) our method achieves the best performance in comparison with all baselines on both datasets; 2) our method sig8397   \u03c6   DenseCaption   Charades-STA   # Steps   Acc@0.5   # Steps   Acc@0.5   1   1.00   25.4%   1.0   17.9%   0.1   5.30   36.1%   5.17   36.3%   0.01   9.96   36.7%   9.97   36.7%   0.001   9.99   36.9%   9.99   36.7%   Table 2: Acc@0.5 and average number of glimpses evaluated   with different \u03c6 settings.   ni\ufb01cantly outperforms previous state-of-the-art methods by   substantial margins: the accuracy is improved by 10.0%   and 10.9% over TALL; 9.5% and 5.9% over MCN; 2.2%   and 4.2% over RL-Loc on DenseCaption and CharadesSTA, respectively. The evaluation results demonstrate that   our model achieves a signi\ufb01cant improvement over all previous state-of-the-art models. This suggests that it is a more   natural formulation of the task by adopting the read, watch,   and move strategy with sequential decision making. As for   the inference computation cost, both our model and RL-Loc   only need to observe up to 10 clips per video. MCN splits   a video into 6 segments and needs to enumerate all 21 possible video clips. TALL adopts sliding windows of 128 and   256 frames with an overlap ratio of 0.8 and needs to examine   45.8 candidates per video on average.   4.4   Ablation Study   In this section, we perform a series of ablation studies to   understand the importance of different factors, reward, and   components. To this end, we train multiple variations of   our model with different settings or con\ufb01gurations. In the   following experiments, we use RGB feature of the videos   on DenseCaption while both RGB and \ufb02ow features on   Charades-STA. Speci\ufb01cally, when two modalities are used,   both the global feature VG and the local feature V (t\u22121)   L   are   the concatenation of RGB and optical \ufb02ow features.   Impact of Penalty Factors   In the reward function, we   have proposed to punish the agent for taking too many steps   to strike a desirable trade-off between grounding accuracy   and computation cost. In this experiment, we examine the   impact of the parameter \u03c6 while keeping \u03b3 \ufb01xed. Table 2   shows the results of Acc@0.5 and the average number of   steps the agent takes. From the results, we can clearly observe that the average number of steps gets higher when \u03c6   gets lower on both datasets, and the grounding accuracy increases when \u03c6 decreases. Specially, when the penalty factor   was set to 1, the average number of steps is 1, which indicates that the agent takes the STOP action at the \ufb01rst step.   On the contrary, if \u03c6 was set to 0.001, the average number   of steps is 9.99, which indicates that the agent has fully utilized the possible exploring steps (which is up-bounded by   Tmax = 10) to adjust the temporal grounding boundaries,   thus the best accuracy is achieved. This shows that \u03c6 can be   utilized to strike a desirable trade-off between the accuracy   and computation cost. For example, setting \u03c6 to 0.1 could be   a balanced choice if we would like to achieve better grounding accuracy in fewer steps.   35.9%   36.3%   36.4%   36.7%   35.5%   35.4%   35.6%   34.2%   32.6%   32.2%   32.4%   35.8%   35.3%   35.6%   36.1%   36.9%   35.8%   36.3%   35.5%   34.0%   33.5%   33.1%   0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1   Acc@0.5   Discounting Factor   Charades-STA   DenseCaption   Figure 3: Results of the methods with different discount factor \u03b3 settings on Charades-STA and DenseCaption.   Impact of Accumulated Reward   In order to build connections inside the action sequence, we propose to trace reward of the current step back to the previous steps, so that   the reward can be accumulated. In this experiment, we \ufb01x \u03c6   to 0.001 and evaluate the impact of the discount factor \u03b3 in   our \ufb01nal reward Rt. We range \u03b3 from 0 to 1 with a step size   of 0.1. Then, we train multiple variations of our model with   different discounting factors, and examine their impact. Figure 3 shows the experimental results. From the results, it is   observed that the grounding accuracy obtained with either 0   or 1 of \u03b3 is lower than most of those obtained with the values   of \u03b3 in between 0 and 1. Intuitively, when \u03b3 is set to 0, it indicates that the reward of the current action has no impact on   its follow-up actions. This is not a plausible reward strategy   for the entire sequential decision making process because   the dependency between successive actions is not modeled.   Conversely, when \u03b3 is set to 1, it indicates that the reward of   the current action largely depends on its successive actions.   This is also not a plausible reward strategy because even if   the tth action is an unsuccessful attempt, the accumulated   reward Rt can be still positive if there is a chance that any   of its successive actions get a positive reward. In this experiment, we empirically observed that when \u03b3 was set to   0.3 on Charades-STA and 0.4 on DenseCaption, the model   achieves the best grounding accuracy on each dataset.   Importance of Different Components   Here we investigate the effectiveness of each component in our proposed   model by conducting an ablation study. In the experiments,   \u03c6 was set to 0.001 to enable the agent to fully explore the   grounding result within the maximum steps allowed, while   \u03b3 was set to 0.3 on Charades-STA and 0.4 on DenseCaption.   We \ufb01rst study the effectiveness of the introduced location   feature. To this end, we train a new model by removing this   feature from the observation network, such that only VG, E   and V (t\u22121)   L   are encoded at the tth step. Then we investigate   the importance of the proposed multi-task learning, which   helps the agent to better capture the representation of the current state of the environment. To accomplish this, we train   multiple variations of our model by disabling the two modules (tIoU regression and location regression) one after the   other while keeping other settings exactly as what they are.   8398   Description: A person ate a sandwich while holding a glass of water   27.37s   9.52s   2.38s   5.95s   20.23s   0s   0s   23.8s   0.90s   16.66s   13.1s   13.30s   Ground Truth   Initial Location   0s   13.1s   STOP   Figure 4: Illustration of the action sequence executed when our model grounds the given description in a video.   We conduct experiments on both datasets and the results are   detailed in Table 3. From the results, we make the following   observations.   First, the performance signi\ufb01cantly drops on both datasets   after disabling all three components, which particularly results in a dramatic decrease of Acc@0.5 by 23.7% (from   36.7% to 13.0%) on Charades-STA. The leave-one-out analysis also reveals that the performance drops by a large margin after removing one or more components. This demonstrates that all the three components can signi\ufb01cantly contribute to the grounding effectiveness.   Second, we compare the models with or without the location feature used as a component of the environment. The   results reveal that the grounding accuracy drops 3.4% and   1.9% in terms of Acc@0.5 on DenseCaption and CharadesSTA, respectively. This veri\ufb01es that explicit location feature   is important to help model the environment for the observation network.   Third, the results show that Acc@0.5 decreases from   36.9% to 35.9% on DenseCaption and from 36.7% to 35.1%   on Charades-STA after disabling the component of location   regression. Moreover, Acc@0.5 further reduces to 34.5% on   DenseCaption and 34.9% on Charades-STA after disabling   both the two components of tIoU regression and location regression. This demonstrates that the overall grounding performance will degrade considerably without the considerCon\ufb01gurations   Acc@0.5   Loc   tIoU-R   Loc-R   DenseCaption   Charades-STA   \u00d7   \u00d7   \u00d7   31.5%   13.0%   \u00d7   \u221a   \u221a   33.5%   34.8%   \u221a   \u221a   \u00d7   35.9%   35.1%   \u221a   \u00d7   \u00d7   34.5%   34.9%   \u221a   \u221a   \u221a   36.9%   36.7%   Table 3: Results with different network con\ufb01gurations. Loc   refers to \u201cExplicit Location Feature\u201d, tIoU-R refers to \u201ctIoU   Regression\u201d, and Loc-R refers to \u201cLocation Regression\u201d.   ation of supervised learning. This also con\ufb01rms the importance of modeling the environment by combining supervised   learning with reinforcement learning in a multi-task learning framework, which can help the agent obtain more accurate information about the environment and better explore   the state space to encounter more reward.   Multi-Modality Experiment   We investigate the effect of   combining both RGB and \ufb02ow features on Charades-STA as   a case study. The experimental results show that the grounding accuracies achieved by the models trained with RGB   feature and \ufb02ow feature are 36.2% and 35.8%, respectively.   However, the grounding accuracy is slightly increased to   36.7% when the two modalities are fused by feature concatenation. The performance could be further improved with   better fusion methods, which we leave as a future work.   4.5   Qualitative Results   To visualize how the agent adjusts grounding boundaries   step by step, we illustrate an action sequence executed by   our agent in the procedure of grounding a natural language   description in a video (see Figure 4). The agent takes \ufb01ve   steps from the initial location and \ufb01nally grounds the description correctly in the video.   5   Conclusion   In this paper, we study the problem of temporally grounding   natural language descriptions in videos. This task can be formulated as a sequential decision making process and it \ufb01ts   naturally into the reinforcement learning paradigm. Therefore, we model this task as controlling an agent to read the   description, to watch the video as well as the current localization, and then to move the temporal grounding boundaries iteratively to \ufb01nd the best matching clip. Combined   with supervised learning in a multi-task learning framework,   our model can be easily trained end-to-end. Experiments   demonstrate that our method achieves a new state-of-the-art   performance on two well-known datasets.   8399", "conf": "AAAI", "year": "2019", "index": 320}, {"title": "Weakly-Supervised Spatio-Temporal Video   Grounding with Variational Cross-Modal   Alignment   Yang Jin1 and Yadong Mu1\u22c6   1Peking University   jiny@stu.pku.edu.cn, myd@pku.edu.cn", "abstract": ". This paper explores the spatio-temporal video grounding   (STVG) task, which aims at localizing a particular object corresponding to a given textual description in an untrimmed video. Existing approaches mainly resort to object-level manual annotations as the supervision for addressing this challenging task. Such a paradigm heavily   constrains the scalability of processing large-scale unlabeled data. To   this end, we present a novel framework that is capable of grounding the   target object relying only on the video-sentence correspondence. Specifically, our model re-formulates the original STVG task as two crossmodal alignment sub-problems: region-phrase and frame-sentence. Since   the absence of ground-truth alignments during the training stage, we   treat them as latent variables and learn to model the joint conditional   distribution by reconstructing the interactions of entities in the video.   The entire framework can be effectively optimized by the variational   Expectation-Maximization (EM) algorithm, which alternates between   two updating steps for progressively maximizing the likelihood of query   sentence, thereby approximating the real cross-modal assignment. Extensive experiments on two video benchmarks (VidSTG and HC-STVG)   further show the effectiveness of the proposed method.   Keywords: visual grounding \u00b7 weakly-supervised learning \u00b7 multi-modality   1", "content": "Visual grounding aims at finding a specific region associated with linguistic   meaning in the visual modality, which serves as a fundamental block in the complicated multi-modal system. In the video scenario, understanding how language   description relates to video content in spatial-temporal dimensions is paramount   for video surveillance [10], video question answering [18], etc. Recently, spatiotemporal video grounding (STVG) was introduced in [43], which requires localizing a spatio-temporal tube (i.e., a sequence of bounding boxes) of the target   object described by language query from the untrimmed video (See Figure 1). In   contrast to prior grounding methods focusing on images, the STVG task poses   a greater difficulty as it involves discerning nuanced characteristic changes of   video instances based solely on textual semantics.   \u22c6Corresponding Author.   2   Jin et al.   An adult pushes a baby walker on the street.   \ud835\udc61!\"#$\"   \ud835\udc61%&'   Region-Phrase   ...   Region Proposals   an    adult    baby    walker   street   Entity Phrases   Assignment   Frame-Sentence   ...   Video Frames   An adult pushes a baby walker    on the street.   Query Sentence   Assignment    Probability   Spatio-Temporal Video grounding   Fig. 1: The STVG task requires localizing the bounding box sequence and the temporal   boundary of the target object described by a query sentence. We factorize this task into   two sub-problems: region-phrase and frame-sentence alignments, which aim to match   the region to the textual phrase in the query sentence and identify which frames belong   to the target video segment, respectively.   To accomplish this complicated task, the prevalent approaches [16,33,38,42,   43] are heavily dependent on fine-grained human annotations such as temporal   boundaries and bounding boxes on each video frame. Despite achieving promising results, the labor-intensive process of annotation in each video frame makes   it challenging to scale these fully-supervised methods to large-scale datasets.   Accordingly, the utilization of weak supervision, where only video-sentence correspondences are available during training, is considered to be more practical in   real-world applications. To this end, we focus on seeking an efficacious paradigm   for weakly-supervised spatio-temporal video grounding in this work.   Although weakly-supervised grounding in images has received significant research attention [1, 11, 21, 36, 37], just a few works [8, 19] have explored this   challenging setting in the STVG task. The weakly-supervised STVG task invokes   several unique challenges associated with the lack of annotation and time-varying   video content. Firstly, the precise alignment of region and phrase is intricate due   to the existence of entities that share similar visual features or perform analogous   actions within a video frame. For example, given the query of \u201cAn adult pushes   a baby walker on the street\u201d as in Figure 1, the absence of spatial supervision   exacerbates the difficulty in distinguishing the target \u201cadult\u201d since there are four   adults in the scene. In this regard, the accurate grounding of the target object   is contingent upon the explicit modeling of the visual relationship (e.g., \u201cpush\u201d)   between entities (\u201cadult\u201d and \u201cbaby walker\u201d) within the video. Secondly, unlike   in images where the visual relationships are static, the interactions in untrimmed   videos are dynamic and will change or disappear over time. However, the query   Video Grounding with Variational Cross-Modal Alignment   3   sentence may only refer to a short-term state of the queried object, e.g. the action   \u201cpush\u201d only occurs in a short video clip. It is of great significance to determine   the temporal boundary of the queried object tubes.   To cope with these unique challenges in the weakly-supervised setting, this   paper presents a novel framework for the STVG task. Specifically, given a video   and its corresponding language description, we parse the text to several noun   phrases that denote objects and extract a set of region proposals as candidates   in each video frame. Then, the STVG task can be interpreted as two cross-modal   alignment problems: region-phrase and frame-sentence. The former entails identifying the region that accurately represents the phrase, while the latter involves   determining which frames correspond to the video segment conveying the semantics of the query sentence. Due to the absence of ground truth in the weaklysupervised context, we regard them as latent variables and proceed to learn   the joint distribution conditioned on the correspondence between the video and   sentence. The entire framework can be efficiently optimized by the variational   Expectation-Maximization (EM) algorithm [26], which alternates between two   learning steps. In the E-step, we introduce a variational posterior distribution   to approximate the real assignment. In the M-step, we learn to reconstruct the   visual relationships between the entity phrases, based on the samples drawn   from the variational assignment distribution. Through multiple optimization iterations aimed at reconstructing the visual relationships between entities, the   model will progressively learn a more precise cross-modal alignment. The main   technical contributions of our work can be summarized as follows:   \u2013 This paper explores the STVG task in the challenging weakly-supervised setting. Specifically, we innovatively bifurcate spatio-temporal grounding into two   distinct cross-modal alignment sub-problems and formulate them by effective   latent variables.   \u2013 We present a methodology that employs the variational Expectation Maximization algorithm to optimize the entire framework. Notably, the proposed   model successfully achieves fine-grained cross-modal alignment without manual annotations during training.   \u2013 We conduct extensive experiments on two large challenging video benchmarks,   namely VidSTG [43] and HC-STVG [34]. The results indicate that our proposed approach surpasses the existing weakly-supervised approaches by a significant margin, thus proving its superiority.   2   Related Work   Fully-Supervised Video Grounding. The video grounding task can be divided into two main categories: temporal and spatio-temporal grounding. In   the past few years, temporal grounding [6, 7, 25, 40] has emerged as a significant area of research interest and has garnered considerable attention from   scholars. Due to the computational complexity of spatial-temporal dimensions   in videos, the STVG task has been explored relatively less. The prevailing approaches [16, 33, 34, 38, 43] for STVG primarily investigate the fully-supervised   4   Jin et al.   paradigm. Zhang et al. [43] develops a spatio-temporal graph that facilitates   message passing across all frame regions. Lately, some approaches [16,38] have   proposed a one-stage approach that directly generates the bounding box at each   frame and predicts the starting and ending timestamps. Notwithstanding, all   of these techniques are substantially dependent on the manual annotations for   both the temporal boundary and spatial bounding box in every video frame. In   contrast, this work is focused on a weakly-supervised setting where the models   solely rely on the video-sentence correspondence, without the annotations of the   temporal and spatial location.   Weakly-Supervised Video Grounding. In the video scenario, the primary   research still focuses on temporal grounding, while only a few studies [2,8,9,19,   23, 32] have devoted to the weakly supervised spatio-temporal grounding task.   The WSSTG [9] developed an attentive interactor that exploits the fine-grained   contextual information to match the pre-extracted object tubes. However, they   only pay attention to spatial grounding at each frame and ignore the temporal   grounding. Chen et al. [8] propose novel spatial and temporal multiple instance   learning frameworks for the untrimmed video grounding. Li et al. [19] develops a multi-modal decomposition tree to perform multi-hierarchy language-tube   matching. Most of the existing works do not account for fine-grained entity interactions, thereby rendering them incapable of resolving semantic ambiguities   in the query sentence and distinguishing the target object.   3   Method   3.1   Problem Formulation   The Spatio-Temporal Video Grounding (STVG) task aims to identify a spatiotemporal tube {bt}te   t=ts in an untrimmed video V consisting of T frames, which   represents the target object described by a given query sentence S. Here, bt   denotes the bounding box in the t-th frame, while ts and te indicate the starting   and ending boundaries of the object tube being retrieved, respectively. In the   weakly-supervised setting, only global video-sentence correspondence (V, S) is   available and the model does not have any information regarding the spatiotemporal location of the queried object during the training stage.   3.2   Overview   Given the video-sentence pair (V, S), we first generate M region proposals for   each video frame and parse the description S to a group of noun phrases and   relationships. Thus, the video V can be denoted by V = {bi,j|i \u2208[1, T], j \u2208   [1, M]}. The query sentence S can be represented as (E, R), where E = {en}N   n=1   is a set of noun phrases that indicate the entities in the video and R = {rk}K   k=1   are the visual relationships that depict the interactions between entities. Then,   the STVG problem can be interpreted by two sub-problems: region-phrase and   frame-sentence alignment. The former seeks to map the entity phrases with their   Video Grounding with Variational Cross-Modal Alignment   5   \u2026   \u2026   \u2026   1   \ud835\udc61!   \ud835\udc61\"   \ud835\udc47   Input Video   An adult in a hat    rides on an elephant   Query Sentence   adult   ride on   in   \u2026   hat   elephant   Temporal Context    Encoder   Spatial Context    Encoder   Multi-Head    Self-Attention   Feed-Forward Network   Video Frame    Assignment \ud835\udc67#   \ud835\udc47   Region-Entity    Assignment \ud835\udc67%   Cross-Attention   Causal Self-Attention   Region Features   Phrase Features   ride   on   [s]   ride   on   [/s]   \u2299   \u2297   \ud835\udc67!   !   \u2299   \u2297   \ud835\udc67\"   !   ...   Features of   adult and elephant   \ud835\udc67!   \"   \ud835\udc61= \ud835\udc47   E-Step   M-Step   Alternating Update   Feature Encoding   Object    Detector   Language    Parser   \ud835\udc5e)(\ud835\udc67%| \u22c5)   \ud835\udc5e)(\ud835\udc67#| \u22c5)   Region Features   Region Features   \ud835\udc67#   \"   \ud835\udc61= 1   Region Proposals   Entities and relations   Fig. 2: The architecture of the proposed weakly-supervised spatio-temporal grounding   framework. Given an input video and a query sentence, it first extracts region proposals   from the video and parses the text to entity and relation phrases. Then, the feature   encoding modal is responsible for aggregating the contextual information among the   regions and phrases. The encoded features are then fed into the q\u03d5 and p\u03b8 networks to   conduct the Variational Expectation-Maximization (EM) optimization process.   corresponding regions, while the latter concerns identifying the frames belonging   to the ground-truth video segment. Since the actual assignment is unobserved   during training, we model it as a latent variable Z = (zS, zT ), where zS =   {zS   t }T   t=1 with zS   t \u2208RN\u00d7M and zT = {zT   t }T   t=1. These two latent variables have   the range of [0, 1], and denote the probability of spatial or temporal assignment.   Based on the above formulation, we propose to estimate the joint conditional   distribution p\u03b8(R, Z|E, V ) in this work. Furthermore, the parameters \u03b8 can be   optimized by maximizing the log-likelihood of the observed visual relationship,   denoted as log p\u03b8(R|E, V ). However, direct maximization of this log-likelihood   is not feasible due to the unavailability of real cross-modal assignment Z during   training. To this end, we instead pursue optimizing the evidence lower bound   (ELBO) of this log-likelihood as follows:     \\ label { e q : elbo} \\be g   i   n { align ed} \\ l   og p_\\t h e   t   a (R | E, V) \\geq \\mathbb {E}_{q_\\phi (Z|E,V)}\\left [ \\log \\frac {p_\\theta (R, Z | E, V)}{q_\\phi (Z|E,V)} \\right ], \\\\ \\end {aligned}    (1)   where q\u03d5(Z|E, V ) is a variational distribution parametrized by \u03d5 that approximates the real posterior distribution p\u03b8(Z|R, E, V ) and the equations holds only   when q\u03d5(Z|E, V ) = p\u03b8(Z|R, E, V ). By maximizing this lower bound, the real   log-likelihood log p\u03b8(R|E, V ) can be accordingly optimized.   Inspired by [3,17,28], this evidence lower bound is optimized via the variational Expectation-Maximization (EM) algorithm [26], which alternates between   E-step and M-step to update \u03d5 and \u03b8, respectively. To be specific, in the Estep, we fix \u03b8 and minimize the KL divergence F(\u03d5) between q\u03d5(Z|E, V ) and   6   Jin et al.   p\u03b8(Z|R, E, V ):     \\m a   t   h   cal {F} ( \\ phi    ) = \\s u m    _{Z} q _\\ p hi (Z|E,V) \\log \\frac {q_\\phi (Z|E,V)}{ p_\\theta (Z | R,E,V)}.    (2)   This step yields an approximation of the real cross-modal assignment. In the   M-step, we freeze \u03d5 and update \u03b8 to maximize the joint log-likelihood function   F(\u03b8) as follows:     \\l a bel {eq:l o glik e} \\m athc a l {F}(\\theta ) = \\mathbb {E}_{q_\\phi (Z|E,V)} \\left [ \\log p_\\theta (R, Z | E, V) \\right ].    (3)   Through the M-step, the model acquires the ability to reconstruct the visual   relationships R and further refines the assignment Z. The insight behind is that if   the interactions between entities depicted by the query sentence can be recovered   perfectly, the model must learn to match the entity phrase with the regions   appropriately. Consequently, the above optimization procedures will contribute   to effective spatio-temporal grounding. The entire framework is illustrated in   Figure 2, which consists of two individual networks parameterized by \u03d5 and   \u03b8. In the subsequent sections, we first present the technical details of feature   encoding and then elaborate on these two updating steps, respectively.   3.3   Contextualized Feature Encoding   We will commence the overall pipeline by introducing feature extraction and   context encoding. As shown in Figure 2, a pre-trained object detector [29] is   employed to extract M region proposals in each video frame. For per region bi,j,   we use the RoI-Align [14] and global average pooling to obtain its region-level   representation, denoted by vij \u2208RD. Additionally, the global frame features   {ft}T   t=1 can also be obtained from the detector backbone. For each entity phrase,   its word-level embeddings are obtained from the Glove model [27] first. Then,   the linguistic representation wn \u2208RD for the phrase en is thus computed by   applying the average pooling to its word-level embeddings.   Video-Context Encoding. After extracting the feature vij for each region proposal in the video, this module employs the multi-head self-attention layer [35]   to capture the contextual information among all regions. For the sake of computational efficiency, the encoding of region context is factorized into spatial   and temporal dimensions, respectively. Specifically, spatial context encoding is   performed among regions within the same frame, thereby fusing local context   information. The temporal interaction is restricted at the tube level, where a   tube is a sequence of regions that may represent the same object. To obtain the   object tube, a linking score slink(bt,i, bt+1,j) is defined to establish the association   of regions between two consecutive video frames:     \\label {e q:link}  s_{\\text {link}} ( b _ {t,i}, b_ {t+1,j}) = \\cos (v_{t,i}, v_{t+1,j}) + \\alpha \\cdot \\text {IoU}(b_{t,i}, b_{t+1,j}),    (4)   where cos(\u00b7) is the cosine similarity of the region features, IoU is the intersectionover-union of two regions and \u03b2 is the hyper-parameter that controls the importance of IoU metric. Based on the linking score, we can leverage the Viterbi   Video Grounding with Variational Cross-Modal Alignment   7   Algorithm [13] to generate M tubes per video by gradually selecting the path   with the highest linking score. Through the message passing within the tube,   each vij absorbs the temporal dynamics of objects across the video. The overall   video context encoding involves alternant spatial and temporal interaction.   Language-Context Encoding. This module utilizes another multi-head self-attention   layer to integrate contextual semantics across all entities within the query description S. Finally, the resulting contextualized visual and linguistic representations are fed into the subsequent modules.   3.4   E-Step: Approximating the Alignment   The exact estimation of the actual posterior distribution is intractable, thus we   introduce a variational distribution q\u03d5(Z|E, V ) to provide the approximation   for the cross-modal alignment in E-step. Following the empirical practice in the   variational inference [5], we utilize the mean-field approximation that assumes   the independence of all the latent assignment variables to formulate q\u03d5(Z|E, V ):     \\labe l  {   e   q   :eq   mf} q   _ \\ph i     (   Z   |E,   V) =    \\ pro d  _{t=1}^{T} q_\\phi (z_t^S|E,V) \\prod _{t=1}^{T} q_\\phi (z_t^T|E,V).    (5)   Specifically, the factorized spatial assignment q\u03d5(zS   t |E, V ) and temporal assignment q\u03d5(zT   t |E, V ) can be parameterized by the following two modules.   Spatial Assignment q\u03d5(zS   t |E, V ). The goal of this module is to estimate the   region-phrase alignment zS   t \u2208RN\u00d7M in each video frame. For the sake of brevity   in the following discussion, we omit the subscript t of the variable zS   t . Given the   contextualized region and phrase representations, two modality-dependent projection layers gV (\u00b7) and gS(\u00b7) are employed to map them into a joint multi-modal   feature space. Here, both gV (\u00b7) and gS(\u00b7) are implemented by fully connected   layers with ReLU activation. Let denote zS   i,j as the assignment variable between   i-th phrase and j-th region in a video frame, the q\u03d5(zS   i,j|E, V ) is computed by:     q_\\   phi (z _ { i   ,j}   ^   S|E,V) = \\fra   c    {   \\ex p \\ left ( g_S (w_{i   })^\\top g_V(v_{j}) \\right )}{\\sum _{j=1}^{M} \\exp \\left ( g_S(w_{i})^\\top g_V(v_{j}) \\right )}.    (6)   Temporal Assignment q\u03d5(zT   t |E, V ). The aforementioned spatial assignment resolves the box grounding at each frame, whereas this module endeavors to estimate the probability of individual frames appearing in the ground-truth video   segment. Intuitively, since the temporal grounding requires determining the accurate video segment corresponding to the semantics of description S, it is of   great significance to capture the interaction between global visual and textual   information. Therefore, following [41,45], the cross-attention layers are leveraged   to conduct multi-modal fusion between the frame features {ft}T   t=1 and sentence   features {wi}N   i=1. The resulting cross-modal global representation fcls is leveraged to produce the center c and width \u03c3 of the temporal boundary [ts, te] by a   8   Jin et al.   prediction layer, which is a linear mapping followed by sigmoid activation. Based   on c and \u03c3, we model q\u03d5(zT   t |E, V ) by a Gaussian distribution:     q_\\   p hi ( z _   t   ^   T|E ,V)       = \\fr a c {   1}{   \\   s q r t { 2\\pi }\\sigma } \\exp \\left ( - \\frac {(t / T - c)^2}{2\\sigma ^2} \\right ), t \\in [0, T].    (7)   Optimization. With the above formulation, the parameter \u03d5 can be optimized by   fixing \u03b8 and minimizing the function F(\u03d5). When \u03b8 is frozen, the log p\u03b8(R|E, V )   is the constant, and minimizing F(\u03d5) is equivalent to maximizing the evidence   lower bound (the expectation term in Eq. 1). By substituting the mean-field   formulation in Eq. 5 into the ELBO and taking the derivative with respect to   each component zS   t or zT   t , the optimal distribution q\u03d5 can be obtained by the   coordinate ascent update rule:     q_\\   p hi ( z _ t^S   |   E,V)    \\pro pto \\   e xp \\l ef t (    \\m   a   thbb {E}_{-z_t^S} [\\log p_\\theta (z_t^S | R, E, V, -z_t^S)] \\right ),    (8)   where \u2212zS   t indicates all the latent components except zS   t . The detailed proof is   presented in the appendix. Based on the independence assumption for Z, the   optimal solution satisfies q\u03d5(zS   t |E, V ) \u2248p\u03b8(zS   t |R, E, V ). Here, we only draw one   sample from p\u03b8(\u00b7) as the pseudo-label in the implementation. Take zS as the   example, the parameter \u03d5 can be updated by the cross entropy loss:     \\la b e   l       {eq:c   r oss } \\ m ath cal {   L }_{ z ^S, \\phi } = - \\sum _t p_\\theta (z_t^S|E,R,V) \\log q_\\phi (z_t^S | E,V).    (9)   Moreover, we also introduce a contrastive objective, which leverages the videosentence alignment as the supervision to further optimize the parameter \u03d5. Concretely, the region-phrase matching score between the i-th phrase and the j-th   region in the t-th frame can be obtained by st,i,j = gS(wi)\u22a4gV (vt,j). Then the   similarity score between video V and sentence S is calculated by:     s_{\\tex t {   m   a   tch   }   }   (V,    S   )    =    \\sum _{t=1}^{T} \\sum _{i=1}^{N} z_t^T\\max _{1 \\leq j \\leq M}s_{t,i,j}.    (10)   The contrastive loss Lcontra,\u03d5 is defined on all the matched and unmatched videosentence pairs in a training batch:     \\mathca l     {   L   }_{   \\te   xt {contr a}, \\p   hi    } = - \\sum _{i=1}   ^{B} \\log \\frac {\\exp (s(V_i, S_i) / \\tau )}{\\sum _{j=1}^B \\exp (s(V_j, S_i) / \\tau )} ,    (11)   where \u03c4 is the temperature parameter and B is the batchsize.   3.5   M-Step: Reconstructing the Relationship   The objective of the M-step is to maximize the joint log-likelihood F(\u03b8) in Eq. 3.   Following [28, 30], the optimization of F(\u03b8) can be further formulated by the   pseudolikelihood function [4]:     \\l a bel {eq:m _ step } \\begi n {s m all } \\math ca l { F}(\\theta ) \\approx \\mathbb {E}_{q_\\phi (Z|E,V)} \\left [ \\log p_\\theta (Z | E, V, R) + \\log p_\\theta (R | E, V, Z) \\right ]. \\end {small}    (12)   Video Grounding with Variational Cross-Modal Alignment   9   Algorithm 1: The optimization of our framework.   Input: The training dataset D with only the correspondence of video-sentence   {Vi, Si}   Output: The learned parameters for the q\u03d5 and p\u03b8.   Initialize both \u03d5 and \u03b8 randomly.   while \u03d5, \u03b8 has not converged do   // Conduct the E-Step   for (Vi, Si) in D do   Compute p\u03b8(Z|E, R, V ) based on the current \u03b8;   Update \u03d5 based on LZ,\u03d5 and Lcontra,\u03d5;   // Conduct the M-Step   for (Vi, Si) in D do   Compute q\u03d5(Z|E, V ) based on current \u03d5;   Update \u03b8 based on LZ,\u03b8 and Lrec,\u03b8;   Predict the grounding result using q\u03d5 and p\u03b8.   It consists of two conditional probability p\u03b8(Z|E, V, R) and p\u03b8(R|E, V, Z) that   estimate the distributions of cross-modal assignment and the relationships between entities.   Optimization of Assignment. This learning objective is to maximize the first   term in Eq.12. Here, we adopt the network implementation similar to q\u03d5 to   formulate p\u03b8(Z|E, V, R). The difference is that p\u03b8 is also conditioned on the   relationships R. Therefore, unlike q\u03d5 that only utilizes entity features, the estimations of zS and zT integrate the representations of all entity and relation   phrases. Finally, zS and zT yielded from q\u03d5 are treated as the targets to update   \u03b8 with a loss function analogous to Eq. 9.   Optimization of Reconstruction. The second optimization term of Eq.12 is obliged   to reconstruct the relationships R between entities E given the cross-modal   assignment Z predicted by q\u03d5. Here, a visual relationship is denoted by the   triplet (es, r, eo) that represents the interaction between subject es and object   eo. Specifically, based on the estimated zS   t \u2208RN\u00d7M and zT   t , the entity-relevant   representation H is calculated by:     H=    \\ { z_t^T    \\od   ot \\ ti   l de  {h_t} \\}_{t=1}^T, \\quad \\tilde {h_t} = z_t^S v^t \\in \\mathcal {R}^{N \\times D} ,    (13)   where vt \u2208RM\u00d7D is the contextualized visual features of M regions in the t-th   frame and zS   t serves as the gate function that modulates each \u02dcht to weaken the   features of irrelevant frames. Then, the feature used to reconstruct the relationship r is obtained by:        h _ t^{r}    =  W_r    [ h _t^{e_s};h_t^{e_o}] + b_r,    (14)   where hes   t and heo   t   are the representations for subject and object entities, respectively. As illustrated in Figure 2, the generated {hr   t}T   t=1 in the overall video will   be fed to a standard transformer decoder [35] with the causal self-attention layer   10   Jin et al.   for recovering the relation r in an autoregressive way. This transformer decoder   is optimized by the cross-entropy loss on the words of r:     \\mat h c   a   l    {L   }_{ \\text {rec},\\theta } = -\\sum _{i=1}^{L} \\log p_\\theta (r_i | r_{\\textless i}, E, V, Z), r   (15)   where ri is the i-th word in the relation r. This design facilitates the grounding,   as the model can only reconstruct the relationship between the subject and   predicate by correctly aligning them with the relevant region (zS) and attending   to the corresponding video segment (zT ).   3.6   Training and Inference   Training. The overall optimization algorithm is summarized in Algorithm 1. It   alternates between E-step and M-step to progressively update q\u03d5 and p\u03b8 until   convergence. In the E-step, \u03b8 is frozen, and we update \u03d5 by LZ,\u03d5 and Lcontra,\u03d5.   In the M-step, \u03d5 is frozen, and parameter \u03b8 is optimized by LZ,\u03b8 and Lrec,\u03b8.   It is worth noting that, during the E-step, q\u03d5 is updated based on p\u03b8 using   LZ,\u03d5, which brings the knowledge acquired through relation construction into   q\u03d5. Similarly, in the M-step, p\u03b8 is updated with q\u03d5 using LZ,\u03b8, which injects the   knowledge gained through contrastive learning into p\u03b8. Consequently, these two   steps work synergistically during the optimization process.   Inference. At inference, our framework produces the bounding boxes and the   starting and ending probabilities based on the prediction of both q\u03d5 and q\u03b8. In   detail, we can generate the estimated assignment variable Z by:     \\lab e l { eq:balance} Z = \\beta Z_{q_\\phi } + (1 - \\beta ) Z_{p_\\theta }.    (16)   The bounding box prediction at each frame is obtained by selecting the region   proposal with the maximal assignment score between the queried object phrase   and all M regions based on zS. The temporal boundary of the object tube is   determined by selecting the segment with the highest zT .   4   Experiments   4.1   Datasets and Metrics   We conduct extensive experiments on two widely-adopted spatio-temporal video   grounding benchmarks, dubbed as VidSTG [43] and HC-STVG [34]. Both   these two datasets comprise manual annotations for the spatio-temporal tubes   concerning a query sentence. For instance, the VidSTG contains a total of   44,808 video segments annotated with 99,943 sentences. The other dataset HCSTVG consists of 5,660 raw video files depicting scenes with multiple people   in the movie, where each video has been annotated the sentence to identify a   relevant human with a specific attribute or interaction with the surrounding   Video Grounding with Variational Cross-Modal Alignment   11   objects. We adopt the same dataset split for training and text following the   previous works [34, 43]. To perform a quantitative evaluation of the grounding   performance, we adopt the criteria proposed in [43], including m_VIoU and   VIoU@R. VIoU score is calculated by: VIoU =   1   |Su|   P   t\u2208Si IoU(\u02c6bt, bt), where Si   and Su represent the intersection and union of the predicted and ground-truth   video segments. And IoU(\u02c6bt, bt) indicates the IoU score between the predicted   \u02c6bt and ground-truth bounding box bt at frame t. The m_VIoU score denotes   the mean value of the VIoU scores across all testing videos and VIoU@R quantifies the proportion of samples in the testing subset whose VIoU score exceeds   a specific threshold R.   4.2   Implementation Details   Consistent with the previous methods, the input video frames are first sampled   at the rate of 5 fps. To handle the overlong videos, we conduct uniform sampling to select at most 200 frames throughout the video. Then, for each frame,   a Faster-RCNN object detector pre-trained on MS-COCO [20] with ResNet101 [15] backbone is leveraged to extract 10 regions as the candidate proposal,   where each region is associated with an RoI align pooled feature. And the representation of each video frame is derived from the C4 block in the ResNet-101.   In regards to the query sentence, we borrow the Stanford CoreNLP tools [24]   to parse a set of entities and relationships. All the representation dimensions in   the model are set to D = 256. Additionally, both the context encoder and reconstruction decoder have two transformer blocks with 8 heads in the attention   module. We empirically set the hyper-parameters in this paper with \u03b1 = 0.5 and   \u03b2 = 0.3. The whole framework is trained for 20 epochs with a batch size of 32   using AdamW [22] optimizer on 4 NVIDIA V100 GPUs, where the learning rate   is warmed up to 5e-5 and then decayed linearly in the remaining iterations.   4.3   Performance Comparison   To fully demonstrate the advantage of the proposed framework, this section provides extensive quantitative comparisons with the existing weakly-supervised   grounding approaches. Here, we consider the following types of methods as the   competitors. (1) Object Similarity. This type of approach predominantly employs the semantic similarity between the region label predicted by the detector and the subject phrase in the query sentence to generate the target object   tube. Essentially, this is the process of grounding by utilizing the knowledge   distilled from external pre-trained object detectors. (2) Factorized Grounding. Since the STVG problem is a compound task, an intuitive way to settle   this problem is to handle the spatial and temporal sub-grounding, respectively.   To this end, we first leverage the weakly-supervised image grounding methods (e.g., GroundeR [31], MATN [44], RAIR [21]) to predict the bounding-box   at each video frame, and then resort to weakly-supervised temporal grounding   techniques (e.g., LCNet [39], CPL [45]) to yield the predicted video segment.   12   Jin et al.   Methods   Declarative Sentences   Interrogative Sentences   m_VIoU VIoU@0.3 VIoU@0.5 m_VIoU VIoU@0.3 VIoU@0.5   Object Similarity   3.41   2.78   1.31   2.87   2.95   0.98   GroundeR [31]+LCNet [12]   7.85   7.96   3.02   6.43   6.58   2.92   MATN [44]+LCNet [12]   8.16   8.03   3.59   6.97   6.64   3.05   GroundeR [31]+CPL [12]   8.28   8.35   3.68   7.16   7.28   3.23   RAIR [21]+CPL [12]   8.67   8.72   4.01   7.68   7.71   3.58   WSSTG [9]   8.85   8.52   3.87   7.12   6.87   2.96   ADWS [8]   8.96   7.86   3.10   8.57   6.84   2.88   Vis-Ctx [32]   9.34   7.32   3.34   8.69   7.18   2.91   WINNER [19]   11.62   14.12   7.40   10.23   11.96   5.46   Ours   14.45   18.57   8.76   13.25   16.74   7.66   Table 1: Performance comparisons of different methods on the VidSTG test set (%).   Methods   m_VIoU VIoU@0.3 VIoU@0.5   Object Similarity   3.25   2.17   0.32   GroundeR [31]+LCNet   4.17   3.28   1.05   MATN [44]+LCNet   4.41   3.53   1.12   GroundeR [31]+CPL   5.23   4.18   1.25   RAIR [21]+CPL   6.88   4.87   1.36   WSSTG [9]   6.52   4.54   1.27   ADWS [8]   8.20   4.48   0.78   Vis-Ctx [32]   9.76   6.81   1.03   WINNER [19]   14.20   17.24   6.12   Ours   14.64   18.60   5.75   Table 2: Performance comparisons of different methods on the HC-STVG test set (%).   (3) Video Grounding. Only a few works have explored the weakly-supervised   video grounding. We compare with them: Vis-Ctx [32], WSSTG [9], ADWS [8],   WINNER [19] on the two STVG benchmarks.   For a fair comparison, all these baselines adopt the same backbone to extract   features. The detailed comparisons are presented in Tables 1 and 2 for two video   benchmarks, respectively. It can be seen that the grounding performance of our   approach surpasses all the competitors on the VidSTG benchmark and achieves   the best results in terms of evaluation metrics: m_VIoU and VIoU@0.3. Furthermore, simply grounding the target object based on the category-phrase similarity will lead to unsatisfactory results. This is mainly because there are usually   multiple objects with the same category in a video scene. The model must figure   out the complicated interactions between these objects to finally identify the   target. The factorized grounding approaches settle the STVG task separately,   which ignores the correlations between the spatial and temporal contextual information and thereby achieves inferior grounding performance. In our work,   the spatial and temporal assignment works collaboratively to recover the subtle   visual relationships between the entities in the video. Moreover, the proposed   method also outperforms most of the existing weakly-supervised video grounding   methods by a large margin. Although they consider the spatio-temporal context   clues throughout the whole video with the attention mechanism by leveraging   Video Grounding with Variational Cross-Modal Alignment   13   Lcontra,\u03d5 Lrec,\u03b8 m_VIoU VIoU@0.3 VIoU@0.5   \u2713   10.83   13.24   5.36   \u2713   12.87   16.32   7.93   \u2713   \u2713   14.45   18.57   8.76   Table 3: Ablation of different network combinations on the test set of VidSTG benchmark (declarative sentences).   Spatial Temporal m_VIoU VIoU@0.3 VIoU@0.5   \u2713   13.16   17.15   7.93   \u2713   14.12   18.21   8.25   \u2713   \u2713   14.45   18.57   8.76   Table 4: Ablation of different context encoding layer on the test set of VidSTG benchmark (declarative sentences).   the multiple instance learning framework, the lack of explicitly modeling the finegrained interactions between entities makes them fail to distinguish the semantic   ambiguities in the query sentence.   4.4   Ablation Study   In this section, we perform various ablations on the VidSTG benchmark to gain   deeper insights into the contributions and design decisions made regarding the   individual components of the proposed framework. More ablation results can be   found in our appendix.   Effect of different components. In the network optimization E-step and M-step,   the contrastive loss Lcontra,\u03d5 and reconstruction loss Lrec,\u03b8 are leveraged to supervise the parameters of q\u03d5 and p\u03b8, respectively. For inference, both the predictions of q\u03d5 and p\u03b8 are responsible for producing the final grounding results.   To explore the effectiveness of these two networks, we designed experiments for   different combinations on the VidSTG benchmark. Based on the quantitative   ablation results presented in Table 3, one can observe that canceling either q\u03d5   or p\u03b8 will weaken the grounding performance. Notably, the network p\u03b8 brings a   more significant performance boost compared to the q\u03d5. This is basically intuitive since the p\u03b8 aims to recover the specific interactions between the grounding   target and other context objects. While the q\u03d5 network only utilizes the global   correspondence of video-sentence and can not benefit from the fine-grained semantics in the description. Nevertheless, the cooperation of these two networks   achieves the best grounding performance.   Effect of the video context encoding. In the contextualized feature encoding module, the spatial and temporal context modeling layers are adopted to capture the   global dynamics of regions across the whole video. We thus design an ablation   experiment in Table 4 to investigate the function of these two layers for the subsequent grounding. From the presented comparison, we can observe a distinct   performance drop without either of these two layers, which further validates the   effectiveness of the spatial and temporal context encoding layer.   14   Jin et al.   Ground-truth   Query:  An adult in a hat is above an elephant.   Prediction   Ground-truth   Query: A child in yellow holds hand of another child in pink in a room.   Prediction   Ground-truth   Query: An adult with watch holds a dish at the stall.   Prediction   adult   hat   elephant   in   above   child   another child   room   in   hold   adult   watch   dish   stall   with   hold   at   Fig. 3: Some illustration examples of the spatio-temporal video grounding predictions   produced by our model on the VidSTG benchmark.   4.5   Visualization Analysis   In this section, we visualize some grounding results on the VidSTG benchmark   to provide a qualitative analysis of the model performance. As illustrated in   Figure 3, the left section displays the predicted sequence bounding boxes for the   target object, while the right section presents the interactions among entities   in the video. From the shown visualizations, one can clearly observe that our   framework can rightly attend to the desired instance depicted by the textual   description. It is worth noting that, even for the hard sample (the second one),   where several instances similar to the target exist in one scene, the model can   still distinguish the queried entity.   5   Conclusion   In this work, an effective framework is proposed for addressing the weaklysupervised spatio-temporal video grounding task. Specifically, we innovatively   formulate the STVG task to two cross-modal alignment sub-problems. To cope   with the absence of fine-grained ground-truth annotation, the assignments between phrase-region and frame-sentence are treated as latent variables optimized   with the variational EM algorithm. Moreover, extensive experimental results on   two video benchmarks further validate the superiority of our approach.   Acknowledgement. This work is supported by National Key R&D Program   of China (2022ZD0160305), an internal grant of Peking University (2024JK28),   and a grant from China Tower Corporation Limited.   Video Grounding with Variational Cross-Modal Alignment   15", "conf": "ECCV", "year": "2024", "index": 250}, {"title": "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding   Tal Shaharabany   Tel Aviv University   shaharabany@mail.tau.ac.il   Lior Wolf   Tel Aviv University   wolf@cs.tau.ac.il", "abstract": "A phrase grounding model receives an input image and   a text phrase and outputs a suitable localization map. We   present an effective way to refine a phrase ground model   by considering self-similarity maps extracted from the latent representation of the model\u2019s image encoder.   Our   main insights are that these maps resemble localization   maps and that by combining such maps, one can obtain   useful pseudo-labels for performing self-training. Our results surpass, by a large margin, the state of the art in   weakly supervised phrase grounding. A similar gap in performance is obtained for a recently proposed downstream   task called WWbL, in which only the image is input, without   any text. Our code is available at https://github.   com/talshaharabany/Similarity-Maps-forSelf-Training-Weakly-Supervised-PhraseGrounding.   1.", "content": "The most important technological foundation of the ongoing revolution in the field of jointly processing images   with text is that of computing a similarity score between an   image and a text phrase. Models such as CLIP [27] are able   to learn powerful similarity functions based on large datasets   that contain pairs of images and suitable captions. Building upon this technology, endless possibilities opened up   for zero-shot and few-shot learning applications, including   image captioning [19,35], image editing [6,13,23,25,41],   and image recognition [24,40].   Phrase grounding is a related image-text task, in which,   given an image and a text phrase, the method identifies   the image region described by the phrase. One can train a   phrase grounding network in a weakly supervised manner,   by employing the CLIP similarity. Such a model would   learn to produce masks that create a foreground region that   is similar to the phrase and a background region that is not.   To avoid trivial solutions, in which the mask includes the   entire image, one should also include a regularization term.   Recently, Shaharabany et al. have obtained state-of-theart results in phrase grounding using this scheme with the   addition of a constraint that considers also the explainability   map of Chefer et al. [8], when applied to the CLIP model.   This map is expected to focus on foreground regions and can,   therefore, provide an additional training cue.   In this work, we focus on the output of the image encoder   of the phrase grounding network of [32]. This encoder creates a spatial image representation that is aligned with the   text encoding of CLIP. In other words, each spatial location   of the tensor that the image encoder outputs is associated   with a vector that is correlated with a representation of the   textual description of the corresponding image location.   As a result, spatial locations that are associated with the   same image object have similar encoding and vice versa.   Building upon this insight, we compute for each image location, the cosine-similarity to the local encoding of all other   locations. These similarity maps (one per location) are textagnostic since they involve only the image input of the phrase   grounding network.   As we demonstrate, these maps capture different objects   in the image, i.e., the similarity map that is obtained by   correlating the image embedding at the spatial location (x, y)   provides a delineation of the object that can be found at this   image location. This is a type of semantic \u201cseed fill\u201d effect,   in which all image regions that are associated with the same   object in the seed location are highlighted. These regions do   not need to be connected, i.e., multiple persons in the image   would be highlighted by a seed placed on one of them.   This localization-map effect is reminiscent of the emergence of segmentation maps in self-supervised transformer   networks [5]. The two effects are, however, different. First,   the effect in the transformer case relies on the attention maps,   which do not exist in the networks we employ. Second, the   attention maps rely on a single CLS token, and our effect is   true to every spatial location. Third, the type of supervision   and the task are both different.   The effect described is also different than the relevancy   maps obtained by explainability methods such as GradCAM [31] or recent transformer explainability methods [8].   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   6925   For example, our similarity maps do not require backtracking   the relevancy scores all the way from the network\u2019s output.   Also, the maps are per seed location and not per label.   By aggregating multiple similarity maps, we obtain fairly   accurate foreground masks. Using these masks as pseudo   labels, we refine the phrase grounding model using a supervised loss term. On all of the known phrase grounding benchmarks, we demonstrate that our refinement method leads to   a marked improvement across multiple scores. Similarly, a   sizable gap over the current state-of-the-art is also shown   in the recently proposed computer vision task of \u201cWhat is   Where by Looking\u201d (WWbL) [32], for which the phrase   grounding network is coupled with a captioning model.   Our contributions are: (i) observing that spatial similarity maps of the latent space of phrase grounding networks   capture the boundaries of objects, (ii) developing a method   for aggregating multiple such similarity maps to obtain a   comprehensive segmentation map, (iii) using the obtained   maps to finetune the phrase grounding network, and (iv) surpassing the current state-of-the-art by a sizable gap in both   phrase grounding and WWbL.   2. Related Work   In the weakly supervised phrase localization task, text   phrases are associated with specific image locations [1,16,   39]. Text embedding is often extracted from a pretrained   language model and is aligned with an image representation   to obtain a shared semantic domain [11, 17, 30]. Recent   contributions employ CLIP [28] for linking text with image   locations [20,32]. Self-supervision methods were also used   to obtain an initial map by means of explainability [3,32].   However, we are unaware of any other phrase-grounding   method that employs pseudo labels that are extracted from a   phrase-grounding network.   Since our method employs the result of a phrase grounding model to collect data that is used to finetune the same   model, our work falls under the category of self-training.   Self-training methods use a model to infer pseudo-labels   for unlabeled or partly labeled samples and use the resulting samples to further improve the same model [2]. For   example, domain adaptation of detection networks can be   performed by labeling samples of the target domain using a   network trained on the source domain [29]. Another example   is a teacher-student framework in which the teacher provides   pseudo labels that are as accurate as possible, and the student   is further regularized by noise injection [38]. Self-training   can also be applied to multiple networks that are trained   jointly, as in the case of a semi-supervised semantic segmentation method that employs two differently initialized   networks that produce labels to one another [10]. We are not   aware of any other self-training work in which the labels are   produced by analyzing the embedding space and not using   the network\u2019s output.   The recently introduced WWbL task extends phrase   grounding by generating both the text and the associated   image regions. The WWbL method [32] employs a multistep procedure on top of phrase grounding, which combines   Selective Search [36] with the image captioning method   BLIP [19] in order to provide captioning to objects in the   scene and then apply phrase grounding to these captions.   WWbL, therefore, provides a novel application for phrase   grounding methods, which does not require any text input,   and serves, in our case, as a way to test the downstream   success of our method.   3. Method   Given an input text t and an RGB image I \u2208R3\u00d7W \u00d7H,   our method\u2019s starting point is the current state-of-the-art   architecture for weakly supervised phrase grounding [32].   We then apply a refinement procedure in order to improve   this model.   Let g be the phrase grounding network of [32], i.e.,     \\lab el {eq:g} M = g(I,Z_t(t))\\,    (1)   where the output mask M \u2208RW \u00d7H contains values between 0 and 1, and Zt is the text encoder of CLIP [27], with   an output dimension of 512.   Network g is based on an encoder-decoder architecture   adapted to support text-based conditioning. The image encoder of g, called ZI, is a VGG network [33] with a receptive   field of size 16\u00d716, i.e., the image is downscaled four times.   The output of ZI has 512 channels, to match the dimensions   of the text encoder Zt.   The map ZI(I) \u2208RW/16\u00d7H/16\u00d7512 is viewed as a spatial field of vectors in R512. Each such vector is normalized   to have a norm of one. To link the image information with   the text information, a dot product is computed between the   normalized vector at each location in the map and the vector   Zt(t) (CLIP also has normalized outputs). Following this   cosine similarity step, a map Zs(I, t) is obtained with values   between -1 and 1. Zs(I, t) has a single channel and the same   spatial dimensions as ZI(I).   The next step of [32] is to spatially re-weight all channels   of ZI(I) by multiplying them pointwise by Zs(I, t). The   result is passed to the decoder U, which consists of three   upsampling blocks, to obtain the single channel map M of   Eq. 1. This map is obtained after applying a Sigmoid and   has values between 0 and 1.   Our proposed solution for phrase-grounding refinement   is based on calculating self-similarity maps based on the   data of the spatial map ZI(I). These maps are aggregated   and serve as pseudo-labels for fine-tuning g and obtaining a   fine-tuned network g++.   Let J be the map that is obtained after normalizing ZI(I)   along the channel index. A set of W/16 \u00d7 H/16 maps is   6926   (a)   (b)   (c)   (d)   (e)   Figure 1. Sample self-similarity maps for given image locations. (a) input image. (b-c) self-similarity maps for the green star and green   diamond image locations (resp.), which are inside the same object. (d-e) self-similarity maps for the locations marked by a red triangle and a   blue square, respectively. Evidently, the similarity map for an image location highlights the object in that region, and two similarity maps   that are associated with image locations on the same object produce similar maps.   obtained by considering every spatial location of J. Specifically, let jx,y \u2208R512 denote the vector of all channels at   one specific map location (x, y). The similarity map Sx,y   has the same spatial dimensions as J and at location (x\u2032, y\u2032)   has the scalar value jx,y \u00b7 jx\u2032,y\u2032.   Sx,y are text-agnostic as the text representation Zt(t)   does not play a role in their creation. Sample maps, for   specified image locations, are depicted in Fig. 1. As can be   seen, the similarity map Sx,y is consistent with the object in   the corresponding image location (16x, 16y) (upsampling   four times, due to the encoder\u2019s downsampling effect).   This is not surprising, since, within the spatial region that   is associated with a single object, the cosine similarity between ZI(I) and Zt(t) is expected to be similar, i.e., high if   that image location is described by the text and low otherwise.   Therefore, the embeddings of two spatial locations from the   same object-associated region would be high. Conversely,   the embeddings at image locations that are associated with   different objects are expected to be low.   Next, our method filters the set of computed maps   {Sx,y}x,y in order to identify the maps that are relevant   to a given input text t. This is done by comparing the output   map M = g(I, Zt(t)) to each of the maps in the set.   Specifically, each map Sx,y is binarized by threshold at   a value of zero (recall that the values of the similarity maps   are cosine similarities between -1 and 1). Similarly, the map   M is binarized using a threshold that is half of the map\u2019s   maximal value. Since M is in the size of the original image,   its binarized version is downscaled by a factor of 16 in each   dimension to match that of Sx,y.   The Intersection Over Union (IOU) score is then computed between the two binary maps, the one based on Sx,y   and the one derived from M. If this score is above a relatively conservative threshold of \u03c4 = 0.6, the map Sx,y is   considered relevant to the text t. Finally, all relevant maps   are averaged to obtain the map \u00af   M. In the case that no location x, y produced a map Sx,y that leads to an IOU above \u03c4,   we select the map with the maximal IOU score.   Fig. 2 presents examples of the mask aggregation process,   which extracts the mask \u00af   M based on the text, image, and   phrase grounding network g. As can be seen, the obtained   masks \u00af   M are often much more comprehensive than the original map M, without covering significantly more of the   background regions. The appendix includes a pseudo-code   outlining the steps involved in our approach.   The finetuning of g to obtain the refined network g++   employs the map \u00af   M as the pseudo-label. Four-loss terms   are used, given the input image I and the text t. The pseudosupervised loss is given by     \\label { eq :   th r e e   }  L_{pse udo}(I,t ,\\bar M) = \\| \\bar M - g^{++}(I,Z_t(t)) \\|^2\\,,    (2)   6927   A   Japanese   woman   Six   children   A person   A boy   A   young   boy   A   pale   gray horse   An   elephant   A   british   airways   airplane   (a)   (b)   (c)   (d)   (e)   (f)   (g)   Figure 2. Sample self-similarity maps and their aggregation. (a) the input text t. (b) the input image I. (c) the output M = g(I, t). (d-f)   three self-similarity maps Sx,y that overlap M. (g) the aggregated map \u00af   M. Evidently, the aggregated map is much more comprehensive.   6928   where \u00af   M is the pseudo label obtained from g when applying   it to image I and text t.   The other three loss terms follow [32]. The foreground   loss Lfore(I, t) is given by     \\label  { e q:one} L_{fo re}(I, t ) = - CLIP(g^{++}(I,Z_t(t))\\odot I, t),    (3)   where \u2299is a pointwise multiplication. This loss maximizes   the CLIP similarity between the foreground region of the   obtained mask g++(I, ZT ) and the text t.   The background loss Lback(I, t) is     \\label  { e q:two} L _{back }(I,t) =  C LIP((1-g^{++}(I,Z_t(t)))\\odot I, t).    (4)   This loss minimizes the CLIP similarity between the background of image I, as denoted by the obtained mask, and the   text t.   Finally regularization loss Lreg(I) is applied to create   compact maps     \\labe l { e q:five}  L_{reg}(I,t)) = \\| g^{++}(I,Z_t(t)) \\|    (5)   The combined loss is defined as an unweighted sum of the   four loss terms, in order to avoid adding hyper-parameters,   i.e., L(I, t, \u00af   M)   =   Lpseudo(I, t, \u00af   M) + Lfore(I, t) +   Lback(I, t) + Lreg(I, t).   4. Experiments   We present our results for both weakly supervised phrase   grounding (WSPG) and WWbL. The networks g and g++   are trained on either MSCOCO 2014 [21] or the Visual   Genome (VG) dataset [18]. Evaluation is done on the test   splits of Flickr30k [26], ReferIt [9,14], and VG.   The training split of Akbari et al. [1] is used for MSCOCO   2014. It consists of 82,783 training images and 40,504 validation images, each image with five captions describing it.   VG contains 77,398 training, 5000 validation, and 5000 test   images. Each image is associated with both free-form text   and annotated bounding boxes.   The Flickr30k Entities dataset [26], which is derived from   Flickr30k, contains 224K phrases describing objects in more   than 31K images. Each image is associated with five captions. For evaluation, we use the 1k images of the test split   of Akbari et al [1]. ReferIt [9,14] consists of 20K images   and 99,535 manually captioned and segmented images. This   data was collected in a two-player game with approximately   130K isolated entity descriptions. Here, too, we use the test   split of Akbari et al. [1].   Implementation details   The proposed network g++ is obtained by fine-tuning network g with the loss terms presented   at Sec. 3. The visual backbone of g is VGG16 [33], which is   also used by the vast majority of the phrase grounding work.   Finetuning takes place using an SGD optimizer with a   batch size of 32 and a learning rate of 0.0001. This learning   a segway scooter   .... ... an atv   ... a mountain biker   ... a deer laying   ... a butcher   (a)   (b)   (c)   (d)   Figure 3. Sample phrase grounding results. (a) the phrase. (b)   the input image and the ground truth bounding box. (c) results   for network g [32] shown as a heatmap. (d) same for the refined   network g++.   rate is 3 times lower than the original learning rate of g. The   optimizer momentum is 0.9 and weight decay regularization   is 0.0001. Finetuning runs for 3000 iterations.   Results   Phrase grounding tasks are evaluated with respect   to the accuracy of the pointing game [39], which is calculated   from the output map by finding the maximum-value location   for the given query and checking whether this point is located   in the region of the object.   Another metric shown (\u201cBBox accuracy\u201d) extracts a   bounding box from the output mask and compares it with the   bounding-box annotations. An accurate prediction is defined   as one for which the IOU is over 0.5. To extract the bounding   6929   a woman walking down the street   a line of parked cars   a person climbing a rock   a forest   a small black dog   a person   (a) input   (b) g   (c) g++   (d) g   (e) g++   Figure 4. Samples WWbL results on images from Flickr1K comparing between g [32](b,d) and our g++(c,e). The automatically generated   captions (these are independent of the phrase grounding network used) are on top of the relevant masks.   Method   VG trained   MS-COCO trained   VG Flickr ReferIt VG Flickr ReferIt   FCVC [12]   14.03 29.03   33.52   VGLS [37]   24.40   TD [39]   19.31 42.40 31.97   SSS [16]   30.03 49.10 39.98   MG-BiLSTM [1] 50.18 57.91 62.76 46.99 53.29   47.89   MG-ELMo [1]   48.76 60.08 60.01 47.94 61.66   47.52   GbS [3]   53.40 70.48 59.44 52.00 72.60   56.10   CLIP+GAE [7]   54.72 72.47 56.76 54.72 72.47   56.76   g [32]   62.31 75.63 65.95 59.09 75.43   61.03   g++ (ours)   66.63 79.95 70.25 62.96 78.10   61.53   Table 1. Weakly Supervised Phrase Grounding (WSPG) results,   showing the \u201cpointing game\" accuracy for multiple benchmarks.   box from an output map M, the procedure of Shaharabany   et al. [32] is employed. First, a threshold of 0.5 is applied to   M, and then the method of Suzuki et al. [34] is applied to   find image contours. A set of bounding boxes is obtained by   considering the enclosing box of each contour. These bounding boxes are scored by summing the values of M within the   contour and those with low scores are discarded. Following   a non-maximal suppression step, the minimal bounding box   that contains the remaining bounding boxes is used.   As mentioned, we use the same training/validation/test   splits as Akbari et al. [1]. For ReferIt, Visual Genome and   Flickr30K, each query is treated as a single sentence.   Tab. 1 lists the results for the Flickr30k, ReferIt, and VG   for the weakly-supervised phrase grounding task. Evidently,   our method is superior to all baselines, whether training   takes place over VG or MS-COCO. In particular, there is a   sizable improvement with respect to the network g that we   refine, which is currently the state-of-the-art [32].   In addition to the pointing game results, Tab. 2 presents   bounding box accuracy for the phrase grounding task (this   data is not available for most baselines). Here, too, our   6930   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   \u03c4   72   73   74   75   76   77   78   79   80   Point Accuracy[   \u00af   M based on g[MS-COCO]   g++[MS-COCO]   \u00af   M based on g[VG]   g++[VG]   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   Maps threshold   40   50   60   70   80   Point Accuracy[   MS-COCO   Visual Genome   (a)   (b)   Figure 5. A parameter sensitivity analysis presenting the pointing game score for the Flickr dataset. (a) sensitivity to parameter \u03c4, in which   the default value is 0.6, and (b) varying the threshold on the map M as a fraction of the maximal value (default is 0.5). Shown in panel (a) is   the performance of using \u00af   M instead of M of g (same as the first ablation) and the performance of the full g++ refinement. In (b), due to   lack of time, only the former appears. The dashed horizontal lines denote the performance of the baseline model g.   VG Trained   MS-COCO Trained   Task   Model   Point Accuracy   Bbox Accuracy   Point Accuracy   Bbox Accuracy   VG   Flickr ReferIt   VG   Flickr ReferIt   VG   Flickr ReferIt   VG   Flickr ReferIt   WWbL   MG [1]   32.15 49.48   38.06   12.23 24.79   16.43   32.91 50.12   36.34   11.48 23.75   13.31   g [32]   43.91 58.59   44.89   17.77 31.46   18.89   44.20 61.38   43.77   17.76 32.44   21.76   g++ (ours)   45.90 62.98   45.14   20.01 33.71   21.07   47.39 65.93   44.52   20.58 36.40   22.07   WSPG   MG [1]   48.76 60.08   60.01   14.45 27.78   18.85   47.94 61.66   47.52   15.77 27.06   15.15   g [32]   62.31 75.63   65.95   27.26 36.35   32.25   59.09 75.43   61.03   27.22 35.75   30.08   g++ (ours)   66.63 79.95   70.25   30.95 45.56   38.74   62.96 78.10   61.53   29.14 46.62   32.43   WSPG ablations   \u00af   M based on g   63.10 77.60   66.61   24.07 26.40   33.33   61.19 77.80   61.15   21.56 22.17   27.41   Only Lpseudo   65.50 78.84   68.49   23.50 39.06   29.16   62.37 78.07   60.15   22.10 40.12   26.62   Lpseudo + Lreg   59.40 73.95   64.31   22.35 26.25   26.25   56.97 74.99   60.03   19.94 22.22   23.55   Lpseudo + 1/3Lreg   65.80 78.94   68.68   30.03 43.46   37.27   60.44 76.81   58.83   27.05 44.99   30.89   Lpseudo + Lfore + Lback 65.47 79.51   69.77   25.71 44.96   34.29   62.61 78.05   60.86   25.51 45.90   30.66   No aggregation   66.22 79.24   70.03   27.36 44.44   35.71   61.72 78.02   59.55   27.28 46.34   31.42   Segmentation encoder   56.52 73.26   61.22   19.72 20.37   23.91   56.53 74.25   59.12   19.78 21.65   22.52   Classification encoder   60.49 74.40   66.67   4.85   4.07   16.50   55.91 72.84   63.08   4.67   4.44   15.41   Table 2. WWbL and Weakly Supervised Phrase Grounding (WSPG) results for the test sets showing both \u201cpointing game\" accuracy and   bounding box accuracy. The ablations show the performance of various reductions of our method on the WSPG task (see text for details).   method outperforms the baseline methods by a wide margin.   Sample results are shown in Fig. 3, compared with g [32].   The WWbL task is an open-world localization task, in   which the only input is a given image: given an input image,   the goal is to localize and describe all the elements composing the scene. To solve this, Shaharabany et al. [32] employ a   multi-stage algorithm, in which selective search [36] is used   to extract object proposals and BLIP is used to caption these   regions. Similar captions are pruned using the Community   Detection (Cd) method [4]. The phrase grounding model   then assigns heatmaps to the filtered list of captions. Since   phrase grounding is applied last, WWbL provides a direct   way to compare phrase grounding methods on automatically   generated captions.   The WWbL task is evaluated with the same two metrics (pointing game and bounding-box accuracy). For each   ground-truth pair of bounding box and caption, the closest   caption in CLIP space from the list of automatically gener6931   ated captions is selected. The associated output map of the   phrase grounding method is then compared to the ground   truth bounding box using the pointing accuracy metric. In addition, bounding boxes are extracted for the output heatmaps,   as described above.   WWbL results are listed in Tab. 2, with the leading WSPG   results provided for reference. The table shows the performance obtained by g, g + +, and a baseline that employs the   phrase grounding method MG [1] as part of the same WWbL   captioning procedure described above. Evidently, g++ obtains the best results among the three across all benchmarks   and the two scores. Fig. 4 presents samples of the results of   g++, compared to those of g [32] for images from Flickr1K.   Ablation Study   We present multiple ablations, each validating a different part of our approach. The results of these   experiments are provided for the phrase grounding task and   are also listed in Tab. 2.   The first ablation considers the pseudo-label generation   process but instead of using it to finetune g to obtain g++, it   uses it as the output of the phrase grounding task. In other   words, this ablation directly provides \u00af   M as the output. As   can be seen, using \u00af   M of g improves upon g in the pointing   game accuracy but not in the bounding box accuracy. Moreover, it scores below our complete solution g++ on all scores   across all benchmarks.   The second ablation employs only the loss of the pseudo   label Lpsudo but not the other three-loss terms. As a general   trend, we can see that it improves the results of g and obtains much of the benefit of g++ (not on all datasets when   considering the bounding box accuracy).   Both these ablations may indicate that the other three   loss terms, including the CLIP-based loss terms Lfore and   Lback and the regularization loss, help maintain a reasonable   bounding box for the resulting mask. To check whether the   regularization loss can elevate Lpsuedo to the level of g++   performance, we conducted an ablation with both Lpseudo +   Lreg. As can be seen, the addition of the regularization term   to the pseudo-label loss hurts performance.   Since the regularization term in this experiment is balanced by a single loss (and not three), this could indicate   that too much regularization takes place. We, therefore, repeat the experiment with a third of the regularization, which   seems to improve, on most benchmarks and scores, above   using only Lpsuedo. Lastly, using the three other loss terms,   without regularization leads to a performance that is sometimes better and sometimes worse than adding a third of the   regularization term to the self-training loss, and still below   that of the full method.   The next ablation \u201cNo aggregation\u201d selects the similarity map Sx,y that has the highest IOU with M instead of   averaging multiple maps that are similar to M. The rest of   the training pipeline of g++ is kept the same. Evidently,   this ablation maintains much of the advantages of g++ over   other phrase grounding methods but obtains slightly lower   accuracy than the complete method.   The last set of ablations employs an alternative encoder   instead of the image encoder of ZI. The purpose of these   experiments is to check whether the emergence of segmentation maps of the latent space is a unique characteristic of   the phrase grounding network we employ or if it is a general   phenomenon. Specifically, we (1) employ the encoder of   a Fully-Convolutional semantic segmentation network [22]   with a ResNet-50 [15] backbone, which is also trained on   the MS-COCO dataset, and (2) employ a latent representation of a VGG16 classification network trained on ImageNet   (the same backbone of g). As can be seen from the ablation   results, these two alternative encoders do not lead to an improvement in the refined network over the baseline network   g. Sample self-similarity maps obtained from the alternative encoders are provided in the supplementary appendix,   showing that the alternative maps do not provide the level of   object-delineation provided by ZI.   Parameter sensitivity   Our method has very few parameters. There is a threshold on the cosine similarity values   that is taken at the middle of the range, i.e., at zero, and   a threshold on M that is taken at half the maximal value.   Both of which are natural choices. Additionally, there is a   threshold \u03c4 on the IOU between the binarized M and the   binarized Sx,y that is set to 0.6. This number is often used   as a relatively conservative threshold for IOU.   Fig. 5 presents results when varying the binarization parameters. These are shown for \u00af   M based on g (same as the   first ablation), which is a direct way to measure the quality   of \u00af   M, and for the refined g++ (the latter is shown only for   \u03c4 due to lack of time). Our method is largely insensitive to   both these thresholds, obtaining similar performance scores   in a relatively wide range of values.   5. Conclusions   Image encoders provide a rich spatial representation of   the input image. Despite the widespread use of such encoders, we are unaware of other contributions that apply   image-processing techniques to this embedding space in order to obtain spatial information. Our work demonstrates that   this is an oversight as the information in the embedding tensor is readily available for further exploitation. By using the   informative correlations between the data in various image   locations, we are able to extract segmentation pseudo-labels   that are extremely beneficial in advancing the performance   in the phrase grounding task beyond the state-of-the-art.   Acknowledgments   This work was supported by a grant from the Tel Aviv   University Center for AI and Data Science (TAD).   6932", "conf": "CVPR", "year": "2023", "index": 466}, {"title": "Detecting and Grounding Multi-Modal Media Manipulation   Rui Shao1,2*, Tianxing Wu2, Ziwei Liu2\u2020   1 School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen)   2 S-Lab, Nanyang Technological University   shaorui@hit.edu.cn, {tianxing.wu, ziwei.liu}@ntu.edu.sg   https://github.com/rshaojimmy/MultiModal-DeepFake   Gordon Brown is forced to resign EU meeting by   Nicolas Sarkozy the French president in Paris.   Detection   Grounding   Detecting and Grounding Multi-Modal Media Manipulation   Image   Text   Manipulated   Bounding Box:   'forced', 'resign'   Manipulated   Tokens:   [FS, FA, TS, TA]   DeepFake   Detection:   Real\u00a0or\u00a0Fake ?   Real\u00a0or\u00a0Fake ?   Multi-Modal   Misinformation   Detection:   Real\u00a0or\u00a0Fake ?   Text Fake News   Detection:            'is welcomed to'   Manipulation   'is forced to resign'   Figure 1. Different from existing single-modal forgery detection tasks, DGM4 not only performs real/fake classification on the input   image-text pair, but also attempts to detect more fine-grained manipulation types and ground manipulated image bboxes and text tokens.   They provide more comprehensive interpretation and deeper understanding about manipulation detection besides the binary classification.   (FS: Face Swap Manipulation, FA: Face Attribute Manipulation, TS: Text Swap Manipulation, TA: Text Attribute Manipulation)", "abstract": "Misinformation has become a pressing issue. Fake media, in both visual and textual forms, is widespread on the   web. While various deepfake detection and text fake news   detection methods have been proposed, they are only designed for single-modality forgery based on binary classification, let alone analyzing and reasoning subtle forgery   traces across different modalities. In this paper, we highlight a new research problem for multi-modal fake media, namely Detecting and Grounding Multi-Modal Media   Manipulation (DGM4). DGM4 aims to not only detect the   authenticity of multi-modal media, but also ground the manipulated content (i.e., image bounding boxes and text tokens), which requires deeper reasoning of multi-modal media manipulation. To support a large-scale investigation,   we construct the first DGM4 dataset, where image-text pairs   are manipulated by various approaches, with rich anno*This work was done at S-Lab, Nanyang Technological University   \u2020Corresponding author   tation of diverse manipulations. Moreover, we propose a   novel HierArchical Multi-modal Manipulation rEasoning   tRansformer (HAMMER) to fully capture the fine-grained   interaction between different modalities. HAMMER performs 1) manipulation-aware contrastive learning between   two uni-modal encoders as shallow manipulation reasoning, and 2) modality-aware cross-attention by multi-modal   aggregator as deep manipulation reasoning. Dedicated manipulation detection and grounding heads are integrated   from shallow to deep levels based on the interacted multimodal information. Finally, we build an extensive benchmark and set up rigorous evaluation metrics for this new research problem. Comprehensive experiments demonstrate   the superiority of our model; several valuable observations   are also revealed to facilitate future research in multi-modal   media manipulation.   1.", "content": "With recent advances in deep generative models, increasing hyper-realistic face images or videos can be auThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   6904   Table 1. Comparison of the proposed DGM4 with existing tasks related to image and text forgery detection.   Problem Setting   Image Forgery   Text Forgery   Multi-Modal   Forgery Detection   Detection   Grounding   Detection   Grounding   DeepFake Detection [26,55]   \"   %   %   %   %   Text Fake News Detection [48,53]   %   %   \"   %   %   Multi-Modal Misinformation Detection [1,25]   %   %   %   %   \"   DGM4   \"   \"   \"   \"   \"   tomatically generated, which results in various security issues [35\u201341, 43, 52] such as serious deepfake problem [7,   12, 21, 34, 42] spreading massive fabrication on visual media. This threat draws great attention in computer vision   community and various deepfake detection methods have   been proposed. With the advent of Large Language Model,   e.g., BERT [6], GPT [31], enormous text fake news [48,53]   can be readily generated to maliciously broadcast misleading information on textual media. Natural Language Processing (NLP) field pays great attention to this issue and   presents diverse text fake news detection methods.   Compared to a single modality, the multi-modal media   (in form of image-text pairs) disseminates broader information with greater impact in our daily life. Thus, multimodal forgery media tends to be more harmful. To cope   with this new threat with a more explainable and interpretable solution, this paper proposes a novel research problem, namely Detecting and Grounding Multi-Modal Media   Manipulation (DGM4). As shown in Table 1 and Fig. 1,   two challenges are brought by DGM4: 1) while current   deepfake detection and text fake news detection methods   are designed to detect forgeries of single modality, DGM4   demands simultaneously detecting the existence of forgery   in both image and text modality and 2) apart from binary   classification like current single-modal forgery detection,   DGM4 further takes grounding manipulated image bounding boxes (bboxes) and text tokens into account.   This   means existing single-modal methods are unavailable for   this novel research problem. A more comprehensive and   deeper reasoning of the manipulation characteristics between two modalities is of necessity. Note that some multimodal misinformation works [1, 25] are developed.   But   they are only required to determine binary classes of multimodal media, let alone manipulation grounding.   To facilitate the study of DGM4, this paper contributes   the first large-scale DGM4 dataset. In this dataset, we study   a representative multi-modal media form, human-centric   news. It usually involves misinformation regarding politicians and celebrities, resulting in serious negative influence. We develop two different image manipulation (i.e.,   face swap/attribute manipulation) and two text manipulation (i.e., text swap/attribute manipulation) approaches to   form the multi-modal media manipulation scenario. Rich   annotations are provided for detection and grounding, including binary labels, fine-grained manipulation types, manipulated image bboxes and manipulated text tokens.   Compared to pristine image-text pairs, manipulated   multi-modal media is bound to leave manipulation traces   in manipulated image regions and text tokens.   All of   these traces together alter the cross-modal correlation and   thus cause semantic inconsistency between two modalities.   Therefore, reasoning semantic correlation between   images and texts provides hints for the detection and   grounding of multi-modal manipulation. To this end, inspired by existing vision-language representation learning   works [16,17,30], we propose a novel HierArchical Multimodal Manipulation rEasoning tRansformer (HAMMER)   to tackle DGM4. To fully capture the interaction between   images and texts, HAMMER 1) aligns image and text embeddings through manipulation-aware contrastive learning   between two uni-modal encoders as shallow manipulation   reasoning and 2) aggregates multi-modal embeddings via   modality-aware cross-attention of multi-modal aggregator   as deep manipulation reasoning. Based on the interacted   multi-modal embeddings in different levels, dedicated manipulation detection and grounding heads are integrated hierarchically to detect binary classes, fine-grained manipulation types, and ground manipulated image bboxes, manipulated text tokens. This hierarchical mechanism contributes   to more fine-grained and comprehensive manipulation detection and grounding. Main contributions of our paper:   \u2022 We introduce a new research problem Detecting and   Grounding Multi-Modal Media Manipulation (DGM4),   with the objective of detecting and grounding manipulations in image-text pairs of human-centric news.   \u2022 We contribute a large-scale DGM4 dataset with samples   generated by two image manipulation and two text manipulation approaches. Rich annotations are provided for   detecting and grounding diverse manipulations.   \u2022 We propose a powerful HierArchical Multi-modal   Manipulation rEasoning tRansformer (HAMMER). A   comprehensive benchmark is built based on rigorous evaluation protocols and metrics. Extensive quantitative and   qualitative experiments demonstrate its superiority.   2. Related Work   DeepFake Detection. To detect face forgery images, current deepfake detection methods are built based on spatial and frequency domains.   Spatial-based deepfake detection methods exploit spatial visual cues, such as blending artifacts [19], textural features [4, 55, 57], 3D information [57], patch consistency [56] and noise character6905   istics [10]. Frequency-based deepfake detection methods   detect spectrum artifacts, like high-frequency components   decomposed from Discrete Fourier Transform (DFT) [8],   subtle frequency discrepancy derived from Discrete Cosine Transform (DCT) [29], up-sampling artifacts hidden   in phase spectrum [23] and frequency-based metric learning [18]. Most of the above deepfake detection methods   only perform binary classification in image media, not to   mention manipulation grounding across multi-modalities.   Multi-Modal Misinformation Detection.   Several existing works study the detection of multi-modal misinformation [1, 2, 13, 15, 25, 49]. Some of them deal with a smallscale human-generated multi-modal fake news [13,15,49],   while others address out-of-context misinformation where   a real image is paired with another swapped text without image and text manipulation [1, 2, 25].   All of these   methods only perform binary classification based on simple   image-text correlation. In contrast, DGM4 studies largescale machine-generated multi-media manipulation, which   is closer to broad misinformation on the web in practice.   Additionally, DGM4 requires not only manipulation detection for binary classification, but also manipulation grounding with more interpretation for multi-modal manipulation.   3. Multi-Modal Media Manipulation Dataset   Most of existing misinformation datasets focus on   single-modal image forgery [7, 12, 20, 34] or text forgery   [44, 48, 53].   Some multi-modal datasets are built, but   they usually contain a small amount of human-generated   fake news [5, 13] or out-of-context pairs [2, 25] for binary   forgery detection. To better facilitate the proposed novel research problem, we present DGM4 dataset, studying largescale machine-generated multi-modal media manipulation.   DGM4 dataset is constructed with diverse manipulation   techniques on both Image and Text modality. All samples   are annotated with rich, fine-grained labels that enable both   Detection and Grounding of media manipulation.   3.1. Source Data Collection   Among all forms of multi-modal media, we specifically   focus on human-centric news, in consideration of its great   public influence. We thus develop our dataset based on the   VisualNews dataset [22], which collects numerous imagetext pairs from real-world news sources (The Guardian,   BBC, USA TODAY, and The Washington Post). To formulate a human-centric scenario with meaningful context, we   further conduct data filtering on both image and text modality, and only keep the appropriate pairs to form the source   pool O = {po|po = (Io, To)} for manipulation.   3.2. Multi-Modal Media Manipulation   We employ two types of harmful manipulations on both   image and text modality. \u2018Swap\u2019 type is designed to include   relatively global manipulation traces, while \u2018Attribute\u2019 type   introduces more fine-grained local manipulations. The manipulated images and texts are then randomly mixed with   pristine samples to form a total of 8 fake plus one original   manipulation classes. Distribution of manipulation classes   and some samples are displayed in Fig. 2 (a).   Face Swap (FS) Manipulation. In this manipulation type,   the identity of the main character is attacked by swapping   his/her face with another person. We adopt two representative face swap approaches, SimSwap [3] and InfoSwap [9].   For each original image Io, we choose one of the two approach to swap the largest face If   o with a random source   face If   celeb from CelebA-HQ dataset [14], producing a face   swap manipulation sample Is. The MTCNN bbox of the   swapped face ybox = {x1, y1, x2, y2} is then saved as annotation for grounding.   Face Attribute (FA) Manipulation.   As a more finegrained image manipulation scenario, face attribute manipulation attempts to manipulate the emotion of the main character\u2019s face while preserving the identity. For example, if   the original face is smiling, we deliberately edit it to the opposite emotion, e.g., an angry face. To achieve this, we first   predict the original facial expression of the aligned face If   o   with a CNN-based network, then edit the face towards the   opposite emotion using GAN-based methods, HFGI [47]   and StyleCLIP [28]. After obtaining the manipulated face   If   emo, we re-render it back to the original image Io to obtain   the manipulated sample Ia. Bbox ybox is also provided.   Text Swap (TS) Manipulation. In this scenario, the text is   manipulated by altering its overall semantic while preserving words regarding main character. Given an original caption To, we use Named Entity Recognition (NER) model to   extract the person\u2019s name as query \u2018PER\u2019. Then we retrieve   a different text sample T \u2032   o containing same \u2018PER\u2019 entity   from the source corpus O. T \u2032   o is then selected as the manipulated text Ts. Note that we compute the semantic embedding of each text using Sentence-BERT [32] and only accept   T \u2032   o that has low cosine similarity with To. This ensures the   retrieved text is not semantically aligned with To, so that the   text semantic regarding the main character in the obtained   pair pm = (Io, Ts) is manipulated. After that, given M   text tokens in Ts, we annotate them with a M-dimensional   one-hot vector ytok = {yi}M   i=1, where yi \u2208{0, 1} denotes   whether the i-th token in Ts is manipulated or not.   Text Attribute (TA) Manipulation. Although news is a   relatively objective media form, we observe that a considerable portion of news samples po \u2208O still carry sentiment bias within the text To, as depicted in Fig. 2 (d). The   malicious manipulation of text attributes, especially its sentiment tendency, could be more harmful and also harder   to be detected as it causes less cross-modal inconsistency   than text swap manipulation. To reflect this specific situation, we first use a RoBERTa [24] model to split the cap6906   (a)   (b)   (e)   (d)   (c)   'applauds'   'looks dejected'   'pros cons of getting   remarried'   'explains the best   products to buy...'   Orig   FS   FA   TS   TA   FS\u00a0+\u00a0TS   FS\u00a0+\u00a0TA   FA\u00a0+\u00a0TS   FA + TA   FS   FA   TA   TS   Figure 2. Statistics of DGM4 dataset. (a) Distribution of manipulation classes; (b) manipulated regions of most images are small-size,   especially for face attribute manipulation; (c) manipulated tokens of text attribute manipulation are fewer than text swap manipulation; (d)   distribution of text sentiment scores in the source pool. (e) number of manipulated samples towards each face/text attribute direction.   tions into positive, negative and neutral sentiment corpora:   {O+, O\u2212, Oneu}. Following [45], we replace all sentiment   words of the original text To with the opposite sentiment   text generated by a B-GST model trained on our own corpora {O+, O\u2212}, obtaining Ta. Similar to text swap manipulation, all text tokens are also annotated with ground-truth   vector ytok.   Combination and Perturbation. Once all single-modal   manipulations are finished, we combine the obtained manipulation samples Is, Ia, Ts and Ta with the original   (Io, To) pairs. This forms a multi-modal manipulated media pool with full manipulation types: P = {pm|pm =   (Ix, Ty), x, y \u2208{o, s, a}}. Each pair pm in the pool is provided with a binary label ybin, a fine-grained manipulation   type annotation ymul, aforementioned annotations ybox and   ytok. ybin describes whether the image-text pair pm is real   or fake, and ymul = {yj}4   j=1 is a 4-dimensional vector denoting whether the j-th manipulation type (i.e., FS, FA, TS,   TA) appears in pm. To better reflect the real-world situation   where manipulation traces may be covered up by noise, we   employ random image perturbations on 50% of the media   pool P, such as JPEG compression, Gaussian Blur, etc.   3.3. Dataset Statistics   The overall statistics of DGM4 dataset are illustrated in   Fig. 2 (a). It consists a total of 230k news samples, including 77,426 pristine image-text pairs and 152,574 manipulated pairs. The manipulated pairs contain 66,722 face swap   manipulations, 56,411 face attribute manipulations, 43,546   text swap manipulations and 18,588 text attribute manipulations.   \u223c1/3 of the manipulated images and \u223c1/2 of   the manipulated text are combined together to form 32,693   mixed-manipulation pairs. Since both image and text attributes can be edited towards two opposite sentiment directions, we deliberately keep a balanced proportion to create   an emotionally-balanced dataset, as shown in Fig. 2 (e).   Furthermore, it can be observed from Fig. 2 (b)-(c) that   the manipulated regions of most images and the number   of manipulated text tokens are relatively small. This indicates DGM4 dataset provides a much more challenging   scenario for forgery detection compared to existing deepfake and multi-modal misinformation datasets.   4. HAMMER   To address DGM4, as illustrated in Fig. 3, we propose a HierArchical Multi-modal Manipulation rEasoning   tRansformer (HAMMER), which is composed of two unimodal encoders (i.e., Image Encoder Ev, Text Encoder Et),   Multi-Modal Aggregator F, and dedicated manipulation   detection and grounding heads (i.e., Binary Classifier Cb,   Multi-Label Classifier Cm, BBox Detector Dv, and Token   Detector Dt). All of these uni-modal encoders and multimodal aggregator are built based on transformer-based architecture [46]. As mentioned above, modeling semantic   correlation and capturing semantic inconsistency between   two modalities can facilitate detection and grounding of   multi-modal manipulation. However, there exist two challenges 1) as discussed in Sec. 3.3 and shown in Fig. 2 (b)(c), a large portion of multi-modal manipulations are minor   and subtle, locating in some small-size faces and a few word   tokens and 2) much visual and textual noise [17] exists in   multi-modal media on the web. As a result, some semantic   inconsistencies caused by manipulation may be neglected   or covered by noise. This demands more fine-grained reasoning of multi-modal correlation. To this end, we devise   HAMMER to perform hierarchical manipulation reasoning which explores multi-modal interaction from shallow   to deep levels, along with hierarchical manipulation detection and grounding. In the shallow manipulation reasoning, we carry out semantic alignment between image and   6907   {'forced', 'resign'}   Image Encoder    Text Encoder    BBox Detector   Multi-Modal   Aggregator    Multi-Label Classifier   Token Detector\u00a0   Binary Classifier   ...   Cross-Att   LPAA   Gordon Brown is forced to resign   EU meeting by Nicolas Sarkozy   the French president in Paris.   Real   Fake   Deep Manipulation Reasoning   Shallow Manipulation Reasoning   Self-Att   FFN   FFN   Self-Att   Self-Att   FFN   Cross-Att   FS   FA   TA   TS   Figure 3. Overview of proposed HAMMER. It 1) aligns image and text embeddings through manipulation-aware contrastive learning   between Image Encoder Ev, Text Encoder Et in shallow manipulation reasoning and 2) further aggregates multi-modal embeddings   via modality-aware cross-attention of Multi-Modal Aggregator F in deep manipulation reasoning. Based on the interacted multi-modal   embeddings in different levels, various manipulation detection and grounding heads (Multi-Label Classifier Cm, Binary Classifier Cb,   BBox Detector Dv, and Token Detector Dt) are integrated to perform their tasks hierarchically. Modules with dashed lines mean they are   the corresponding momentum versions of Image Encoder, Text Encoder, Multi-Modal Aggregator and Token Detector, respectively.   text embeddings through Manipulation-Aware Contrastive   Loss LMAC, and conduct manipulated bbox grounding under Image Manipulation Grounding Loss LIMG.   In the   deep manipulation reasoning, based on deeper interacted   multi-modal information generated by Multi-Modal Aggregator, we then detect binary classes with Binary Classification Loss LBIC, fine-grained manipulation types with   Multi-Label Classification Loss LMLC, and ground manipulated text tokens via Text Manipulation Grounding Loss   LT MG. By combing all the above losses, manipulation reasoning is performed hierarchically, contributing to a joint   optimization framework as follows:   L = LMAC + LIMG + LMLC + LBIC + LT MG   (1)   4.1. Shallow Manipulation Reasoning   Given an image-text pair (I, T) \u223cP, we patchify and   encode image I into a sequence of image embeddings via   self-attention layers and feed-forward networks in Image   Encoder as Ev(I) = {vcls, vpat}, where vcls is the embedding of [CLS] token, and vpat = {v1, ..., vN} are embeddings of N corresponding image patches. Text Encoder   extracts a sequence of text embeddings of T as Et(T) =   {tcls, ttok}, where tcls is the embedding of [CLS] token,   and ttok = {t1, ..., tM} are embeddings of M text tokens.   Manipulation-Aware Contrastive Learning. To help two   uni-modal encoders better exploit the semantic correlation   of images and texts, we align image and text embeddings   through cross-modal contrastive learning.   Nevertheless,   some subtle multi-modal manipulations cause minor semantic inconsistency between two modalities, which are   hardly unveiled by normal contrastive learning.   To emphasize the semantic inconsistency caused by manipulations, HAMMER proposes manipulation-aware contrastive   learning on image and text embeddings. Different from normal cross-modal contrastive learning pulling embeddings   of original image-text pairs close while only pushing those   of unmatched pairs apart, manipulation-aware contrastive   learning pushes away embeddings of manipulated pairs as   well so that semantic inconsistency produced by them can   be further emphasized. Following InfoNCE loss [27], we   formulate image-to-text contrastive loss by:   Lv2t(I, T +, T \u2212) = \u2212Ep(I,T )   \"   log   exp(S(I, T +)/\u03c4)   PK   k=1 exp(S(I, T \u2212   k )/\u03c4)   #   (2)   where \u03c4   is a temperature hyper-parameter,   T \u2212   =   {T \u2212   1 , ..., T \u2212   K} is a set of negative text samples that are not   matched to I as well as that belong to manipulated imagetext pairs. Since [CLS] token serves as the semantic representation of the whole image and text, we use two projection   heads hv and \u02c6ht to map [CLS] tokens of two modalities   to a lower-dimensional (256) embedding space for similarity calculation: S(I, T) = hv(vcls)T\u02c6ht(\u02c6tcls). Inspired by   MoCo [11], we learn momentum uni-modal encoders \u02c6Ev,   \u02c6Et (an exponential-moving-average version) and momentum projection heads for two modalities respectively. Two   queues are used to store the most recent K image-text pair   embeddings. Here \u02c6tcls are [CLS] tokens from text momentum encoders and \u02c6ht(\u02c6tcls) means projected text embeddings   from text momentum projection head. Similarly, text-toimage contrastive loss is as follows:   Lt2v(T, I+, I\u2212) = \u2212Ep(I,T )   \"   log   exp(S(T, I+)/\u03c4)   PK   k=1 exp(S(T, I\u2212   k )/\u03c4)   #   (3)   where I\u2212= {I\u2212   1 , ..., I\u2212   K} is a queue of K recent negative image samples that are not matched to T as well as   that belong to manipulated image-text pairs. S(T, I) =   ht(tcls)T\u02c6hv(\u02c6vcls). Inspired by [51], to maintain reasonable   semantic relation within each single modality, we further   carry out intra-modal contrastive learning in both modalities. We incorporate all the losses to form ManipulationAware Contrastive Loss as follows:   LMAC = 1   4 [Lv2t(I, T +, T \u2212) + Lt2v(T, I+, I\u2212)+   Lv2v(I, I+, I\u2212) + Lt2t(T, T +, T \u2212)]   (4)   6908   Manipulated Image Bounding Box Grounding. As mentioned above, FS or FA swaps identities or edits attributes of   faces in images. This alters their correlation to corresponding texts in terms of persons\u2019 names or emotions. Given   this, we argue that the manipulated image region could   be located by finding local patches that have inconsistencies with the text embeddings. In this regard, we perform   cross-attention between image and text embeddings to obtain patch embeddings that contain image-text correlation.   Attention function [46] is performed on normalized query   (Q), key (K), and value (V ) features as:   Attention(Q, K, V ) = Softmax(KT Q/   \u221a   D)V   (5)   Here we cross-attend the image embedding with text embedding, by treating Q as image embedding, K and V as   text embedding as follows:   Uv(I) = Attention(Ev(I), Et(T), Et(T)) + Ev(I)   (6)   where Uv(I) = {ucls, upat}. upat = {u1, ..., uN} are N   image patch embeddings interacted with text information.   Unlike [CLS] token ucls, the patch tokens upat are generated with position encoding [46]. This means they possess richer local spatial information and thus are more suitable for manipulated image bbox grounding. Based on this   analysis, we propose Local Patch Attentional Aggregation   (LPAA) to aggregate the spatial information in upat via an   attentional mechanism. This aggregation is performed by   cross-attending a [AGG] token with upat as follows:   uagg = Attention([AGG], upat, upat)   (7)   Different from previous work [54] directly using [CLS] token for bbox prediction, we perform the manipulated bbox   grounding based on the attentionally aggregated embedding uagg. Specifically, we input uagg into BBox Detector   Dv and calculate Image Manipulation Grounding Loss by   combing normal \u21131 loss and generalized Intersection over   Union (IoU) loss [33] as follows:   LIMG =E(I,T )\u223cP [\u2225Sigmoid(Dv(uagg)) \u2212ybox\u2225   + LIoU(Sigmoid(Dv(uagg)) \u2212ybox)]   (8)   4.2. Deep Manipulation Reasoning   Manipulated token grounding is a tougher task than manipulated bbox grounding as it requires deeper analysis and   reasoning on the correlation between images and texts. For   example, as illustrated in Fig. 3, we are able to detect the   manipulated tokens in T, i.e., \u2018force\u2019 and \u2018resign\u2019, only   when we are aware of such negative words mismatching   the positive emotion (i.e., smiling faces) in I. Besides, we   need to summarize multi-modal information to detect finegrained manipulation types and binary classes. This demands a comprehensive information summary at this stage.   To this end, we propose deep manipulation reasoning.   Manipulated Text Token Grounding. To model deeper   multi-modal interaction, as depicted in Fig. 3, we propose modality-aware cross-attention to further lead text embedding Et(T) to interact with image embedding Ev(I)   through multiple cross-attention layers in Multi-Modal Aggregator F.   This generates aggregated multi-modal embedding F(Ev(I), Et(T)) = {mcls, mtok}. In particular,   mtok = {m1, ..., mM} represent the deeper aggregated embeddings corresponding to each token in T. At this stage,   each token in T has passed through multiple self-attention   layers in Et and cross-attention layers in F. In this way,   each token embedding in mtok not only entirely explores   the context information of text, but also fully interacts with   image features, which fits manipulated text tokens grounding. Moreover, grounding manipulated tokens is equal to   labeling each token as real or fake. This is similar to sequence tagging task in NLP. Notably, unlike existing sequence tagging task mainly studied in text modality, manipulated text token grounding here can be regarded as a novel   multi-modal sequence tagging since each token is interacted   with two modality information. In this case, we use a Token   Detector Dt to predict the label of each token in mtok and   calculate cross-entropy loss as follows:   Ltok = E(I,T )\u223cP H(Dt(mtok), ytok)   (9)   where H(\u00b7) is the cross-entropy function. As mentioned,   news on the web is usually noisy with texts unrelated to   paired images [17]. To alleviate over-fitting to noisy texts,   as shown in Fig. 3, we further learn momentum versions for   Multi-Modal Aggregator and Token Detector, respectively,   denoted as \u02c6F and \u02c6Dt. We can obtain the multi-modal embedding from momentum modules as \u02c6F( \u02c6Ev(I), \u02c6Et(T)) =   { \u02c6mcls, \u02c6mtok}. Based on this, momentum Token Detector   generates soft pseudo-labels to modulate the original token   prediction, by calculating the KL-Divergence as follows:   Lmom   tok   = E(I,T )\u223cP KL   h   Dt(mtok)\u2225\u02c6Dt( \u02c6mtok)   i   (10)   The final Text Manipulation Grounding Loss is a weighted   combination as follows:   LT MG = (1 \u2212\u03b1)Ltok + \u03b1Lmom   tok   (11)   Fine-Grained Manipulation Type Detection and Binary   Classification.   Unlike current forgery detection works   mainly performing real/fake binary classification, we expect our model to provide more interpretation for manipulation detection.   As mentioned in Sec. 3.2, two image   and two text manipulation approaches are introduced in   DGM4 dataset. Given this, we aim to further detect four   fine-grained manipulation types. As different manipulation   types could appear in one image-text pair simultaneously,   we treat this task as a specific multi-modal multi-label classification. Since [CLS] token mcls aggregates multi-modal   information after modality-aware cross-attention, it can be   utilized as a comprehensive summary of manipulation characteristics. We thus concatenate a Multi-Label Classifier   Cm on top of it to calculate Multi-Label Classification Loss:   LMLC = E(I,T )\u223cP H(Cm(mcls), ymul)   (12)   Naturally, we also conduct a normal binary classification   based on mcls as follows:   LBIC = E(I,T )\u223cP H(Cb(mcls), ybin)   (13)   6909   Table 2. Comparison of multi-modal learning methods for DGM4.   Categories   Binary Cls   Multi-Label Cls   Image Grounding   Text Grounding   Methods   AUC   EER   ACC   mAP   CF1   OF1   IoUmean   IoU50   IoU75   Precision   Recall   F1   CLIP [30]   83.22   24.61   76.40   66.00   59.52   62.31   49.51   50.03   38.79   58.12   22.11   32.03   ViLT [16]   85.16   22.88   78.38   72.37   66.14   66.00   59.32   65.18   48.10   66.48   49.88   57.00   Ours   93.19   14.10   86.39   86.22   79.37   80.37   76.45   83.75   76.06   75.01   68.02   71.35   Table 3. Comparison of deepfake detection methods for DGM4.   Categories   Binary Cls   Image Grounding   Methods   AUC   EER   ACC   IoUmean   IoU50   IoU75   TS [26]   91.80   17.11   82.89   72.85   79.12   74.06   MAT [55]   91.31   17.65   82.36   72.88   78.98   74.70   Ours   94.40   13.18   86.80   75.69   82.93   75.65   Table 4. Comparison of sequence tagging methods for DGM4.   Categories   Binary Cls   Text Grounding   Methods   AUC   EER   ACC   Precision   Recall   F1   BERT [6]   80.82   28.02   68.98   41.39   63.85   50.23   LUKE [50]   81.39   27.88   76.18   50.52   37.93   43.33   Ours   93.44   13.83   87.39   70.90   73.30   72.08   Table 5. Ablation study of image modality.   Categories   Binary Cls   Image Grounding   Methods   AUC   EER   ACC   IoUmean   IoU50   IoU75   Ours-Image   93.96   13.83   86.13   75.58   82.44   75.80   Ours   94.40   13.18   86.80   75.69   82.93   75.65   Table 6. Ablation study of text modality.   Categories   Binary Cls   Text Grounding   Methods   AUC   EER   ACC   Precision   Recall   F1   Ours-Text   75.67   32.46   72.17   42.99   33.68   37.77   Ours   93.44   13.83   87.39   70.90   73.30   72.08   5. Experiments   Please refer to appendix for implementation details and   rigorous setup of evaluation metrics.   5.1. Benchmark for DGM4   Comparison with multi-modal learning methods.   We   adapt two SOTA multi-modal learning methods to DGM4   setting for comparison. Specifically, CLIP [30] is one of   the most popular dual-stream approaches where two modalities are not concatenated at the input level. For adaptation,   we make outputs of two streams interact with each other   through cross-attention layers.   Detection and grounding   heads are further integrated on top of them. In addition,   ViLT [16] is a representative single-stream approach where   cross-modal interaction layers are operated on a concatenation of image and text inputs. We also adapt it by concatenating detection and grounding heads on corresponding   outputs of the model. We tabulate comparison results in   Table 2. The results show that the proposed method significantly outperforms both baselines in terms of all evaluation   metrics. This demonstrates that hierarchical manipulation   reasoning is more able to accurately and comprehensively   model the correlation between images and texts and capture   semantically inconsistency caused by manipulation, contributing to better manipulation detection and grounding.   Comparison with deepfake detection and sequence tagging methods. We compare our method with competitive   uni-modal methods in two single-modal forgery data splits,   respectively. For a fair comparison, in addition to the original ground-truth regarding binary classification, we further integrate manipulation grounding heads into uni-modal   models with corresponding annotations of grounding. For   image modality, we tabulate the comparison with two SOTA   deepfake detection methods in Table 3. For text modality,   we compare two widely-used sequence tagging methods in   NLP to ground manipulated tokens along with binary classification. We report the comparison results in Table 4. Tables 3 and 4 show that HAMMER performs better than   uni-modal methods for single-modal forgery detection by a   large margin. This indicates our method trained with multimodal media also achieves promising manipulation detection and grounding performance in each single modality.   5.2. Experimental Analysis   Ablation study of two modalities. To validate the importance of multi-modal correlation for our model, we perform   ablation study by only keeping corresponding input and network components with respect to image (Ours-Image) or   text (Ours-Text) modality. We tabulate results in Tables 5   and 6, showing the performance of complete version of our   model surpasses its ablated parts, especially in text modality. This suggests the performance degrades once one of   the two modalities is missing without cross-modal interaction. This is to say, through exploiting correlation between   two modalities via our model, more complementary information between them can be dug out to promote our task.   Particularly, this correlation is more essential for manipulation detection and grounding in text modality.   Ablation study of losses. The considered losses and corresponding results obtained for each case are tabulated in Table 7. As evident from Table 7, removing the task-general   loss, i.e., LMAC, nearly all the performance degenerates.   This implies manipulation-aware contrastive learning is indispensable for our task. After getting rid of any one of taskspecific losses, i.e., LMLC, LIMG and LT MG, not only the   performance of the corresponding task degrades dramatically, but also the overall binary classification performance   probably becomes lower. Comparatively, our model with   the complete loss function obtains the best performance in   most of cases, indicating the effectiveness and complementarity of all losses. In particular, the first row of Table 7   represents the current multi-modal misinformation detection scenario where only LBIC is used. Our method substantially outperforms this baseline on binary classification,   6910   Table 7. Ablation study of losses in the proposed method.   Losses   Binary Cls   Multi-Label Cls   Image Grounding   Text Grounding   BIC   MLC   MAC   IMG   TMG   AUC   EER   ACC   mAP   CF1   OF1   IoUmean   IoU50   IoU75   Precision   Recall   F1   \"   91.04   16.91   83.81   20.79   33.84   33.48   4.81   0.33   0.00   15.95   78.70   26.53   \"   \"   \"   \"   91.74   16.08   84.39   27.22   30.81   27.62   74.05   81.34   72.59   74.30   66.84   70.37   \"   \"   \"   \"   92.77   14.53   86.01   85.52   79.09   79.86   75.98   83.37   75.25   77.82   61.83   68.91   \"   \"   \"   \"   93.21   14.30   86.28   86.29   79.37   80.32   4.69   0.17   0.00   75.72   67.44   71.34   \"   \"   \"   \"   92.99   14.62   86.15   86.06   79.06   79.93   76.51   83.73   76.05   13.93   58.87   22.53   \"   \"   \"   \"   \"   93.19   14.10   86.39   86.22   79.37   80.37   76.45   83.75   76.06   75.01   68.02   71.35   Figure 4. Efficacy of local patch   attentional aggregation (LPAA).   Figure 5. Performance of   each manipulation type.   implying more manipulation grounding tasks in DGM4 facilitate binary classification as well.   Efficacy of LPAA. Regarding manipulated bbox grounding, we compare the usage of [CLS] token [54] with proposed LPAA in Fig. 4. Fig. 4 shows LPAA yields better   performance under all metrics, verifying its efficacy.   Details of manipulation type detection. We plot the classification performance of each manipulation type based on   the output of Multi-Label Classifier in Fig. 5. The results   deliver more interpretation that text manipulation detection   is harder than image modality and TA is the hardest case.   Visualization of manipulation detection and grounding.   We provide some visualized results of manipulation detection and grounding in Fig. 6. Fig. 6 (a)-(b) show our method   can accurately ground manipulated bboxes and detect correct manipulation types for both FA and FS. Furthermore,   most of the manipulated text tokens in TS and all of those   in TA are successfully grounded in Fig. 6 (c)-(d). All of   them visually verify effective manipulation detection and   grounding can be achieved by HAMMER.   Visualization of attention map. We provide Grad-CAM   visualizations of our model regarding manipulated text tokens in Fig. 7. Fig. 7 (a) shows our model pays attention to   surroundings of the character in image. These surroundings   indicate the character is giving a speech, which is semantically distinct from text tokens manipulated by TS. As for   TA, Fig. 7 (b) shows the per-word visualization with respect   to the manipulated word (\u2018mourn\u2019). It implies our model   focuses on the smiling face in image that is semantically   inconsistent to the sad sentiment expressed from the manipulated word (\u2018mourn\u2019). These samples prove our model can   indeed capture the semantic inconsistency between images   and texts to tackle DGM4.   Caroline Flint trebled her majority in Don Valley   in the general election.   (a) GT: Fake-FS, Pred: Fake-FS   Thousands gathered in Liverpool on Wednesday   evening for a vigil to remember the victims.   (b) GT: Fake-FA, Pred: Fake-FA   French minister of the interior Bernard Cazeneuve   and his British counterpart Theresa May shake hands   at the Eurotunnel terminal in Calais.   (c) GT: Fake-TS, Pred: Fake-TS   BJP supporters protest against the party\u2019s attack   outside their office in Gauhati India.   (d) GT: Fake-TA, Pred: Fake-TA   Figure 6. Visualization of detection and grounding results. Ground   truth annotations are in red, and prediction results are in blue.   Airbus employees throw flower petals to welcome   Nick Clegg to their facility in Bangalore India.   (a) Attention map in TS   Kevin Garnett the head of teammate KarlAnthony   Towns cries as they mourn during the closing   moments of a win on Nov 27 2015.   (b) Attention map in TA   Figure 7. Grad-CAM visualizations on manipulated text tokens.   6. Conclusion   This paper studies a novel DGM4 problem, aiming to detect and ground multi-modal manipulations. We construct   the first large-scale DGM4 dataset with rich annotations. A   powerful model HAMMER is proposed and extensive experiments are performed to demonstrate its effectiveness.   Acknowledgements   This study is supported by the Ministry of Education,   Singapore, under its MOE AcRF Tier 2 (MOE-T2EP202210012), NTU NAP, and under the RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP)   Funding Initiative, as well as cash and in-kind contribution   from the industry partner(s).   6911", "conf": "CVPR", "year": "2023", "index": 326}, {"title": "HT-Step: Aligning Instructional   Articles with How-To Videos   Triantafyllos Afouras\u2217   Effrosyni Mavroudi\u2217   Tushar Nagarajan   Huiyu Wang   Lorenzo Torresani   FAIR, Meta   {afourast, emavroudi, tusharn, huiyuw, torresani}@meta.com", "abstract": "We introduce HT-Step, a large-scale dataset containing temporal annotations of   instructional article steps in cooking videos. It includes 116k segment-level annotations over 20k narrated videos (approximately 2.1k hours) of the HowTo100M   dataset. Each annotation provides a temporal interval, and a categorical step label from a taxonomy of 4, 958 unique steps automatically mined from wikiHow   articles which include rich descriptions of each step. Our dataset significantly   surpasses existing labeled step datasets in terms of scale, number of tasks, and   richness of natural language step descriptions. Based on these annotations, we   introduce a strongly supervised benchmark for aligning instructional articles with   how-to videos and present a comprehensive evaluation of baseline methods for   this task. By publicly releasing these annotations and defining rigorous evaluation   protocols and metrics, we hope to significantly accelerate research in the field of   procedural activity understanding.   1. In a large mixing bowl, whisk together    the flour, salt, and baking soda. Mixing    your dry ingredients together separately    ensures that \u2026   2. Beat the butter in a separate bowl until    it becomes light and fluffy. You can do    this using a whisk or an electric mixer.   3. Beat the white and brown sugar for    about 2 minutes. You might find it easier    to mix if you add the sugar in a little bit at    a time \u2026.   4. Whisk in the vanilla extract and the    milk. Keep whisking until everything is    evenly combined.   5. Beat in the flour mixture. At this point,    the mixture will start to stick together to    form a ball. If you were using a whisk, \u2026   66:00   86:00   90:00   104:00   106:00 125:00   Is full match? Yes   Is full match? No   130:00 140:00   Non-groundable    steps:   Step 2   Step 4   How-To Video   time   Figure 1: HT-Step is a dataset of annotations aimed towards instructional video-article alignment.   Given an untrimmed how-to video and an instructional article containing a list of step descriptions, the   goal is to temporally localize the steps shown in the video and reject the ones not demonstrated. Our   dataset includes over 116k segment-level annotations for this task, as well as labels indicating whether   a temporal segment is a full or a partial match to the step, i.e., whether the segment demonstrates all   substeps and ingredients mentioned in the step headline (shown in bold).   \u2217equal contribution   37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.   1", "content": "Instructional videos have become a popular way to learn or improve our skills, providing an entertaining and dynamic alternative to traditional written manuals. For example, how-to videos are listed   as one of the top-four watched categories on the YouTube platform [1], where viewers can access   narrated visual demonstrations for a wide range of activities, from cooking to crafts and DIY projects.   In addition to being a valuable learning medium for humans, how-to videos are a great source of   training data for computer vision models [6, 25, 30]. Since many of these instructional videos focus   on complex activities entailing a sequence of steps, these repositories offer great potential for learning   task graphs [47] and training procedural activity models [42, 50]. However, although the videos are   often accompanied by informative closed captions or ASR transcriptions, they lack structured step   annotations that clearly denote which video segments correspond to what steps of the activity.   An alternative source of information for acquiring new skills is instructional article collections, such   as wikiHow [20]. These typically provide a list of procedural steps and have the advantage of being   more comprehensive, incorporating variations and detailed explanations. Although they often contain   still-pictures or graphic illustrations, they lack the dynamic demonstrations offered by videos.   In this work we introduce HT-Step, the largest existing dataset with labeled step segments on   instructional videos. We leverage wikiHow articles [20] to guide the annotation on a large subset   of the open-sourced HowTo100M dataset [31]. These cross-dataset associations provide us with   a taxonomy of steps well fitting the data and with an automatic mechanism to identify the set of   potential steps represented in each individual video. Through a meticulous process of manual labeling   by professional annotators we obtain 116k temporal step labels over 20k videos (approximately 2.1k   hours). Our benchmark significantly surpasses the largest existing datasets of labeled steps [41, 50]   in procedural videos along multiple axes: scale (2.6\u00d7 the number of segments annotated, 4.8\u00d7 the   number of video hours), taxonomy granularity (7\u00d7 the number of steps), and scope (2\u00d7 the number   of activities).   We share these annotations with the research community and present a benchmark for the task   of temporal article grounding. Given as input a how-to video and an instructional article listing   a sequence of potential steps, the objective is to temporally ground the subset of steps that are   demonstrated in the video and to detect outliers (i.e., the steps not executed). We note that this   task differs from traditional temporal grounding task which require localizing a single query that is   known to be present in the video. Instead, in our problem setting the system is given the complete   sequence of potential steps. We believe that this formulation may encourage the design of methods   that can perform global reasoning over all the steps and take full advantage of ordering constraints   and sequence priors to disambiguate the grounding. Furthermore, on average about 58% of the steps   listed in the associated wikiHow article are not demonstrated in the given video. This implies that   the model must have strong capability to recognize the steps that are not groundable. Finally, we   include in our benchmark a test set over unseen tasks, i.e., activities that are not represented in the   training set. Traditional activity detection methods requiring a taxonomy known a priori cannot   generalize to unseen activities. By introducing this setting we hope to promote the development of   language-based temporal alignment models that can learn to ground the rich textual descriptions of   steps in instructional articles even for never-seen tasks.   In summary, our contributions are the following: (1) We introduce a large collection of step annotations on videos from the popular HowTo100M dataset. (2) We propose a scalable pipeline for   annotating procedural videos that leverages an automatic strategy for associating potential steps to   video and for inferring the step taxonomy. (3) We introduce splits, evaluation protocols and metrics   that evaluate the ability of models to predict the temporal extent of steps in videos, and reject steps   that are not visually groundable. (4) We train and evaluate state-of-the-art approaches and baselines   that tackle our task from three different angles: activity detection, single-sentence temporal grounding,   and temporal article grounding. (5) We discuss properties and unique aspects of HT-Step through   comprehensive analyses and experiments.   2   Related Work   Procedural and Instructional Video Datasets. Procedural videos portray humans performing   sequential steps in some constrained yet non-unique order, with the aim of achieving a specific   goal, such as preparing a dish. Understanding the content of such videos has been an active area of   2   Table 1: Comparison between HT-Step and existing procedural video understanding (top), temporal   grounding (middle), and instructional video (bottom) datasets. Note that HT-Step seen validation and   test sets introduced in [28] are subsumed by HT-Step.   Dataset   Duration (h)   # Videos   # Segments   # Activities   Domain   Procedural Activity Datasets   MPII [35]   10   44   5.6k   cooking   50Salads [40]   4.5   50   899   cooking   TACoS [34]   \u223c600   127   18.8k   cooking   Breakfast [22]   77   2k   8.5k   10   cooking   Ikea-FA [16]   4   101   1.9k   1   furniture   EPIC-KITCHENS [9]   100   700   90k   cooking   EgoProcel [3]   62   298   1k   16   multiple   Assembly101 [36]   513   4.3k   1M   101   assembly   Temporal Grounding Datasets   ActivityNet Captions [21]   849   20k   100k   multiple   Charades-STA [14]   \u223c90   10k   18k   157   multiple   QV-highlights [23]   845   10.2   16.1k   multiple   Instructional Video Datasets   YouCook [10]   2   88   cooking   YouCook2 [48]   176   2k   13.8k   cooking   YouwikiHow [8]   47k   1398   multiple   CrossTask [50]   213   2.8k   21k   18   multiple   COIN [41]   476   11.8k   46.4k   180   multiple   HT-Step Seen Val+Test Set [28]   124   1.2k   7k   177   cooking   HT-Step (Ours, Full)   2.1k   19.7k   116k   433   cooking   HT-Step splits   HT-Step Train   1.9k   17.4k   103k   401   cooking   HT-Step Val seen   64   600   3.4k   120   cooking   HT-Step Test seen (S1)   61   600   3.6k   120   cooking   HT-Step Test unseen (S2)   116   1000   5.7k   32   cooking   research over the last decade [2, 6, 11, 13, 22, 34, 37, 47], largely driven by the release of benchmark   datasets [22, 41, 50]. Early datasets such as Breakfast [22] and 50Salads [40] are manually recorded,   small-scale (spanning at most 100 hours) and limited in activity diversity (e.g., covering only   breakfast recipes [22] or IKEA furniture assembly [16]). More recent datasets, such as COIN [41]   and CrossTask [50], have capitalized on public video collections, including videos sourced from   YouTube, in order to substantially increase dataset scale and activity diversity. A key characteristic of   these videos is that they are instructional, i.e., they typically involve an individual teaching a complex   task by narrating the sequence of steps.   Our HT-Step dataset also provides temporal annotations for steps of instructional videos. However,   it surpasses existing datasets in terms of scale and activity diversity, as the steps are sourced from   real-world instructional articles (wikiHow [20]) rather than being manually defined. This approach   leads to much richer step descriptions, including a paragraph providing detailed instructions. Despite   its exclusive focus on the cooking domain, HT-Step is significantly larger in scale and step description   complexity. Our work significantly expands the small-scale val/test set that was recently released [28]   for evaluating step grounding approaches trained without annotated temporal segments. While the   focus of that work was the design of weakly supervised methods leveraging narrations as \u201cfree\u201d signal   for step-to-video alignment, here we share the first large-scale dataset enabling strongly-supervised   training of methods for temporal article grounding. Our annotations over a large-scale training set, and   a new test set of unseen tasks (not included in the training set) provide almost 20\u00d7 more annotated   videos and number of segments. We also include additional metadata per step (indicating full or   partial segment-step matches). Furthermore, we include a comprehensive experimental evaluation of   strongly supervised baselines on this task.   Instructional Video Understanding There exist various tasks related to instructional video understanding, such as step classification [25], step detection [41, 46], action segmentation [13],   captioning [49], video retrieval [15], visual object grounding [29], and temporal grounding [26].   In this paper, we focus on the recently-introduced task of temporal article grounding, which has   been tackled only with weakly-supervised approaches so far [8, 28]. In particular Chen et al. [8] is   the first work to train and evaluate models for the temporal article grounding task. However this   work was limited to weakly-supervised training due to the lack of a strongly labeled dataset, and   used CrossTask as a proxy dataset for evaluation, with noisy mapping between article steps and   CrossTask labels. In contrast, we introduce a large amount of strong temporal segment annotations   3   \u201cPlace an apple onto    each square piece.\u201d   Make Apple Dumplings   \u201cPlace the dumplings on    the prepared baking tray.\u201d   \u201cSlowly pour the sauce    over the dumplings.\u201d   \u201cCoat the chicken with \u2028   salt and pepper.\u201d   Make Chicken Piccata   \"Dredge the chicken    in  \ufb02our, coating    completely.\u201d   \u201cCook the chicken on    both sides for 3-4    minutes.\u201d   \u201cAdd the butter once the water\u2028   evaporates and small bubbles\u2028   form in the pan.\u201d   Make Crepes Suzette   \u201cFold the crepes into quarters\u2028   and add them to the skillet to\u2028   \u201cheat them.\u201d   \u201cMove the crepes with the    wooden spatula to help    incorporate the sauce while    heating the crepes.\u201d   \u201cAdd the onions to the pan   and  saut\u00e9 them until    golden brown.\u201d   Make Naadan Kadala Curry   \u201cAdd the tomatoes and water,    Then put on the pan and let    cook or 10 minutes.\u201d   \u201cPut in the cooker chickpeas   Into the gravy and stir well.\u201d   Step headlines   Step headlines   Figure 2: Sample frames for 12 videos spanning 4 tasks from the HT-Step training set. For each   video, we show relevant frames for three steps. By annotating multiple videos of the same task, we   obtain multiple instances of each step, which have high variations in appearance and viewpoints.   for training, and propose a metric that evaluates both the recall and precision of models, and is thus   better suited to the task of temporal article grounding which involves multiple ungroundable steps.   The temporal article grounding task is closely related to two other grounding tasks: (a) temporal   grounding of a single natural language query [14, 23] (also known as moment retrieval) and (b) video   paragraph grounding [4]. Benchmarks [14, 21, 23] for these tasks typically consist of video captions   or questions that are loosely related to each other and which were obtained by human annotators   based on the video contents. Thus, most approaches assume that each of the queries is groundable in   the video. For example, video paragraph grounding aims at predicting a single temporal segment for   each sentence of the paragraph. Instead, in our HT-Step benchmark the step queries originate from   wikiHow articles, which might contain steps that are not shown in the video (non-groundable steps)   or steps that are partially executed (partial matches). This makes it challenging to adapt temporal   grounding approaches for temporal article grounding in our benchmark.   3   HT-Step   The creation of HT-Step involved pairing videos from HowTo100M with instructional articles from   wikiHow and then annotating segments in each video with steps of the associated article. wikiHow2   is an online knowledge base that houses thousands of articles with step-by-step instructions for a   large variety of activities. Every article is typically structured as a series of steps, each step anchored   by a descriptive headline and further supplemented with a detailed paragraph. HowTo100M, on   the other hand, contains an extensive compilation of instructional videos that were collected using   titles of wikiHow articles as search keywords for a subset of activities deemed to contain visual   demonstrations. As a result of this sourcing procedure, each video in HowTo100M comes with a task   label that corresponds to a specific wikiHow article. These pairings create a natural opportunity for   annotating the HowTo100M videos with the step labels of the corresponding wikiHow articles, which   we exploited for creating HT-Step. Next we describe in detail our annotation process.   3.1   Annotation workflow   Activity selection. HowTo100M spans a variety of domains, including cooking, DIY, crafts, sports,   and gardening. As in previous works [17], we chose to focus on cooking activities as i) they account   for approximately a third of the HowTo100M videos, and ii) they are relatively low-complexity tasks   which can be annotated by non-experts. Using metadata, we selected 495 cooking activities (detailed   list in the Appendix) for which HowTo100M contains at least 70 videos.   Step Taxonomy. We automatically sourced the step descriptions for all the activities from wikiHow.   For every activity, we extracted the steps from the corresponding wikiHow article, forming a step   taxonomy that includes 4, 958 unique steps. We list the full taxonomy in the Appendix.   2https://www.wikihow.com/   4   Video validation. For every video, the annotators were given the corresponding activity title and step   taxonomy. They were then asked to watch the whole video and determine whether it contains the   specified activity. If the video was found not matching the task, it was rejected.   Temporal annotation. During the temporal annotation loop annotators were asked to provide   temporal boundaries for the wikiHow steps represented in all the non-rejected videos. An example   annotation is illustrated in Figure 1.   Task variations. wikiHow articles often list several variations for a given task. For example, the   instructional recipe for \u201cMake Taco Salad\u201d includes three versions: basic, deluxe and vegetarian. For   such activities, we asked annotators to choose the variation that best fits the video and to annotate   only the steps listed in that variation.   Full vs partial matches. We asked the annotators to mark a step as a \u201cfull\u201d match if (i) all the   instructions and ingredients listed in the step headline are shown in the segment and (ii) the step is   brought to full completion. Conversely, \u201cpartial\u201d indicates that some sub-steps in the step headline   are skipped, some ingredients are not used, or that a step is interrupted and continued later.   QA process. To ensure the quality of the annotations, we followed a rigorous multi-stage Quality   Assurance (QA) process. In the first stage, the videos were annotated by a single annotator. These   initial annotations were then reviewed by more experienced annotators, who either approved all the   annotations on a given video (all the marked steps were correct and no steps were missing) or marked   it for re-doing, with specific comments indicating the necessary correction. At the last stage of the QA   process, the annotations that were marked as incorrect were redone by a third set of annotators. Due   to budget constraints, the full QA was enforced on a sample of roughly 13% of the total annotations.   Annotation output. The annotation process took approximately 33k person-hours by 90 professional   human annotators. Overall, 34% of the videos were rejected as not matching the specified activity,   resulting in annotations for 433 of the initial 494 activities. We summarize the statistics of the   resulting annotations and compare to existing datasets in Table 1.   3.2   Training, validation, and test splits   We formed two types of test sets: one for seen activities, i.e., activities that are also included in the   training set, and one for unseen activities, i.e., novel activities not appearing in the training set. For   seen activities we also formed a validation (val) set containing the same activities.   Seen val/test set (S1). We directly adopt the HT-Step validation and test splits [28] as the seen   evaluation sets of our dataset, each containing 600 videos, 5 samples for each of 120 activities.   Unseen test set (S2). To create the unseen test set, we selected 32 activities that do not appear in the   training set or the seen val/test set. We include all the annotations on videos of these activities that   went through the QA review, resulting in a test set of 1000 annotated videos. This set is imbalanced   w.r.t. activities, with the video count per activity ranging from 17 to 70.   Training set. All the other annotated videos are included in the final training set, which contains   more than 17k videos and 100k annotated segments. We summarize the split statistics in the bottom   part of Table 1.   3.3   Properties   Article steps. As previously mentioned, step descriptions in wikiHow articles have two parts: a   headline and a paragraph. The headline is brief and may not reveal enough information for grounding;   the paragraph contains additional details about the execution of the step. Annotators were asked to   localize each step by considering both. This allowed us to obtain annotations where the paragraph   description is essential for grounding the steps. We show some examples in Figure 3.   Composite steps. Many wikiHow steps are composite, i.e., include multiple sub-steps. Two examples   of such steps are shown in Figure 4 (b,c). We introduced the labeling of partial vs full matches   (discussed in Section 3.1) to account for the fact that composite steps are rarely fully represented in   videos or they may be carried out in disjoint time intervals of the video.   Non-groundable steps. Some of the steps listed in the wikiHow article may not be represented   at all in the video. An example is illustrated in Figure 1. Temporal grounding models that are   trained/evaluated on this data must support handling queries with no associated temporal segments.   3.4   Statistics   In this section we provide insightful statistics about the dataset. See Appendix for more.   5   Dice the celery, carrot, and pickle. Use a sharp knife to    carefully dice enough red onion to measure 1/4 cup (40    g). You'll also need to dice enough celery to measure 1/4    cup (55 g) and enough pickle to measure 1/4 cup (35 g).    Transfer the vegetables to the bowl with the mashed    chickpeas.   Make the cake ball dough. Place two-inch (5-cm) cubes    of cake into the food processor and pulse until the cake is    broken up into fine crumbs. Transfer the crumbs to a    mixing bowl. [\u2026] If you don\u2019t have a food processor,    place the chunks of cake into a mixing bowl and mash    them with a fork or break them up with your hands.   Figure 3: Qualitative examples where the paragraph is essential to localize the step. The paragraph   sentences that are visually demonstrated in the video are highlighted in yellow.    (c) Step: Add the broccoli, carrots, and onions.      (a) Step: Fill the mini-Stromboli with meat, cheese, and seasoning.    (d) Step: Cook the chicken on both sides for 3-4 minutes.    (b) Step: Pile on the roast pork.    Figure 4: Examples of composite steps and full/partial matches. Frames annotated as \u201cfull matches\u201d   are shown in green, \u201cpartial matches\u201d are shown in orange. (a) An atomic step, fully matched. (b) A   composite step: it is labeled as a full match as all the ingredients and the substeps mentioned in the   paragraph are shown in the video. (c) Another composite step: here not all the described ingredients   are utilized and thus it is labeled as a \u201cpartial match.\u201d (d) An example of step fragmentation: the step   is executed over two disjoint segments which are consequently labeled as \u201cpartial matches.\u201d   Overview. Table 1 summarizes the statistics of HT-Step. It is larger and has a much richer step   taxonomy than existing datasets. Compared to COIN, it has 4.8\u00d7 the duration, 2.6\u00d7 the number of   annotated segments, 7\u00d7 the taxonomy size (4958 vs 778 unique steps), and 2\u00d7 the number of videos.   Segment duration. According to Figure 5a, the average segment duration is approx. 15 seconds,   with most of the annotated segments at 10 seconds or shorter and a long tail of rare longer segments.   Video coverage. We define video coverage as the percentage of a video that is covered by annotations.   Figure 5b shows the distribution of the average video coverage per activity, with a mean of 22%.   Step coverage. Step coverage denotes the fraction of unique steps from the associated wikiHow   article that are annotated in a video and is on average 42%. Figure 5c shows that even for the activities   with the highest step coverage, many article steps have no relevant temporal segments in each video.   Distribution of segments over activities. Figure 5d shows how the number of annotations is   distributed over the 4, 958 steps. On average we get 24.6 segments per step. Since the choice of   steps to be annotated was determined by the data, several steps have very few training samples, as   indicated by the large concentration in the first bucket. We believe that this property will encourage   the development of methods that can perform zero- or few-shot detection, as well as techniques that   approach the task as language-based temporal grounding as opposed to class-based detection.   Length of text descriptions. We calculate the number of words as a proxy for the complexity of the   step descriptions. In Figure 5e we provide the distribution of this metric averaged over activities, for   HT-Step and COIN [41]. It is clear that HT-Step contains richer text descriptions, even when only the   headlines are used (8.5 words vs 4.9 for COIN on average). When paragraph text is included in this   calculation, the word count increases even further, to 17.1 words on average.   Step ordering variation. The steps in a wikiHow article are listed according to a recommended order   of execution. In order to capture real-world deviations from these canonical orderings, we measure   the Normalized Edit Distance (NED) between the annotated step sequence and the wikiHow listing,   normalized over the length of the annotated sequence in each video. We calculate the mA-NED as the   mean of the average NED per activity. We compute the same metric for the COIN dataset, using the   ordering provided with the taxonomy of this dataset. Steps in our dataset are much more frequently   executed out of order compared to COIN (mA-NED is 0.37 vs 0.27), as shown in Figure 5f. This   presents a much more challenging temporal grounding setting.   6   0   10   20   30   40   50   60   70   80   Segment duration (seconds)   0K   2K   5K   8K   10K   12K   15K   18K   20K   # Segments   Mean: 14.94s   (a) Distribution of segment durations. The average segment duration is about 15 seconds, with most   of the annotated segments at 10 seconds or shorter, and a long tail of   rare longer segments.   0.0   0.1   0.2   0.3   0.4   0.5   0.6   Video coverage   0   5   10   15   20   25   30   35   40   # Activities   Mean: 0.22   (b) Distribution of video coverage,   averaged per activity. Video coverage denotes the percentage of a   video that is covered by step annotations. It exhibits a normal distribution around the mean.   Make Potstickers (Guo Tie)   Make Vegetable Manchurian   Make Crepes Suzette   Make Focaccia   Make a Galaxy Mirror Cake   Make Taho   Make Chicken Piccata   Make Conchas   Make Orange Chicken   Make Planet Cookies   Make Naadan Kadala Curry   Make Saltimbocca   Make Ciabatta Bread   Make Tinga De Pollo   Make King's Cake   Make Beignets   Make Hush Puppies   Make Vegetable Paniyaram   Make Lobster Bisque   Make Nigerian Style Jollof Rice   Activity   0   5   10   15   20   25   # Unique steps   (c) Average number of unique   steps annotated per video for the 20   activities with most unique steps.   0   25   50   75   100 125 150 175 200   # Annotations   0   500   1000   1500   2000   2500   # Steps   Mean: 24.6   (d) Distribution of annotations.   Annotations follow a long-tail distribution which should encourage the   development of zero/few-shot detection or text grounding methods.   0   10   20   30   40   50   60   70   80   # Words   0   200   400   600   800   # Steps   Mean: 17.1 words   Mean: 8.5 words   Mean: 4.9 words   Ours - headl. + par.   Ours - head.   COIN   (e) Comparison of text description   lengths. HT-Step contains long step   descriptions, almost twice as long   as those of COIN, even when only   the headline text is considered.   0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8   NED   0   1   2   3   4   # Activities   Mean: 0.37   Mean: 0.23   Ours   COIN   (f) NED. Step sequences in HT-Step   have a 0.37 NED compared to the   canonical step ordering on average,   which makes it more challenging   compared to previous benchmarks.   Figure 5: HT-Step statistics. We provide various statistics for a qualitative overview of the dataset.   Full vs partial matches. Of the resulting annotated segments, 27% are full and 73% are partial   matches. Some examples of full and partial match annotations are shown in Figure 4.   Variations. Most tasks (335 out of 433) involve a single listing of steps in wikiHow. However there   is a significant number of activities (98) that was annotated according to more than one (and up to 6)   sequences of steps. Refer to Appendix for an overview of the variations distribution.   4   Benchmark & Experiments   Table 2: Article grounding mAP. Comparison of state-of-the-art approaches on our benchmark. TS   denotes the TimeSformer backbone.   Method   Training data   Video Backbone Text Backbone   Seen (S1)   Unseen (S2)   \u2191mAP@IOU   \u2191mAP@IOU   @0.3 @0.5 @0.7 @[0.3-0.7] @0.3 @0.5 @0.7 @[0.3-0.7]   Detection   ActionFormer [46]   HT-Step   TS [5, 25]   32.9   23.6   12.6   23.1   ActionFormer [46]   HT-Step   S3D [30, 44]   35.1   25.6   14.8   25.4   Grounding   UMT [26]   HT-Step   S3D [30, 44]   Word2Vec [32]   8.0   4.0   1.5   4.4   4.9   2.3   0.8   2.6   UMT [26]   HT-Step   TS [5, 25]   CLIP [33]   15.7   8.7   3.2   9.1   9.4   4.9   1.7   5.3   MT+BCE   HT-Step   S3D [30, 44]   Word2Vec [32]   31.5   19.6   8.1   19.7   18.6   10.3   3.7   10.6   MT+BCE   HT-Step   TS [5, 25]   Word2Vec [32]   26.2   16.5   6.0   16.1   13.5   6.8   2.5   7.4   ActionFormer-T   HT-Step   S3D [30, 44]   Word2Vec [32]   27.8   20.0   10.6   19.6   10.8   6.4   3.1   6.6   ActionFormer-T   HT-Step   S3D [30, 44]   MPNet [39]   36.9   26.5   15.3   26.3   27.4   18.3   9.3   18.4   VINA [28]   HT100M   S3D [30, 44]   Word2Vec [32]   12.6   4.7   1.2   5.9   8.2   3.1   0.6   3.8   MT+BCE (VINA) HT100M+HT-Step   S3D [30, 44]   Word2Vec [32]   46.2   29.9   12.9   29.8   31.6   18.7   7.7   19.3   ActionFormer-T   HT100M+HT-Step   S3D [30, 44]   MPNet [39]   41.2   30.8   18.3   30.2   29.7   20.3   10.7   20.4   7   Task definition. The annotated data of HT-Step can be used to train and evaluate models for a plethora   of tasks, from temporal article grounding to task-graph-based step localization [47], narration-based   step grounding [28], captioning[21], and step forecasting [25]. In this paper, we focus on the temporal   article grounding task: given an untrimmed video and an instructional article containing a sequence   of steps, a model must predict temporal segments for the steps demonstrated in the video, along with   a confidence score for every segment.   Evaluation protocol. For evaluating models, we introduce the article-grounding mAP \u2013 a variant of   the popular mean Average Precision (mAP) metric from object detection [12, 24]. AP is separately   computed for each activity by treating all steps within the activity as class-agnostic text queries. The   per-activity APs are then averaged to obtain the article-grounding mAP. Following prior work [18],   we evaluate AP under different temporal Intersection-over-Union (tIoU) thresholds.   4.1   Results   We evaluate state-of-the-art approaches and baselines from step detection, single-step grounding, and   multi-step grounding on our task. Full architecture, training, and implementation details can be found   in Appendix.   ActionFormer [46] is a state-of-the-art, multi-scale action detection model that operates on a fixed   taxonomy of steps (i.e., it does not use language), and as a result, can only detect seen steps (S1 split).   ActionFormer-T is a variant of the ActionFormer model which we adapted to enable open-vocabulary   step detection (i.e. unseen step grounding). It is constructed by replacing the classification head with   a dot product between the textual embeddings of every step and the visual embeddings obtained from   the temporal feature pyramid.   UMT [26] is a transformer-based model built for joint moment retrieval and highlight detection, and   represents the state-of-the-art in single-step grounding.   VINA [28] is a model for temporal article grounding. Unlike traditional single-step grounding models,   it can capture step ordering and the relationship between steps, and was trained with weak supervision   from ASR transcripts and video-article pairs.   MT+BCE is our modification of VINA for fully-supervised temporal article grounding. It embeds the   full sequence of step headlines along with the video clips, and feeds them together to a multimodal   transformer. We train it with a per-frame binary cross entropy loss (BCE) and produce segment   predictions using a 1D blob detection routine [43].   Baselines comparison. Table 2 compares the performance of the aforementioned models for different   choices of visual and textual backbones on the seen and unseen article test splits. Among all models   trained from scratch on our training set, ActionFormer-T achieves the best performance on the seen   test set (S1), while the performance of ActionFormer is slightly lower but comparable (26.3% vs   25.4% mAP). We conjecture that the structure of our training set, which has multiple video examples   per activity, allows the ActionFormer to learn step representations without explicitly requiring   language descriptions. However, modelling the language explicitly with ActionFormer-T yields a   small boost, as it enables capturing similarities and subtle differences between step descriptions.   More importantly, ActionFormer cannot be applied to detect steps of novel activities and articles   in the unseen test set (S2). On this test set, our vanilla MT+BCE baseline that leverages the article   structure (in contrast to grounding each step independently) performs reasonably well, outperforming   UMT, even though the latter has been trained with a regression loss for more accurate temporal   boundaries. This shows that leveraging the article step structure is very helpful for grounding novel   articles in video. However, although ActionFormer-T has the disadvantage of performing single-step   grounding, it outperforms all models in the unseen split too, presumably because it combines an   architecture tailored to multi-scale detection with language modelling.   Weakly supervised pretraining. The state-of-the-art weakly supervised VINA model trained   on HowTo100M ASR captions and wikiHow articles yields a low mAP metric since it was not   trained with accurate temporal boundaries. However, using the same pretrained VINA model as   initialization to finetune our MT+BCE baseline on HT-Step leads to a remarkable performance boost,   outperforming all previous baselines by a large margin, achieving 29.8 and 19.3 mAP on S1 and S2   respectively. We perform a similar experiment by pre-training the ActionFormer-T (AF-T) model on   the ASR captions of HowTo100M, then fine-tuning the model with the HT-Step annotations. The   results are similar, with the model achieving very good performance in the high IOU evaluation   settings, leading to the best overall results, i.e. 30.2 and 20.4 mAP on S1 and S2 respectively. The   VINA-pretrained model still shows better performance on the low-IOU evaluation, reflecting the   strong localization abilities of the original model. These experiments clearly showcase the great   8   potential in combining strong step labels from HT-Step with noisy ASR captions from HowTo100M   for training grounding models.   Table 3: Ablation of text used for step description.   Method   Text backbone   Text input   \u2191mAP @ [0.3-0.7]   Hdl. Prg. Act. seen (S1) unseen (S2)   ActionFormer-T   MPNet   \u2713   \u2717   \u2717   26.3   18.4   ActionFormer-T   MPNet   \u2713   \u2713   \u2717   26.1   18.0   ActionFormer-T   MPNet   \u2713   \u2713   \u2713   24.2   14.7   ActionFormer-T   MPNet   \u2713   \u2717   \u2713   24.0   13.7   ActionFormer-T   MPNet   \u2717   \u2713   \u2717   22.8   13.9   ActionFormer-T   MPNet   \u2717   \u2713   \u2713   21.1   11.3   MT + BCE   Word2Vec   \u2713   \u2717   \u2717   19.4   10.1   MT + BCE   Word2Vec   \u2713   \u2713   \u2717   19.2   10.3   MT + BCE   Word2Vec   \u2713   \u2717   \u2713   19.7   10.6   MT + BCE   Word2Vec   \u2717   \u2713   \u2717   17.0   8.3   MT + BCE   Word2Vec   \u2713   \u2713   \u2713   19.8   10.1   Table 4: Partial vs full matches.   Method   seen   unseen   partial full partial full   Detection   ActionFormer [46]   20.4   29.6   Grounding   MT + BCE [17]   16.5   24.7   8.8   8.7   What text information is important for article grounding? We experiment with different combinations of a step\u2019s headline, paragraph and activity name to form its description and summarize the   results in Table 3 (details for how every model combines text sources are provided in Appendix). It   is evident that the headline contains the most valuable information. Paragraph information alone is   enough for learning the task, however performance when using only paragraphs is lower than with   headlines. Results from incorporating the activity name in the step descriptions vary depending on   the model. Overall, although paragraph information is essential for accurate grounding, as shown   in Figure 3, combining headline and paragraph provides marginal improvements. In addition, with   all the approaches, there is a significant performance gap between the seen and the unseen test set.   We conclude that temporal article grounding on our benchmark presents a challenging and exciting   problem that will hopefully inspire the development of better models by the community.   What are the results on partial matches compared to full matches? To understand the differences   between partial and full matches, we take models trained on the complete set and test them on subsets   containing only partial-match or only full-match labels (details in Appendix). We present the results   in Table 4. When evaluating on seen activities (S1), the performance of all models is significantly   better on the full-match subset. This aligns with our intuition that full-match annotations should be   easier to ground, as their step descriptions align better to what is visually demonstrated in the video.   However, this gap disappears when evaluating on unseen activities (S2). We conjecture this may be   due to full step matches being visually more consistent, resulting in models overfitting them on seen   activities.   Table 5: Comparison to SOTA for zero-shot   step grounding on the CrossTask dataset.   Method   \u2191Avg. R@1 (%)   Zhukov [50]   22.4   HT100M [31]   33.6   VideoCLIP [45]   33.9   MCN [7]   35.1   DWSA [38]   35.3   MIL-NCE [30]   40.5   VT-TWINS [19]   40.7   UniVL [27]   42.0   AF-T (HT100M)   37.1   AF-T (HT100M + HT-Step)   48.5   Table 6: Zero-shot article grounding evaluation   on CrossTask.   Training data   \u2191Avg. R@1 (%) mAP@[0.3-0.7]   HT100M   37.1   5.2   HT100M + COIN   41.4   6.7   HT100M + HT-Step   48.5   9.5   Do models trained on HT-Step generalize to other tasks? We showcase the potential of leveraging   HT-Step for model pre-training by evaluating one of our models on zero-shot atomic step localization.   In particular, we pre-trained the ActionFormer-T (AF-T) model on the ASR captions of HowTo100M   (following prior work [27, 30, 45]), then fine-tuned the model with the HT-Step annotations. We   perform evaluation on the CrossTask dataset, following standard protocol [50], i.e., we compute the   recall per task based on a single predicted timestamp per step, computed over 20 random subsets   of the training videos. As Table 5 demonstrates, it is clear that training on HT-Step results in a   9   huge boost in performance, namely a +11.4% absolute improvement, but also surpasses the previous   state-of-the-art on this benchmark by a large margin (+6.5%).   To further motivate the usefulness of the HT-Step annotations compared to existing datasets, we train   the same model on the COIN dataset [41], and perform zero-shot evaluation on CrossTask, using both   the standard recall metric (for single timestamp prediction evaluation) as well as mAP (for temporal   segment and precision evaluation). The results are shown in Table 6. It is clear that fine-tuning on   the HT-step annotations results in a much more substantial performance improvement compared to   fine-tuning on COIN (+7.1% R@1 and +2.8 mAP).   5   Conclusion   We have introduced HT-Step, a dataset containing temporal annotations of instructional article steps   in cooking videos. The dataset offers a large-scale training resource for the task of temporal article   grounding, presenting new challenges and encouraging the development of alignment methods that   leverage order and procedural structure. By releasing the annotations and providing benchmark   protocols, we aim to spur new research in this domain and advance the field of procedural activity   understanding.   Limitations and societal impact   We acknowledge that HT-Step is intended for research purposes   and should not be regarded as a comprehensive dataset encompassing the full range of human   activities. Models trained on our dataset may exhibit biases towards the specific activities included in   the dataset, resulting in a limited coverage of our everyday living scenarios.   Acknowledgements.   We thank Mandy Toh, Yale Song, Gene Byrne, Fu-Jen Chu, Austin Miller,   and Jiabo Hu for helpful discussions and invaluable engineering support.", "conf": "IJCAI", "year": "2023", "index": 509}, {"title": "HT-Step: Aligning Instructional   Articles with How-To Videos   Triantafyllos Afouras\u2217   Effrosyni Mavroudi\u2217   Tushar Nagarajan   Huiyu Wang   Lorenzo Torresani   FAIR, Meta   {afourast, emavroudi, tusharn, huiyuw, torresani}@meta.com", "abstract": "We introduce HT-Step, a large-scale dataset containing temporal annotations of   instructional article steps in cooking videos. It includes 116k segment-level annotations over 20k narrated videos (approximately 2.1k hours) of the HowTo100M   dataset. Each annotation provides a temporal interval, and a categorical step label from a taxonomy of 4, 958 unique steps automatically mined from wikiHow   articles which include rich descriptions of each step. Our dataset significantly   surpasses existing labeled step datasets in terms of scale, number of tasks, and   richness of natural language step descriptions. Based on these annotations, we   introduce a strongly supervised benchmark for aligning instructional articles with   how-to videos and present a comprehensive evaluation of baseline methods for   this task. By publicly releasing these annotations and defining rigorous evaluation   protocols and metrics, we hope to significantly accelerate research in the field of   procedural activity understanding.   1. In a large mixing bowl, whisk together    the flour, salt, and baking soda. Mixing    your dry ingredients together separately    ensures that \u2026   2. Beat the butter in a separate bowl until    it becomes light and fluffy. You can do    this using a whisk or an electric mixer.   3. Beat the white and brown sugar for    about 2 minutes. You might find it easier    to mix if you add the sugar in a little bit at    a time \u2026.   4. Whisk in the vanilla extract and the    milk. Keep whisking until everything is    evenly combined.   5. Beat in the flour mixture. At this point,    the mixture will start to stick together to    form a ball. If you were using a whisk, \u2026   66:00   86:00   90:00   104:00   106:00 125:00   Is full match? Yes   Is full match? No   130:00 140:00   Non-groundable    steps:   Step 2   Step 4   How-To Video   time   Figure 1: HT-Step is a dataset of annotations aimed towards instructional video-article alignment.   Given an untrimmed how-to video and an instructional article containing a list of step descriptions, the   goal is to temporally localize the steps shown in the video and reject the ones not demonstrated. Our   dataset includes over 116k segment-level annotations for this task, as well as labels indicating whether   a temporal segment is a full or a partial match to the step, i.e., whether the segment demonstrates all   substeps and ingredients mentioned in the step headline (shown in bold).   \u2217equal contribution   37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.   1", "content": "Instructional videos have become a popular way to learn or improve our skills, providing an entertaining and dynamic alternative to traditional written manuals. For example, how-to videos are listed   as one of the top-four watched categories on the YouTube platform [1], where viewers can access   narrated visual demonstrations for a wide range of activities, from cooking to crafts and DIY projects.   In addition to being a valuable learning medium for humans, how-to videos are a great source of   training data for computer vision models [6, 25, 30]. Since many of these instructional videos focus   on complex activities entailing a sequence of steps, these repositories offer great potential for learning   task graphs [47] and training procedural activity models [42, 50]. However, although the videos are   often accompanied by informative closed captions or ASR transcriptions, they lack structured step   annotations that clearly denote which video segments correspond to what steps of the activity.   An alternative source of information for acquiring new skills is instructional article collections, such   as wikiHow [20]. These typically provide a list of procedural steps and have the advantage of being   more comprehensive, incorporating variations and detailed explanations. Although they often contain   still-pictures or graphic illustrations, they lack the dynamic demonstrations offered by videos.   In this work we introduce HT-Step, the largest existing dataset with labeled step segments on   instructional videos. We leverage wikiHow articles [20] to guide the annotation on a large subset   of the open-sourced HowTo100M dataset [31]. These cross-dataset associations provide us with   a taxonomy of steps well fitting the data and with an automatic mechanism to identify the set of   potential steps represented in each individual video. Through a meticulous process of manual labeling   by professional annotators we obtain 116k temporal step labels over 20k videos (approximately 2.1k   hours). Our benchmark significantly surpasses the largest existing datasets of labeled steps [41, 50]   in procedural videos along multiple axes: scale (2.6\u00d7 the number of segments annotated, 4.8\u00d7 the   number of video hours), taxonomy granularity (7\u00d7 the number of steps), and scope (2\u00d7 the number   of activities).   We share these annotations with the research community and present a benchmark for the task   of temporal article grounding. Given as input a how-to video and an instructional article listing   a sequence of potential steps, the objective is to temporally ground the subset of steps that are   demonstrated in the video and to detect outliers (i.e., the steps not executed). We note that this   task differs from traditional temporal grounding task which require localizing a single query that is   known to be present in the video. Instead, in our problem setting the system is given the complete   sequence of potential steps. We believe that this formulation may encourage the design of methods   that can perform global reasoning over all the steps and take full advantage of ordering constraints   and sequence priors to disambiguate the grounding. Furthermore, on average about 58% of the steps   listed in the associated wikiHow article are not demonstrated in the given video. This implies that   the model must have strong capability to recognize the steps that are not groundable. Finally, we   include in our benchmark a test set over unseen tasks, i.e., activities that are not represented in the   training set. Traditional activity detection methods requiring a taxonomy known a priori cannot   generalize to unseen activities. By introducing this setting we hope to promote the development of   language-based temporal alignment models that can learn to ground the rich textual descriptions of   steps in instructional articles even for never-seen tasks.   In summary, our contributions are the following: (1) We introduce a large collection of step annotations on videos from the popular HowTo100M dataset. (2) We propose a scalable pipeline for   annotating procedural videos that leverages an automatic strategy for associating potential steps to   video and for inferring the step taxonomy. (3) We introduce splits, evaluation protocols and metrics   that evaluate the ability of models to predict the temporal extent of steps in videos, and reject steps   that are not visually groundable. (4) We train and evaluate state-of-the-art approaches and baselines   that tackle our task from three different angles: activity detection, single-sentence temporal grounding,   and temporal article grounding. (5) We discuss properties and unique aspects of HT-Step through   comprehensive analyses and experiments.   2   Related Work   Procedural and Instructional Video Datasets. Procedural videos portray humans performing   sequential steps in some constrained yet non-unique order, with the aim of achieving a specific   goal, such as preparing a dish. Understanding the content of such videos has been an active area of   2   Table 1: Comparison between HT-Step and existing procedural video understanding (top), temporal   grounding (middle), and instructional video (bottom) datasets. Note that HT-Step seen validation and   test sets introduced in [28] are subsumed by HT-Step.   Dataset   Duration (h)   # Videos   # Segments   # Activities   Domain   Procedural Activity Datasets   MPII [35]   10   44   5.6k   cooking   50Salads [40]   4.5   50   899   cooking   TACoS [34]   \u223c600   127   18.8k   cooking   Breakfast [22]   77   2k   8.5k   10   cooking   Ikea-FA [16]   4   101   1.9k   1   furniture   EPIC-KITCHENS [9]   100   700   90k   cooking   EgoProcel [3]   62   298   1k   16   multiple   Assembly101 [36]   513   4.3k   1M   101   assembly   Temporal Grounding Datasets   ActivityNet Captions [21]   849   20k   100k   multiple   Charades-STA [14]   \u223c90   10k   18k   157   multiple   QV-highlights [23]   845   10.2   16.1k   multiple   Instructional Video Datasets   YouCook [10]   2   88   cooking   YouCook2 [48]   176   2k   13.8k   cooking   YouwikiHow [8]   47k   1398   multiple   CrossTask [50]   213   2.8k   21k   18   multiple   COIN [41]   476   11.8k   46.4k   180   multiple   HT-Step Seen Val+Test Set [28]   124   1.2k   7k   177   cooking   HT-Step (Ours, Full)   2.1k   19.7k   116k   433   cooking   HT-Step splits   HT-Step Train   1.9k   17.4k   103k   401   cooking   HT-Step Val seen   64   600   3.4k   120   cooking   HT-Step Test seen (S1)   61   600   3.6k   120   cooking   HT-Step Test unseen (S2)   116   1000   5.7k   32   cooking   research over the last decade [2, 6, 11, 13, 22, 34, 37, 47], largely driven by the release of benchmark   datasets [22, 41, 50]. Early datasets such as Breakfast [22] and 50Salads [40] are manually recorded,   small-scale (spanning at most 100 hours) and limited in activity diversity (e.g., covering only   breakfast recipes [22] or IKEA furniture assembly [16]). More recent datasets, such as COIN [41]   and CrossTask [50], have capitalized on public video collections, including videos sourced from   YouTube, in order to substantially increase dataset scale and activity diversity. A key characteristic of   these videos is that they are instructional, i.e., they typically involve an individual teaching a complex   task by narrating the sequence of steps.   Our HT-Step dataset also provides temporal annotations for steps of instructional videos. However,   it surpasses existing datasets in terms of scale and activity diversity, as the steps are sourced from   real-world instructional articles (wikiHow [20]) rather than being manually defined. This approach   leads to much richer step descriptions, including a paragraph providing detailed instructions. Despite   its exclusive focus on the cooking domain, HT-Step is significantly larger in scale and step description   complexity. Our work significantly expands the small-scale val/test set that was recently released [28]   for evaluating step grounding approaches trained without annotated temporal segments. While the   focus of that work was the design of weakly supervised methods leveraging narrations as \u201cfree\u201d signal   for step-to-video alignment, here we share the first large-scale dataset enabling strongly-supervised   training of methods for temporal article grounding. Our annotations over a large-scale training set, and   a new test set of unseen tasks (not included in the training set) provide almost 20\u00d7 more annotated   videos and number of segments. We also include additional metadata per step (indicating full or   partial segment-step matches). Furthermore, we include a comprehensive experimental evaluation of   strongly supervised baselines on this task.   Instructional Video Understanding There exist various tasks related to instructional video understanding, such as step classification [25], step detection [41, 46], action segmentation [13],   captioning [49], video retrieval [15], visual object grounding [29], and temporal grounding [26].   In this paper, we focus on the recently-introduced task of temporal article grounding, which has   been tackled only with weakly-supervised approaches so far [8, 28]. In particular Chen et al. [8] is   the first work to train and evaluate models for the temporal article grounding task. However this   work was limited to weakly-supervised training due to the lack of a strongly labeled dataset, and   used CrossTask as a proxy dataset for evaluation, with noisy mapping between article steps and   CrossTask labels. In contrast, we introduce a large amount of strong temporal segment annotations   3   \u201cPlace an apple onto    each square piece.\u201d   Make Apple Dumplings   \u201cPlace the dumplings on    the prepared baking tray.\u201d   \u201cSlowly pour the sauce    over the dumplings.\u201d   \u201cCoat the chicken with \u2028   salt and pepper.\u201d   Make Chicken Piccata   \"Dredge the chicken    in  \ufb02our, coating    completely.\u201d   \u201cCook the chicken on    both sides for 3-4    minutes.\u201d   \u201cAdd the butter once the water\u2028   evaporates and small bubbles\u2028   form in the pan.\u201d   Make Crepes Suzette   \u201cFold the crepes into quarters\u2028   and add them to the skillet to\u2028   \u201cheat them.\u201d   \u201cMove the crepes with the    wooden spatula to help    incorporate the sauce while    heating the crepes.\u201d   \u201cAdd the onions to the pan   and  saut\u00e9 them until    golden brown.\u201d   Make Naadan Kadala Curry   \u201cAdd the tomatoes and water,    Then put on the pan and let    cook or 10 minutes.\u201d   \u201cPut in the cooker chickpeas   Into the gravy and stir well.\u201d   Step headlines   Step headlines   Figure 2: Sample frames for 12 videos spanning 4 tasks from the HT-Step training set. For each   video, we show relevant frames for three steps. By annotating multiple videos of the same task, we   obtain multiple instances of each step, which have high variations in appearance and viewpoints.   for training, and propose a metric that evaluates both the recall and precision of models, and is thus   better suited to the task of temporal article grounding which involves multiple ungroundable steps.   The temporal article grounding task is closely related to two other grounding tasks: (a) temporal   grounding of a single natural language query [14, 23] (also known as moment retrieval) and (b) video   paragraph grounding [4]. Benchmarks [14, 21, 23] for these tasks typically consist of video captions   or questions that are loosely related to each other and which were obtained by human annotators   based on the video contents. Thus, most approaches assume that each of the queries is groundable in   the video. For example, video paragraph grounding aims at predicting a single temporal segment for   each sentence of the paragraph. Instead, in our HT-Step benchmark the step queries originate from   wikiHow articles, which might contain steps that are not shown in the video (non-groundable steps)   or steps that are partially executed (partial matches). This makes it challenging to adapt temporal   grounding approaches for temporal article grounding in our benchmark.   3   HT-Step   The creation of HT-Step involved pairing videos from HowTo100M with instructional articles from   wikiHow and then annotating segments in each video with steps of the associated article. wikiHow2   is an online knowledge base that houses thousands of articles with step-by-step instructions for a   large variety of activities. Every article is typically structured as a series of steps, each step anchored   by a descriptive headline and further supplemented with a detailed paragraph. HowTo100M, on   the other hand, contains an extensive compilation of instructional videos that were collected using   titles of wikiHow articles as search keywords for a subset of activities deemed to contain visual   demonstrations. As a result of this sourcing procedure, each video in HowTo100M comes with a task   label that corresponds to a specific wikiHow article. These pairings create a natural opportunity for   annotating the HowTo100M videos with the step labels of the corresponding wikiHow articles, which   we exploited for creating HT-Step. Next we describe in detail our annotation process.   3.1   Annotation workflow   Activity selection. HowTo100M spans a variety of domains, including cooking, DIY, crafts, sports,   and gardening. As in previous works [17], we chose to focus on cooking activities as i) they account   for approximately a third of the HowTo100M videos, and ii) they are relatively low-complexity tasks   which can be annotated by non-experts. Using metadata, we selected 495 cooking activities (detailed   list in the Appendix) for which HowTo100M contains at least 70 videos.   Step Taxonomy. We automatically sourced the step descriptions for all the activities from wikiHow.   For every activity, we extracted the steps from the corresponding wikiHow article, forming a step   taxonomy that includes 4, 958 unique steps. We list the full taxonomy in the Appendix.   2https://www.wikihow.com/   4   Video validation. For every video, the annotators were given the corresponding activity title and step   taxonomy. They were then asked to watch the whole video and determine whether it contains the   specified activity. If the video was found not matching the task, it was rejected.   Temporal annotation. During the temporal annotation loop annotators were asked to provide   temporal boundaries for the wikiHow steps represented in all the non-rejected videos. An example   annotation is illustrated in Figure 1.   Task variations. wikiHow articles often list several variations for a given task. For example, the   instructional recipe for \u201cMake Taco Salad\u201d includes three versions: basic, deluxe and vegetarian. For   such activities, we asked annotators to choose the variation that best fits the video and to annotate   only the steps listed in that variation.   Full vs partial matches. We asked the annotators to mark a step as a \u201cfull\u201d match if (i) all the   instructions and ingredients listed in the step headline are shown in the segment and (ii) the step is   brought to full completion. Conversely, \u201cpartial\u201d indicates that some sub-steps in the step headline   are skipped, some ingredients are not used, or that a step is interrupted and continued later.   QA process. To ensure the quality of the annotations, we followed a rigorous multi-stage Quality   Assurance (QA) process. In the first stage, the videos were annotated by a single annotator. These   initial annotations were then reviewed by more experienced annotators, who either approved all the   annotations on a given video (all the marked steps were correct and no steps were missing) or marked   it for re-doing, with specific comments indicating the necessary correction. At the last stage of the QA   process, the annotations that were marked as incorrect were redone by a third set of annotators. Due   to budget constraints, the full QA was enforced on a sample of roughly 13% of the total annotations.   Annotation output. The annotation process took approximately 33k person-hours by 90 professional   human annotators. Overall, 34% of the videos were rejected as not matching the specified activity,   resulting in annotations for 433 of the initial 494 activities. We summarize the statistics of the   resulting annotations and compare to existing datasets in Table 1.   3.2   Training, validation, and test splits   We formed two types of test sets: one for seen activities, i.e., activities that are also included in the   training set, and one for unseen activities, i.e., novel activities not appearing in the training set. For   seen activities we also formed a validation (val) set containing the same activities.   Seen val/test set (S1). We directly adopt the HT-Step validation and test splits [28] as the seen   evaluation sets of our dataset, each containing 600 videos, 5 samples for each of 120 activities.   Unseen test set (S2). To create the unseen test set, we selected 32 activities that do not appear in the   training set or the seen val/test set. We include all the annotations on videos of these activities that   went through the QA review, resulting in a test set of 1000 annotated videos. This set is imbalanced   w.r.t. activities, with the video count per activity ranging from 17 to 70.   Training set. All the other annotated videos are included in the final training set, which contains   more than 17k videos and 100k annotated segments. We summarize the split statistics in the bottom   part of Table 1.   3.3   Properties   Article steps. As previously mentioned, step descriptions in wikiHow articles have two parts: a   headline and a paragraph. The headline is brief and may not reveal enough information for grounding;   the paragraph contains additional details about the execution of the step. Annotators were asked to   localize each step by considering both. This allowed us to obtain annotations where the paragraph   description is essential for grounding the steps. We show some examples in Figure 3.   Composite steps. Many wikiHow steps are composite, i.e., include multiple sub-steps. Two examples   of such steps are shown in Figure 4 (b,c). We introduced the labeling of partial vs full matches   (discussed in Section 3.1) to account for the fact that composite steps are rarely fully represented in   videos or they may be carried out in disjoint time intervals of the video.   Non-groundable steps. Some of the steps listed in the wikiHow article may not be represented   at all in the video. An example is illustrated in Figure 1. Temporal grounding models that are   trained/evaluated on this data must support handling queries with no associated temporal segments.   3.4   Statistics   In this section we provide insightful statistics about the dataset. See Appendix for more.   5   Dice the celery, carrot, and pickle. Use a sharp knife to    carefully dice enough red onion to measure 1/4 cup (40    g). You'll also need to dice enough celery to measure 1/4    cup (55 g) and enough pickle to measure 1/4 cup (35 g).    Transfer the vegetables to the bowl with the mashed    chickpeas.   Make the cake ball dough. Place two-inch (5-cm) cubes    of cake into the food processor and pulse until the cake is    broken up into fine crumbs. Transfer the crumbs to a    mixing bowl. [\u2026] If you don\u2019t have a food processor,    place the chunks of cake into a mixing bowl and mash    them with a fork or break them up with your hands.   Figure 3: Qualitative examples where the paragraph is essential to localize the step. The paragraph   sentences that are visually demonstrated in the video are highlighted in yellow.    (c) Step: Add the broccoli, carrots, and onions.      (a) Step: Fill the mini-Stromboli with meat, cheese, and seasoning.    (d) Step: Cook the chicken on both sides for 3-4 minutes.    (b) Step: Pile on the roast pork.    Figure 4: Examples of composite steps and full/partial matches. Frames annotated as \u201cfull matches\u201d   are shown in green, \u201cpartial matches\u201d are shown in orange. (a) An atomic step, fully matched. (b) A   composite step: it is labeled as a full match as all the ingredients and the substeps mentioned in the   paragraph are shown in the video. (c) Another composite step: here not all the described ingredients   are utilized and thus it is labeled as a \u201cpartial match.\u201d (d) An example of step fragmentation: the step   is executed over two disjoint segments which are consequently labeled as \u201cpartial matches.\u201d   Overview. Table 1 summarizes the statistics of HT-Step. It is larger and has a much richer step   taxonomy than existing datasets. Compared to COIN, it has 4.8\u00d7 the duration, 2.6\u00d7 the number of   annotated segments, 7\u00d7 the taxonomy size (4958 vs 778 unique steps), and 2\u00d7 the number of videos.   Segment duration. According to Figure 5a, the average segment duration is approx. 15 seconds,   with most of the annotated segments at 10 seconds or shorter and a long tail of rare longer segments.   Video coverage. We define video coverage as the percentage of a video that is covered by annotations.   Figure 5b shows the distribution of the average video coverage per activity, with a mean of 22%.   Step coverage. Step coverage denotes the fraction of unique steps from the associated wikiHow   article that are annotated in a video and is on average 42%. Figure 5c shows that even for the activities   with the highest step coverage, many article steps have no relevant temporal segments in each video.   Distribution of segments over activities. Figure 5d shows how the number of annotations is   distributed over the 4, 958 steps. On average we get 24.6 segments per step. Since the choice of   steps to be annotated was determined by the data, several steps have very few training samples, as   indicated by the large concentration in the first bucket. We believe that this property will encourage   the development of methods that can perform zero- or few-shot detection, as well as techniques that   approach the task as language-based temporal grounding as opposed to class-based detection.   Length of text descriptions. We calculate the number of words as a proxy for the complexity of the   step descriptions. In Figure 5e we provide the distribution of this metric averaged over activities, for   HT-Step and COIN [41]. It is clear that HT-Step contains richer text descriptions, even when only the   headlines are used (8.5 words vs 4.9 for COIN on average). When paragraph text is included in this   calculation, the word count increases even further, to 17.1 words on average.   Step ordering variation. The steps in a wikiHow article are listed according to a recommended order   of execution. In order to capture real-world deviations from these canonical orderings, we measure   the Normalized Edit Distance (NED) between the annotated step sequence and the wikiHow listing,   normalized over the length of the annotated sequence in each video. We calculate the mA-NED as the   mean of the average NED per activity. We compute the same metric for the COIN dataset, using the   ordering provided with the taxonomy of this dataset. Steps in our dataset are much more frequently   executed out of order compared to COIN (mA-NED is 0.37 vs 0.27), as shown in Figure 5f. This   presents a much more challenging temporal grounding setting.   6   0   10   20   30   40   50   60   70   80   Segment duration (seconds)   0K   2K   5K   8K   10K   12K   15K   18K   20K   # Segments   Mean: 14.94s   (a) Distribution of segment durations. The average segment duration is about 15 seconds, with most   of the annotated segments at 10 seconds or shorter, and a long tail of   rare longer segments.   0.0   0.1   0.2   0.3   0.4   0.5   0.6   Video coverage   0   5   10   15   20   25   30   35   40   # Activities   Mean: 0.22   (b) Distribution of video coverage,   averaged per activity. Video coverage denotes the percentage of a   video that is covered by step annotations. It exhibits a normal distribution around the mean.   Make Potstickers (Guo Tie)   Make Vegetable Manchurian   Make Crepes Suzette   Make Focaccia   Make a Galaxy Mirror Cake   Make Taho   Make Chicken Piccata   Make Conchas   Make Orange Chicken   Make Planet Cookies   Make Naadan Kadala Curry   Make Saltimbocca   Make Ciabatta Bread   Make Tinga De Pollo   Make King's Cake   Make Beignets   Make Hush Puppies   Make Vegetable Paniyaram   Make Lobster Bisque   Make Nigerian Style Jollof Rice   Activity   0   5   10   15   20   25   # Unique steps   (c) Average number of unique   steps annotated per video for the 20   activities with most unique steps.   0   25   50   75   100 125 150 175 200   # Annotations   0   500   1000   1500   2000   2500   # Steps   Mean: 24.6   (d) Distribution of annotations.   Annotations follow a long-tail distribution which should encourage the   development of zero/few-shot detection or text grounding methods.   0   10   20   30   40   50   60   70   80   # Words   0   200   400   600   800   # Steps   Mean: 17.1 words   Mean: 8.5 words   Mean: 4.9 words   Ours - headl. + par.   Ours - head.   COIN   (e) Comparison of text description   lengths. HT-Step contains long step   descriptions, almost twice as long   as those of COIN, even when only   the headline text is considered.   0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8   NED   0   1   2   3   4   # Activities   Mean: 0.37   Mean: 0.23   Ours   COIN   (f) NED. Step sequences in HT-Step   have a 0.37 NED compared to the   canonical step ordering on average,   which makes it more challenging   compared to previous benchmarks.   Figure 5: HT-Step statistics. We provide various statistics for a qualitative overview of the dataset.   Full vs partial matches. Of the resulting annotated segments, 27% are full and 73% are partial   matches. Some examples of full and partial match annotations are shown in Figure 4.   Variations. Most tasks (335 out of 433) involve a single listing of steps in wikiHow. However there   is a significant number of activities (98) that was annotated according to more than one (and up to 6)   sequences of steps. Refer to Appendix for an overview of the variations distribution.   4   Benchmark & Experiments   Table 2: Article grounding mAP. Comparison of state-of-the-art approaches on our benchmark. TS   denotes the TimeSformer backbone.   Method   Training data   Video Backbone Text Backbone   Seen (S1)   Unseen (S2)   \u2191mAP@IOU   \u2191mAP@IOU   @0.3 @0.5 @0.7 @[0.3-0.7] @0.3 @0.5 @0.7 @[0.3-0.7]   Detection   ActionFormer [46]   HT-Step   TS [5, 25]   32.9   23.6   12.6   23.1   ActionFormer [46]   HT-Step   S3D [30, 44]   35.1   25.6   14.8   25.4   Grounding   UMT [26]   HT-Step   S3D [30, 44]   Word2Vec [32]   8.0   4.0   1.5   4.4   4.9   2.3   0.8   2.6   UMT [26]   HT-Step   TS [5, 25]   CLIP [33]   15.7   8.7   3.2   9.1   9.4   4.9   1.7   5.3   MT+BCE   HT-Step   S3D [30, 44]   Word2Vec [32]   31.5   19.6   8.1   19.7   18.6   10.3   3.7   10.6   MT+BCE   HT-Step   TS [5, 25]   Word2Vec [32]   26.2   16.5   6.0   16.1   13.5   6.8   2.5   7.4   ActionFormer-T   HT-Step   S3D [30, 44]   Word2Vec [32]   27.8   20.0   10.6   19.6   10.8   6.4   3.1   6.6   ActionFormer-T   HT-Step   S3D [30, 44]   MPNet [39]   36.9   26.5   15.3   26.3   27.4   18.3   9.3   18.4   VINA [28]   HT100M   S3D [30, 44]   Word2Vec [32]   12.6   4.7   1.2   5.9   8.2   3.1   0.6   3.8   MT+BCE (VINA) HT100M+HT-Step   S3D [30, 44]   Word2Vec [32]   46.2   29.9   12.9   29.8   31.6   18.7   7.7   19.3   ActionFormer-T   HT100M+HT-Step   S3D [30, 44]   MPNet [39]   41.2   30.8   18.3   30.2   29.7   20.3   10.7   20.4   7   Task definition. The annotated data of HT-Step can be used to train and evaluate models for a plethora   of tasks, from temporal article grounding to task-graph-based step localization [47], narration-based   step grounding [28], captioning[21], and step forecasting [25]. In this paper, we focus on the temporal   article grounding task: given an untrimmed video and an instructional article containing a sequence   of steps, a model must predict temporal segments for the steps demonstrated in the video, along with   a confidence score for every segment.   Evaluation protocol. For evaluating models, we introduce the article-grounding mAP \u2013 a variant of   the popular mean Average Precision (mAP) metric from object detection [12, 24]. AP is separately   computed for each activity by treating all steps within the activity as class-agnostic text queries. The   per-activity APs are then averaged to obtain the article-grounding mAP. Following prior work [18],   we evaluate AP under different temporal Intersection-over-Union (tIoU) thresholds.   4.1   Results   We evaluate state-of-the-art approaches and baselines from step detection, single-step grounding, and   multi-step grounding on our task. Full architecture, training, and implementation details can be found   in Appendix.   ActionFormer [46] is a state-of-the-art, multi-scale action detection model that operates on a fixed   taxonomy of steps (i.e., it does not use language), and as a result, can only detect seen steps (S1 split).   ActionFormer-T is a variant of the ActionFormer model which we adapted to enable open-vocabulary   step detection (i.e. unseen step grounding). It is constructed by replacing the classification head with   a dot product between the textual embeddings of every step and the visual embeddings obtained from   the temporal feature pyramid.   UMT [26] is a transformer-based model built for joint moment retrieval and highlight detection, and   represents the state-of-the-art in single-step grounding.   VINA [28] is a model for temporal article grounding. Unlike traditional single-step grounding models,   it can capture step ordering and the relationship between steps, and was trained with weak supervision   from ASR transcripts and video-article pairs.   MT+BCE is our modification of VINA for fully-supervised temporal article grounding. It embeds the   full sequence of step headlines along with the video clips, and feeds them together to a multimodal   transformer. We train it with a per-frame binary cross entropy loss (BCE) and produce segment   predictions using a 1D blob detection routine [43].   Baselines comparison. Table 2 compares the performance of the aforementioned models for different   choices of visual and textual backbones on the seen and unseen article test splits. Among all models   trained from scratch on our training set, ActionFormer-T achieves the best performance on the seen   test set (S1), while the performance of ActionFormer is slightly lower but comparable (26.3% vs   25.4% mAP). We conjecture that the structure of our training set, which has multiple video examples   per activity, allows the ActionFormer to learn step representations without explicitly requiring   language descriptions. However, modelling the language explicitly with ActionFormer-T yields a   small boost, as it enables capturing similarities and subtle differences between step descriptions.   More importantly, ActionFormer cannot be applied to detect steps of novel activities and articles   in the unseen test set (S2). On this test set, our vanilla MT+BCE baseline that leverages the article   structure (in contrast to grounding each step independently) performs reasonably well, outperforming   UMT, even though the latter has been trained with a regression loss for more accurate temporal   boundaries. This shows that leveraging the article step structure is very helpful for grounding novel   articles in video. However, although ActionFormer-T has the disadvantage of performing single-step   grounding, it outperforms all models in the unseen split too, presumably because it combines an   architecture tailored to multi-scale detection with language modelling.   Weakly supervised pretraining. The state-of-the-art weakly supervised VINA model trained   on HowTo100M ASR captions and wikiHow articles yields a low mAP metric since it was not   trained with accurate temporal boundaries. However, using the same pretrained VINA model as   initialization to finetune our MT+BCE baseline on HT-Step leads to a remarkable performance boost,   outperforming all previous baselines by a large margin, achieving 29.8 and 19.3 mAP on S1 and S2   respectively. We perform a similar experiment by pre-training the ActionFormer-T (AF-T) model on   the ASR captions of HowTo100M, then fine-tuning the model with the HT-Step annotations. The   results are similar, with the model achieving very good performance in the high IOU evaluation   settings, leading to the best overall results, i.e. 30.2 and 20.4 mAP on S1 and S2 respectively. The   VINA-pretrained model still shows better performance on the low-IOU evaluation, reflecting the   strong localization abilities of the original model. These experiments clearly showcase the great   8   potential in combining strong step labels from HT-Step with noisy ASR captions from HowTo100M   for training grounding models.   Table 3: Ablation of text used for step description.   Method   Text backbone   Text input   \u2191mAP @ [0.3-0.7]   Hdl. Prg. Act. seen (S1) unseen (S2)   ActionFormer-T   MPNet   \u2713   \u2717   \u2717   26.3   18.4   ActionFormer-T   MPNet   \u2713   \u2713   \u2717   26.1   18.0   ActionFormer-T   MPNet   \u2713   \u2713   \u2713   24.2   14.7   ActionFormer-T   MPNet   \u2713   \u2717   \u2713   24.0   13.7   ActionFormer-T   MPNet   \u2717   \u2713   \u2717   22.8   13.9   ActionFormer-T   MPNet   \u2717   \u2713   \u2713   21.1   11.3   MT + BCE   Word2Vec   \u2713   \u2717   \u2717   19.4   10.1   MT + BCE   Word2Vec   \u2713   \u2713   \u2717   19.2   10.3   MT + BCE   Word2Vec   \u2713   \u2717   \u2713   19.7   10.6   MT + BCE   Word2Vec   \u2717   \u2713   \u2717   17.0   8.3   MT + BCE   Word2Vec   \u2713   \u2713   \u2713   19.8   10.1   Table 4: Partial vs full matches.   Method   seen   unseen   partial full partial full   Detection   ActionFormer [46]   20.4   29.6   Grounding   MT + BCE [17]   16.5   24.7   8.8   8.7   What text information is important for article grounding? We experiment with different combinations of a step\u2019s headline, paragraph and activity name to form its description and summarize the   results in Table 3 (details for how every model combines text sources are provided in Appendix). It   is evident that the headline contains the most valuable information. Paragraph information alone is   enough for learning the task, however performance when using only paragraphs is lower than with   headlines. Results from incorporating the activity name in the step descriptions vary depending on   the model. Overall, although paragraph information is essential for accurate grounding, as shown   in Figure 3, combining headline and paragraph provides marginal improvements. In addition, with   all the approaches, there is a significant performance gap between the seen and the unseen test set.   We conclude that temporal article grounding on our benchmark presents a challenging and exciting   problem that will hopefully inspire the development of better models by the community.   What are the results on partial matches compared to full matches? To understand the differences   between partial and full matches, we take models trained on the complete set and test them on subsets   containing only partial-match or only full-match labels (details in Appendix). We present the results   in Table 4. When evaluating on seen activities (S1), the performance of all models is significantly   better on the full-match subset. This aligns with our intuition that full-match annotations should be   easier to ground, as their step descriptions align better to what is visually demonstrated in the video.   However, this gap disappears when evaluating on unseen activities (S2). We conjecture this may be   due to full step matches being visually more consistent, resulting in models overfitting them on seen   activities.   Table 5: Comparison to SOTA for zero-shot   step grounding on the CrossTask dataset.   Method   \u2191Avg. R@1 (%)   Zhukov [50]   22.4   HT100M [31]   33.6   VideoCLIP [45]   33.9   MCN [7]   35.1   DWSA [38]   35.3   MIL-NCE [30]   40.5   VT-TWINS [19]   40.7   UniVL [27]   42.0   AF-T (HT100M)   37.1   AF-T (HT100M + HT-Step)   48.5   Table 6: Zero-shot article grounding evaluation   on CrossTask.   Training data   \u2191Avg. R@1 (%) mAP@[0.3-0.7]   HT100M   37.1   5.2   HT100M + COIN   41.4   6.7   HT100M + HT-Step   48.5   9.5   Do models trained on HT-Step generalize to other tasks? We showcase the potential of leveraging   HT-Step for model pre-training by evaluating one of our models on zero-shot atomic step localization.   In particular, we pre-trained the ActionFormer-T (AF-T) model on the ASR captions of HowTo100M   (following prior work [27, 30, 45]), then fine-tuned the model with the HT-Step annotations. We   perform evaluation on the CrossTask dataset, following standard protocol [50], i.e., we compute the   recall per task based on a single predicted timestamp per step, computed over 20 random subsets   of the training videos. As Table 5 demonstrates, it is clear that training on HT-Step results in a   9   huge boost in performance, namely a +11.4% absolute improvement, but also surpasses the previous   state-of-the-art on this benchmark by a large margin (+6.5%).   To further motivate the usefulness of the HT-Step annotations compared to existing datasets, we train   the same model on the COIN dataset [41], and perform zero-shot evaluation on CrossTask, using both   the standard recall metric (for single timestamp prediction evaluation) as well as mAP (for temporal   segment and precision evaluation). The results are shown in Table 6. It is clear that fine-tuning on   the HT-step annotations results in a much more substantial performance improvement compared to   fine-tuning on COIN (+7.1% R@1 and +2.8 mAP).   5   Conclusion   We have introduced HT-Step, a dataset containing temporal annotations of instructional article steps   in cooking videos. The dataset offers a large-scale training resource for the task of temporal article   grounding, presenting new challenges and encouraging the development of alignment methods that   leverage order and procedural structure. By releasing the annotations and providing benchmark   protocols, we aim to spur new research in this domain and advance the field of procedural activity   understanding.   Limitations and societal impact   We acknowledge that HT-Step is intended for research purposes   and should not be regarded as a comprehensive dataset encompassing the full range of human   activities. Models trained on our dataset may exhibit biases towards the specific activities included in   the dataset, resulting in a limited coverage of our everyday living scenarios.   Acknowledgements.   We thank Mandy Toh, Yale Song, Gene Byrne, Fu-Jen Chu, Austin Miller,   and Jiabo Hu for helpful discussions and invaluable engineering support.", "conf": "NIPS", "year": "2023", "index": 509}, {"title": "Affordance Grounding from Demonstration Video to Target Image   Joya Chen   Difei Gao   Kevin Qinghong Lin   Mike Zheng Shou\u2020   Show Lab, National University of Singapore   {joyachen,qinghonglin}@u.nus.edu   {daniel.difei.gao,mike.zheng.shou}@gmail.com", "abstract": "Humans excel at learning from expert demonstrations   and solving their own problems. To equip intelligent robots   and assistants, such as AR glasses, with this ability, it is   essential to ground human hand interactions (i.e., affordances) from demonstration videos and apply them to a   target image like a user\u2019s AR glass view. This video-toimage affordance grounding task is challenging due to (1)   the need to predict fine-grained affordances, and (2) the limited training data, which inadequately covers video-image   discrepancies and negatively impacts grounding. To tackle   them, we propose Affordance Transformer (Afformer),   which has a fine-grained transformer-based decoder that   gradually refines affordance grounding.   Moreover, we   introduce Mask Affordance Hand (MaskAHand), a selfsupervised pre-training technique for synthesizing videoimage data and simulating context changes, enhancing affordance grounding across video-image discrepancies. Afformer with MaskAHand pre-training achieves state-of-theart performance on multiple benchmarks, including a substantial 37% improvement on the OPRA dataset. Code is   made available at https://github.com/showlab/afformer.   1.", "content": "Humans frequently learn from observing others interact   with objects to enhance their own experiences, such as following an online tutorial to operate a novel appliance. To   equip AI systems with this ability, a key challenge lies in   comprehending human interaction across videos and images. Specifically, a robot must ascertain the points of interaction (i.e., affordances [19]) in a demonstration video and   apply them to a new target image, such as the user\u2019s view   through AR glasses.   This process is formulated as video-to-image affordance   grounding, recently proposed by [13], which presents a   more challenging setting than previous affordance-related   tasks, including affordance detection [11, 59], action-toimage grounding [18,40,41,44], and forecasting [16,35,36].   \u2020Corresponding Author.   Demonstration     Video   Target    Image   \u2461Fine-grained    Affordance   Heatmap   Decoding   \u2460Self-supervised   Affordance Pretraining on Videos   action: pull   action: press   Video-to-Image    Affordance Grounding    Intelligent Robots/AR glasses   Figure 1. This figure demonstrates the video-to-image affordance   grounding task, which aims to identify the area of human hand interaction (i.e., affordance) in a demonstration video and map it to   a target image (e.g., AR glass view). Our contributions include (1)   proposing a self-supervised pre-training approach for affordance   grounding, and (2) establishing a new model that excels remarkably in fine-grained heatmap decoding.   The complexity of this setting stems from two factors: (1)   Fine-grained grounding: Unlike conventional grounding   tasks that usually localize coarse affordance positions (e.g.,   identifying all buttons related to \u201cpress\u201d), video-to-image   affordance grounding predicts fine-grained positions specific to the query video (e.g., only buttons pressed in the   video). (2) Grounding accross various video-image discrepancies: Demonstration videos and images are often captured in distinct environments, such as a store camera\u2019s perspective versus a user\u2019s view in a kitchen, which complicates the grounding of affordances from videos to images.   Moreover, annotating for this task is labor-intensive, as it   necessitates thoroughly reviewing the entire video, correlating it with the image, and pinpointing affordances. As   a result, affordance grounding performance may be limited   by insufficient data on diverse video-image discrepancies.   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   6799   To enable fine-grained affordance grounding, we propose a simple yet effective Affordance transformer (Afformer), which progressively refines coarse-grained predictions into fine-grained affordance grounding outcomes. Previous methods [13,40,44] either simply employ large stride   upsampling or deconvolution for coarse-grained affordance   heatmap prediction (e.g., 8 \u00d7 8 \u2192256 \u00d7 256 [13]), or just   evaluate at low resolution (e.g., 28 \u00d7 28 [40, 44]). As a   result, these methods struggle with fine-grained affordance   grounding, particularly when potential affordance regions   are closely situated (e.g., densely packed buttons on a microwave). Our approach employs cross-attention [4, 5] between multi-scale feature pyramids, to facilitate gradual decoding of fine-grained affordance heatmaps.   To address the limited data issue that inadequately covers   video-image differences and hampers affordance grounding performance, we present a self-supervised pre-training   method, Masked Affordance Hand (MaskAHand), which   can leverage vast online videos to improve video-to-image   affordance grounding. MaskAHand can automatically generate target images from demonstration videos by masking hand interactions and simulating contextual differences   between videos and images. In the generated target image, the task involves estimating the interacting hand regions by watching the original video, thereby enhancing   the similarity capabilities crucial for video-to-image affordance grounding. Our approach uniquely simulates context   changes, an aspect overlooked by previous affordance pretraining techniques [18,36]. Furthermore, we also rely less   on external off-the-shelf tools [1,39,48,50] used in [18,36].   We conducted comprehensive experiments to evaluate   our Afformer and MaskAHand methods on three video-toimage affordance benchmarks, namely OPRA [13], EPICHotspot [44], and AssistQ [57]. Compared to prior architectures, our most lightweight Afformer variant achieves a relative 33% improvement in fine-grained affordance grounding (256\u00d7256) on OPRA and relative gains of 10% to 20%   for coarse-grained affordance prediction (28\u00d728) on OPRA   and EPIC-Hotspot. Utilizing MaskAHand pre-training for   Afformer results in zero-shot prediction performance that is   comparable to previously reported fully-supervised methods on OPRA [13], with fine-tuning further enhancing the   improvement to 37% relative gains. Moreover, we demonstrate the advantage of MaskAHand when the data scale for   downstream tasks is limited: on AssistQ, with only approximately 600 data samples, MaskAHand boosts Afformer\u2019s   performance by a relative 28% points.   2. Related Work   Visual Affordance. The term \u201caffordance\u201d was coined in   1979 [17] to refer to potential action possibilities offered   by the environment. With the progress of computer vision,   learning visual affordances has gained significant attention   in several domains, such as 2D/3D human-object interaction [9, 11, 46, 50], robotic manipulation/grasping [22, 42,   43], and instructional video understanding [13, 44].   Affordance detection [11, 59], forecasting [16, 35, 36], and   grounding [13, 18, 40, 41, 44] are some of the well-known   visual affordance tasks.   Affordance Grounding. Affordance grounding tasks are   primarily divided into two categories: (1) Action-to-Image   Grounding [18,40,41,44], which aims to identify image regions corresponding to a specific action query (e.g., \u201cpress\u201d   \u2192all microwave buttons). (2) Video-to-Image Affordance   Grounding [13, 40, 44], which involves predicting the interaction region and associated action label in a target image, based on a video (e.g., pressing the power button on   a microwave in a video \u2192the same region in a target image). Our paper concentrates on the latter problem, which   presents greater challenges than action-to-image grounding.   Predicting Affordance Heatmap.   Affordance regions   have typically been represented by pixel-wise heatmaps or   masks in prior research [11,13,18,40,41,44,59]. To predict   these pixel-wise affordance maps, upsampling and deconvolution with a large stride (e.g., 32\u00d7) are frequently employed [13, 18, 59]. Additionally, some weakly-supervised   approaches [40, 41, 44] utilize Grad-CAM [49] to estimate   the affordance heatmap through gradient activations. However, these techniques struggle to generate fine-grained affordance heatmaps, as they only learn affordance in lowresolution feature maps (e.g., 7 \u00d7 7).   We get inspiration from multi-scale segmentation transformers [5, 6, 55, 58] to design a pyramid transformer decoder, which performs cross attention across pyramid feature levels to gradually refine the affordance heatmap estimations. This architecture can better support fine-grained   affordance heatmap prediction.   Self-supervised Affordance Pre-training. Affordance regions within or across objects can be highly diverse and irregular in shape, complicating the prediction process. Several affordance pre-training methods [18,36] have been introduced to improve affordance heatmap prediction. HoIforecast [36] generates self-supervised annotations for the   current frame using future frames by: (1) employing handobject interaction detection [50], skin segmentation [48],   and hand landmark estimation [1] to produce affordance   points, and (2) utilizing SIFT [39] matching to map affordance points back to the current frame. HandProbes [18]   also leverages [50] to select frames for \u201cmasked hand-object   intersection prediction\u201d.   In contrast to HoI-forecast [36] and HandProbes [18],   which primarily focus on minimally altered egocentric images/videos [31] and rely on detecting common objects,   our proposed MaskAHand enhances affordance grounding   from video to image across substantial contextual variations, eliminating the need for common object detection.   6800   Query   Key   time/2   Encoder   (ResNet   /ViTDet)   Demonstration Video   Target Image   Transformer    Decoder   1.  Afformer as a supervised SOTA baseline    MAD   Layer   Refined Decoding   in This Level   Coarse Decoding from Last Level   Upsample   cross-att   self-att    mlp   Add   Affordance Heatmap   on Target Image   Fine-grained Decoder    (FgD)   Action: Turn   Fine-grained Decoder Layer (FgD Layer)   Deconv   MAD   Layer   MAD   Layer   FgD   Layer   FgD   Layer   FgD   Layer   MLP   spatial\u00d72   spatial\u00d72   time/2   Figure 2. Our proposed Afformer is a simple yet effective model for video-to-image affordance grounding. The Afformer takes a demonstration video and a target image as inputs and produces an affordance heatmap on the target image. It employs an encoder to extract video   and image features, followed by a multi-scale, transformer-based decoder to progressively refine fine-grained affordance heatmaps.   3. Affordance Transformer   We introduce Affordance Transformer (Afformer), a   novel model for video-to-image affordance grounding. See   Figure 2, our model employs a multi-scale, transformerbased decoder to handle fine-grained affordance grounding.   We now proceed to discuss the Afformer in detail.   3.1. Formulation   We present the problem formulation for video-to-image   affordance grounding [13]. Given a demonstration video   V \u2208Rt\u00d73\u00d7hV \u00d7wV and a target image I \u2208R3\u00d7hI\u00d7wI as   inputs, the goal of video-to-image affordance grounding is   to predict an affordance heatmap H \u2208RhI\u00d7wI over the target image, accompanied by an action label A \u2208{1, . . . , c},   where c denotes the number of action classes. Here, t represents the number of sampled video frames, h and w indicate   the height and width of the image or video, respectively.   The Afformer F can be described as a function that maps   the demonstration video and target image to the affordance   heatmap and action label:     \\b ol d sym bol {F}(V, I) \\rightarrow (H, A). \\label {equation1}    (1)   3.2. Afformer Architecture   As shown in Figure 2, the Afformer F consists of a   video/image shared encoder Fe, a fine-grained decoder Fd,   and a prediction head Fh, i.e. F = Fh \u25e6(Fd \u25e6Fe).   Shared Encoder. Our Afformer employs a shared encoder   for both video and image feature representation, mitigating overfitting issues in small video-to-image affordance   grounding datasets [13,44,57]. During the encoding phase,   we only utilize a spatial network [12,21] to process images   and sampled video frames, reducing computational costs   caused by temporal modeling. To enable multi-scale decoding, we preserve multi-scale features during the encoding   process. This can be expressed as:     \\b o l dsy   m bo l {F_ e }(V   )  \\rightarrow \\{E_l^V\\}, \\boldsymbol {F_e}(I) \\rightarrow \\{E_l^I\\}, \\label {equation2}    (2)   where {EV   l } ({EI   l }) represents the set of features extracted   from the video (image) input at different scales l. Their   shapes are El   V \u2208Rt\u00d7C\u00d7hV   l \u00d7wV   l and El   I \u2208RC\u00d7hI   l \u00d7wI   l . C   is the feature dimension. Following [32], l is the stride with   respect to the input scale, e.g. l \u2208{2, 3, 4, 5}. We adopt   multi-scale feature pyramid networks such as ResNet with   FPN [21,32] and ViTDet [12,29] as the encoder.   Fine-grained Affordance Decoder. With multi-scale encodings {EV   l } and {EI   l }, our fine-grained decoder produces heatmap decoding Dlmin by     \\bol   d sy mbo   l  { F _d}(\\{E_l^V\\}, \\{E_l^I\\}) \\rightarrow D_{l_{min}}. \\label {equation3}    (3)   where lmin represents the minimal stride, which corresponds to the pyramid level of the highest feature resolution.   For example, if the encoder uses lmin = 2, then Dlmin will   be in RC\u00d7(hI/22)\u00d7(wI/22). Previous methods [13,44] typically use single-step, small-scale decoding (e.g., lmin = 5)   and a large deconvolution to predict large-scale heatmaps.   However, this approach loses spatial information and produces coarse predictions. In comparison, our decoder utilizes a multi-scale strategy to gradually decode the heatmap,   which leads to more detailed affordance heatmaps.   We first outline single-scale decoding before extending   it to multi-scale. Video-to-image affordance grounding can   6801   be considered a process where, at each spatial location in   the target image, we search the video to determine if the location corresponds to an affordance region. Consequently,   this process can be intuitively modeled as a cross-attention   operation, with image encodings serving as the query and   video encodings as the key and value. Unlike previous studies [13,44], our method explicitly incorporates spatial modeling, making it more suitable for heatmap grounding. The   core operation of multi-head cross-attention (MCA) is     \\b egin     {al igne d }    \\bol d   symb   o   l    {MC A}(\\ hat       {Q_l},\\hat {K_l},\\hat {V_l}) = \\sigma (\\frac {\\hat {Q_l}(\\hat {K_l})^\\mathrm {T}}{\\sqrt {C}} + R_l)\\hat {V_l}W_l^{MCA}, \\end {aligned} (4)   where \u02c6Ql, \u02c6   Kl, \u02c6Vl denote flattened, layer-normalized, and   linearly mapped query, key, and value features from   EI   l , EV   l , EV   l , respectively. Rl represents the decomposed   relative positional encoding [29,30]. W MCA is a learnable   linear mapping, and \u03c3 is the softmax operation along the   sequence of the \u02c6   Kl axis. For simplicity, we only display a   single attention head here, while the actual attention head   used is a common C/64. According to [53], the complete   transformer decoder layer also consists of a multi-head selfattention layer (MSA) and an MLP layer (MLP). We perform MSA on image features, rather than video features,   to reduce memory usage, as video feature sequences are   longer due to the temporal dimension. The single-scale decoding Dl is obtained by     D _l = flatt   e n ( E^I_ l) + \\boldsymbol {MSA}(\\hat {Q_l}), \\label {equation5}    (5)     D _l  = D_l +     \\bo ldsymbol {MCA}(D_l,\\hat {K_l},\\hat {V_l}), \\label {equation6}    (6)     D _l  = D _l + \\boldsymbol {MLP}(D_l). \\label {equation7}    (7)   We proceed with multi-scale decoding.   The query in   Equation 5 should incorporate heatmap decoding at the   pyramid level (l + 1) for coarse-to-fine refinement. Furthermore, we find that maintaining a fixed resolution video   pyramids stabilize training. We select l = 3 to balance resolution and semantics for video pyramids. Meanwhile, as the   encoder only processes videos spatially, we apply temporal   sampling to video features to aggregate temporal information before decoding, as depicted in Figure 2. Consequently,   the multi-scale decoding can be expressed as     D _l = flatt   e n ( E^I_ l) +  \\boldsymbol {MSA}(\\hat {Q_l} + UP(D_{l+1})), \\label {equation8}    (8)     D _l  = D_l +     \\b   olds ymb   ol {MCA}(D_l,\\hat {K_l}^{C3D},\\hat {V_l}^{C3D}), \\label {equation9}    (9)     D _l  = D _l + \\boldsymbol {MLP}(D_l). \\label {equation10}    (10)   where UP denotes to the nearest spatial upsampling used in   FPN [32]. C3D denotes spatial-temporal convolution [52]   with a kernel size of 3 and a stride of 2 applied only in the   temporal dimension. By multi-scale decoding we can get   the final fine-grained heatmap decoding Dlmin.   Heatmap and Action Prediction. The predictor computes   Fh(Dlmin) \u2192H, A.   We utilize appropriate deconvolutional layers to reconstruct the heatmap H by decoding   Dlmin (e.g., C \u00d764\u00d764 7\u21921\u00d7256\u00d7256). A MLP network   and adaptive 2D pooling are employed for action classification A. Thus, we have concisely presented the Afformer as   F (V, I) \u2192H, A.   Training Loss Function. Our Afformer predicts action logits for c classes and the corresponding hI\u00d7wI heatmap. In a   supervised setting, Afformer is trained with action classification loss La and heatmap regression loss Lh. The former   uses multi-class cross-entropy loss, while the latter employs   KL divergence as the heatmap loss [2,13]:     \\   ma   t   h   ca   l       {L}_{h} = \\sum _i^ {h^I}\\su   m _j^{w^   I} 1_{g_{i,j}>0} \\sigma _{g}(g)_{i,j}\\log \\frac {\\sigma _g(g)_{i,j}}{\\sigma _{h}(h)_{i,j}}, \\label {equation11}    (11)   where   g   represents   the   ground-truth   heatmap,   \u03c3g(g)i,j   =   gi,j/ P g   normalizes   by   sum,   and   \u03c3h(h)i,j   =   exp(hi,j)/ P exp(h) normalizes by softmax. The total loss for optimization is L = La + Lh.   Our proposed Afformer attains fine-grained heatmap decoding via multi-scale cross-attention, making it an effective model for video-to-image affordance grounding. However, the limited available training data [13, 44, 57] (<   20k) restricts context variation and may impair the Afformer\u2019s ability to ground affordances across video-image   discrepancies. While some modified cross-attention techniques [23,34] aim to address the low-data issue [23], they   focus on feature aspects and are limited to few-shot settings. We propose to tackle the problem from data, yielding   a more versatile solution applicable to various challenges.   In the following section, we introduce a self-supervised pretraining method for video-to-image affordance grounding.   4. Masked Affordance Hand Grounding   We introduce Masked Affordance Hand (MaskAHand),   a self-supervised pre-training approach for video-to-image   affordance grounding.   As depicted in Figure 3, our   method leverages the consistency between hand interaction   and affordance regions, approximating affordance grounding to hand interaction grounding.   MaskAHand can be   viewed as an extension of the \u201cmasked image modeling\u201d   paradigm [20,54,56] to \u201cmasked hand grounding\u201d. We now   elaborate on the MaskAHand method in detail.   4.1. Formulation   As described in Section 3.1, the video-to-image affordance grounding can be expressed as Equation 1:   F(V, I)   \u2192   H, A.   Since action prediction is a wellestablished problem in action recognition [14,15,25,52,60],   we focus on the challenging task of affordance heatmap   6802   KL-Div Loss   Frames contain interaction   Perspective Transformation   Hand Context Mask   Video   Afformer   Random Context Mask   Target    Image    \u2460Video and Target Image Synthesis including Hand Interaction    \u2461Video-to-Image Masked Hand Grounding   Hand Mask GT   Hand Interaction    score: 0.998 > 0.99   Long Demonstration Video   Hand Interaction    Clip Mining   Figure 3. MaskAHand pre-training focuses on \u201cvideo-to-image masked interaction hand grounding\u201d, acting as a proxy task for videoto-image affordance grounding. There are two steps: (1) video and target image synthesis including hand interaction, and (2) training   Afformer with the generated data to learn \u201cvideo-to-image masked interaction hand grounding\u201d. GT stands for \u201cground-truth\u201d.   grounding, denoted by F (V, I) \u2192H. However, annotating   for this heatmap is labor-intensive, as it necessitates thoroughly reviewing the entire video, correlating it with the   image, and pinpointing affordances. Consequently, current   video-to-image affordance grounding datasets contain limited training samples, with fewer than 20k in total.   We consider solving the limited data problem in the task   F (V, I) \u2192H. We observe that all affordance regions are   interacted with by hands. The human hand exhibits a distinct visual pattern, making it easier to detect than irregular   affordance regions. The interaction state can also be readily distinguished using the hand interaction detector [50].   Consequently, we focus on a related task, F \u2032(V, I) \u2192H\u2032,   which is simpler to gather data for, where H\u2032 represents the   \u201dimagined\u201d hand in the target image that interacts with the   affordance region shown in the video. As demonstrated, the   capabilities of F \u2032 and F are closely related, allowing us to   obtain F by fine-tuning the F \u2032 network or even considering   F \u2032 as F for zero-shot grounding.   However, training F \u2032(V, I) \u2192H\u2032 still needs the demonstration video V , the target image I, and interaction hand   annotation heatmap H\u2032 (e.g., hand box mask). But thanks to   our approximation for the original task, the data preparation   becomes much simpler and allows us to do self-supervised   pre-training. We refer to our approach as Masked Affordance Hand (MaskAHand), illustrated in Figure 3 and described in the following section.   4.2. Affordance-related Data Synthesis   Hand Interaction Detection.   Our MaskAHand relies   solely on a hand interaction detector, eliminating the need   for an object detector as required by [18, 36].   We employ a Faster R-CNN [32, 47] trained on the 100DOH   dataset [8,28,50,51] to detect hand bounding boxes and output binary hand states (i.e., interacting or not). Our trained   detector achieves an 84.9 AP in hand interaction detection   on the 100DOH test set, demonstrating its accuracy and reliability for synthesizing hand interaction data.   Hand Interaction Clip Mining. We extract multiple hand   interaction clips from a long demonstration video, each containing 32 consecutive frames and regarding as V , guaranteeing the presence of an interacting hand in at least one   frame. To avoid redundancy, we apply a stride of 16 frames   between successive clips. We only set a high interaction   score threshold 0.99 to reduce false positives.   Target Image Synthesis and Transformation. Inspired by   SuperPoint [10], we synthesize the target image I from V   by simulating video-image context changes, as illustrated in   Figure 3. The process consists of four steps: (1) Select the   corresponding frame from V involving the interaction hand;   (2) Apply a mask to conceal the hand, as the target image   should not include it, making the hand context mask Mh   larger than the detected hand box by a factor of > 1 (e.g.,   1.5 times); (3) Introduce a random context mask Mr with   the same scale as Mh to enhance MaskAHand pre-training   difficulty, preventing the model from simply predicting the   masked region; (4) Apply a random perspective transformation to simulate perspective change between the demonstration video and target image (e.g., egocentric vs. exocentric).   4.3. Video-to-Image Masked Hand Grounding   Given a masked and perspective-transformed interaction   frame as the target image I and a mined interaction clip as   the video V , the Afformer without action predictor F \u2032(V, I)   6803   Target Image   Prediction   Demonstration Video From AssistQ Buttons   Prediction   Demonstration Video From OPRA   Target Image   Figure 4. Afformer\u2019s video-to-image affordance grounding visualization. MaskAHand\u2019s visualization is in the supplementary material.   takes them as input and produces a heatmap H\u2032. Unlike   the supervised setting, the ground truth for H\u2032 is derived   from the detected hand box mask, subjected to the same   perspective transformation as I (see Figure 3).   During   MaskAHand pre-training, the network is tasked with videoto-image masked hand grounding, which requires observing the video to \u201cmatch\u201d the unmasked context in the target image and predict the precise hand box position. Thus,   this pre-training shares abilities essential for video-to-image   affordance grounding, including context matching between   the video and image to ground affordances in the image.   For MaskAHand pre-training, we still utilize the KLD   loss, as shown in Equation 11.   The ground-truth hand   box masks are gaussian blurred, following video-to-image   affordance grounding [13]. After training Afformer with   MaskAHand, we can directly perform zero-shot evaluation   on video-to-image affordance grounding, or fine-tune using   supervised data and subsequently conduct evaluation.   5. Experiments   5.1. Experimental settings   Datasets. We conduct experiments on three datasets:   \u2022 OPRA [13] consists of YouTube product review videos   for appliances (e.g., washing machine, stove). Each data   sample consists of a video clip V (e.g., holding a frying   pan), paired with a target image I (e.g., ads picture of the   frying pan), an action label A (e.g., holding) belonging to a   total of 7 actions, and ten annotated points on the image representing the affordance region (e.g., ten points around the   frying pan handle). The ten points are always produced as   ground-truth heatmap H by applying a gaussian blur with a   kernel size of 3 [13,40,44]. The dataset comprises roughly   16k training samples and 4k testing samples, each represented in the form of (V, I, A, H).   \u2022 EPIC-Hotspot [44] is made up of EPIC-Kitchens [8],   which contains egocentric videos of kitchen activities.   EPIC-Kitchens provides an action label A and annotations   for interacted objects in video V , but no target image.   EPIC-Hotspot chooses one frame in V to be the target image I that corresponds to the object class and appearance.   Follow [44] for more information. They crowdsource annotations for ground-truth heatmaps H after I is chosen,   yielding 1.8k annotated instances across 20 actions and   31 objects.   The data sample format in EPIC-Hotspot is   (V, I, A, H), which is the same as OPRA.   \u2022 AssistQ [57] is a benchmark to solve user egocentric   query [27] according to instructional videos. It includes   fine-grained button regions for a wide range of everyday devices (e.g., microwave), which require precise affordance prediction to distinguish very close buttons. In AssistQ [57], we consider the instructional video to be the active video and the user view to be the inactive image. Using transcript timestamps, we divide the instructional video   into multiple clips V and manually annotate the interacted   button on the inactive image I. We finally get 650 training samples from 80 videos and 91 testing samples from 20   videos, and each of sample contains active video clip, inactive image, and active-to-inactive button bounding-boxes.   The ground-truth heatmap H is generated using gaussian   blur for the button center point map. Because the action   class in AssistQ is mostly limited to \u201cpress\u201d or \u201cturn\u201d, the   data sample format is (V, I, H) without action.   We report results on the test sets of these datasets and   perform ablation studies on the largest OPRA dataset.   Implementation Details.   We train the Afformer model   with a ResNet encoder using a batch size of 16 and 5k iterations, employing the AdamW optimizer [38] and a cosine   annealing learning rate scheduler [37] with an initial learning rate of 3 \u00d7 10\u22124. As per [13], we set the spatial size of   both images and videos to 256. The ground-truth heatmap is   generated using annotation points (box center for AssistQ)   mapped to a Gaussian blur kernel size of \u221a256 \u00d7 256/3,   following [13,44]. For the Afformer with a ViTDet encoder,   we adjust the learning rate to 2\u00d710\u22124 and the spatial size to   1024 to accommodate the pre-trained positional encodings   from [29]. All encoders are initialized with COCO [33]   detection weights [29, 47], following [13]. These hyperparameters remain consistent across all datasets, including   MaskAHand pre-training.   Evaluation. We report saliency metrics [3] as KLD, SIM,   and AUC-J. Please refer to [13,44] for more details.   6804   Method   Variants   OPRA (256 \u00d7 256)   Heatmap KLD \u2193   Action Top-1 Acc \u2191   Demo2Vec [13]   LSTM   3.45   20.41   ConvLSTM   3.31   30.20   TSA + ConvLSTM   3.34   38.47   Motion + TSA + ConvLSTM   2.34   40.79   Naive Baseline (Ours)   ResNet-50-Deconv   2.20   45.66   Afformer (Ours)   ViTDet-B-Afformer   1.51   52.27   ResNet-50-Afformer   1.55   52.14   MaskAHand (Ours)   Zero-shot   ResNet-50-Deconv   2.89   n/a   ResNet-50-Afformer   2.36   n/a   Fine-tune   ResNet-50-Deconv   1.74   48.93   ResNet-50-Afformer   1.48   52.50   Table 1. Video-to-image affordance grounding performance of our Afformer and MaskAHand models on the OPRA dataset (fine-grained,   256\u00d7256): Afformer reduces heatmap KLD errors by over 30%; MaskAHand\u2019s zero-shot pre-training results are comparable to [13] (2.36   vs. 2.34); further fine-tuning yields the best performance on OPRA.   Method   Method   OPRA (28 \u00d7 28)   EPIC (28 \u00d7 28)   KLD \u2193   SIM \u2191   AUC-J \u2191   KLD \u2193   SIM \u2191   AUC-J \u2191   Weakly   Supervised   EGOGAZE [24,44]   2.43   0.25   0.65   2.24   0.27   0.61   MLNET [7,44]   4.02   0.28   0.76   6.12   0.32   0.75   DEEPGAZEII [26,44]   1.90   0.30   0.72   1.35   0.39   0.75   SALGAN [44,45]   2.12   0.31   0.77   1.51   0.40   0.77   Hotspot [44]   1.42   0.36   0.81   1.26   0.40   0.79   HAG-Net (+Hand Box) [40]   1.41   0.37   0.81   1.21   0.41   0.80   Self-supervised   Zero-shot   Center Bias (action agnostic)   11.13   0.21   0.63   10.66   0.22   0.63   MaskAHand (action agnostic)   1.86   0.28   0.76   1.32   0.37   0.76   Supervised   Img2heatmap [44]   1.47   0.36   0.82   1.40   0.36   0.80   Demo2Vec [13]   1.20   0.48   0.85   n/a   n/a   n/a   Afformer (Ours)   1.05   0.53   0.89   0.97   0.56   0.88   Table 2. Performance of Afformer and MaskAHand models on OPRA and EPIC-Hotspot datasets (coarse-grained, 28 \u00d7 28). MaskAHand   can surpass many weakly-supervised methods in KLD. Afformer achieve the best performance among supervised methods.   Method   KLD \u2193   SIM \u2191   Top-1 Acc \u2191   Afformer   1.13   0.44   0.41   +MaskAHand   1.01(-12%)   0.54(+23%)   0.57 (+28%)   Table 3. Results on AssistQ Buttons [57]. \u201cAcc\u201d refers to the   accuracy of button classification.   5.2. Main Results   Fine-grained Video-to-image Affordance Grounding. In   Table 1, the prior method Demo2Vec [13] achieves a   KLD error of 2.34 on OPRA, which is comparable to our   naive ResNet-50 [21] + deconvolution baseline with 2.20   KLD. However, our proposed Afformer significantly reduces KLD error. Utilizing ResNet-50 and ViTDet-B [29]   backbones, Afformer attains 1.55 and 1.51 KLD, respectively, surpassing previous results by over 30%.   We attribute Afformer\u2019s success to its better design for finegrained affordance heatmaps, which also boosts action classification accuracy.   We also assess MaskAHand pre-training as Table 1 reveals, surprisingly, its zero-shot results are already comparable to Demo2Vec (2.36 vs. 2.34), demonstrating its effectiveness as a proxy task for supervised video-to-image   affordance tasks. Furthermore, fine-tuning the MaskAHand   pre-trained Afformer on OPRA leads to the lowest KLD error of 1.48, a 37% improvement over Demo2Vec.   Coarse-grained   Affordance   Grounding   on   OPRA,   EPIC-Hotspots. We adopt the evaluation protocol from   [44] to assess our method\u2019s performance on coarse-grained   affordance grounding at low resolution (28 \u00d7 28).   We   downsample the standard resolution 256 \u00d7 256 prediction   heatmap to 28 \u00d7 28 using bilinear interpolation during   both training and inference phases. Table 2 demonstrates   that Afformer, despite not being explicitly designed for   lower resolution, outperforms other methods. Moreover, the   self-supervised MaskAHand zero-shot results surpass some   weakly-supervised approaches.   Video-to-image Grounding on small-scale Data. We train   our Afformer and fine-tune MaskAHand models on AssistQ   Buttons, which contains only 600+ training samples, posing challenges for data-hungry deep neural networks. As   demonstrated in Table 3, MaskAHand pre-training substantially reduces the heatmap KLD error and increases SIM   metric, increasing the relative button classification accuracy   by 28%. Thus, MaskAHand self-supervised pre-training is   a viable option when dealing with limited video-to-image   affordance grounding data.   6805   Spatial I   Spatial V   KLD \u2193   82   82   1.88   162   1.73   322   1.65   642   1.65   322   162   1.63   322   1.62   642   1.63   (a) Single pyramid. I: image, V: video.   Spatial I   Spatial V   KLD \u2193   322   322   1.62   162 \u2192322   1.63   322   322   1.62   162 \u2192322   1.60   82 \u2192162 \u2192322   322   1.59   162 \u2192322 \u2192642   1.57   82 \u2192... \u2192642   1.57   (b) Multiple pyramids. I: image, V: video.   Spatial   Temporal   KLD \u2193   Mem \u2193   162   \u2193   322   \u2193   642   T   1.57   \u22120.0 G   T \u2192T   2 \u2192T   2   1.56   \u22122.7 G   T \u2192T   2 \u2192T   4   1.55   \u22123.8 G   (c) Temporal pyramids and reduced memory.   Table 4. Ablation study results for Afformer on the OPRA dataset at a 256\u00d7256 scale, comparing default settings (image spatial pyramids:   162 \u2192322 \u2192642, video spatial pyramid: single 322, no temporal downsampling, and one cross/self-attention module in decoding).   Enhanced by multi-scale, high-resolution decoding, performance significantly improves (1.57 vs. 1.88), while temporal pyramids further   reduce KLD error and decrease GPU memory consumption.   # Hand Mask   (#Mh <= 1)   # Random   Mask (#Mr)   Mask   Scale   Zero-shot KLD   0   0   n/a   4.35   1   0   1.0\u00d7   4.29   1   0   1.5\u00d7   2.68   1   0   2.0\u00d7   2.54   1   0   3.0\u00d7   3.42   1   1   1.5\u00d7   2.48   1   1   2.0\u00d7   2.75   1   2   2.0\u00d7   2.98   (a) Ablations on masking ratio and number of masks.   Masking   Distortion   Zero-shot KLD   Fine-tune KLD   #Mh = 1,   #Mr = 1,   1.5\u00d7   0   2.48   1.53   0.25   2.40   1.50   0.5   2.36   1.48   1.0   n/a   n/a   (b) Ablations on perspective transformation. \u201cn/a\u201d: network divergence.   Table 5. Ablation studies of MaskAHand pre-training on OPRA.   5.3. Ablation Studies   Afformer Fine-grained Decoder. We evaluate our finegrained decoder on the OPRA dataset (256 \u00d7 256 resolution). Table 4(a) reveals that the highest image pyramid resolution (642) reduces the KLD error; however, for videos, a   322 resolution is more effective, potentially due to weaker   semantics in high-resolution pyramids. Table 4(b) indicates   that preserving a fixed video pyramids when building lowto-high resolution image pyramids also decreases KLD error. As per Table 4(c), constructing video temporal pyramids results in considerable memory savings and slight performance enhancement. These findings suggest the significance of our fine-grained decoder in heatmap decoding.   Context Masking in MaskAHand. We examine the context masking effects in MaskAHand (Table 5(a)). Without   masking (#Mh = 0 and #Mr = 0), pre-training degenerates into hand saliency detection (Figure 3), providing no   benefit to video-to-image affordance grounding and resulting in a large KLD error 4.35. A similar outcome occurs   with hand masking only: when #Mh = 1 and #Mr = 0,   the network can directly predict the masked region for a low   training loss, yielding a meaningless result (KLD 4.29).   However, increasing the hand mask region to 1.5\u00d7 significantly reduces the zero-shot KLD error to 2.68, indicating that the network learns a useful representation for   video-to-image affordance grounding. Extending the mask   scale to 3.0\u00d7 causes performance degradation, likely due to   the overly large masked region creating challenging context   matching between video and image. An additional random   mask offers a similar effect as enlarging the hand mask but   with more diversity, preventing the network from merely   predicting a region within the hand mask. Therefore, we   use #Mh = 1, #Mr = 1, and 1.5\u00d7 hand box masking as   the default MaskAHand pre-training setting.   Perspective Transformation in MaskAHand. Our context masking strategy is to enhance the grounding ability   across the video-image context differences. However, it is   also crucial to consider perspective transformation, such as   enabling simulation of egocentric and exocentric views. As   shown in Table 5(b), perspective transformation can lead to   performance improvements when the distortion ratio is in a   reasonable range.   6. Conclusion   In this paper, we introduce the Affordance Transformer   (Afformer), a simple and effective model for video-toimage affordance grounding, utilizing multi-scale decoding to generate fine-grained affordance heatmaps. We also   propose a pre-training technique, Masked Affordance Hand   (MaskAHand), that employs a proxy task of masked hand   interaction grounding, facilitating data collection while   benefiting video-to-image affordance grounding. Our extensive experiments show Afformer significantly outperforms previous methods, and MaskAHand pre-training impressively improves performance on small datasets.   Acknowledgment   This project is supported by the   National Research Foundation, Singapore under its NRFF   Award NRF-NRFF13-2021-0008, and Mike Zheng Shou\u2019s   Start-Up Grant from NUS. The computational work for this   article was partially performed on resources of the National   Supercomputing Centre, Singapore.   6806", "conf": "CVPR", "year": "2023", "index": 308}, {"title": "INTRA: Interaction Relationship-aware   Weakly Supervised Affordance Grounding   Ji Ha Jang1\u2217, Hoigi Seo1\u2217, and Se Young Chun1,2\u2020   1Dept. of Electrical and Computer Engineering, 2INMC & IPAI   Seoul National University, Republic of Korea   {jeeit17, seohoiki3215, sychun}@snu.ac.kr", "abstract": ". Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised   affordance grounding teaches agents the concept of affordance without   costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired   exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose   INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through   contrastive learning with exocentric images only, eliminating the need   for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text,   designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with   our text synonym augmentation. Our method outperformed prior arts on   diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable   domain scalability for synthesized images / illustrations and is capable   of performing affordance grounding for novel interactions and objects.   Project page: https://jeeit17.github.io/INTRA   Keywords: Affordance grounding \u00b7 Weak supervision \u00b7 Exocentric image \u00b7 Contrastive learning \u00b7 Interaction relation   1", "content": "Affordance [17] refers to the perceived possible interactions based on an object\u2019s   inherent or recognized properties (e.g., the rim of a wine glass affords sipping   while stem of it affords holding). Humans can identify affordances of objects   and interact with proper parts despite the diversity in their physical attributes.   * Authors contributed equally. \u2020 Corresponding author.   2   Jang & Seo et al.   INTRA (Ours)   LOCATE   GT   Object Image   Drink with   Sit on   Pour   Ride   INTRA (Ours)   LOCATE   GT   Object Image   Fig. 1: Prior works on weakly-supervised affordance grounding like LOCATE [25] often failed to ground different affordances for the same object. However, our proposed   INTRA yielded finer and more accurate grounding results for them that are closer to   the ground truth (GT) by considering interaction relationship among them.   This ability can be acquired through individual learning, by directly interacting   with objects, and observational learning [8], by observing others\u2019 interactions.   The sense of affordance enables effective interaction in new environments or with   novel objects, without step-by-step instructions [57]. Affordance plays an essential role across numerous applications involving intelligent agents, enabling them   to provide flexible and timely responses in complex, dynamic environments [5].   These applications include task planning, robot grasping, manipulation, scene   understanding and action prediction [2,6,7,16,51,62].   Affordance grounding is the task to teach intelligent systems how to locate   possible action regions in objects for a certain interaction. While fully supervised learning [4, 18, 42, 58] is the most straightforward approach, its reliance   on costly annotations may limit its applicability across diverse contexts. Another approach is weakly supervised learning, similar to human\u2019s observational   learning [8], that does not require GT, but weak labels. In this setting, exocentric images illustrating human-object interactions, along with corresponding   egocentric images depicting the objects, are provided during training. During   inference, intelligent systems perform affordance grounding on the egocentric   images, identifying object parts relevant to the given interactions. Recent advances in weakly supervised affordance grounding [25, 31, 32, 41] proposed to   use pairs of exocentric and egocentric images, yielding great performance. The   deep neural networks learn affordances by pulling features from exocentric and   egocentric images closer, aiming to focus on object parts related to interactions.   However, weakly supervised affordance grounding remains challenging. Firstly,   the requirement for current weak labels with pairs of exocentric and egocentric   images is still strong. Note that human observational learning does not usually require egocentric images. Secondly, a complex relationship between interactions exists, which has not been adequately addressed in prior works. Many   instances in object-interaction relationships exhibit intricate many-to-many associations, occasionally with one entailing another. For example, some distinct   Interaction Relationship-aware Affordance Grounding   3   interactions represent the same affordance regions (e.g., \u2018wash\u2019 and \u2018brush with\u2019   a tooth brush), and there are closely related interactions that always come together (e.g., \u2018sip\u2019 usually includes \u2018hold\u2019. \u2018ride\u2019 usually includes \u2018sit on\u2019). This   complexity poses challenges in extracting interaction-relevant features based on   image-level affordance labels, introducing biases towards objects in affordance   grounding as illustrated in Fig. 1 (LOCATE [25] often yielded similar affordance   grounding with different interactions for the same object).   Here, we propose a novel weakly supervised affordance grounding method,   INTRA (INTeraction Relationship-aware weakly supervised Affordance grounding) to address these unexplored challenges. While previous studies [31,41] solved   the weak supervision problem as supervised learning by pulling object features   of exocentric and egocentric images closer and LOCATE [25] enhanced this approach by generating more localized pseudo labels based on prior information for   exocentric images for supervised learning (i.e., containing human, object part,   and background), our INTRA framework recasts the weak supervision problem   as representation learning. This novel reformulation allows us to use weaker labels (i.e., exocentric images only) for training so that the requirement to use   pairs of exocentric / egocentric images is now alleviated. Moreover, unlike prior   works, our INTRA method actively exploits large language model (LLM) as well   as the text encoder of the vision-language model (VLM) to leverage linguistic information and existing textual knowledge on affordances, which further enhances   our interaction relationship-guided contrastive learning. This novel scheme also   allows excellent scalability for unseen objects across diverse domains as well as   zero-shot inference for novel interactions, which was not possible in prior arts.   In summary, our main contributions are three-fold as follows:   \u2013 We propose a novel approach for weakly supervised affordance grounding   by recasting the problem as representation learning and by leveraging VLM,   leading to relaxing the need for paired training datasets for more weak supervision and enhancing scalability across domains for unseen objects.   \u2013 We proposed INTRA, a novel method that consists of text synonym augmentation and text-conditioned affordance map generation module along   with interaction relationship-guided contrastive learning, so that inference   on unseen interactions is possible.   \u2013 Our INTRA outperforms the prior arts in weakly supervised affordance   grounding on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD,   demonstrating both qualitative and quantitative excellence (see Fig. 1).   2   Related Works   2.1   Affordance Grounding   Supervised affordance grounding. Supervised affordance grounding methods [10,14] analyze interaction videos / images to predict affordance regions on   an object, trained with pixel-level GT masks / heat maps. Though successful in   localizing fine-grained affordance regions through supervised learning, they are   4   Jang & Seo et al.   limited by the costly GT mask annotation process and their limited generalizability to unseen objects. Furthermore, they require paired demonstration videos   and target object images, making real-world application challenging.   Weakly supervised affordance grounding. Weakly supervised affordance   grounding methods [13,20,23,25,31\u201333,41,47] offer the advantage of not requiring   GT, but requiring weak labels such as exocentric images with interaction text   labels. Prior works [25,31,32,41] mainly align interaction-relevant object features   from both egocentric and exocentric images without considering the intrinsic   properties of interactions. The framework in [41] predicts object features engaged   in interactions by analyzing human-object interaction videos. The works of [31,   32] preserve the correlation of affordance features from exocentric and egocentric   images to learn affordances. The work of [25] enhances object feature extraction   by adopting DINO-ViT [9] based Class Activation Maps (CAM) [64] and k-means   clustering [35] for more explicit guidance. However, focusing solely on object   features may introduce biases towards object, hindering the inference of multiple   affordances for a single object. Our INTRA addresses this issue by considering   the complex relationships between interactions using interaction relationshipguided contrastive loss, while ensuring the network remains attentive to the   objects using object-variance mitigation loss.   2.2   Foundation Models for Affordance Grounding   Self-supervised transformer. Self-supervised transformers, extensively trained   on large-scale datasets and scalability, possess robust representation power. Previous works [25,52] have explored their potential in affordance grounding. DINOViT [9], a vision transformer foundation model trained in a self-supervised manner, can identify both high-semantics such as overall information of the image   and low-semantics such as details regarding specific object parts. This versatility has led advancements in various tasks, including classification, semantic   segmentation [4,24] and semantic correspondence [61]. LOCATE [25] leverages   DINO-ViT to extract low-semantic information, resulting in performance improvements in affordance grounding. Our INTRA employed DINOv2 [46] as an   image encoder to extract information about objects and their constituent parts.   Vision-language model. The Vision-Language Model (VLM) is a class of   models jointly pretrained on visual and language data for various downstream   tasks [45,56,60]. VLM text encoders, trained through contrastive learning with   image-text pairs, capture representations in the joint space of the images and   text [26\u201328, 50]. These text encoders, incorporating visual information, have   demonstrated excellent performance across multiple tasks. ALBEF [28] notably   enhances vision and language representation learning by aligning image and   text features before fusing them. While supervised affordance grounding methods leveraging VLM text encoders [44] have been explored, their application in   weakly supervised affordance grounding remains underexplored. We propose a   Interaction Relationship-aware Affordance Grounding   5   \u201c Take photo\u201d    \u201cBoxing\u201d    \u201cCut with\u201d   \u201cHold\u201d     Interaction Labels   \u2026   Exocentric Images   \u2026   X   Interaction relationship-guided    contrastive learning   Projection   \u2131!\"#   Pairwise   cosine   similarity   CAM   Affordance    Map   \u2131!$#   CAM   Affordance    Map   \u2131!\"#!   \u2131\"#$   [   ]   X   PartSelect   X   (a) LOCATE (Prior work)   (b) INTRA (Ours)   Selection   \u2026   \u2026   \u2026   \u2026   Paired   \u2026   [CLS]   \u2131\"#$   \u2131!\"#!   [CLS]   !!\"\"   Transformer   Encoder   Conv layers   \u2131#$#!%   &   Conv layers   Linear   Affordance   Map   Generation   Module   Pull? Push?   Conv layers   Text    Synonym    Augmentation   Text   Encoder   Image   Encoder   Exocentric Images   Egocentric Images   Image   Encoder   Image   Encoder   \u201c Take photo\u201d    \u201cBoxing\u201d    \u201cCut with\u201d   \u201cHold\u201d     Interaction Labels   [   ]   \u2131!$!%&   \"\"#$   !!\"\"   #'($   Affordance    Map   Fig. 2: Overall frameworks of (a) LOCATE [25] and (b) INTRA (Ours). LOCATE   takes paired exocentric and egocentric images to generate interaction-aware affordance   maps (CAMs) for predefined interactions and then selects an interaction-related CAM   by the given interaction label. In contrast, INTRA takes only exocentric images and   interaction labels to yield an affordance map through our affordance map generation   module. Training is done via interaction relationship-guided contrastive learning on   exocentric features from affordance maps. Note that all encoder parameters are frozen.   framework leveraging the text encoder of ALBEF to enable novel interactions,   diverging from prior arts limited to inferring predetermined sets of affordances.   Large language model. Understanding affordance relationships is crucial for   affordance grounding, as it enables extending and linking learned visual cues, and   reasoning about affordances for new objects, interactions, or situations. While   prior works like [19] leverage semantically similar object properties and [32]   utilize affordance feature correlation, none directly exploit these relationships.   We use these intricate relationships in affordance learning by adopting Large   Language Models (LLMs). LLMs have gained prominence in robotics due to   their profound natural language understanding, providing valuable priors about   interactions and their complex relationships. Previous works [3,29,54,63] focus   on extracting action knowledge, deriving task-specific plans, and grounding them   in the physical world. LLMs have also been widely used in previous affordance   studies [37,55], demonstrating their exceptional understanding of interactions.   3   Method   Prior arts in weakly supervised affordance grounding [25, 31, 32, 41] typically   align object features of paired exocentric (interaction with object) and egocentric (object only) images to learn interaction-related features. For example, as   illustrated in Fig. 2(a), LOCATE [25] generates CAMs (affordance maps) from   exocentric and egocentric images for a pre-determined interaction label, extracts   egocentric feature as well as exocentric object parts feature selected by PartSelect module (pseudo label), and then trains the model by optimizing cosine   6   Jang & Seo et al.   similarity to align (pull) egocentric and exocentric object parts features. In contrast, we propose an alternative approach, INTRA, whose overall framework is   illustrated in Fig. 2(b). Our text-conditioned affordance grounding framework   of INTRA leverages VLM text encoder in affordance map generation module   and employs text synonym augmentation to enhance robustness, as will be described in Sec. 3.1. Then, INTRA learn affordance grounding via our interaction   relationship-guided contrastive learning, detailed in Sec. 3.2. The framework of   INTRA as depicted in Fig. 2(b) clearly suggests two advantages over prior arts   including LOCATE [25]: 1) it exploits exocentric images only and 2) INTRA   admits novel interactions outside the pre-defined interaction set.   3.1   Text-conditioned Affordance Grounding Framework   To utilize the semantic meanings inherent in interaction labels and enable flexible   inference on novel verbs, our text-conditioned affordance grounding framework   generates affordance maps by conditioning image features with text features via   our affordance map generation module where text and image features extracted   from separately pre-trained text and image encoders are fused. In specific, as   depicted in Fig. 2(b), deep features Fexo \u2208R(h\u00d7w)\u00d7d are obtained from the   input exocentric images using DINOv2 [46], where h and w represent the height   and width of the affordance map, and d refers to the dimension of the feature.   The text feature Ftext of the given interaction is obtained using the ALBEF text   encoder [28]. See the supplementary material for further details on the rationale   for employing DINOv2 and the ablation study on the text encoder.   Affordance map generation module. Before fusing text and image features,   the class token of Ftext passes through a single linear layer to align the separately pre-trained image and text embedding spaces and connect them, as shown   to be effective in previous works [30,65]. Subsequently, image features Fexo and   the class token of text features are concatenated and processed through a transformer encoder for conditioning. The image feature part of the resulting vector   is then projected using a multi-layered convolutional network and normalized   using min-max normalization to obtain the affordance map Maff \u2208Rh\u00d7w. This   affordance map represents the image regions in exocentric images most relevant   to interactions. During inference, Maff functions directly as an output heatmap,   indicating the image regions in egocentric images most relevant to interactions.   Text synonym augmentation. To enhance the robustness of text conditioning, we integrate text synonym augmentation into our interaction embeddings.   Initially, we generate ks synonyms for each interaction label using LLM. Subsequently, any synonyms overlapping with other interaction labels are removed.   These synonyms are then randomly selected to substitue the text conditioning   interaction embedding, while the original interaction label is retained for interaction relationship-guided contrastive learning. This module enhances overall   performance by providing models with enriched interpretations of interactions.   Interaction Relationship-aware Affordance Grounding   7   3.2   Interaction Relationship-guided Contrastive Learning   Our INTRA learns via interaction relationship-guided contrastive learning by   comparing exocentric image features across diverse interactions. Our contrastive   learning consists of two key components, 1) extracting exocentric image features   with affordance map and 2) designing loss for interaction relationship-guided   contrastive learning, that enable the grounding of multiple affordances on a   single object.   Exocentric image feature extraction with affordance map. As described   in Sec 3.1, a text-conditioned affordance map, Maff, is generated to represent   interaction-relevant image regions of exocentric images. Then, the exocentric   image features fexo corresponding to the affordance map are extracted as follows:     \\c e ntering    f_{ex   o} = (1/hw ){ \\ Sigma _ {i = 1}^{h}\\Sigma _{j=1}^{w}\\mathcal {F}_{exo}(i,j) \\cdot \\mathcal {M}_{aff}(i, j)} \\in \\mathbb {R}^{d}. \\label {eq:eq1_feature}    (1)   The resulting fexo is then projected and normalized to obtain the exocentric   image feature zexo using an MLP layer, which will be used for training. This   projection layer was also used in previous works [11,12,59], which have demonstrated the necessity and efficiency of it.   Loss design for interaction relationship-guided contrastive learning.   Supervised contrastive learning [21] effectively derives good representations for   each class by focusing on common characteristics in positive pairs while disregarding those in negative pairs like other classes. However, in affordance grounding tasks, treating all other interaction classes as negative pairs may be inadequate due to the complex relationship among interactions. To mitigate this issue,   we propose interaction relationship-guided contrastive loss, Linter. Furthermore,   considering the subtle meaning variations within single interaction classes depending on the object and context, we also propose object-variance mitigation   loss, Lobj. Thus, the total loss for our INTRA is formulated as follows:     \\cen t ering \\ mathcal {L}_{total} = \\mathcal {L}_{inter} + \\lambda _{obj} \\mathcal {L}_{obj} \\label {eq:eq3_total_loss}    (2)   where \u03bbobj denotes the control parameter of Lobj.   Interaction relationship-guided contrastive loss. In affordance grounding,   treating all other interaction classes as negative pairs is inadequate due to the   intricate relationships between interactions. For example, \u2018Wash\u2019 and \u2018Brush   with\u2019 toothbrush or \u2018Pour\u2019 and \u2018Seal\u2019 bottle represent distinct interactions but act   on the same object parts. Manually finding these relationships is time-consuming   and impractical as the number of pairs grows quadratically with the number of   interaction (see the supplementary). Moreover, although linguistic relationships   like synonyms or co-occurrence were considered as substitutes, they are often   inadequate and degrade performance. For example, \u2018Sip\u2019 entails \u2018Hold\u2019, but they   act on different part of objects, and \u2018Wash\u2019 and \u2018Cut with\u2019 a knife have different   8   Jang & Seo et al.   Think of five common objects    that can be interacted with    sip and hold.   (Eat, Drink with) \u2013 \u201cPositive\u201d   (Hold, Hit) \u2013 \u201cPositive\u201d   (Cut with, Cut) \u2013 \u201cNegative\u201d   (Drink, Drink with) \u2013 \u201cNegative\u201d   (Boxing, Kick) - \u201cPositive\u201d   (Sit on, Lie on) \u2013 \u201cPositive\u201d   (Push, Ride) \u2013 \u201cNegative\u201d   ...   LLM generated relationships   Step 1   Step 2   What are the parts of the    objects corresponding to    the interactions?   \u201cHold\u201d   \u201cSip\u201d   \u201cHold\u201d   \u201cSip\u201d   \u2026   Step 3   Are the object    parts same?   No, Negative   Think of five common objects    that can be interacted with    brush with and wash.   What are the parts of the    objects corresponding to    the interactions?   Are the object    parts same?   Yes, Positive   \u201cBrush with\u201d   \u201cWash\u201d \u201cBrush with\u201d   \u201cWash\u201d   LLM   LLM   LLM   LLM   LLM   LLM   Coffee cup, mug cup, wine    glass, bottle, wine bottle   Tooth brush, brush, comb,    shoe brush, makeup brush   For mug cup, \u201csip\u201d: Rim/lip,    \u201chold\u201d: body/handle   For toothbrush, \u201cbrush with\u201d:    bristle head, \u201chold\u201d: handle   \u2026   \u2026   \u2026   Fig. 3: The overall scheme of interaction-relationship map (R) generation. LLM classifies all pairs of interactions in the dataset as positive or negative through chain of   thoughts. This process is based on reasoning if interactions occur on same object parts.   meanings, but they act on the same blade. To mitigate this, we leverage LLM   to determine if interaction pairs act on the same object part. Through Chain of   Thoughts (CoT), interaction pairs are categorized as positive or negative in three   steps as described in Fig. 3. In Step 1, LLM deduces five different objects where   both interactions could be performed. In Step 2, LLM identifies object parts   where these interactions could occur by considering five objects one by one,   not simultaneously. In Step 3, if the identified parts of the interaction pair are   the same, the pair is classified as positive; otherwise, negative. Positive pairs are   assigned 1 in the interaction-relationship map R, and negative pairs are assigned   0. We propose interaction-relationship guided contrastive loss by integrating R   into supervised contrastive learning as follows:     \\sma l   l    \\   mat   hc   al { L }   _{   i   nte   r} = \\su m  \\l   imi ts    _{i = 1}   ^{2N}    \\f   r   ac    {-1}{ 2 N_{ y_{i}} - 1}\\sum \\limits _{j=1}^{2N}\\mathcal {R}_{(y_i, y_j)} \\cdot \\log {\\frac {\\exp {(z_{exo}^i \\cdot z_{exo}^j / \\tau )}}{\\sum \\limits _{k=1}^{2N} \\mathbf {1}_{i \\neq k} \\cdot \\exp {(z_{exo}^i \\cdot z_{exo}^k / \\tau )}}} \\label {eq:eq2_interaction_loss}    (3)   where i, j are sample indices, yi, yj are class labels, Nyi is the number of samples in the batch labeled with yi, N is the total number of distinct samples in   the batch, zj   exo is the exocentric image feature vector of each sample, \u03c4 is the   temperature, and R(yi,yj) is the value of (yi, yj) pair in interaction-relationship   map.   Object-variance mitigation loss. In the context of affordance, the interpretation of the same interaction can vary significantly based on the object and   context. For instance, \u2018Hold\u2019 a baseball bat and a cup may seem similar since   both involve grasping an object. However, the former involves gripping the bat\u2019s   slender part, while the latter entails holding the cup\u2019s rounded, protruding part.   To address this variance within the same interaction category, we implemented   Interaction Relationship-aware Affordance Grounding   9   an object-variance mitigation loss Lobj as follows:        \\   sma   ll    \\su m     \\l   i   mit   s _{i= 1 }^{   2N}  \\f   rac  {1}{2N_   {o   _   {i}   }-1}\\ s um \\limit s  _{j=1}^{2N}\\mathbf {1}_{o_{i}=o_{j}} \\cdot \\log {\\frac {\\exp {(z_{exo}^i \\cdot z_{exo}^j / \\tau )}}{\\sum \\limits _{k=1}^{2N} \\mathbf {1}_{i \\neq k} \\cdot \\exp {(z_{exo}^i \\cdot z_{exo}^k / \\tau )}}} \\label {eq:eq3_obj_loss}    (4)   where oi, oj denote object class of i and j.   4   Experiments   4.1   Experimental Setting   Dataset and metrics. We conducted an evaluation of our method using the   Affordance Grounding Dataset (AGD20K) [32]. AGD20K comprises both exocentric and egocentric images, with 20,061 exocentric images and 3,755 egocentric images labeled with 36 affordances. The dataset support evaluation under   two settings: 1) the \u2018Seen\u2019 setting, where the object categories of the training and   testing sets are identical, and 2) the \u2018Unseen\u2019 setting, where no objects overlap   between the training and test sets. Our approach only used exocentric images in   training for all experiments, while other approaches were trained using both egocentric and exocentric images. We employed three evaluation metrics commonly   employed in previous affordance grounding methodologies: 1) Kullback-Leibler   Divergence (KLD), 2) Similarity (SIM), 3) and Normalized Scanpath Saliency   (NSS). These metrics were utilized to quantify the similarity between the distributions of ground truth heatmaps and predicted affordance grounding.   Implementation details. We employed DINOv2 as the image encoder and   ALBEF, fine-tuned with RefCOCO+, as the text encoder. ChatGPT-4 [1] served   as the LLM. Images were resized to 384\u00d7384, then cropped to 336\u00d7336. Training   utilized the Adam optimizer [22] with a learning rate of 2e-4 and a batch size of   256. The hyperparameter \u03bbobj was set to 4, and all experiments were conducted   on a single NVIDIA A100 GPU. More details are provided in the supplementary.   4.2   Comparison to State-of-the-art Methods   To comprehensively assess our method, we conduct quantitative and qualitative   comparisons with state-of-the-art weakly-supervised grounding methods, incorporating a user study. We further expand our experiments to include additional   datasets [40,43,53] for a comprehensive evaluation. Refer to the supplementary   materials for more details on the experimental settings.   Quantitative results. We evaluated previous works [15, 25, 31, 32, 36, 41, 48]   and our method based on the metrics mentioned above. Tab. 1 shows the quantitative comparison results of our method with prior arts. In both \u2018Seen\u2019 and   \u2018Unseen\u2019 setting, our approach surpasses the baseline performances across all   three metrics: KLD, SIM, and NSS, thereby setting a new state-of-the-art.   10   Jang & Seo et al.   Table 1: Quantitative results of ours and other baselines [15, 25, 31, 32, 36, 41, 48] on   the AGD20K dataset. \u2191/ \u2193indicates that higher / lower the metric is, the better the   model performs. INTRA outperformed all baselines, despite being trained only with   exocentric images, whereas other models incorporated both exocentric and egocentric   images during training.   Prior works   Seen   Unseen   mKLD\u2193mSIM\u2191mNSS\u2191mKLD\u2193mSIM\u2191mNSS\u2191   Weakly Supervised   Object Localization   EIL [36]   1.931   0.285   0.522   2.167   0.227   0.330   SPA [48]   5.528   0.221   0.357   7.425   0.167   0.262   TS-CAM [15]   1.842   0.260   0.336   2.104   0.201   0.151   Weakly Supervised   Affordance Grounding   Exo+Ego   Hotspots [41]   1.773   0.278   0.615   1.994   0.237   0.557   Cross-view-AG [32]   1.538   0.334   0.927   1.787   0.285   0.829   Cross-view-AG+ [31]   1.489   0.342   0.981   1.765   0.279   0.882   LOCATE [25]   1.226   0.401   1.177   1.405   0.372   1.157   Exo   INTRA (Ours)   1.199   0.407   1.239   1.365   0.375   1.209   Table 2: Quantitative results on the modified IIT-AFF, CAD, and UMD dataset for   our method and other baselines [25,31,32]. Models were trained in the \u2018Seen\u2019 setting of   AGD20K and tested on the datasets without additional training. INTRA outperformed   all baselines on all metrics across all datasets. * Objects with affordances that prior   works are unable to predict were eliminated from the datasets for fairness, wheares our   method can infer affordances on novel interactions.   IIT-AFF* [43]   CAD* [53]   UMD* [40]   mKLD\u2193mSIM\u2191mNSS\u2191mKLD\u2193mSIM\u2191mNSS\u2191mKLD\u2193mSIM\u2191mNSS\u2191   Cross-View-AG [32]   3.856   0.096   0.849   2.568   0.173   0.589   4.721   0.014   1.287   Cross-View-AG+ [31]   3.920   0.095   1.072   2.529   0.176   0.663   4.753   0.013   1.227   LOCATE [25]   3.315   0.115   1.709   2.528   0.187   0.558   4.083   0.026   2.699   INTRA(Ours)   2.663   0.148   2.511   2.095   0.243   1.259   3.081   0.062   4.195   Results on additional datasets. We evaluated the generalization and robustness of the INTRA framework, along with previous works [25,31,32] trained in   the \u2018Seen\u2019 setting of AGD20K, on the IIT-AFF [43], CAD [53], and UMD [40]   datasets. The experiment was conducted in the \u2018Seen\u2019 setting due to overlapping   objects between these datasets and AGD20K. Each GT was processed in the   same way as when evaluating the AGD20K test set. Despite significant domain   gaps across datasets, INTRA outperformed in all metrics on all datasets, demonstrating its superior generalizability as shown in Tab. 2. Further details of the   experiment can be found in the supplementary material.   Qualitative results. Fig. 4 and Fig. 5 show our superior grounding precision   compared to the baselines, being closer to the GT and finer in granularity. INTRA precisely identifies the exact object part for a given affordance, unlike the   baselines, which ground the same parts regardless of the affordances provided.   User study. Affordance grounding can be ambiguous depending on context   and interpretation, thus relying solely on metrics for evaluation has limitaInteraction Relationship-aware Affordance Grounding   11   INTRA (Ours)   LOCATE   GT   Cross-View-AG   Cross-View-AG+   Hold   Ride   Sit on   Cut with   Object Image   Fig. 4: Qualitative results of INTRA (Ours) and baseline models [25,31,32] on grounding affordances of multiple potential interactions on a single object. INTRA precisely   localizes relevant interaction spots for each interaction. For example, with a knife,   it grounds the handle for \u2018Hold\u2019 and the blade for \u2018Cut with\u2019. For a motorcycle, it   accurately grounds the saddle for \u2018Sit on\u2019. Additionally, for \u2018Ride\u2019, it grounds both   the handle and saddle, slightly deviating from the GT but still producing reasonable   results, as we usually interacts with handle and saddle to \u2018Ride\u2019 a motorcycle.   INTRA (Ours)   LOCATE   GT   Cross-View-AG+   Open   Drag   Drink with   Hit   INTRA (Ours)   LOCATE   GT   Cross-View-AG+   Ride   Hold   Take photo   Push   Object Image   Seen   Unseen   Object Image   Fig. 5: Qualitative results comparison between our approach and other baselines [25,   31,32]. Our approach, INTRA, demonstrates superior precision and detail in grounding   affordances compared to the baselines. For instance, in the example of \u2018Drag\u2019, while   baselines either fail to localize the handle or erroneously ground several other parts,   INTRA accurately identifies and grounds the handle of a suitcase with finesse.   12   Jang & Seo et al.   Table 3: The result of user study on validity, finesse, and separability. Users were   asked to score a 5-point scale, and we averaged it for mean opinion score (MOS).   Validity   Finesse   Separability   Cross-View-AG+ [31]   2.897   3.022   2.732   LOCATE [25]   3.054   2.573   2.651   INTRA (Ours)   3.134   3.112   3.221   Ground Truth   2.905   3.334   3.160   Table 4: Quantitative results of ablation   study on our loss design. We incrementally   added each component of the losses to examine their impact.   Seen   Unseen   mKLD\u2193mSIM\u2191mNSS\u2191mKLD\u2193mSIM\u2191mNSS\u2191   baseline   1.678   0.338   0.891   1.581   0.300   1.100   Linter   1.439   0.334   1.031   1.569   0.292   1.133   Lobj   1.336   0.387   1.218   1.521   0.334   1.042   Linter+Lobj   1.199   0.407   1.239   1.365   0.375   1.209   Table 5: Quantitative results of ablation   study   on   different   R.   LW ordNet,   LW ord2V ec are calculated using word similarity from WordNet [39], Word2Vec [38],   respectively.   LCo\u2212occur.   used   cooccurrence probability in GloVe [49].   Seen   Unseen   mKLD\u2193mSIM\u2191mNSS\u2191mKLD\u2193mSIM\u2191mNSS\u2191   LW ordNet   1.701   0.282   0.710   1.698   0.277   0.937   LCo\u2212occur.   1.519   0.309   0.988   1.639   0.274   1.101   LW ord2V ec   1.547   0.302   0.958   1.679   0.270   0.980   Linter(Ours) 1.439   0.334   1.031   1.569   0.292   1.133   tions. Hence, we conducted a user study comparing Cross-View-AG+ [31], LOCATE [25], GT, and INTRA (Ours) across three categories: 1) Validity: assessing   heatmap reasonableness, 2) Finesse: measuring heatmap detail, 3) Separability:   determining the accuracy of the heatmap when different affordances are assigned   to the same object. A total of 936 responses were collected for randomly selected   samples from 104 respondents. Results presented in Tab. 3 demonstrate that our   approach outperforms baselines and par on GT based on human perception.   4.3   Ablation Studies   We validate our pipeline design choices and parameters with ablation studies.   This section includes ablation studies on loss design, adoption of LLM, and text   synonym augmentation. Refer to the supplementary for further ablation studies.   Ablation study on loss design. To assess the individual impact of the components comprising loss on its overall performance, we analyzed by incrementally   adding components. We started with the most basic element: a normal supervised   contrastive loss. Subsequently, we sequentially added an interaction relationshipguided loss and an object-variance mitigation loss. The performance outcomes of   these incremental modifications were thoroughly evaluated to understand their   contributions, as represented in the Tab. 4.   Ablation study on adoption of LLM. Adopting LLM to create the relationship map was essential given the intricate nature of affordances. We experimented with various methods to create the relationship map to validate this   Interaction Relationship-aware Affordance Grounding   13   (b) Feature t-SNE before (left) / after (right) training   (a) Overall training scheme   Repel   Attract   Fig. 6: An illustration of interaction relationship-guided contrastive learning and tSNE [34] visualization of feature distribution. (a) In interaction relationship-guided   contrastive learning, positive interaction pairs attract each other, while others repel.   (b) t-SNE visualization of DINOv2 [46] class token and fexo from INTRA, showing   that features of positive interaction pairs become closer as learning progresses.   choice. We measured the similarity of interaction pairs using WordNet [39] and   Word2Vec [38], or computed co-occurence probability of interaction pairs with   Glove [49]. Based on these measurements, we created an Interaction-relationship   Map and trained the INTRA framework. The results are in the Tab. 5.   Ablation study on text synonym augmentation. We conducted an ablation study on the effectiveness of text synonym augmentation on overall performance. We compared performance with and without the module. The module   improved performance by up to 21.93%, particularly in the \u2018Unseen\u2019 setting,   enriching models with varied meanings of interactions. Additionally, to test its   effectiveness on novel verb inference, we deliberately omitted the subset \u2018Hold\u2019   (24.17% of training data) and then performed inference on \u2018Hold\u2019. The module   boosted performance for novel verbs by up to 58.06%. Similar tendencies were   observed for other verbs. Detailed results are available in the supplementary.   5   Discussion   5.1   Effect of Interaction Relationship-guided Contrastive Loss   Our rationale for learning affordance grounding solely with exocentric images   relies on the consistent presence of humans within these images. By repelling   common features of negative pairs, such as human parts, the images effectively   exclude irrelevant elements. Conversely, positive pairs, sharing the desired feature of the object\u2014specifically, the rim of the object near the face\u2014facilitate   learning by attracting these relevant features (see Fig. 6(a)). To visualize the effectiveness of our loss in learning interaction-relevant features in similar images,   we examine the feature distributions of \u2018Hold\u2019 and \u2018Sip\u2019 a wine glass, involving   distinct affordances. Prior to training, these distributions overlap. However, after training with our loss function, the feature distribution for \u2018Hold wine glass\u2019   aligns more closely with \u2018Hold baseball bat\u2019 than with \u2018Sip wine glass\u2019. This indicates that our loss function effectively discriminates between the characteristics   of different interactions without exhibiting bias towards objects (see Fig. 6(b)).   Detailed explanation is illustrated in the supplementary.   14   Jang & Seo et al.   INTRA (Ours)   LOCATE   Stick   INTRA (Ours)   Lie on   LOCATE   Ride   INTRA (Ours)   LOCATE   Sit on   INTRA (Ours)   LOCATE   Take photo   Hold   LOCATE   INTRA (Ours)   Ride   INTRA (Ours)   LOCATE   Write   (a) Qualitative results on images with large domain gap   (b) Qualitative results on novel object   (c) Qualitative results on novel interaction   Fig. 7: Qualitative results of feasibility study: (a) Inference on diverse images with   significant domain gap such as pixel arts and paintings. (b) Inference on novel objects   that were not in the training data. (c) Inference on unseen novel interactions. INTRA demonstrates superior grounding accuracy in (a)-(c) compared to LOCATE [25],   showing proper affordance region inference without explicit training.   5.2   Feasibility Study on Generalization Property of INTRA   INTRA excels in affordance grounding on images with large domain gaps, such   as pixel art and paintings, as illustrated in Fig. 7(a). Furthermore, our method   showcases strong generalization abilities for novel objects like a horse and quill,   not present in the training set, as shown in Fig. 7(b). Additionally, despite deliberately not being trained on specific interaction classes like \u2018Hold\u2019 and \u2018Take   photo\u2019 for experiment, INTRA successfully infers their affordances, as depicted   in Fig. 7(c). More results and detailed experimental settings are in the supplementary. One possible explanation for this generalization property is that our   INTRA employs VLM so that diverse domains and novel object can be dealt with   without explicitly tuning for them. Another explanation is INTRA\u2019s contrastive   training that may achieve better representation learning.   6   Conclusion   In this paper, we introduce INTRA, a novel framework reformulating the weakly   supervised affordance grounding with representation learning. We suggest interaction relationship-guided contrastive learning, informed by affordance knowledge from LLM. Furthermore, INTRA actively leverages VLM text embedding   in proposed text-conditioned affordance map generation for flexible affordance   grounding, further bolstered by text synonym augmentation for robustness. INTRA achieves state-of-the-art performance across diverse datasets, relying solely   on exocentric images for training, unlike prior methods that also use egocentric   images. Moreover, our method demonstrates generalization feasibility on novel   objects, interactions, and images with significant domain gaps.   Interaction Relationship-aware Affordance Grounding   15   Acknowledgements   This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT)   [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul   National University)], the National Research Foundation of Korea(NRF) grant   funded by the Korea government(MSIT) (No. NRF-2022M3C1A309202211) and   AI-Bio Research Grant through Seoul National University. Also, the authors acknowledged the financial support from the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University.", "conf": "ECCV", "year": "2024", "index": 295}, {"title": "Contrastive Learning for Weakly Supervised   Phrase Grounding   Tanmay Gupta1, Arash Vahdat3, Gal Chechik2,3, Xiaodong Yang3,   Jan Kautz3, and Derek Hoiem1   1 University of Illinois Urbana-Champaign   2 Bar Ilan University   3 NVIDIA", "abstract": ". Phrase grounding, the problem of associating image regions   to caption words, is a crucial component of vision-language tasks. We   show that phrase grounding can be learned by optimizing word-region   attention to maximize a lower bound on mutual information between   images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in   the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct e\ufb00ective negative captions for learning through language model guided word substitutions.   Training with our negatives yields a \u223c10% absolute gain in accuracy   over randomly-sampled negatives from the training data. Our weakly   supervised phrase grounding model trained on COCO-Captions shows   a healthy gain of 5.7% to achieve 76.7% accuracy on Flickr30K Entities benchmark. Our code and project material will be available at   http://tanmaygupta.info/info-ground.   Keywords: Mutual Information, InfoNCE, Grounding, Attention   1", "content": "Humans can learn from captioned images because of their ability to associate   words to image regions. For instance, humans perform such word-region associations while acquiring facts from news photos, making a diagnosis from MRI   scans and radiologist reports, or enjoying a movie with subtitles. This wordregion association problem is called word or phrase grounding and is a crucial   capability needed for downstream applications like visual question answering,   image captioning, and text-image retrieval.   Existing object detectors can detect and represent object regions in an image, and language models can provide contextualized representations for noun   phrases in the caption. However, learning a mapping between these continuous,   independently trained visual and textual representations is challenging in the   absence of explicit region-word annotations. We focus on learning this mapping   from weak supervision in the form of paired image-caption data without requiring laborious grounding annotations.   2   T. Gupta et al.   \ud835\udc58\"# Image-Caption Pair    in Training Data   \ud835\udc58   Object   Detector   Language   Model   Region-word alignment defined by an    attention mechanism with parameters \ud835\udf03   Chocolate1 donut2 in3   front4 of5 a6 computer7.   \ud835\udc64&   '   \ud835\udc64(   '   \ud835\udc64)   '   \ud835\udc5f&   '   \ud835\udc5f(   '   \ud835\udc5f+   '   \ud835\udc5f,   '   chocolate   donut   computer   Region    Features   (top detections)   Contextualized   Word Features   (nouns and adjectives)   \ud835\udc79\ud835\udc8c:   \ud835\udc64&   '   \ud835\udc64(   '   \ud835\udc64)   '   \ud835\udc5f&   '   \ud835\udc5f(   '   \ud835\udc5f+   '   \ud835\udc5f,   '   chocolate   donut   computer   Chocolate1 donut2 in3   front4 of5 a6 computer7.   \ud835\udc79\ud835\udc8c0:   \ud835\udc64&   '   \ud835\udc64(   '   \ud835\udc64)   '   \ud835\udc5f&   '0   \ud835\udc5f(   '0   \ud835\udc5f+   '0   \ud835\udc5f,   '0   chocolate   donut   computer   \ud835\udc5f1   '0   Chocolate1 cookie2 in3   front4 of5 a6 computer7.   \ud835\udc64(   '2 cookie   \ud835\udf194(\ud835\udc79'0, \ud835\udc647   ')   \ud835\udf194(\ud835\udc79', \ud835\udc647   ')   \ud835\udf194(\ud835\udc79', \ud835\udc647   '2)   \ud835\udcdb\ud835\udc8d\ud835\udc82\ud835\udc8f\ud835\udc88(\ud835\udf3d)   \ud835\udcdb\ud835\udc8a\ud835\udc8e\ud835\udc88(\ud835\udf3d)   min   \ud835\udf3d   \ud835\udcdb\ud835\udc8a\ud835\udc8e\ud835\udc88\ud835\udf3d+ \ud835\udcdb\ud835\udc8d\ud835\udc82\ud835\udc8f\ud835\udc88\ud835\udf3d   Contrastive Training   \ud835\udf194 \ud835\udc79, \ud835\udc64:   Compatibility   between   set   of   region   features   \ud835\udc79   from   an   image   and   contextualized   word representation \ud835\udc64that uses   the region-word attention.   \ud835\udc5f&   '   \ud835\udc5f(   '   \ud835\udc5f+   '   \ud835\udc5f,   '   Fig. 1. Overview of our contrastive learning framework. We begin by extracting   region and word features using an object detector and a language model respectively.   Contrastive learning trains a word-region attention mechanism as part of a compatibility function \u03c6\u03b8 between the set of region features from an image and individual   contextualized word representations. The compatibility function is trained to maximize a lower bound on mutual information with two losses. For a given caption word,   Limg learns to produce a higher compatibility for the true image than a negative image   in the mini-batch. Llang learns to produce a higher compatibility of an image with a   true caption-word than with a word in a negative caption. We construct negative captions by substituting a noun word like \u201cdonut\u201d in the true caption with contextually   plausible but untrue words like \u201ccookie\u201d using a language model.   Current state-of-the-art approaches [11,1,33] formulate weakly supervised   phrase grounding as a multiple instance learning (MIL) problem [25,18]. The   image can be viewed as a bag of regions. For a given phrase, all images with captions containing the phrase are treated as positive bags while remaining images   are treated as negatives. Models aggregate per region features or phrase scores   to construct image-level predictions that can be supervised with image-level labels in the form of phrases or captions. Common aggregation approaches include   max or mean pooling, noisy-OR [13], and attention [11,18]. Popular training objectives include binary classi\ufb01cation loss [13] (whether the image contain the   phrase) or caption reconstruction loss [33] (generalization of binary classi\ufb01cation to caption prediction) or ranking objectives [1,11] (do true image-caption   or image-phrase pairs score higher than negative pairs).   Fig. 1 provides an overview of our proposed contrastive training. We propose   a novel formulation of the weakly supervised phrase grounding problem as that of   maximizing a lower bound on mutual information between set of region features   extracted from an image and contextualized word representations. We use pretrained region and word representations from an object detector and a language   model and perform optimization over parameters of word-region attention instead of optimizing the region and word representations themselves. Intuitively,   to compute mutual information with a word\u2019s representation, attention must   Contrastive Learning for Weakly Supervised Phrase Grounding   3   discard nuisance regions in the word-conditional attended visual representation,   thereby selecting regions that match the word. For any given word, the learned   attention thus functions as a soft selection or grounding mechanism over regions.   Since computing MI is intractable, we maximize the recently introduced InfoNCE lower bound [30] on mutual information. The InfoNCE bound requires a   compatibility score between each caption word and the image to contrast positive image and caption word pairs with negative pairs in a minibatch. We use   two objectives. The \ufb01rst objective (Limg in Fig. 1) contrasts a positive pair with   negative pairs with the same caption word but di\ufb00erent image regions. The second objective (Llang in Fig. 1) contrasts a positive pair with negative pairs with   the same image but di\ufb00erent captions. We show empirically that sampling negative captions randomly from the training data to optimize Llang does not yield   any gains over optimizing Limg only. Instead of random sampling, we propose   to use a language model to construct context-preserving negative captions by   substituting a single noun word in the caption.   We design the compatibility function using a query-key-value attention   mechanism. The queries and keys, computed from words and regions respectively, are used to compute a word-speci\ufb01c attention over each region which acts   as a soft alignment or grounding between words and regions. The compatibility score between regions and word is computed by comparing attended visual   representation and the word representation.   Our key contributions are: (i) a novel MI based contrastive training framework for weakly supervised phrase grounding; (ii) an InfoNCE compatibility   function between a set of regions and a caption word designed for phrase grounding; and (iii) a procedure for constructing context-preserving negative captions   that provides \u224810% absolute gain in grounding performance.   1.1   Related Work   Our work is closely related to three active areas of research. We now provide an   overview of prior arts in each.   Weakly Supervised Phrase Grounding. Weakly supervised phrase localization is typically posed as a multiple instance learning (MIL) problem [25,18]   where each image is considered as a bag of region proposals. Images whose captions mention a word or a phrase are treated as positive bags while rest of the   images are treated as negatives for that word or phrase. Features or scores for a   phrase or the entire caption are aggregated across all regions to make a prediction for the image. Common methods of aggregation are max or average pooling,   noisy-OR [13], or attention [33,18]. With the ability to produce image-level scores   for pairs of images and phrases or captions, the problem becomes an image-level   fully-supervised phrase classi\ufb01cation problem [13] or an image-caption retrieval   problem [1,11]. An alternatives to the MIL formulations is the approach of Ye et   al. [44] which uses statistical hypothesis testing approach to link concepts detected in an image and words mentioned in the sentence. While all the above   approaches assume paired image-caption data, Wang et al. [42] recently address   4   T. Gupta et al.   the problem of phrase grounding without access to image-caption pairs. Instead   they assume access to a set of scene and color classi\ufb01ers, and object detectors to   detect concepts in the scene and use word2vec [27] similarity between concept   labels and caption words to achieve grounding.   MI-based Representation Learning. Recently MI-based approaches have   shown promising results on a variety representation learning problems. Computing the MI between two representations is challenging as we often have access   to samples but not the underlying joint distribution that generated the samples.   Thus, recent e\ufb00orts rely on variational estimation of MI [3,20,6,30]. An overview   of such estimators is discussed in [31,40] while the statistical limitations are   reviewed in [26,34].   In practice, MI-based representation learning models are often trained by   maximizing an estimation of MI across di\ufb00erent transformations of data. For   example, deep InfoMax [17] maximizes MI between local and global representation using MINE [6]. Contrastive predictive coding [30,16] inspired by noise   contrastive estimation [14,29] assumes an order in the features extracted from an   image and uses summary features to predict future features. Contrastive multiview coding [39] maximizes MI between di\ufb00erent color channels or data modalities while augmented multiscale Deep InfoMax [5] and SimCLR [8] extract views   using di\ufb00erent augmentations of data points. Since the infoNCE loss is limited   by the batch size, several previous work rely on memory banks [43,28,15] to   increase the set of negative instances.   Joint Image-Text Representation Learning. With the advances in both   visual analysis and natural language understanding, there has been a recent   shift towards learning representation jointly from both visual and textual domains [23,35,24,37,38,45,22,9,2,36]. ViLBERT [24] and LXMERT [38] learn representation from both modalities using two-stream transformers, applied to image and text independently. In contrast, UNITER [9], VisualBERT [23], UnicoderVL [22], VL-BERT [35] and B2T2 [2] propose a uni\ufb01ed single architecture that   learns representation jointly from both domains. Our method is similar to the   \ufb01rst group, but di\ufb00ers in its fundamental goal. Instead of focusing on learning a   task-agnostic representation for a range of downstream tasks, we are interested   in the quality of region-phrase grounding emerged by maximizing mutual information. Moreover, we rely on the language modality as a weak training signal   for grounding, and we perform phrase-grounding without any further \ufb01netuning.   2   Method   Consider the set of region features and contextualized word representation as two   multivariate random variables. Intuitively, estimating MI between them requires   extracting the information content shared by these two variables. We model this   MI estimation as maximizing a lower bound on MI with respect to parameters of   a word-region attention model. This maximization forces the attention model to   Contrastive Learning for Weakly Supervised Phrase Grounding   5   downweight regions from the image that do not match the word, and to attend   to the image regions that contain the most shared information with the word   representation.   Sec. 2.1 describes MI and the InfoNCE lower bound. Sec. 2.2 introduces   notation and InfoNCE based objective for learning phrase grounding from paired   image caption data. Sec. 2.3 presents the design of a word-region attention based   compatibility function which is part of the InfoNCE objective.   2.1   InfoNCE Lower Bound on Mutual Information   Let x \u2208X and y \u2208Y be random variables drawn from a joint distribution with   density p(x, y). The MI between x and y measures the amount of information   that these two variables share:   MI(x, y) = E(x,y)\u223cp(x,y)   \u0014   log p(x, y)   p(x)p(y)   \u0015   ,   (1)   which is also the Kullback\u2013Leibler Divergence from p(x, y) to p(x)p(y).   However, computing MI is intractable in general because it requires a complete knowledge of the joint and marginal distributions. Among the existing MI   estimators, the InfoNCE [30] lower bound provides a low-variance estimation   of MI for high dimensional data, albeit being biased [31]. The appealing variance properties of this estimator may explain its recent success in representation   learning [8,30,16,36]. InfoNCE de\ufb01nes a lower bound on MI by:   MI(x, y) \u2265log(k) \u2212Lk(\u03b8).   (2)   Here, Lk is the InfoNCE objective de\ufb01ned in terms of a compatibility function   \u03c6 parametrized by \u03b8: \u03c6\u03b8 : X \u00d7Y \u2192R. The lower bound is computed over a minibatch B of size k, consisting of one positive pair (x, y) \u223cp(x, y) and k\u22121 negative   pairs {(x\u2032   i, y)}k\u22121   i=1 where x\u2032 \u223cp(x):   Lk(\u03b8) = EB   \"   \u2212log       e\u03c6\u03b8(x,y)   e\u03c6\u03b8(x,y) + Pk\u22121   i=1 e\u03c6\u03b8(x\u2032   i,y)   !#   .   (3)   Oord et al. [30] showed that maximizing the lower bound on MI by minimizing   Lk with respect to \u03b8 leads to a compatibility function \u03c6\u03b8\u2217that obeys   e\u03c6\u03b8\u2217(x,y) \u221dp(x|y)   p(x)   =   p(x, y)   p(x)p(y),   (4)   where \u03b8\u2217is the optimal \u03b8 obtained by minimizing Lk.   2.2   InfoNCE for Phrase Grounding   Recent work [11] has shown that pre-trained object detectors such as FasterRCNN [32] and language models such as BERT [12] provide rich representations   6   T. Gupta et al.   \ud835\udc5f!   \ud835\udc5f\"   \ud835\udc5f#   \ud835\udc5f$   \ud835\udc5f%   \ud835\udc5f&   A donut and a coffee mug in front of a computer.   \ud835\udc64\"   \ud835\udc64&   \ud835\udc64!!   \ud835\udc5e! \ud835\udc64\"   \ud835\udc63! \ud835\udc64\"   \ud835\udc5e! \ud835\udc64#   \ud835\udc63! \ud835\udc64#   \ud835\udc5e! \ud835\udc64$$   \ud835\udc63! \ud835\udc64$$   \ud835\udc4e$\"   \ud835\udc4e#\"   \ud835\udc4e%\"   \ud835\udc4e&\"   \ud835\udc4e'\"   \ud835\udc4e\"\"   \ud835\udc58( \ud835\udc5f$   \ud835\udc63( \ud835\udc5f$   \ud835\udc58( \ud835\udc5f#   \ud835\udc63( \ud835\udc5f#   \ud835\udc58( \ud835\udc5f%   \ud835\udc63( \ud835\udc5f%   \ud835\udc58( \ud835\udc5f&   \ud835\udc63( \ud835\udc5f&   \ud835\udc58( \ud835\udc5f'   \ud835\udc63( \ud835\udc5f'   \ud835\udc58( \ud835\udc5f\"   \ud835\udc63( \ud835\udc5f\"   Region features &   Contextualized    Word    Representations   Query-Key-Values   for Attention   Word-Region    Attention   Scoring   function   Input Image    &   Caption   \ud835\udc63'(( \ud835\udc79, \ud835\udc64& = '   )*!   &   \ud835\udc4e)& \ud835\udc63+ \ud835\udc5f)   \ud835\udf19, \ud835\udc79, \ud835\udc64& = \ud835\udc63. \ud835\udc64& \ud835\udc63'(((\ud835\udc79, \ud835\udc64&)   Object Detector   BERT   Attended Visual   Representation    Compatibility    Function   Fig. 2. Compatibility function \u03c6\u03b8 with word-region attention. The \ufb01gure shows   compatibility computation between the set of image regions and the word \u201cmug\u201d in   the caption. The compatibility function consists of learnable query-key-value functions kr, vr, qw, vw. The query constructed from contextualized representation of the   word \u201cmug\u201d is compared to keys created from region features to compute attention   scores. The attention scores are used as weights to linearly combine values created   from region features to construct an attended visual representation for \u201cmug\u201d. The   compatibility is de\ufb01ned by the dot product of the attended visual representation and   value representation for \u201cmug\u201d.   in the visual and textual domains for the phrase grounding problem. Inspired by   this, we aim to maximize mutual information between region features generated   by an object detector and contextualized word representation extracted by a   language model.   Let us denote image region features for an image by R = {ri}m   i=1 where m is   the number of regions in the image with each ri \u2208Rdr. Similarly, caption word   representations are denoted as W = {wj}n   j=1 where n is the number of words in   the caption with each word represented as wj \u2208Rdw.   We maximize the InfoNCE lower bound on MI between image regions and   each individual word representation denoted by MI(R, wj). Thus using Eq. 2 we   maximize the following lower bound:   n   X   j=1   MI(R, wj) \u2265n log(k) \u2212   n   X   j=1   Lkj(\u03b8).   (5)   We empirically show that maximizing the lower bound in Eq. 5 with an appropriate choice of compatibility function \u03c6\u03b8 results in learning phrase grounding   without strong grounding supervision. The following section details the design   of the compatibility function.   Contrastive Learning for Weakly Supervised Phrase Grounding   7   2.3   Compatibility Function with Attention   The InfoNCE loss in our phrase grounding formulation requires a compatibility   function between the set of region feature vectors R and the contextualized   word representation wj. To de\ufb01ne the compatibility function, we propose to   use a query-key-value attention mechanism [41]. Speci\ufb01cally, we de\ufb01ne neural   modules kr, vr : Rdr \u2192Rd to map each image region to keys and values and   qw, vw : Rdw \u2192Rd to compute query and values for the words. The query   vectors for each word are used to compute the attention score for every region   given a word using   a(ri, wj) =   es(ri,wj)   Pm   i\u2032=1 es(ri\u2032,wj) ,   (6)   where s(ri, wj) = qw(wj)T kr(ri)/   \u221a   d. The attention scores are used as a soft   selection mechanism to compute a word-speci\ufb01c visual representation using a   linear combination of region values   vatt(R, wj) =   m   X   i=1   a(ri, wj)vr(ri).   (7)   Finally, the compatibility function is de\ufb01ned as \u03c6\u03b8(R, wj) = vT   w(wj)vatt(R, wj),   where \u03b8 refers to the parameters of neural modules kr, vr, qw, and vw, implemented using simple feed-forward MLPs. Following Eqs. 3 & 5, the InfoNCE   loss for phrase grounding is de\ufb01ned as   Limg(\u03b8) = EB   \uf8ee   \uf8f0\u2212   n   X   j=1   log       e\u03c6\u03b8(R,wj)   e\u03c6\u03b8(R,wj) + Pk\u22121   i=1 e\u03c6\u03b8(R\u2032   i,wj)   !\uf8f9   \uf8fb.   (8)   which is marked using subscript img as negative pairs are created by replacing   image regions from a positive pair with regions extracted from negative instance   in the mini-batch.   Remark: We enforce compatibility between each word and all image regions   using MI(R, wj) in Eq. 5, but not between a region and all caption words   (MI(ri, W)). This is because the words only describe part of the image, so there   will be regions with no corresponding word in the caption.   2.4   Context-Preserving Negative Captions   The objective in Eq. 8 trains the compatibility function by contrasting positive   regions-word pairs against pairs with replaced image regions. We now propose a   complementary objective function that contrasts the positive pairs against negative pairs whose captions are replaced with plausible negative captions. However,   extracting negative captions that are related to a captions is challenging as it   requires semantic understanding of words in a caption. Here, we leverage BERT   as a pretrained bidirectional language model to extract such negative captions.   8   T. Gupta et al.   Caption   Negatives Selected   After Reranking   Candidates Rejected    After Reranking   A man is seated at a counter with all types of delicious looking foods,    yet is completely unaffected, casually reading his newspaper.   menu, books, phone, scripts,    email, messages, bible, tablet   newspaper, paper, journal, article,    magazine   A BMX bike rider in red clothing and a helmet is riding his bike next    to a wooden fence.   bench, pole, statue, door, table,    chair, sign, platform, piano   fence, gate, wall, railing, screen   A man in a blue jumpsuit stands next to a red van pulling a trailer.   bike, sedan, horse, jeep, cart, car,    tractor, bull, engine, motorcycle   van, trailer, vehicle, light, truck   A man and a boy are playing with a dog in the evening.    girl, lady, mother, woman,    teenager, child, teacher, mom   man, boy, guy, couple, youth   A woman in a brown sweater sits at a table covered with food.   boy, guy, gentleman, kid, nurse,    soldier, waiter, priest, child   woman, female, person, face, lady   A man with shorts and a hat is holding onto a little boy and a dog.   gloves, glasses, coat, trousers,    bags, apron, moustache, beard   shorts, ties, pants, stripes, jeans   Fig. 3. Context-preserving negative captions. We construct negative captions   which share the same context as the true caption but substitute a noun word. We   choose the substitute using a language model such that it is plausible in the context   but we reject potential synonyms or hypernyms of the original word by a re-ranking   procedure.   For a caption with a noun word s and context c, we de\ufb01ne a contextpreserving negative caption as one which has the same context c but a di\ufb00erent   noun s\u2032 with the following properties: (i) s\u2032 should be plausible in the context;   and (ii) the new caption de\ufb01ned by the pair (s\u2032, c) should be untrue for the image. For example, consider the caption \"A man is walking on a beach\" where   s is chosen as \"man\" and c is de\ufb01ned by \"A [MASK] is walking on a beach\"   where [MASK] is the token that denotes a missing word. A potential candidate   for a context-preserving negative caption might be \"A woman is walking on a   beach\" where s\u2032 is woman. However, \"A car is walking on a beach\" and \"A   person is walking on a beach\" are not negative captions because car is not   plausible given the context, and the statement with person is still true given   that the original caption is true for the image.   Constructing context-preserving negative captions. We propose to use   a pre-trained BERT language model to construct context-preserving negative   captions for a given true caption. Our approach for extracting such words consists   of two steps: First, we feed the context c into the language model to extract 30   most likely candidates {s\u2032   l}30   l=1 for the masked word using probabilities p(s\u2032|c)   predicted by BERT. Intuitively, these words correspond to those that \ufb01ll in the   masked word in caption according to BERT. However, the original masked word   or its synonyms may be present in the set as well. Thus, in the second step,   we pass the original caption into BERT to compute q(s\u2032   l|s, c) which we use as a   proxy for how true (s\u2032   l, c) is given that (s, c) is true. We re-rank the candidates   using the score   p(s\u2032|c)   q(s\u2032|s,c) and we keep the top 25 captions {(s\u2032   l, c)}25   l=1 as negatives   for the original caption (s, c).   We empirically \ufb01nd that the proposed approach is e\ufb00ective in extracting   context-preserving negative captions. Fig. 3 shows a context-preserving negatives   for a set of captions along with candidates that were rejected after re-ranking.   Contrastive Learning for Weakly Supervised Phrase Grounding   9   Note that the selected candidates match the context and the rejected candidates   are often synonyms or hypernyms of the true noun.   Training with context-preserving negative captions. Given the contextpreserving negative captions, we can train our compatibility function by contrasting the positive pairs against negative pairs with plausible negative captions. We   use a loss function similar to InfoNCE to encourage higher compatibility score   of an image with the true caption than any negative caption. Let w and {w\u2032   l}25   l=1   denote the contextualized representation of the positive word s and the corresponding negative noun words {s\u2032   l}25   l=1. The language loss is de\ufb01ned as   Llang(\u03b8) = EB   \"   \u2212log       e\u03c6\u03b8(R,w)   e\u03c6\u03b8(R,w) + P25   l=1 e\u03c6\u03b8(R,w\u2032   l)   !#   .   (9)   For captions with multiple noun words, we randomly select s from the noun   words for simplicity.   2.5   Implementation Details   Regions and Visual Features. We use the Faster-RCNN object detector provided by Anderson et al. [4] and used for extracting visual features in the current   state-of-the-art phrase grounding approach Align2Ground [11]. The detector is   trained jointly on Visual Genome object and attribute annotations and we use   a maximum of 30 top scoring bounding boxes per image with 2048 dimensional   ROI-pooled region features.   Contextualized Word Representations. We use a pretrained BERT language model to extract 768 dimensional contextualized word representations for   each caption word. Note that BERT is trained on a text corpora using masked   language model training where words are randomly replaced by a [MASK] token   in the input and the likelihood of the masked word is maximized in the distribution over vocabulary words predicted at the output. Thus, BERT is trained   to model distribution over words given context and hence suitable for modeling   p(s|c) de\ufb01ned in Sec. 2.4 for constructing context-preserving negative captions.   Query-Key-Value Networks. We use an MLP with 1 hidden layer for each   of kr, vr, qw, vw for all experiments except the ablation in Fig. 4. We use BatchNorm [19] and ReLU activations after the \ufb01rst linear layer. The hidden layer has   the same number of neurons as the input dimensions of these networks which   are 2048 for (kr, vr), and 768 for (qw, vw). The output layer is 384 (= 768/2) for   all networks.   Losses. Since we only care about grounding noun phrases, we compute Limg   only for noun and adjective words in the captions as identi\ufb01ed by a POS tagger   instead of all caption words for computation e\ufb03ciency.   10   T. Gupta et al.   Optimization. We optimize Limg + Llang computed over batches of 50 imagecaption pairs using the ADAM optimizer [21] with a learning rate of 10\u22125. We   compute Limg for each image using other images in the batch as negatives.   Attention to phrase grounding. We use the BERT tokenizer to convert   captions into individual word or sub-word tokens. Attention is computed per   token. For evaluation, the phrase-level attention score for each region is computed   as the maximum attention score assigned to the region by any of the tokens in   the phrase. The regions are then ranked according to this phrase level score.   3   Experiments   Our experiments compare our approach to state-of-the-art on weakly supervised   phrase localization (Sec. 3.2), ablate gains due to pretrained language representations and context-preserving negative sampling using a language model   (Sec. 3.3), and analyse the relation between phrase grounding performance and   the InfoNCE bound that we optimize as a proxy for phrase grounding (Sec. 3.4).   3.1   Datasets and Metrics   We train our models on image-caption pairs from COCO training set which consists of \u223c83K training images. We use the validation set with \u223c41K images for   part of our analysis. Each image is accompanied with 5 captions. For evaluation,   we use the Flickr30K Entities validation set for model selection (early stopping)   and test set for reporting \ufb01nal performance. Both sets consist of 1K images with   5 captions each. We report two metrics:   Recall@k which is the fraction of phrases for which the ground truth bounding   box has an IOU \u22650.5 with any of the top-k predicted boxes.   Pointing accuracy which requires the model to predict a single point location   per phrase and the prediction is counted as correct if it falls within the ground   truth bounding box for the phrase. Unlike recall@k, pointing accuracy does not   require identifying the extent of the object. Since our model selects one of the   detected regions in the image, we use use center of the selected bounding box as   the prediction for each phrase for computing pointing accuracy.   3.2   Performance on Flickr30K Entities   Tab. 1 compares performance of our method to existing weakly supervised   phrase grounding approaches on the Flickr30K Entities test set. A few existing approaches train on Flickr30K Entities train set and report recall@1 while   recent methods use COCO train set and report pointing accuracy. Further, all   approaches use di\ufb00erent visual features making direct comparison di\ufb03cult. For   a fair comparison to state-of-the-art, we use Faster-RCNN trained on Visual   Contrastive Learning for Weakly Supervised Phrase Grounding   11   Table 1. Grounding performance on Flickr30K Entities test set. We make our   approach directly comparable to the current state-of-the-art, Align2Ground [11]. The   performance of older methods are reported for completeness but the use of di\ufb00erent   visual features makes direct comparison di\ufb03cult.   Method   Training Data   Visual Features   R@1   R@5 R@10 Accuracy   GroundeR (2015) [33]   Flickr30K Entities   VGG-det (VOC)   28.94   Yeh et al.(2018) [44]   Flickr30K Entities   VGG-cls (IN)   22.31   Yeh et al.(2018) [44]   Flickr30K Entities   VGG-det (VOC)   35.90   Yeh et al.(2018) [44]   Flickr30K Entities   YOLO (COCO)   36.93   KAC Net+Soft KBP (2018) [7] Flickr30K Entities   VGG-det (VOC)   38.71   Fang et al.(2015) [13]   COCO   VGG-cls (IN)   29.00   Akbari et al.(2019) [1]   COCO   VGG-cls (IN)   61.66   Akbari et al.(2019) [1]   COCO   PNAS Net (IN)   69.19   Align2Ground (2019) [11]   COCO   Faster-RCNN (VG)   71.00   Ours   Flickr30K Entities Faster-RCNN (VG) 47.88 76.63 82.91   74.94   Ours   COCO   Faster-RCNN (VG) 51.67 77.69 83.25   76.74   Table 2. Bene\ufb01ts of language modeling. The \ufb01rst two rows show the gains due to   pretrained language representations. The next three rows show gains from each step in   our proposed context-preserving negative caption construction.   Negative Captions   Language Model R@1   R@5 R@10 Accuracy   None   BERT (Random)   25.66   59.57   75.16   57.37   None   BERT (Pretrained)   35.74   72.91   82.07   66.89   Random   BERT (Pretrained)   36.32   72.42   81.81   66.92   Contextually plausible   BERT (Pretrained)   48.05 76.78 82.97   74.91   Excluding near-synonyms & hypernyms BERT (Pretrained)   51.67 77.69 83.25   76.74   Genome object and attribute annotations used in Align2Ground [11] and report   performance for models trained on either datasets on both recall and pointing   accuracy metrics.   Using the same training data and visual feature architecture, our model shows   a 5.7% absolute gain in pointing accuracy over Align2Ground. Learning using   our contrastive formulation is also quite sample e\ufb03cient as can be seen by only   a 2 to 3 points drop in performance when the model is trained on the much   smaller Flickr30K Entities train set which has approximately one-third as many   image-caption pairs as COCO.   3.3   Bene\ufb01ts of Language Modeling   Our approach bene\ufb01ts from language modeling in two ways: (i) using the   pretrained language model to extract contextualized word representations, and   (ii) using the language model to sample context-preserving negative captions.   Tab. 2 evaluates along both of these dimensions.   12   T. Gupta et al.   3.20   3.25   3.30   3.35   3.40   3.45   3.50   3.55   InfoNCE bound on COCO - validation   64   66   68   70   72   74   76   Pointing accuracy on Flickr30k - validation (%)   52K iters   36K iters   16K iters   4K Iters   80K Iters   Best Accuracy   Linear   MLP w/ 1 hidden layer   MLP w/ 2 hidden layers   Fig. 4. Relation between InfoNCE lower bound and phrase grounding performance with   training iterations for 3 di\ufb00erent choices of key-value modules in the compatibility   function \u03c6\u03b8. Each epoch is \u223c8K iterations. The scattered points visualize the measured   quantities during training. The dashed lines are created by applying moving average   to highlight the trend.   Gains from pretrained word representations. In Tab. 2, BERT (Random)   refers to the BERT architecture initialized with random weights and \ufb01netuned on   COCO image-caption data along with parameters of the attention mechanism.   BERT (Pretrained) refers to the o\ufb00-the-shelf pretrained BERT model which   is used as a contextualized word feature extractor during contrastive learning   without \ufb01netuning. We observe a \u223c10% absolute gain in both recall@1 and   pointing accuracy by using pretrained word representations from BERT.   Gains from context-preserving negative caption sampling. Our contextpreserving negative sampling has two steps. The \ufb01rst step is drawing negative   noun candidates given the context provided by the true caption. The second step   is re-ranking the candidates to \ufb01lter out likely synonyms or hypernyms that are   also true for the image.   First, note that randomly sampling negative captions from training data for   computing Llang performs similarly to only training using Limg. Model trained   with contextually plausible negatives signi\ufb01cantly outperforms random sampling   by \u22658% gain in recall@1 and pointing accuracy. Excluding near-synonyms and   hypernyms yields another \u223c3 points gain in recall@1 and accuracy.   3.4   Is InfoNCE a good proxy for learning phrase grounding?   The fact that optimizing our InfoNCE objective results in learning phrase   grounding is intuitive but not trivial. Fig. 4 shows that maximizing the InfoNCE lower bound correlates well with phrase grounding performance on a   Contrastive Learning for Weakly Supervised Phrase Grounding   13   heldout dataset. We make several interesting observations: (i) As training progresses (from left to right), InfoNCE lower bound (Eq. 5) mostly keeps increasing   on the validation set. This indicates that there is no over\ufb01tting in terms of the   InfoNCE bound. (ii) With the increase in InfoNCE lower bound, phrase grounding performance \ufb01rst increases until peak performance and then decreases. This   shows that the InfoNCE bound is correlated with the grounding performance   but maximizing it fully does not necessarily yield the best grounding. A similar   observation has been made in [39] for representation learning. (iii) The peak   performance and the number of iterations needed for the best performance depends on the choice of key-value-query modules. One and two layer MLPs hit   the peak faster and perform better than linear functions. We refer the reader to   the supplementary material for a discussion of limitations of our approach.   3.5   Qualitative Results   Fig. 5 visualizes the word-region attention learned by our model. The qualitative results demonstrate the following abilities: (i) localizing di\ufb00erent objects   mentioned in the same caption with varying degrees of semantic relatedness,   e.g., man and canine in row 1 vs. man and woman in row 3; (ii) disambiguation   between two instances of the same object category using caption context. For   example, boy and another in row 4 and bride and groom from other men and   women in row 3; (iii) localizing object parts such as toddler\u2019s shirt in row 2 and   instrument\u2019s mouthpiece in row 5; (iv) handling occlusion, e.g., table covered   with toys in row 6; (v) handling uncommon words or categories like ponytail   and mouthpiece in row 5 and hose in row 7.   4   Conclusion   In this work, we o\ufb00er a novel perspective on weakly supervised phrase grounding   from paired image-caption data which has traditionally been cast as a multiple   instance learning problem. We formulate the problem as that of estimating mutual information between image regions and caption words. We demonstrate that   maximizing a lower bound on mutual information with respect to parameters   of a region-word attention mechanism results in learning to ground words in   images. We also show that language models can be used to generate contextpreserving negative captions which greatly improve learning in comparison to   randomly sampling negatives from training data.   Acknowledgement   This work was done partly at NVIDIA and is partly supported by ONR MURI   Award N00014-16-1-2007   14   T. Gupta et al.   A man and a canine both stand on    a snowy plane looking out into    the distance.   man:0.26,0.17,0.09   canine:0.33,0.25,0.06   A toddler in a yellow shirt   standing in front of a living    complex next to a baby carriage.   toddler:0.59,0.20,0.06   shirt:0.63,0.17,0.08   #boxes:34   #boxes:27   A man in a tuxedo and a woman in    a bride's gown are leaving a    church.   man:0.17,0.09,0.09   woman:0.19,0.15,0.08   #boxes:50   One boy follows another at the    park.   boy:0.25,0.18,0.18   another:0.12,0.10,0.08   #boxes:37   A man with a ponytail wearing a    blue collared shirt is playing    an instrument's mouthpiece.   ponytail:0.29,0.19,0.10   mouthpiece:0.32,0.16,0.09   #boxes:12   Two kids sitting at a table full    of toys.   kids:0.22,0.17,0.16   table:0.33,0.15,0.10   #boxes:34   A curly-haired little girl    watering plants with a hose.   haired:0.27,0.17,0.16   hose:0.29,0.16,0.16   #boxes:24   (1)   (2)   (3)   (4)   (5)   (6)   (7)   Fig. 5. Visualization of attention. We show all detected regions and top-3 attended   regions with attention scores for two words highlighted in each caption. More qualitative   results can be found on our project page http://tanmaygupta.info/info-ground/   Contrastive Learning for Weakly Supervised Phrase Grounding   15", "conf": "ECCV", "year": "2020", "index": 35}, {"title": "Explainable Video Entailment with Grounded Visual Evidence   Junwen Chen and Yu Kong   Golisano College of Computing and Information Sciences   Rochester Institute of Technology   Rochester, NY, USA   {jc1088, yu.kong}@rit.edu", "abstract": "Video entailment aims at determining if a hypothesis   textual statement is entailed or contradicted by a premise   video. The main challenge of video entailment is that it   requires \ufb01ne-grained reasoning to understand the complex   and long story-based videos. To this end, we propose to   incorporate visual grounding to the entailment by explicitly linking the entities described in the statement to the   evidence in the video. If the entities are grounded in the   video, we enhance the entailment judgment by focusing on   the frames where the entities occur.   Besides, in the entailment dataset, the entailed/contradictory (also named as   real/fake) statements are formed in pairs with subtle discrepancy, which allows an add-on explanation module to   predict which words or phrases make the statement contradictory to the video and regularize the training of the entailment judgment. Experimental results demonstrate that our   approach outperforms the state-of-the-art methods.   1.", "content": "Bridging the gap between computer vision and natural   language processing is a rapid growing research area in various tasks including visual captioning [40, 34], VQA [20,   1, 33], and visual-textual retrieval [22, 23]. Liu et al. [25]   introduced a new video entailment problem to infer the semantic entailment between a premise video and a textual   hypothesis. As shown in Fig. 1, video entailment [25] task   aims at determining whether a textual statement is entailed   or contradicted by a video. In Fig. 1, the label for the \ufb01rst   statement with the premise is entailment because the statement can be concluded from the dialog of the \ufb01rst clip in   which \u201cthe woman wearing jeans\u201d appears. On the contrary, the second statement is labeled as contradiction, because the premise does not have evidence to conclude the   statement. In this paper, we aim to address the video entailment with a faithful explanation.   The main challenge of video entailment is that it re[00:12.920, 00:16.430]    \u201chi yeah I'm cool hey    you do me a favor\u201d   Dialog   [00:01.709, 00:04.579]    \"got pony lessons in    an hour I'm hearing\"   [00:30.390, 00:32.479    \u201cthe season's over no    no no no no Pamela\u201d   [00:12.920, 00:16.430    \u201cplease give me a    glass of water\u201d   Video   Prediction   Entailment   Statement   (a) The woman wearing jeans  has kids that have pony lessons in hour.   Ground to clip 1&4   Focus on clip 1&4   (b) The woman wearing jeans has kids that have school tomorrow.   Contradiction   Figure 1. Video entailment aims at judging if a statement is entailed or contradicted by a video and its aligned textual dialog. A   pair of real and fake statements have the similar structure and sutble difference (marked by the red dot line). We incorporate visual   grounding into the entailment judgment. The entity grounding,   e.g., \u201cA woman wearing jeans\u201d guides the entailment judgment   module to focus on the entity-relevant frames and the corresponding sentences in the dialog (marked by blue in the temporal axis)   to make a correct judgment. Best viewed in color.   quires \ufb01ne-grained reasoning to understand the complex   story-based videos and then make a correct judgment. The   story-based videos are also accompanied by the textual dialog (subtitles) (see Fig. 1). In the existing method for video   entailment [25], video frames are less exploited than dialog, because it lacks of a \ufb01ne-grained understanding of the   video and the model does not know which frames in the   long video are related to the statement. However, the entities in the textual statement are usually people with their attributes, e.g., \u201cA woman wearing jeans\u201d (see Fig. 1), which   should be implied in the video frames instead of the dialog.   To this end, we propose to enhance the entailment judgment by introducing a visual grounding model that links the   entity described in a statement to the evidence in the video.   This is motivated by the fact that the statement is usually   only related to a small subset of the long and untrimmed   video. Based on this, a visual grounding module for the entities described in the statement is developed to localize the   clips where the entity appears and guide the judgment to focus on the entity\u2019s occurring clips as well as the aligned sentences in the dialog. For example, the statements in Fig. 1   are linked to the \ufb01rst and fourth clips and sentences, consid2021   ering the entity \u201dThe woman wearing jeans\u201d. By highlighting the relevant clips and sentences, the details can be better   understood compared to [25] that does not have grounding   guidance and equally considers all of the frames.   Visual   grounding   has   been   attempted   in   many   video+language tasks, such as image captioning [38]   and VQA [21]. However, it cannot be directly generalized   to the entailment task, because the bounding box annotations of grounding are not provided in the entailment   dataset.   Therefore, we resort to the existing weaklysupervised object grounding methods [15, 5] to address the   training of the grounding module. But these methods are   limited to explicit natural objects (e.g., \u201capple\u201d, \u201criver\u201d).   Our grounding is more demanding, as we target at the   described entities with \ufb01ne-grained attributes, such as hair,   clothes and gender, to be grounded to the challenging   story-telling videos.   Furthermore, we aim at improving the faithfulness of the   entailment model by evaluating if the entailment is judged   based on correct evidence.   A faithful entailment model   should tell not only whether the statement is contradictory   to the video but also which words or phrases in the statement make it contradictory to the video. A pair of real/fake   statements usually have a similar structure and only have   very subtle differences, with only a small number of words\u2019   replacement, e.g. \u201cpony lessons in hour\u201d and \u201cschool tomorrow\u201d marked by the red dot line in Fig. 1. Thus, we   propose to regularize the training of the entailment judgment module by encouraging the local explanation on the   contribution of the words in the statement to conform to the   subtle difference.   Our main contribution is threefold. First, we propose   a novel approach to address video entailment with visually grounded evidence. Second, we exploit the pairwise   real/fake statements to add the explainability to the entailment model, which can tell the speci\ufb01c words or phrases   that make the statement contradictory to the video. Third,   extensive results demonstrate that our method outperforms   the state-of-the-art video entailment method.   2. Related Work   2.1. Visual Entailment   Natural language inference [9, 8, 26, 3] is the task of   understanding if a hypothesis sentence is entailed or contradicted by a premise sentence, which is a fundamental task   in natural language understanding. Inspired by the textual   entailment, recently visual entailment is proposed to extend   NLI to the visual domain. In visual entailment, the premise   is an image or a video. And the goal is to predict if the   textual hypothesis can be con\ufb01rmed in the visual premise.   Recently, researchers began to solve visual entailment   mainly on image premise. SNLI-VE [35] is a visual entailment dataset combining the textual entailment [2] and   Flickr30k image caption [36]. It also provides a solution   model that utilizes ROI generation and models the \ufb01negrained cross-modal information. However, the hypothesis (e.g., \u201cThe two women are holding packages\u201d) is much   more straightforward compared to the hypothesis in our   video entailment. e-SNLI-VE-2.0 [12] appends and corrects SNLI-VE [35] by the human-written language hypothesis. It also provides the explanation ground-truth of   why the hypothesis is entailed/contradicted by the premise.   NLVR2 [32] is another image entailment dataset that requires quantitative and comparing reasoning. But similar   to SNLI-VE [35], it also mainly focuses on objects in the   natural images.   Recently, Liu et al. [25] proposed VIOLIN dataset that   focuses on video entailment. Video entailment is a challenging task as the complex temporal dynamics occur in the   video. A \ufb01ne-grained reasoning of the social relations, human motions and intentions is necessary to understand the   story-based content and make a correct judgment.   2.2. Grounding for Video+Language Reasoning   Recently, many video+language tasks have been trying   to explicitly link the language sentence to the evidence in   the video. Zhou et al. [38] proposed a video description   dataset with the annotation of the bounding boxes of the   referred objects. With this dataset, a good captioning model   is desirable by attending to appropriate video regions. For   video question answering, Lei et al. [21] built a dataset with   the spatio-temporal grounding annotation, which requires   the model to localize the temporal moments, detected the   referred object, and answer the questions.   Different from captioning and VQA, video entailment   needs a \ufb01ne-grained understanding of the entities with detailed attributes. Meanwhile, the existing video entailment   does not provide the grounding annotation. Thus, we propose to achieve the entity grounding in a weakly-supervised   manner.   2.3. Weakly-supervised Entity Grounding   Visual grounding is to localize the described entity to its   occurring regions visually. Since the annotation of bounding boxes is very expensive, sundry efforts have been made   to achieve object grounding in a weakly-supervised manner [15, 5, 29], mainly based on multiple instance learning.   It also has been extended to video domain [39, 30, 14, 7,   6, 4], to achieve spatio-temporal grounding of entities in an   untrimmed video.   In the video entailment task, the visually related entities   are mainly characters, while the existing grounding methods aim at grounding natural objects. Our grounding requires a \ufb01ne-grained understanding of human gender, dress,   hair and other attributes. Therefore, we cannot directly gen2022   [00:01.709, 00:04.579]   \"got pony lessons in an hour I'm hearing\u201d   [00:12.920, 00:16.430]   \u201chi yeah I'm cool hey you do me a favor\u201d   \u2026   [00:16.440, 00:18.439]   \u201cplease give me a glass of water\u201d   Video   Dialog   Statement   Vid Det   STATE   Vid C3D   Entity Grounding   Entity   Dialog   Entailment Judgment with Grounded Evidence   Explanation   temporal segmentation   !\"\u00d7$%   !\u00d7$&   Key Dialog   Key C3D   !\u00d7$%   !\"\u00d7$&   Key   Key D   ST   Output:   Entailment   Output:   Contradiction   contradictory statement   The woman wearing jeans has kids that have    school tomorrow.   The woman wearing    jeans has kids that have    school tomorrow.   Statement   The woman wearing jeans   Parser   Contradiction Local Explanation   Video   features   Dialog   embedding   Key frames   Key Dialog   Focus   Statement   Fusion   Fuse   Judge   Output: Heatmap   Problem Setup   Approach   Figure 2. Given a video, its aligned dialog in text, and a textual statement for the video as input, our goal is to predict if the statement is   entailed or contradicted by the video and dialog. Our model consists of three sub-networks: Entity Grounding, Entailment Judgment with   Grounded Evidence, and Contradiction Local Explanation. The entity grounding module helps to \ufb01nd if the described entity occurs in the   video clips. Moreover, entity grounding guides the judgment module to focus on the entity-relevant clips and the corresponding sentences   in the dialog (marked as \u201cKey\u201d), to make a correct judgment. If judged as \u201ccontradiction\u201d, our model can also explain which words or   phrases in the statement make it contradictory to the video by generating an explanation heatmap.   eralize the existing grounding methods to video entailment.   2.4. Multi-modal VQA   Different from image entailment, video entailment is   supposed to understand story-based video content, such as   movies. This is more challenging than the plain videos as   multiple factors such as human interactions, emotions, motivation, and scenes appear. Similar to existing videoQA   datasets [21, 22], the input to our entailment task is multimodal, including both videos and textual subtitles.   For   multi-modal VQA, early fusion was commonly used in   merging different modalities [27]. Recent methods mainly   leverage late fusion approaches [18, 16].   Another aspect [17] is to utilize the content of QA pairs to shift to   the relevant modality and constrain the contribution of the   irrelevant ones.   Video entailment requires a \ufb01ne-grained understanding.   The statement may only relate to the details in a long and   untrimmed video.   Thus, we propose to ground the described entities to their occurring clips and highlight the dialog sentence aligned to those clips for entailment judgment.   3. Our Approach   Given a story-like video aligned with a textual dialog   (subtitles) and a hypothesis statement, the entailment task is   to predict if the hypothesis statement is entailed or contradicted by the premise video (see the left of Fig. 2). The right   part of Fig. 2 shows the overall pipeline of the proposed   method. We decompose our model into three sub-networks:   entity grounding, entailment judgment with grounded evidence, and contradiction local explanation, to address entailment in a modularized manner.   The motivation of grounding entities described in the   statement (e.g., \u201ca woman wearing a red cape\u201d) to frames   comes from the observation that video modality is not   well exploited compared to dialog modality in the existing   method [25]. However, many contradictory statements such   as the incorrect attributes should be determined from the   frames instead of the dialog, (e.g., \u201ca woman wearing a blue   cape\u201d) in Fig. 3. Moreover, the statements are written about   different aspects of a video [25], and a statement is usually   related to a small subset of video frames. The entity grounding helps to \ufb01nd the entity-relevant frames and then guides   the entailment judgment module to highlight these frames.   To learn a credible entailment judgment model, we propose   to not only judge the semantic entailment but also explain   which words or phrases make the statement contradictory   to the video by a heatmap that indicates the contribution of   each word in the statement to the model prediction.   3.1. Preliminaries   Text Representation. Following VIOLIN [25], we use   BERT encoder [10] provided by VIOLIN to represent the   statement and dialog, resulting in a 768-dimension vector   for each word. Then using a bi-directional LSTM for both   statement and dialog, each word is also embedded to ddimension. A statement is tokenized into a word sequence,   in the length of Nl. A textual dialog is also tokenized and   represented as a word sequence. Then, by encoding, the   statement is represented as R = {ri}Nl   i=1 in which ri indicates the i-th word\u2019s representation. The dialog is represented as H = {hj}Ns   j=1, in which hj indicates the j-th   2023   Positive Entity   A man wearing a red cape is \ufb02ying    while holding another man.   Negative Entity   A man in a red shirt walks    through a store that's on \ufb01re.   Golden Negative Entity   A man wearing a blue cape is \ufb02ying    while holding another man.   en   e\u2032!n   e*n   person     detection   \u2026   1 2 3   K   matching score   s(V, en) > s(V, e\u2032!n)   s(V, en) > s(V, e*n )   vt   1 vt   2   vt   1 vt   2   vt   N   v1 v2   vK   V   en   e\u2032!n   e*n   Figure 3. Training of the entity grounding module. We extract the positive entity en from the statement aligned with the video and the   negative entity e\u2032   n from a statement unaligned with the video. Besides, real and fake statements are formed in pairs. Thus, the entity in   the fake statement can be utilized as a golden negative entity e\u2217   n, which is slightly different from en and is a hard sample to enhance the   grounding model\u2019s training. The training process encourages the matching score of the positive entity to be larger than any negative entity.   Best viewed in color.   word\u2019s representation. Ns denotes the number of words in   the long dialog. The starting time tj   s and the ending time tj   e   of the j-th sentence are also provided, which can be aligned   with the video frames.   Video Representation. Following VIOLIN [25], we extract a sequence of visual features from video frames and   then encode the visual features by a bi-directional LSTM   layer. The video is then represented as C \u2208RT \u00d7d, where   T is the number of frames, and d is the feature dimension   of each frame.   To realize grounding, we \ufb01rst detect the people in the   input video. Speci\ufb01cally, we extract the frames of the middle timestamps corresponding to each sentence (tj   s + tj   e)/2   and apply Faster R-CNN [28] pretrained on COCO [24] to   detect all of the people from each frame and extract their   features. Each person is represented by a 4096-dimension   vector, denoted as vk. Then each video is formed as a set of   persons V = {vk}K   k=1, where vk encodes the k-th person.   3.2. Entity Grounding Module   In the existing video entailment method [25], the performance gain of video modality is limited compared to dialog modality. Visual information needs \ufb01ne-grained understanding, but the existing work equally considers all of the   frames even if the frames are not relevant to the statement.   Video modality should be responsible for a lot of information described in the statement such as entity attributes   (e.g., gender and clothes). We propose to leverage entity   grounding in the video modality to improve the entailment   judgment in a modularized manner (see Fig. 2). First, our   grounding module is developed to achieve spatio-temporal   grounding of the subject entity described in the statement.   The predicted temporal occurrences of the entity are used to   guide the following cross-modal entailment judgment.   However, two technical challenges need to be handled   to leverage visual grounding for the entailment task. First,   spatial-temporal annotations of entities are typically not   available for the entailment task so that existing fullysupervised grounding-based VideoQA methods [21] cannot   be directly leveraged. We resort to multiple instance learning [39] to achieve entity grounding in a weakly-supervised   fashion. Second, detailed visual attributes (e.g., clothes and   hair) of entities are essential for the entailment task but they   are typically ignored by the existing object grounding methods [30, 39, 4].   To extract the entity and its attributes from a textual   statement, we employ a constitute parsing method [19].   For example, in Fig. 2, \u201cThe woman wearing jeans\u201d is   an entity extracted from the corresponding statement \u201cThe   woman wearing jeans has kids that have pony lessons in   hour\u201d. The extracted entities in a statement are denoted as   E = {en}Ne   n=1, where Ne is the total number of entities and   en indicates the n-th entity.   To ground the entity to its occurring frames, we compute   the matching score s(V, en) between video V and an entity   en as:   s(V, en) = 1   K   K   X   k=1   \u03c3(FC1(vk||en))   (1)   where FC1 is a fully-connected layer and \u03c3 is the sigmoid   activation. We take average of the scores of the K people   as the entity-video matching score s(V, en).   Following the existing visual-textual matching work [23,   4, 39], we formulate the weakly-supervised learning of   grounding as:   Lga = \u2212log(1 \u2212s(V, e\u2032   n)) \u2212log(s(V, en)),   (2)   where e\u2032   n is a \u201cnegative entity\u201d extracted from a randomly   sampled statement from another video, which is different   from en. Eq. 2 encourages that the aligned video-entity pair   (V, en) to better matched and the unaligned pair (V, e\u2032   n) to   be less matched.   2024   Different from weakly-supervised video grounding [4,   39], the entailment task consists of the real/fake statements   in pairs. Thus, we have the opportunity to obtain hard negative samples, which is the entity described in the fake statement but NOT described in the real statement. As shown in   Fig. 3, the negative version is \u201ca man wearing a blue cape\u201d,   which is very similar to the positive one \u201ca man wearing   a red cape\u201d but is contradicted by the video. We name it   as \u201cgolden negative entities\u201d e\u2217   n and use it in training the   grounding module:   Lgb = \u2212log(1 \u2212s(V, e\u2217   n)) \u2212log(s(V, en)),   (3)   Lgb encourages the video V to match more to its aligned   entity en and less to the golden negative entity e\u2217   n. To sum   up, we train the grounding model by the grounding loss Lg   which balances the negative entities and the golden negative   entities by \u03b2.   Lg = Lga + \u03b2Lgb,   (4)   During the inference, if the matching score s(vk, en) =   \u03c3(FC1(vk||en)) between a person vk and an entity en exceeds a threshold, we consider that the k-th people is en.   The temporal grounding result will be used to guide the entailment judgment in Sec 3.3.   3.3. Entailment Judgment with Grounded Evidence   Statements are usually related to a small subset of the   video, instead of the entire video. For example, in Fig. 2,   the clause in the statement \u201ckids that have school tomorrow\u201d should be judged from the \ufb01rst sentence in the dialog.   Thus, we utilize the entity grounding result to highlight the   frames and the corresponding textual dialog in the temporal range that the entity occurs, since the frames and dialog   are aligned by temporal boundaries. The highlighted frame   and dialog embeddings are concatenated and marked as key   embedding CO, HO.   The model takes three streams in different modalities as   input: video frames, dialog, and statements. We leveraged   the visually grounded evidence to make our model \ufb01xate its   attention on the frames where the entity appears. Then, we   fuse the multi-modal data and predict whether the statement   is entailed or contradicted by the video.   To bridge the modal discrepancy between the video   frames and textual content, we use heterogeneous reasoning [37] to fuse the statement representation R with different context embedding, including video embeddings C,   dialog embeddings H and key embeddings CO, HO (see   Fig. 4) respectively. The heterogeneous reasoning is based   on a graph convolution layer [7]:   P\u2217= A\u2217\u2192sX\u2217W\u2217s,   (5)   where \u2217denotes one of the context among video C, dialog   D and key CO, HO and Adjacency matrix A\u2217\u2192s contains   Figure 4. Our multi-task learning framework for entailment judgment and its explanation. Given the video and dialog embedding,   we use heterogeneous reasoning to fuse them and update the statement representation. Then, the statement representation is incorporated into two branches: the judgment branch to predict if it is   entailed or contradicted and the explanation branch to generate a   heatmap that shows the contribution of words in the statement in   making it fake. GT abbreviates ground-truth.   the similarity between the statement R and the context embedding X\u2217. Eq. 5 projects the context X\u2217to an R-shaped   embedding P\u2217by a learnable linear layer W\u2217s. Then, to   avoid forgetting, we learn a gating function z\u2217by a linear   operation W\u2217, b\u2217and constrained activation sigmoid,   z\u2217= sigmoid(W\u2217[R, P\u2217] + b\u2217),   (6)   and incorporate the projected embedding P\u2217of different   context into the statement representation by:   Q\u2217s = z\u2217\u2299R + (1 \u2212z\u2217) \u2299P\u2217.   (7)   Eq. 7 respectively results in three statement representations   Qcs, Qhs, Qcos, Qhos speci\ufb01c to the video, dialog and key   context. \u2299indicates element-wise product. We concatenate   them and update the statement representation as:   Q = [R; Qhs; Qcs; Qhos; Qcos] ,   (8)   The updated statement representation Q is passed through   a function f that contains a linear layer with 1-dimensional   output and a sigmoid activation to predict the score of the   statement to be real.   3.4. Explainable Entailment   The local explanation for judging a textual statement is   de\ufb01ned as the contribution of each word, which is in form   2025   Table 1. Entailment Accuracy Comparison. We report the Accuracy (%) of all statements, real statements, fake statements, human-written   statements, and adversarially sampled statements. 2/3 of fake statements are human-written and the remaining 1/3 are adversarially   sampled. Not that \u201cVisual\u201d column denotes the visual features used in the entailment judgment stage.   Method   Visual   Accuracy   Real   Fake   Human-written   Adv-sampled   VIOLIN [25]   C3D   67.23   74.66   57.73   61.99   67.60   Ours   C3D   68.15   79.21   57.08   61.33   79.43   VIOLIN [25]   Resnet   67.60   79.10   56.10   59.15   84.49   Ours   Resnet   68.39   79.52   57.25   60.11   84.94   of a heatmap for a sentence. Our method aims to regularize   the training of entailment judgment with its local explanation to promote the model\u2019s faithfulness and generalization   ability (see the explanation branch in Fig. 4) [13]. We encourage the entailment model to focus more on the words   that actually make the statement contradictory to the video,   instead of memorizing the dataset-speci\ufb01c artifacts.   In VIOLIN dataset [24], more than half of the fake statements were collected by modifying a small subset of the real   statement to be contradicted by the video [25], which makes   the difference between the real and fake statements subtle   and alleviates the bias. We propose to exploit the subtle difference as a kind of supervision signal for the local explanation. During training, we have access to the real/fake statements that are formed in pairs. For example, a pair of real   and fake statements are: \u201cA man in a black jacket gets off   his white motorcycle\u201d and \u201cA man in a black jacket gets off   the bell towel.\u201d respectively. By a simple \u201cdiff\u201d operation   between them, the contradictory items are \u201cthe bell towel\u201d.   The indexes of the different words between the real and fake   statements obtained by the \u201cdiff\u201d operation are de\ufb01ned as   the ground-truth of local explanation. We mark it as a binary vector oe \u2208RNl\u00d71 that is in length of the statement.   Speci\ufb01cally, we form the entailment judgment (see 3.3)   and its explanation as multi-task learning. The explanation   branch in Fig. 4 takes the updated statement representation   Q as input and generates a heatmap ue \u2208RNl that indicates   the contribution of each word to the model prediction f(Q).   The explanation loss Lr is de\ufb01ned as:   Lr =   Nl   X   i=1   oi   e(\u2212log(ue)) + (1 \u2212oi   e)(\u2212log(1 \u2212ue)), (9)   which aligns the generated heatmap ue with the local explanation ground-truth oe. The overall objective function Le is   de\ufb01ned as:   Le = Lcls + \u03bbLr,   (10)   in which Lcls is the binary cross entropy loss for entailment   judgment. It balances entailment judgment and its explanation by constraint \u03bb. If a statement is justi\ufb01ed as real, each   word should be entailed by the premise. Thus, during training, we only regularize the fake statements. During inference, if a statement is predicted as \u201ccontradiction\u201d, the explanation module will be triggered to generate the heatmap   for the statement.   4. Experiments   4.1. Dataset   To our best knowledge, VIOLIN [25] is the only dataset   for video entailment task. VIOLIN contains 15, 887 video   clips and each video clip is annotated with 3 pairs of   real/fake statements, resulting in 95, 322 statements in total. Statements are in random lengths and have 18 words   on average. The \ufb01rst two fake statements of each video   are human-written by modifying a small portion of the   corresponding real statements.   Thus, the human-written   real/fake statements have very subtle differences, such as   one or two words replacement. The third negative statement   is adversarially sampled and has a relatively larger difference compared to the real statement. Following the original   paper, we split the VIOLIN dataset into 80% for training,   10% for validation, and 10% for testing.   4.2. Implementation Details   We use the pre-trained Bert [11] features of both dialog   subtitles and statements provided by [25]. For grounding, a   Faster R-CNN framework [28] with VGG-Net [31] as backbone pre-trained on COCO [24] is applied to extract persons and their features across frames. The entity grounding   threshold is set to 0.5. Both the visual and textual input are   embedded into d-dimension for fusion, and d is set as 256.   We sample the frames corresponding to the middle timestamp of each sentence for grounding. Adam with a learning   rate of 1e\u22123 is used for optimization. The constraint weight   of grounding module \u03b2 is set to 1. We set batch size as 8   in training. The entities in the statements of other videos in   the batch are sampled as the negative samples for training   the entity grounding module.   For the contradiction explanation module, we only use   the human-written samples for training. Adam with a learning rate of 1e\u22124 is used for optimization. Constraint weight   of multi-task learning \u03bb is set to 1.   2026   Table 2. Ablation Study of Entity Grounding for Entailment (%).   Method   Accuracy   Real Accuracy   Fake Accuracy   v1   66.72   73.60   59.83   v2   67.60   75.50   59.71   v3   66.53   77.78   48.01   Ours   68.39   79.52   57.25   4.3. Comparison Methods   We compare our method with the only existing method   proposed for the video entailment task, to our best knowledge. VIOLIN [25] dataset provides a visual/language fusion model to address entailment judgment. The statement   representations are jointly modeled with its video and subtitle by an attention-based fusion module.   Experimental results on VIOLIN dataset are shown in Table 1. Our proposed explainable entailment model along   with grounded evidence given by our method outperforms   the previous video entailment method. Because we precisely model the alignment between the video frames and   dialog based on grounded evidence. We also evaluate the in\ufb02uence of different visual features following VIOLIN [25].   The results demonstrate that our method works for both   image-based features \u201cResnet\u201d and motion-based features   \u201cC3D\u201d.   4.4. Ablation Study   4.4.1   How does grounding help in entailment?   To exhibit the effectiveness of entity grounding in entailment judgment, we compare our proposed method with the   following variants. (1) v1: Removing the \ufb01rst contradiction judgment from the entity grounding module. Then,   entity grounding is only used to provide temporal guidance.   (2) v2: Removing the temporal grounding guidance on entailment judgment. We substitute the Eq. 8 by   Q = [R; Qhs; Qcs]. Each frame contributes to the statement without being highlighted. (3) v3: Removing Lg. The   grounding module is trained without golden negative statements.   Table 2 summarizes the results of the aforementioned   variants. Comparing \u201cOurs\u201d and v3, adding golden negative   entities brings more than 1% performance improvement, as   it improves the grounding quality. Comparing \u201cOurs\u201d and   v2, adding temporal grounding\u2019s guidance is necessary for   making an accurate judgment. The contradiction judgment   from the entity grounding module also brings performance   gain by comparing \u201cOurs\u201d to v1.   4.4.2   How does explanation help in entailment?   To explore the contribution of the add-on entailment explanation module, we conduct the ablation study with the following variants: (1) v4: Using both the adversarial stateTable 3. Ablation Study of the Add-on Explanation Module for   Entailment (%).   Method   Accuracy   Real Accuracy   Fake Accuracy   v4   67.65   78.75   56.54   v5   67.32   80.63   54.02   Ours   68.39   79.52   57.25   Table 4. Quantitative Result for Contradiction Explanation (%).   Method   Explanation Accuracy   v6   72.42   Ours   75.20   ments and human-written statements in training the explanation model. (2) v5: Removing the explanation regularizer   Lr and only use Lcls.   Table 3 illustrates the results of the ablation study on the   explanation module. The proposed method outperforms the   variant v5 without explanation module by 0.83%, which   shows that the multi-task learning boosts the performance   of entailment judgment. By the outperformance to the variant v4, it is wise to train the explanation model with only   human-written samples instead of the adversarial samples,   since the adversarial samples are very different from its   paired real statement in sentence structure.   4.5. Contradiction Explanation Result   Since the real and fake statements are formed in pairs, we   can get access to the ground-truth of the items (in words or   phrases) that make the statement contradictory to the video.   For human-written fake statements, the annotators manually change a small portion of words or phrases in the real   statement, which makes the paired real and fake statements   have similar grammar and very tiny differences. Thus, the   ground-truth of the contradictory items can be obtained by a   simple \u201cdiff\u201d operation between a real/fake pair. But in the   adversarial sampled pairs, the real and fake statements are   mostly different in structure. Thus, we only use the humanwritten pairs for training the explanation module. But we   test all of the statements either human-written or adversarially sampled.   We quantitatively evaluate the local explanation on the   fake statements that are human-written.   The evaluation   metric is de\ufb01ned as the percentage of the number of words   that are correctly explained over the overall number of   words in the statement. The explanation results are exhibited in Table 4. We achieve 75.2% accuracy in contradiction   explanation, which indicates that more than three-quarters   of fake words can be found by our explanation model.   We also compare the proposed explanation method with   a variant v6.   v6 is the variant that explains the entailment of the statement by \ufb01nding the contradictory constitutes instead of the contradictory words. Constitute parsing   method [19] that was used in obtaining entities in Sec.3.3   2027   00:39.580, 00:40.000    'You look like me'   00:37.380, 00:38.760    'I need my throat for    talking, so thank you'   00:22.980, 00:25.680    \"An angel who's getting    really close\"   00:18.120, 00:20.000    \"She's..... She's Like an    Angel\"   00:16.760, 00:18.000    'What were they talking    about?'   00:27.240, 00:28.940    'Quem \u00e9 Voc\u00ea? O que    voc\u00ea est\u00e1 fazendo aqui'   True Statement   Prediction   False Statement   Prediction   A male blue bird gazes admirably at a female blue bird.   A female blue bird gazes admirably at a male blue bird.   (Entail)   A female blue bird immediately mistrusts the male blue    bird.   A female blue bird immediately trusts the male blue bird.   The female blue bird speaks to the male blue bird with    suspicion.   The man at the phone booth used a handkerchief to    use the phone as to not leave any \ufb01ngerprints.   (Entail)   (Entail)   (Contradict)   (Entail)   (Contradict)   0.7873   0.5722   0.6577   Grounded Entities: male blue bird, female blue bird   Ungrounded Entities: man at phone booth, handkerchief.   The blonde girl wants to go home and sleep in her own    bed.   The man in the dark jacket and a man in a light blue suit    and yellow apron laugh about a dropped tray of food.   The man in the dark jacket and a man in a light blue suit    and yellow apron laugh about a funny comedian's joke.   The man in the dark jacket is drinking milk in the kitchen    when he hears a loud crash coming from upstairs.   The man in the dark jacket is drinking milk in the kitchen    when he hears a dark barking outside.   Prediction   (Contradict)   (Entail)   The man in the dark jacket mistakes a man in a light blue    suit for his mother as he walks upstairs.   (Entail)   Prediction   (Contradict)   (Contradict)   (Contradict)   Grounded Entities: man in dark jacket,  man in light blue suit   Ungrounded Entities: blonde girl   True Statement   False Statement   00:08.130, 00:17.080    'no huh hi Jimbo you    thought I was mom'   00:19.770, 00:22.370    'curls outhouse getting    mom some supper'   00:22.380, 00:26.500    'she doesn't feel too    well'   00:26.510, 00:29.120    'what you doing drop it    yeah'   00:29.130, 00:40.000    'she dropped it yeah I    better clean it up'   0.9540   0.6374   0.9409   0.8900   0.5863   0.9880   0.9219   0.9980   The game is being played to pick the godparent for the    baby of the woman wearing the gold dress.   The man kisses the back of the woman's head when he    hears that she got a job.   Prediction   (Entail)   Prediction   (Contradict)   True Statement   False Statement   Grounded Entities: Woman, Man   Ungrounded Entities: woman wearing the gold dress   00:04.249, 00:05.579    'Hey,babe.'   00:07.769, 00:09.219    'I am a working girl'   00:09.249, 00:11.179    'bree asked me to join    her company.'   00:15.049, 00:16.739    'Hey,you're still gonna    cook for me,right?'   00:16.779, 00:18.759    'Are you kidding?You're    my guinea pig'   The woman is putting cards in a box when her husband    arrives home.   The woman is cooking dinner when her husband arrives    home.   The man is carrying a magazine in his hands when he    arrives home.   The man is carrying a suitcase in his hands when he    arrives home.   (Entail)   (Entail)   (Contradict)   (Contradict)   0.9164   0.9112   0.7039   0.5803   0.6914   Figure 5. Visualization of the entailment judgment and its explanation with grounded evidence. Strikethrough indicates that the video   does not contain the described entity and thus is judged as \u201ccontradiction\u201d. The contradictory items are marked by the underline with the   predicted scores.   is applied to extract constitutes from statement. The result   demonstrates that a plain word-level explanation is better   than using the constitute.   4.6. Explainable Entailment Result   Fig. 5 presents several entailment judgment examples using our method. Our model can successfully ground the   described entities to the speci\ufb01c regions and the relevant   frames, even if the grounding annotation is not provided   in training. Our model also has the resilience to the entities in the fake statements that are absent in the video.   The two fake statements contain the entities that are missing (e.g., \u201cthe blonde girl\u201d, \u201cthe woman wearing the golden   dress\u201d), marked by strikethrough, and are judged as fake in   the grounding stage. The predicted fake items are marked   by the underline with explanation scores. We \ufb01nd if the   statement is correctly judged as fake, the explanation result   is more reliable.   5. Conclusion   In this paper, we present a novel approach for video   entailment and its local explanation. Entity grounding is   highly incorporated into our task from two aspects. First,   we train a weakly-supervised entity video grounding module to judge a statement as \u201ccontradiction\u201d if the statement   consists of an entity absent in the video. Then if the entity   is present in the video, we infer the temporal occurrence of   that entity to guide the entailment judgment module focusing on the entity-relevant clips. In addition to entailment   judgment, our method is also developed to explain which   words or phrases make the statement contradictory to the   video. We formulate the local explanation as a regularizer   to the decision-making of entailment to improve the model\u2019s   faithfulness. Extensive results on VIOLIN dataset demonstrate the resulting model consistently outperforms the existing methods.   Acknowledgement: This project is supported in part by the   NSF SaTC award 1949694, and the Army Research Of\ufb01ce   under grant number W911NF-21-1-0236. The views and   conclusions contained in this document are those of the authors and should not be interpreted as representing the of\ufb01cial policies, either expressed or implied, of the Army Research Of\ufb01ce or the U.S. Government.   2028", "conf": "ICCV", "year": "2021", "index": 133}, {"title": "Proceedings of NAACL-HLT 2019, pages 582\u2013595   Minneapolis, Minnesota, June 2 - June 7, 2019. c\u20dd2019 Association for Computational Linguistics   582   CLEVR-Dialog:   A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog   Satwik Kottur1, Jos\u00b4e M.F. Moura1, Devi Parikh2,3, Dhruv Batra2,3, Marcus Rohrbach2   1Carnegie Mellon University, 2Facebook AI Research, 3Georgia Institute of Technology   {skottur,moura}@andrew.cmu.edu,{parikh,dbatra}@gatech.edu,mrf@fb.com", "abstract": "Visual Dialog is a multimodal task of answering a sequence of questions grounded   in an image (using the conversation history   as context).   It entails challenges in vision,   language, reasoning, and grounding.   However, studying these subtasks in isolation on   large, real datasets is infeasible as it requires   prohibitively-expensive complete annotation   of the \u2018state\u2019 of all images and dialogs.   We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning   in visual dialog. Speci\ufb01cally, we construct a   dialog grammar that is grounded in the scene   graphs of the images from the CLEVR dataset.   This combination results in a dataset where all   aspects of the visual dialog are fully annotated.   In total, CLEVR-Dialog contains 5 instances   of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.   We use CLEVR-Dialog to benchmark performance of standard visual dialog models; in   particular, on visual coreference resolution (as   a function of the coreference distance). This   is the \ufb01rst analysis of its kind for visual dialog models that was not possible without this   dataset. We hope the \ufb01ndings from CLEVRDialog will help inform the development of future models for visual dialog. Our code and   dataset are publicly available1.   1", "content": "The focus of this work is on intelligent systems   that can see (perceive their surroundings through   vision), talk (hold a visually grounded dialog), and   reason (store entities in memory as a dialog progresses, refer back to them as appropriate, count,   compare, etc.). Recent works have begun studying   such systems under the umbrella of Visual Dialog   (Das et al., 2017a; de Vries et al., 2017), where   1https://github.com/satwikkottur/clevr-dialog   an agent must answer a sequence of questions   grounded in an image. As seen in Fig. 1, this entails   challenges in \u2013 vision (e.g., identifying objects and   their attributes in the image), language/reasoning   (e.g., keeping track of and referencing previous   conversation via memory), and grounding (e.g.,   grounding textual entities in the image).   In order to train and evaluate agents for Visual   Dialog, Das et al. (2017a) collected a large dataset   of human-human dialog on real images collected   between pairs of workers on Amazon Mechanical Turk (AMT). While such large-scale realistic   datasets enable new lines of research, it is dif\ufb01cult   to study the different challenges (vision, language,   reasoning, grounding) in isolation or to break down   the performance of systems over different challenges to identify bottlenecks, because that would   require prohibitively-expensive complete annotation of the \u2018state\u2019 of all images and dialogs (all   entities, co", "conf": "NAACL", "year": "2019", "index": 71}, {"title": "Preprint   FERRET:   REFER   AND GROUND ANYTHING ANYWHERE AT ANY GRANULARITY   \uf8ffHaoxuan You1\u2020, Haotian Zhang2\u2020, Zhe Gan2, Xianzhi Du2, Bowen Zhang2, Zirui Wang2,   Liangliang Cao2, Shih-Fu Chang1, Yinfei Yang2   1Columbia University, 2Apple AI/ML   haoxuan.you@cs.columbia.edu, {haotian zhang2,zhe.gan,yinfeiy}@apple.com   What animal is in area [region0]?   It is a ferret [box0].   Cool, what is the animal [region1]?    Are they the same one?   No, the animal [box1] is a ferret,    which is a different one from the    one [box0]. They are not the same    ferret.   How to make the food [region0]   with available ingredients?   To make a sandwich [box0] with available ingredients,   1. You can prepare the sandwiches by placing the   sliced turkey [box1] on one bread, and the sliced   cheese [box2] on another.   2. To assemble the sandwiches, press the two halves   of the sandwich together, and you can use the knife   [box3] or your hands [box4] to hold them together if   needed. Once you have a complete sandwich, you can   use the knife to cut it into quarters or halves, and serve   it on a plate [box5].   region0   box0   box1     box2     box3     box4     box5     Input Referring   Output Grounding   Ferret   region1   region0   box0    box1    Figure 1: Ferret enables referring and grounding capabilities for multimodal large language model   (LLM). In terms of referring, a user can refer to a region or an object in point, box, or any free-form   shape. The regionN in the input will be replaced by the proposed hybrid representation before being   fed into the LLM. In terms of grounding, Ferret is able to accurately ground any open-vocabulary   descriptions. The boxN in the output denotes the predicted bounding box coordinates.", "abstract": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image   and accurately grounding open-vocabulary descriptions. To unify referring and   grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features   jointly to represent a region in the image. To extract the continuous features   of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept   diverse region inputs, such as points, bounding boxes, and free-form shapes. To   bolster the desired capability of Ferret, we curate GRIT, a comprehensive referand-ground instruction tuning dataset including 1.1M samples that contain rich   hierarchical spatial knowledge, with 95K hard negative data to promote model   robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs   in region-based and localization-demanded multimodal chatting. Our evaluations   also reveal a significantly improved capability of describing image details and a   remarkable alleviation in object hallucination. Code and data will be available at   https://github.com/apple/ml-ferret.   \uf8ffWork done during an internship at Apple. \u2020Equal contribution.   1   arXiv:2310.07704v1  [cs.CV]  11 Oct 2023   Preprint   1", "content": "In vision-language learning, how to enable spatial understanding of models is a fundamental research problem. Two desired capabilities stem from this problem: referring and grounding. Referring demands that the model can accurately comprehend the semantics of specific given regions (Krahmer & Van Deemter, 2012; Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016;   Zellers et al., 2019), whereas grounding necessitates the model to localize the region in accordance   with the given semantic description (Luo & Shakhnarovich, 2017; Nagaraja et al., 2016; Yu et al.,   2017; Kamath et al., 2021).   Essentially, referring and grounding demand the same type of knowledge: alignment of spatial   information and semantics. Despite that, existing works mostly learn referring and grounding individually (Li et al., 2022; Wu et al., 2022; Yu et al., 2017). In comparison, humans can learn from one   task and generalize the shared knowledge to the other task effortlessly, and are able to seamlessly   integrate referring/grounding capabilities with daily dialogue and reasoning (Zellers et al., 2019).   Inspired by the above gap, in this paper, we study three main questions: (i) How to unify referring   and grounding in one framework, and will they benefit each other? (ii) How to represent versatile   types of regions that humans usually use for referring, such as point, box, scribble, and even freeform shapes? (iii) How to make referring and grounding open-vocabulary, instruction-following,   and robust, which are crucial for practical applications?   Targeting these three questions, we introduce Ferret, a novel refer-and-ground Multimodal Large   Language Model (MLLM). First of all, we choose MLLM as the bedrock of Ferret due to their powerful vision-language global understanding capability (Zhu et al., 2023a; Liu et al., 2023b; Li et al.,   2023c). To unify referring and grounding, Ferret first represents the coordinates of regions in natural   language numerical form,1 as illustrated in Figure 3. However, it is inefficient to use single point   or box coordinates to represent versatile shapes of regions, such as strokes, scribbles, or complex   polygons. These shapes are essential for more universal and precise human-model interaction. To   solve this problem, we further propose a spatial-aware visual sampler to acquire the visual features   for regions in any shape, taking care of the varying sparsity in those shapes. Then, the discrete   coordinates and the continuous visual features are combined together to represent the visual regions   in the input, composing a hybrid region representation in Ferret. Equipped with above methods,   Ferret can deal with input that mixes referred regions with free-form text, and is able to ground the   mentioned objects in its output by seamlessly generating the coordinates for each groundable object   along with generating text. To our best knowledge, Ferret is the first work that is able to process   free-formed region inputs in MLLMs.   In order to make the refer-and-ground capability in Ferret open-vocabulary, instruction-following,   and robust, we collect GRIT, a Ground-and-Refer Instruction-Tuning dataset with 1.1M samples.   GRIT contains multiple levels of spatial knowledge, covering objects, relationships, region descriptions, and complex reasoning. It includes both text-in location-out (grounding) and location-in textout (referring) data, as well as data that mixes location and text in both input and output. The majority of the dataset is converted from existing vision(-language) tasks like object detection (Krishna   et al., 2017) and phrase grounding (Yu et al., 2016; Plummer et al., 2015) with carefully designed   templates to make it instruction-following. Additionally, 34K refer-and-ground instruction-tuning   conversations are collected via the help of ChatGPT/GPT-4 (OpenAI, 2023b) to facilitate training   an instruction-following and open-vocabulary refer-and-ground generalist. Moreover, we conduct   spatial-aware negative data mining, which further promotes model robustness.   Ferret subsumes strong open-vocabulary capabilities of spatial understanding and localization.   When evaluated on conventional referring and grounding tasks, it achieves superior performance.   More than that, we believe refer-and-ground capabilities should be integrated into daily conversations of humans, e.g., people refer to something they don\u2019t know and ask what it is used for (like   Figure 1). To evaluate this new capability, we introduce Ferret-Bench, covering three new types of   tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark   existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.   Moreover, Ferret demonstrates an intriguing property of alleviating object hallucinations.   1Note that there is no additional vocabulary or position encoders introduced in Ferret model.   2   Preprint   Table 1: Comparison of Ferret v.s. recent MLLMs integrating spatial awareness. \u2018Convention\u2019 refers   to a comprehensive collection of publicly available data that has been transformed using templates,   \u2018GPT-Generate\u2019 signifies the generated refer/ground datasets employing GPT, and \u2018Robustness\u2019 denotes datasets aimed at mitigating hallucination and improving robustness. Section 4 explains more   details about each.   Model   Input Types   Output   Grounding   Data Construction   Quantatitive Eval.   of Refer/Ground   w. Chat   Point   Box   Free-form   Convention   GPT-Generate   Robustness   BuboGPT   \u2717   \u2717   \u2717   \u2714   \u2714   \u2717   \u2717   \u2717   Vision-LLM   \u2717   \u2717   \u2717   \u2714   \u2714   \u2717   \u2717   \u2717   Kosmos-2   \u2717   \u2714   \u2717   \u2714   \u2714   \u2717   \u2717   \u2717   Shikra   \u2714   \u2714   \u2717   \u2714   \u2714   \u2714   \u2717   \u2717   GPT4-ROI   \u2717   \u2714   \u2717   \u2717   \u2714   \u2717   \u2717   \u2717   PVIT   \u2717   \u2714   \u2717   \u2717   \u2714   \u2714   \u2717   \u2714   Ferret   \u2714   \u2714   \u2714   \u2714   \u2714   \u2714   \u2714   \u2714   In summary, our contributions are threefold. (i) We propose Ferret, that uses a hybrid region representation equipped with a novel spatial-aware visual sampler, to enable fine-grained and openvocabulary referring and grounding in MLLM. (ii) We construct GRIT, a large-scale ground-andrefer instruction tuning dataset, for model training. It also contains additional spatial negative samples to enhance model robustness. (iii) We introduce Ferret-Bench, to evaluate tasks jointly requiring referring/grounding, semantics, knowledge, and reasoning. Our model exhibits superior   performance in a wide range of tasks and reduces object hallucination.   2   RELATED WORK   Multimodal large language models (MLLMs).   Large Language Models (LLMs), including   GPTs (Brown et al., 2020; OpenAI, 2023a), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al.,   2022), and LLaMA (Touvron et al., 2023a;b), have revolutionized research in NLP, spurring significant advances in multimodal language models as well.   Early models primarily focused on   large-scale image-text pre-training.   Notable examples include SimVLM (Wang et al., 2022c),   GIT (Wang et al., 2022a), PaLI (Chen et al., 2022b), PaLI-X (Chen et al., 2023c), BLIP-2 (Li   et al., 2023c), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023), CM3 (Aghajanyan   et al., 2022), and CM3Leon (Yu et al., 2023). Flamingo, in particular, pioneered the integration   of a pre-trained CLIP image encoder with LLMs through gated cross-attention blocks, showcasing   emergent multimodal in-context few-shot learning capabilities. Its open-sourced variants, such as   OpenFlamingo (Awadalla et al., 2023) and IDEFICS (Laurenc\u00b8on et al., 2023), have garnered significant attention. Typically, these models undergo pre-training using millions or even billions of   image-text pairs and interleaved image-text datasets (Zhu et al., 2023b).   On the other hand, recent research has increasingly focused on using pre-trained LLMs for visual   instruction tuning. Prominent examples include LLaVA (Liu et al., 2023b), MiniGPT-4 (Zhu et al.,   2023a), mPLUG-Owl (Ye et al., 2023), Otter (Li et al., 2023a), InstructBLIP (Dai et al., 2023),   to name a few. In addition to text generation, recent models like FROMAGe (Koh et al., 2023b),   GILL (Koh et al., 2023a), Emu (Sun et al., 2023), have also enabled MLLMs for image retrieval and   image generation. Please refer to Chapter 5 of Li et al. (2023b) for a detailed review.   MLLMs for referring and grounding. In the realm of existing literature, works such as Kosmos2 (Peng et al., 2023) and Shikra (Chen et al., 2023b), closely resemble ours as they also enable   MLLMs for fine-grained image comprehension and open-world referring and grounding. Additional works in this direction include GPT4ROI (Zhang et al., 2023), PVIT (Chen et al., 2023a),   BuboGPT (Zhao et al., 2023), VisionLLM (Wang et al., 2023), and ContextDET (Zang et al., 2023).   Nevertheless, pivotal distinctions set our model apart. First, prior endeavors supported only bounding boxes (and points in Shikra) as input. Conversely, due to Ferret\u2019s innovative hybrid region representation, we accommodate a broader range of free-form shapes for referring, encompassing points,   boxes, sketches, scribbles, polygons, and more. Second, we meticulously curate an extensive referand-ground instruction tuning dataset. Third, we introduce Ferret-Bench to facilitate forthcoming   3   Preprint   research and enhance evaluation benchmarks in this direction. Lastly, our model exhibits superior   performance compared to previous works, notably mitigating object hallucination to a significant   extent. A more straightforward side-by-side comparison is shown in Tab. 1.   Unifying grounding and VL understanding. Our work is also related to previous work that aims   to unify text and bounding box output for vision-language (VL) models, such as UniTAB (Yang   et al., 2022), OFA (Wang et al., 2022b), and Unified-IO (Lu et al., 2022), which also represent   bounding boxes using a set of additional discrete tokens as proposed in Pix2Seq (Chen et al., 2021;   2022a). Ferret is unique in that (i) our model is built upon LLMs, marrying the power of LLMs and   grounding, thus unlocking new capabilities such as grounded instruction tuning, and (ii) we handle   bounding box coordinates as regular text tokens, avoiding the need for extra specialized tokens   dedicated to representing boxes.   3   METHOD   We start with detailing the proposed hybrid region representation to depict regions of various shapes   and formats. Then, we present the model architecture of Ferret.   3.1   HYBRID REGION REPRESENTATION   The object [obj0] is a pistol, and    the object [obj1] is a knife   What is the object [region0]    and object [region1]?    region0   region1   Figure 2: Bounding box   v.s.   Free-from Shape.   These two objects have   almost the same bounding box, causing ambiguity when relying on the   box to refer to. Equipped   with hybrid region representation, Ferret can separate them.   When referring to specific regions, three primary formats are generally   used: point, box, and free-form shapes. While the point and box formats can be succinctly represented by coordinates (e.g., [x, y] for a point,   and [xmin, ymin, xmax, ymax] for a box) as in Peng et al. (2023); Chen et al.   (2023b), the free-form shape is more versatile, encompassing a variety   of region types such as scribbles, polygons, and masks. The advantage   of free-form shape is straightforwardly illustrated in Figure 2. Depicting free-form shapes through coordinates is computationally expensive and   obscure, and its complexity hinders the model learning to establish a clear   correlation between the provided coordinates and the corresponding regions.   To generalize across all three distinct formats, we propose a hybrid region   representation that synergizes discrete coordinates with continuous visual   features to refer to a particular region, which is shown in the top-left of   Figure 3. For coordinates, following Chen et al. (2021); Yang et al. (2022),   we quantize each coordinate into one of the nbins discrete bins.2 Regarding   continuous visual features, for a given region R, we first construct a 2D   binary mask M of the same size as the image, marking a value of 1 inside   the targeted region and 0 outside of the region. Then, the binary mask M,   jointly with the extracted image feature map Z, is sent into our proposed   spatial-aware visual sampler s(\u00b7), which will be detailed in Section 3.2, to   extract the visual continuous feature f = s(M, Z).   Finally, we represent a point with {x, y, fRp}, where the region Rp is a circle centered in {x, y} with   a fixed radius.3 A box or a free-form shape can both be represented by {xmin, ymin, xmax, ymax, fRbox},   where xmin/xmax denotes the minimum/maximum x-axis coordinate of the region, and so forth for   y-axis. Rbox denotes the input region.   3.2   MODEL ARCHITECTURE   As illustrated in Figure 3, Ferret is mainly composed of (i) an image encoder to extract image   embeddings, (ii) the proposed spatial-aware visual sampler to extract regional continuous features,   and (iii) an LLM to jointly model image, text, and region features.   2nbins = 1000 by default. The value is input invariant, which means for any input image size, the original   coordinate will be mapped to the new coordinates. This makes the model robust to different input resolutions.   3Radius is set to 5 by default.   4   Preprint   Large Language Model    Image Encoder   Embedding   Image Input   Text w/", "conf": "ICLR", "year": "2024", "index": 194}, {"title": "Weakly Supervised Grounding for VQA in   Vision-Language Transformers   Aisha Urooj Khan1 , Hilde Kuehne2,3 , Chuang Gan3 , Niels Da Vitoria   Lobo1 , and Mubarak Shah1   1 University of Central Florida, Orlando, FL, USA   2 Goethe University Frankfurt, Frankfurt, Hesse, Germany   3 MIT-IBM Watson AI Lab, Cambridge, MA, USA", "abstract": ". Transformers for visual-language representation learning have   been getting a lot of interest and shown tremendous performance on visual   question answering (VQA) and grounding. However, most systems that   show good performance of those tasks still rely on pre-trained object   detectors during training, which limits their applicability to the object   classes available for those detectors. To mitigate this limitation, this paper   focuses on the problem of weakly supervised grounding in the context   of visual question answering in transformers. Our approach leverages   capsules by transforming each visual token into a capsule representation   in the visual encoder; it then uses activations from language self-attention   layers as a text-guided selection module to mask those capsules before   they are forwarded to the next layer. We evaluate our approach on   the challenging GQA as well as VQA-HAT dataset for VQA grounding.   Our experiments show that: while removing the information of masked   objects from standard transformer architectures leads to a significant   drop in performance, the integration of capsules significantly improves   the grounding ability of such systems and provides new state-of-the-art   results compared to other approaches in the field4.   Keywords: multimodal understanding, visual grounding, visual question   answering, vision and language   1", "content": "Empowering VQA systems to be explainable is important for a variety of applications such as assisting visually-impaired people to navigate [16,64] or helping   radiologists in early diagnosis of fatal diseases [1,65]. A system that only produces   a good answering accuracy will not be sufficient in these applications. Instead,   VQA systems for such uses should ideally also provide an answer verification   mechanism and grounding is a convincing way to obtain this direct verification.   On the heels of success in natural language processing and multi-modal   understanding, a variety of transformer-based methods have been introduced for   4 Code is available at https://github.com/aurooj/WSG-VQA-VLTransformers   2   A. Urooj et al.   Fig. 1: (a) Proposed architecture: Given the question-image pair, grid features are   used to obtain visual capsules using a Capsule encoding layer. Output embedding for   [CLS] token from text transformer layer is then used to do capsule features selection.   Selected capsules encodings with position information is then input to the visual   encoder. We use sentence embedding from each textual transformer layer to select   capsules for each visual transformer layer. The selected capsules are then input to   the next visual transformer layer along with the output of the previous layer in the   visual encoder. Finally, a cross-attentional block allows for a fine-grained interaction   between both modalities to predict the answer. (b) Attention from the proposed   vision-language transformer with VQA supervision. We look at the self-attention   of the [IMG] token on the heads of the last layer. These maps show that the model   automatically learns to ground relevant objects with VQA-only supervision leading to   weakly-supervised grounded VQA (GVQA). blue box:ground truth, orange:predicted box.   joint representation learning of vision and language and respective downstream   tasks, including VQA. Such approaches, e.g., [34, 40, 57] are usually trained   based on region masks of detected objects generated by a pre-trained object   detector [26]. The assumption that object masks are provided at input time   limits detection to pretrained objects only and comes at the risk of image context   information being unused.   Detector-free methods avoid this bias toward pre-trained object classes while   being simpler and faster because they do not need to extract region-based   features from a pre-trained object detector. Other works [10, 22, 28, 49] have   therefore focused on removing the dependency on object detectors while achieving   comparable if not better performance (e.g., on retrieval and VQA tasks). Among   those, [10] and [49] also show good visual representations by qualitative examples,   but do not provide an evaluation of their answer (or question) grounding ability.   Weakly Supervised Grounding for VQA in Vision-Language Transformers   3   In this work, we address this issue and focus on the problem of weakly   supervised grounding for the VQA task in visual-language transformer based   systems. For the input image-question pair, we want to answer the question as well   as localize the relevant question and answer objects with only VQA supervision.   Compared to detector-free referential expression grounding [7,38,61], VQA does   not assume that the region description is always part of the input phrase as the   answer word may not be present in the input question. It is therefore inadequate   to only learn a direct mapping between text and image features. Instead, it   requires processing multiple image-text mapping steps with the correct relation   between them.   To address the task of VQA grounding in transformer-based architectures with   the question-answering supervision alone, we propose a generic extension of the   visual encoder part of a visual-language transformer based on visual capsule   layers together with a text-guided selection module. Capsule networks learn to   group neurons into visual entities. They thus try to capture entities (objectness)   and relationships among them with promising results on various visual detection   and segmentation tasks [12,13,31]. To make use of this ability in the context of   transformers, we transfer inputs as well as intermediate layers\u2019 feature tokens   to capsule encodings, from which the most relevant ones will be selected by the   textual tokens of a parallel language encoder. This text-guided selection facilitates   choosing features at entity-level, similar to attending object features, instead   of an independent feature selection. We interleave transformer layers with such   masked residual capsule encodings. This extension provides a combination of   visual input routing and text-based masking which significantly improves the   visual grounding ability of such systems.   We evaluate existing methods as well as the proposed approach on the challenging   GQA and VQA-HAT datasets. To this end, we consider the attention output   obtained from these methods and evaluate it on various metrics, namely overlap,   intersection over union, and pointing game accuracy. Our results on the original   architectures show a significant gap between task accuracy and grounding of   existing methods indicating that existing vision-language systems are far from   learning an actually grounded representation. The proposed method bridges   the gap and outperforms the best contenders in terms of overlap, intersection   over union, and pointing game accuracy achieving SOTA performance on the   GQA dataset. It also achieves best mean-rank correlation score on VQA-HAT [8]   dataset among methods which do not use attention supervision.   We summarize the contributions of our architecture as follows: a) we propose a   capsule encoding layer to generate capsule-based visual tokens for transformers;   b) we propose a text-guided capsule selection with residual connections to guide   the visual features at each encoding step; and c) the proposed generic interleaved   processing of capsules and self-attention layers can be integrated in various vision   language architectures.   4   A. Urooj et al.   2   Related Work   Visual-language Representation Learning. Learning a robust visual-language   representation is currently an active area of research [27] with impressive progress   on downstream tasks including VQA [6, 32, 34, 35, 39, 40, 43, 56, 57]. A majority of these methods rely on object detections making the downstream task   simpler. Some works have attempted to avoid this dependency on object detections and show comparable performance using spatial features or image   patches [21, 22, 28, 33]. Our work falls into the later category and uses grid   features as input.   Weakly-supervised Grounding and VQA. Weakly-supervised visual grounding is well studied for phrase-grounding in images [3, 5, 7, 9, 38, 58, 61]. Some   works also focused on phrase grounding in videos [20, 55, 62]. However, less   attention has been paid to grounding in VQA despite having significance for   many critical applications. There is much research on making questions visually   grounded [45,50,54,59,67,68], but only a handful of works focus on evaluating   their grounding abilities [8,24,25,48,52,54]. GQA leverages scene graphs from   Visual Genome dataset providing visual grounding labels for question and answer   making it feasible to evaluate VQA logic grounding. Recently, xGQA [46] has been   introduced as a multilingual version of the GQA benchmark. GQA [24] and [25]   discuss the evaluation of VQA systems for grounding ability. VQA-HAT [8] on   the other hand, provides human attention maps used for answering the question   in a game-inspired attention-annotation setup. A handful of methods [41,48,63]   evaluate their systems for correlation between machine-generated attention and   human attention maps on VQA-HAT. With the emergence of transformers as the   current SOTA, the focus moves towards grounding abilities of those systems for   the VQA task. Unfortunately, none of these transformer-based methods have yet   focused on the evaluation of weakly supervised grounding. Additionally, the fact   that only few real-world datasets provide grounding labels makes this task challenging. We therefore finetune three existing detector-free transformer methods   on GQA and evaluate them for the weak grounding task.   Transformers with Capsules. Some research has focused on the idea of   combining transformers and capsules [11,15,37,42,44,47,60]. For instance, [60]   studies text-summarization, image-processing, and 3D vision tasks; [44,47] uses   capsule-transformer architecture for image-classification, and [37] uses capsulestransformers for predicting stock movements. To the best of our knowledge, the   combination of capsules with transformers for VQA grounding has not been   studied.   3   Proposed Approach   Given an input image-question pair with image I and question Q, we want to   localize the relevant question and answer objects with only VQA supervision. We   start from a two stream visual-language model (fig. 1) where the language encoder   Le guides input and intermediate respresentations of the visual encoder Ve. The   Weakly Supervised Grounding for VQA in Vision-Language Transformers   5   (a) Capsule Encoding layer   (b) Capsule layer   Fig. 2: (a) Capsule encoding layer: grid features X\u2032 \u2208Rh\u00d7w\u00d7d are transformed into   capsules Xc for each spatial position. Output embedding h1   cls for [CLS] token from the   first text encoder layer generates a mask m1 for capsule selection. The selected capsules   X1   mc are flattened along capsule dimension to get a set of visual tokens (of length h \u2217w)   where each token is denoted by x1   jmc, j = {1, 2, ..., hw}; x1   jmc \u2208Rdc where dc=capsule   dimension, is then upsampled to model dimension d using a fully connected layer. A   position embedding is added to visual tokens with the special token [IMG] at position   0. The output capsule encodings are then input to the visual transformer for future   steps. (b) Capsule layer prepares the input for the next visual transformer layer i by   combining previous layer\u2019s output (layer i \u22121) with selected capsules using output for   [CLS] token hi   cls from the ith text encoder layer. Similar to the Capsule encoding, input   tokens X\u2032 are first transformed into capsules Xc. The aggregated output feature hi   cls   from the ith text encoder layer generates a mask mi to select certain capsules for input   to the ith visual encoder layer. The resulting capsules are then flattened and upsampled   (denoted by \u02c6xi   jmc) and added to the output hi\u22121   vj   of the previous visual transformer   layer i \u22121 to obtain input features \u02c6hi   vj for visual transformer layer i.   input text to language encoder Le is a sequence of word tokens from a vocabulary   V appended with special tokens [CLS] and [SEP] at the start and end of the   word tokens. As input to the visual encoder, our model takes convolutional   features as image embeddings. The convolutional features X \u2208Rh\u00d7w\u00d7d1 are   extracted from a pre-trained ResNet model, h, w are the feature height and width,   and d1 is the extracted features dimension. A 2D convolutional layer then yields   an embedding X\u2032 of size Rh\u00d7w\u00d7d, where d is the model dimension size. These   input embeddings produce capsule encodings Xc as explained in section 3.2.   In the following, we first explain the motivation to use capsules in Sec. 3.1   followed by details about the capsule encoding in Sec. 3.2, the text-guided   selection of the capsules in Sec. 3.3, as well as the text-based residual connection   in Sec. 3.4. We close the section with an overview of the pretraining procedure in   Sec. 3.5 and describe the details of the VQA downstream task in Sec. 4.1.   3.1   Capsule Networks   Standard neural networks lack the ability to dynamically represent a distinct   part-whole hierarchy tree structure for each image [17]. This inability motivated   the introduction of a type of model called Capsule Networks [18] which was later   formalized in [53]. A Capsule Network is a neural network that is designed to   6   A. Urooj et al.   model part-whole hierarchical relationships more explicitly than Convolutional   Neural Networks (CNNs), by using groups of neurons to encode entities and   learning the relationships between these entities. The promising performance of   capsules can be attributed to their ability to learn part-whole relationships for   object entities via routing-by-agreement [53] between different capsule layers. A   capsule is represented by a group of neurons; each capsule layer is comprised of   multiple capsules and multiple capsule layers can be stacked together. Capsule   routing is a non-linear, iterative and clustering-like process that occurs between   adjacent capsule layers, dynamically assigning part capsules i in layer \u2113to object   capsules j in layer \u2113+ 1 , by iteratively calibrating the routing coefficients \u03b3 [51].   Unlike most previous works which use a loss over object classes to learn a set of   capsule classes, we do not have any object level supervision available for capsules,   but instead combine the power of transformers and capsules by interleaving   capsules as intermediate layers within the transformer and use VQA supervision   to model visual entities as capsules.   3.2   Capsule Encodings   We use matrix capsules [19] as follows: given an image embedding X\u2032 \u2208Rh\u00d7w\u00d7d,   matrix capsules Xc \u2208Rh\u00d7w\u00d7dc, as shown in Figure 2(a), are obtained as follows:   The image embedding X\u2032 is input to a convolutional layer producing primary   capsules Xp where each capsule has a pose matrix of size K \u00d7K and an activation   weight. The primary capsule layer outputs Cp number of capsules for each spatial   location. The output dimensions for poses is Rh\u00d7w\u00d7Cp\u00d7K\u00d7K and for activation   is Rh\u00d7w\u00d7Cp\u00d71. To treat each capsule as a separate entity, the pose matrix and   activation are grouped together for each capsule. Hence, the primary capsules   Xp have the dimensions Rh\u00d7w\u00d7dp where dp = Cp \u00d7 (K \u00d7 K + 1). The primary   capsules are then passed through an EM-Routing layer to vote for capsules in   the next layer. Assuming we have Cv number of capsules in the next layer, the   routing yields capsule encodings Xc where Xc \u2208Rh\u00d7w\u00d7dc, dc = Cv \u00d7(K \u00d7K +1).   We use an equal number of capsules in both layers, i.e., C = Cp = Cv. Our   system employs the capsule representation Xc as visual embeddings.   Since transformers take a sequence of tokens as input, we flatten the capsule   embeddings across spatial dimension to get a sequence of visual tokens of length   h\u2217w, where each visual token is denoted by xj \u2208Rdc for j \u22081, 2, ..., hw. A special   trainable token [IMG] is then concatenated to these tokens to form the final   set of visual tokens {[IMG], x1, x2, ..., xhw}. A learnable position embedding is   added to these visual tokens to keep the notion of spatial position in the sequence.   Each of the visual tokens except [IMG] is represented by C capsules.   3.3   Text-guided Capsule Selection   As the language encoder is attending different words at each layer, we select   visual capsules based on the text representation at each visual encoder layer.   Let hi   cls be the feature output corresponding to [CLS] token from the ith text   encoder layer; we take the feature output h1   cls corresponding to [CLS] token from   Weakly Supervised Grounding for VQA in Vision-Language Transformers   7   the first text encoder layer and input it to a fully connected layer \u03d5. The output   is C logits followed by a softmax function to learn presence probability m1 \u2208RC   of attended words at layer 1. This mask is applied to Xc to select the respective   capsules and mask out the rest resulting in the masked capsule representation   X1   mc.     \\ label {eq:1} m^1 = softmax(\\phi (h^{1}_{cls})). x   (1)        \\l a be l  {eq:2} X^1_{mc} = m^1 \\odot X_c    (2)   The masking is only applied to the visual tokens xj without affecting [IMG]   token.   3.4   Text-based Residual Connection   To keep the capsule representation between intermediate layers, we add capsules   via a residual connection to the inputs of each intermediate visual encoder layer.   The input capsules to the intermediate layer are also selected based on the   intermediate features output from the text encoder. Let mi be the probability   mask for attended words in the text feature output hi   cls from the ith layer:     \\ label {eq:3} m^i = softmax(\\phi (h^{i}_{cls})), \\forall i \\in \\{1,2,...,L\\}, x   (3)   and xi   jmc denotes the jth token with visual capsules selected using mask mi.   x    \\la b el  {eq:4} x^i_{j_{mc}} = m^i \\odot X_c,    (4)   The ith visual encoder layer takes features from the (i \u22121)th layer to produce   features hi   vj for the jth position. Let f i   v be the ith layer in the visual encoder.   The output and input follow the notation below:        \\l a b e   l {eq:   5}  h^i_{v_j} = f^i_v (h^{i-1}_{v_j}).    (5)   To keep information flowing from text to image, we propose to add the   residual connection from visual capsules for the jth token by adding xi   jmc to the   input of the ith encoder layer. However, there is a dimension mismatch between   xi   jmc \u2208Rdc and hi\u22121   vj   \u2208Rd. We upsample xi   jmc to dimension size d using a fully   connected layer \u03c3 and get the upsampled capsule-based features \u02c6xi   jmc \u2208Rd. The   input to the visual encoder layer will be as follows:     \\la   be   l  {   eq:6}    \\h   a tx{   h}^{i-1}_{v_j} = f^i_v (h^{i-1}_{v_j} + \\hat {x}^i_{j_{mc}}).    (6)   The output feature sequences from both encoders are then input to our cross   attentional module which allows token-level attention between the two modalities.   The aggregated feature outputs corresponding to [CLS] and [IMG] tokens after   cross attention are input to a feature pooling layer followed by respective classifiers   for pretraining and downstream tasks. We discuss the implementation about   modality-specific encoders, feature pooling, and cross attention in detail in the   supplementary material.   8   A. Urooj et al.   3.5   Training   To perform well, transformers require pretraining on large-scale datasets before   finetuning for the downstream tasks, i.e., GQA and VQA-HAT in our case. Therefore, we first pretrain our capsules-transformer backbone on three pretraining   tasks: image-text matching (ITM), masked language modeling (MLM), and visual   question answering (VQA). The system is pre-trained in two stages: first, we do   joint training of modality-specific encoders only to learn text-guided capsules   representation; the representation learned in encoders is kept fixed during the   second stage of pre-training where we add cross-attentional blocks on top of   the modality encoders allowing token-level interaction between text features   and visual features. While the first stage of pretraining uses pooled features   from text and from visual encoders, the second stage pools features after cross   attention: therefore, the second stage pre-training tasks uses cross-modal inputs   as language and image features. For details about pretraining tasks in context   of our method, refer to section 1.2 in supplementary. We finally finetune the   pretrained capsules-transformer backbone to solve VQA as our downstream task.   4   Experiments and Results   4.1   Datasets   Pre-training. We use image-caption pairs from MSCOCO [36] and Visual   Genome [30] for pretraining our backbone. Specifically, we use the same data   as [57] which also include MSCOCO-based VQA datasets: Visual7W, VQAv2.0,   and Visual Genome based GQA. However, we exclude the GQA validation set   from pretraining and finetuning as we evaluate grounding on this set because   scene graphs for GQA test and testdev are not publicly available. We use train   sets of MSCOCO and VG with \u223c7.5M sentence-image pairs for pretraining.   MSCOCO val set is used for validating pretraining tasks.   Downstream. We consider two datasets for the downstream task, GQA [24]   and VQA-HAT [8].   GQA poses visual reasoning in the form of compositional question answering. It   requires multihop reasoning to answer the question, so GQA is a special case of   VQA. GQA is more diverse than VQA2.0 [14] in terms of coverage of relational,   spatial, and multihop reasoning questions. It has 22M QA pairs with 113K images.   GQA provides ground truth boxes for question and answer objects making it a   suitable test bed for our task.   VQA-HAT dataset provides human attention maps for VQA task. This dataset   is based on VQA v1.0 [2] dataset and provides 1374 QA pairs with 488 images   in the validation set. To evaluate on this dataset, we train our system on VQA   v1.0. The answer vocabulary of VQA train set has a long tail distribution. We   follow previous works [2,8] and use 1000 most frequent answers. We first combine   training (248,349 QA pairs) and validation data (121,512 QA pairs) to get a total   of 368,487 QA pairs. We then filter out the questions with out-of-vocabulary   answers resulting in 318,827 QA pairs.   Weakly Supervised Grounding for VQA in Vision-Language Transformers   9   4.2   Evaluation Metrics   For GQA, VQA accuracy is reported for task accuracy. For grounding task on   transformers, we take attention scores from [IMG] token to visual tokens from   the last cross-attentional layer for all heads. Answer (or question) grounding   performance is evaluated in terms of the following: Overlap\u2013 overlap between   the ground truth bounding box for answer object and the detected attention   region is reported in terms of precision (P), recall (R), and F1-score (F1); IOU\u2013   intersection over union (IOU) between the ground truth object and detected   region is reported in terms of P, R, and F1-score. Pointing Game\u2013 proposed   by [66] is a metric for weakly-supervised visual grounding methods. For pointing   game, we consider the point detected from each head as a part of distribution,   and perform k-means clustering (k=1) on those points. The cluster center is   considered as the detected point from the system and used for evaluating accuracy.   For VQA-HAT, we report mean rank correlation between system generated   attention and human attention maps to compare with previous methods. Mean   rank correlation is an order-based metric for finding the degree of association   between two variables based on their ranks and thus is invariant to absolute   spatial probability values [8].   4.3   Implementation details.   We use L = 5 layers in both text and image encoders, and 2 layers in crossattention module. The transformer encoder layers have the same configuration as   BERT with 12 heads and feature dimension d = 768. A batch size of 1024 with   learning rate lr = 1e\u22124 is used for pretraining. First stage pretraining is done for   20 epochs and further trained for 10-15 epochs during the second stage. We use   Imagenet pre-trained ResNet model to extract features of dimensions 7\u00d77\u00d72048.   For finetuning on GQA, we use batch size=32 with lr = 1e \u22125 and 5-10 training   epochs. For VQA-HAT, we use batch size=64 with lr = 4e \u22125 trained for 20   epochs. To evaluate the grounding results, we follow [4] and consider the last   cross-attentional layer\u2019s output for the attention map. To compute overlap and   IOU for GQA, we threshold over the attention map with an attention threshold of   0.5 to get high attention regions. Each connected region is considered a detection.   For pointing game, we find the single cluster center over maximum attention   points from all heads and use it for evaluation. We ignore the test samples with   empty ground truth maps for pointing game since there is no ground truth   bounding box to check for a hit or a miss. For the VQA-HAT evaluation, we   follow [8] and use mean rank correlation between the generated attention maps   and the ground truth.   4.4   Comparison to State-of-the-Art   We compare the performance of our method to other best-performing methods   in the field of weakly supervised VQA grounding and VQA in general, namely   MAC [23] and MAC-Caps [25] as representation of visual reasoning architectures,   10   A. Urooj et al.   Method   Layer Pointing Game Acc.   Random   18.80   Center   33.30   MAC [23]   mean   8.90   MAC-Caps [25]   mean   28.46   LXMERT [57]   last   29.00   ALBEF [33]-GC   last   32.13   ALBEF [33]-ATN   last   32.11   ViLT [28]   last   11.99   Ours-no-init (C=16) last   34.59   Ours-no-init (C=32) last   34.43   Ours-nogqa (C=32)   last   37.04   Table 1: Pointing game accuracy for GQA. For MAC and MAC-Caps, mean attention   maps over reasoning steps are used. For transformer-based methods, maximum attention   points from all heads are used for clustering. The cluster center is then used for the   pointing game evaluation. For ALBEF, GC=GradCAM output, ATN=attention output.   Ours-no-init is the full model trained from scratch (no initialization from BERT or ViT),   Ours-nogqa uses no GQA samples at pretraining stage. Numbers are in percentages.   and LXMERT [57], ViLT [28], and ALBEF [33] as state-of-the-art transformer   architectures without object features.   For LXMERT, we take the provided backbone pre-trained on object features   and finetune it using image patches of size 32 \u00d7 32 \u00d7 3 on GQA. In case of ViLT,   we use the provided pre-trained backbone and finetune it on GQA. Following   ViLT, we generate a heat map using the cosine similarity between image and   word tokens evaluating the similarity scores as well as raw attention for grounding   performance for all three metrics. For ALBEF we report results on the last layer   as well as on layer 8 which is specialized for grounding [33] using the visualizations   from gradcam as well as raw attention maps.   GQA We first look at the results of our evaluation on GQA, considering pointing   game accuracy in Table 1 and for overlap and IOU in Table 2. Our method   outperforms both MAC and MAC-Caps for answer grounding on the last attention   map. We achieve an absolute gain of 16.47% (overlap F1-score) and 2.67% increase   in IOU F1-score, and an improvement of 25.69% in pointing game accuracy   for MAC. When compared with MAC-Caps, our best method (C=16, no-init)   improves overlap F1-score by 15.71% \u2191, IOU F1-score by 2.32% \u2191, and pointing   game accuracy by 6.13% \u2191. Similar performance gain is observed for question   grounding with an improvement of 38.2% \u2191for overlap F1-score and 3.67% \u2191   gain for IOU F1-score.   To evaluate LXMERT finetuned on image patches (LXMERT-patches), we   take the attention score maps from the last cross modality layer. We improve   over LXMERT by 12.01% \u2191absolute points w.r.t overlap F1-score, 2.23% \u2191   w.r.t. IOU F1-score and 5.43% \u2191gain in pointing game accuracy. For question   grounding, LXMERT achieves an overlap F1-score: 43.08% (vs. ours 59.69%) and   IOU F1-score of 4.62% (vs. ours 5.96%).   Weakly Supervised Grounding for VQA in Vision-Language Transformers   11   Overlap   IOU   Method   Obj.   Backbone   Pre-training   Layer   Acc.   P   R   F1   P   R   F1   MAC [23]   A   ResNet   last   57.09 5.05 24.44 8.37   0.76 3.70 1.27   MAC-Caps [25]   A   ResNet   last   55.13 5.46   27.9   9.13   0.97 4.94 1.62   LXMERT-patches   A   Faster RCNN MSCO,VG   last   48.65 7.13 64.21 12.83 0.95 8.66 1.71   ALBEF [33]-GC   A   ViT+BERT   last   64.16 6.94 99.92 12.98 0.89 13.43 1.67   ALBEF [33]-ATN   A   ViT+BERT MSCO,VG,   last   64.20 5.13 99.92 9.75   0.64 12.98 1.21   ALBEF [33]-GC   A   ViT+BERT   SBU,GCC   8   64.20 4.41 99.92 8.44   0.54 12.85 1.04   ALBEF [33]-ATN   A   ViT+BERT   8   64.20 4.82 99.92 9.19   0.60 12.88 1.14   ViLT [29]   A   ViT   MSCO,VG, last-cos 66.33 0.34   6.13   0.65   0.04 0.63 0.07   ViLT [29]   A   ViT   SBU,GCC last-ATN 66.33 0.28   4.10   0.53   0.08 1.20 0.15   Ours (C=16)   A   ResNet   MSCO,VG   last   57.21 14.53 85.47 24.84 2.30 13.61 3.94   MAC [23]   Q   ResNet   last   57.09 10.79 16.38 13.01 1.39 2.09 1.67   MAC-Caps [25]   Q   ResNet   last   55.13 17.39 28.10 21.49 1.87 2.96 2.29   LXMERT-patches   Q   Faster RCNN MSCO,VG   last   48.65 32.46 64.02 43.08 3.48 6.87 4.62   ALBEF [33]-GC   Q   ViT+BERT   last   64.20 22.15 99.90 36.26 1.96 9.22 3.24   ALBEF [33]-ATN   Q   ViT+BERT MSCO,VG,   last   64.20 16.50 99.90 28.33 1.40 8.90 2.43   ALBEF [33]-GC   Q   ViT+BERT   SBU,GCC   8   64.20 14.21 99.90 24.88 1.19 8.71 2.09   ALBEF [33]-ATN   Q   ViT+BERT   8   64.20 15.51 99.90 26.85 1.31 8.77 2.27   ViLT [29]   Q   ViT   MSCO,VG, last-cos 66.33 1.02   5.64   1.73   0.10 0.54 0.17   ViLT [29]   Q   ViT   SBU,GCC last-ATN 66.33 0.34   1.56   0.56   0.08 0.38 0.14   Ours (C=16)   Q   ResNet   MSCO,VG   last   57.21 47.03 81.67 59.69 4.72 8.08 5.96   Table 2: Results on GQA validation set (for last layer). All methods are evaluated   for weak VQA grounding task. For transformer-based models, attention was averaged   over all heads. Results are based on grounding of objects referenced in the answer (A)   and the question (Q). C=no.of capsules, we report results from our best model with   C=16. Refer to table 4 for more variants. For ViLT, we obtain results using cosine   similarity (cos.) between text and image features as proposed by the authors as well   as from raw attention scores (ATN). For ALBEF, GC is the gradcam output used for   evaluation, ATN is the attention output. ALBEF uses layer 8 as grounding layer, we   also report grounding performance on this layer. Our method outperforms all baselines   for overlap F1-score and IOU F1-score. See section 4.4 for more details. Numbers are in   percentages.   ViLT outperforms all methods in terms of VQA accuracy of 66.33%. However,   on the grounding task, it demonstrates the lowest performance for all metrics   (table 1: row 7, table 2: rows 8-9). Similar behavior is observed for the question   grounding task.   ALBEF produces visualization using GradCAM. We compare with ALBEF   using both GradCAM output and attention maps. ALBEF has a very high recall   (R) both in terms of overlap and IOU. However, it lacks in precision (P) leading   to lower F1-scores for both metrics. Our best model outperforms ALBEF-VQA   by a significant margin on both answer grounding and question grounding.   VQA-HAT We further evaluate our system on the VQA-HAT dataset. To this   end, we follow the protocol of VQA-HAT and resize the human attention maps   and the output attention maps from our system to the common resolution of   14x14. We then rank both of them. VQA-HAT val set provides three human   attention maps for each question. We compute the rank correlation of generated   attention map with each human attention map and take the average score. Mean   rank correlation score over all QA pairs is reported.   12   A. Urooj et al.   Method   Mean Rank-Correlation   Random   0.000 \u00b1 0.001   Human   0.623 \u00b1 0.003   Unsupervised   SAN [63]   0.249 \u00b1 0.004   HieCoAtt [41]   0.264 \u00b1 0.004   Ours (C=16)   0.479 \u00b1 0.0001   Supervised   HAN [48]   0.668 \u00b1 0.001   Table 3: Results on VQA-HAT val dataset. Unsupervised: no attention supervision,   Supervised: use attention refinement.   Fig. 3: Success cases for VQA-HAT dataset. VQA-HAT provides 3 human   attention maps for each image. Here, we show the best matched ground truth   map (GT HAT).   We compare our approach on VQA-HAT with three different baselines:   SAN [63] and HieCoAtt [41] as unsupervised bounding box free systems, and   HAN [48] which uses attention supervision during training. The evaluation is   shown in table 3. It shows that the proposed system is able to significantly   outperform both methods using VQA-only supervision.   Without any attention supervision during training, we are able to narrow   the gap between unsupervised methods and methods such as HAN, which use   human ground truth attention maps during training. Figure 3 shows success cases   on VQA-HAT, comparing our generated attention result to the closest human   attention map.   4.5   Ablations and Analyses   Impact of Residual Connections. We compare our full system with an   ablated variant without residual connections. We observe a drop in performance   Weakly Supervised Grounding for VQA in Vision-Language Transformers   13   Overlap   IOU   Pointing   Method   Acc.   P   R   F1   P   R   F1   Game   (1) no skip(C=32)   56.83 | 11.06 77.60 19.37 | 1.39   9.85   2.43   29.81   (2) w/skip (C=32)   55.41 | 10.09 71.95 17.70 | 1.41 10.09 2.47   34.43   (3) w/skip (C=16)   57.21 | 14.53 85.47 24.84 | 2.30 13.61 3.94   34.59   (4) w/skip (C=24)   56.26 | 10.90 74.03 19.00 | 1.54 10.56 2.69   31.08   (5) w/skip (C=32)   55.41 | 10.09 71.95 17.70 | 1.41 10.09 2.47   34.43   (6) w/skip (C=48)   53.65 | 10.28 68.94 17.89 | 1.59 10.73 2.78   29.70   (7) no-init (C=32)   55.41 | 10.09 71.95 17.70 | 1.41 10.09 2.47   34.43   (8) vit-bert init (C=32) 58.86 | 11.11 74.67 19.34 | 1.55 10.44 2.69   27.06   Table 4: Ablations over the design choices for the proposed architecture on GQA val   set. Average attention over all heads in the last transformer layer is used to evaluate   the grounding performance. We perform ablation study with C=32 caspules except   rows 3-6 where we train the proposed architecture with varying number of capsules.   Ablation (1) no skip is our system without residual connections, (2) w/skip is the full   model. Results for the final design choices are shown in bold.   in terms of overlap, but a slight increase in terms of IOU. Without residual   connections, pointing game accuracy is lower than with residual connections   (4.62% \u2193in table 4). We conclude that using residual connections is beneficial for   pointing game.   Number of capsules. We ablate our system with varying number of capsules.   We train the proposed system with C=16, 24, 32, and 48 capsules. We observe   that increase in number of capsules not only decreases VQA accuracy, but also   hurts the overlap and IOU in terms of precision, recall and F1-score. Our best   method uses 16 capsules with residual connections and pre-trained from scratch.   ViT + BERT + Ours. ViLT and ALBEF initialize their image and text   encoders from ViT and/or BERT weights. Although our model is shallower than   both models (5 layers in modality specific encoders compared to 12 layers in   ViLT and ALBEF), we experimented to initialize our text encoder with BERT   weights and image encoder with ViT weights from last 5 layers. We find a gain   in VQA accuracy (58.86% vs. 57.21%) but it get less grounding performance.   4.6   Qualitative Analysis   In figure 4, for all examples including the ones where our system mispredicted the   answer, the grounding attention was correct (row 1,4 and 5). Also, the answers   are plausible. For instance, in row 3, the correct answer is \u2019aircraft\u2019, and our   method predicted it as \u2019airplane\u2019 with the correct localization. Overall, we notice   that compared to our method, the baselines were either attending most of the   image (ALBEF in rows 1,3, and 5 which explains the high recall in table 2), or   generate small attention maps (MAC-Caps, ViLT) or look at the wrong part of   the image (LXMERT). More examples and analysis are in the supplementary   material.   14   A. Urooj et al.   Fig. 4: Qualitative comparison: each row shows the input example, and the last   layer\u2019s attention visualizations (averaged over all heads) with the predicted answer   from all methods. Column 1 shows the question and ground truth answer, column 2 is   the input image, column 3 shows the attention (grounding) output from our method,   column 4-7 are results from the baselines. Blue box is the ground truth bounding box   for the answer object, orange boxes are the detected regions from each system. We can   see that ours is attending relevant answer object with the plausible predicted answer   even when the prediction mismatches with the ground truth answer (row 3-5). In row   4, the question is vague; therefore we can say, except LXMERT, all methods choose   the correct answer. ALBEF has attention spread over all image which explains the   high recall it achieves for overlap and IOU. Refer to section 4.6 for more details and   discussion. Best viewed in color.   5   Conclusion   In this work, we show the trade-off between VQA accuracy and the grounding   abilities of the existing SOTA transformer-based methods. We use text-guided   capsule representation in combination with transformer encoder layers. Our results demonstrate significant improvement over all baselines for all grounding   metrics. Extensive experiments demonstrate the effectiveness of the proposed   system over the baselines.   Acknowledgements. Aisha Urooj is supported by the ARO grant W911NF-19-1-0356. The U.S.   Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein   are those of the authors and should not be interpreted as necessarily representing the official policies   or endorsements, either expressed or implied, of ARO, IARPA, DOI/IBC, or the U.S. Government.   Weakly Supervised Grounding for VQA in Vision-Language Transformers   15", "conf": "ECCV", "year": "2022", "index": 152}, {"title": "Improving Visual Grounding by Encouraging Consistent Gradient-based   Explanations   Ziyan Yang   Rice University   zy47@rice.edu   Kushal Kafle   Adobe Research   kkafle@adobe.com   Franck Dernoncourt   Adobe Research   dernonco@adobe.com   Vicente Ordonez   Rice University   vicenteor@rice.edu", "abstract": "We propose a margin-based loss for tuning joint visionlanguage models so that their gradient-based explanations   are consistent with region-level annotations provided by   humans for relatively smaller grounding datasets. We refer to this objective as Attention Mask Consistency (AMC)   and demonstrate that it produces superior visual grounding results than previous methods that rely on using visionlanguage models to score the outputs of object detectors.   Particularly, a model trained with AMC on top of standard   vision-language modeling objectives obtains a state-of-theart accuracy of 86.49% in the Flickr30k visual grounding   benchmark, an absolute improvement of 5.38% when compared to the best previous model trained under the same   level of supervision. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension where it obtains 80.34% accuracy in   the easy test of RefCOCO+, and 64.55% in the difficult split.   AMC is effective, easy to implement, and is general as it can   be adopted by any vision-language model, and can use any   type of region annotations.   1.", "content": "Vision-language pretraining using images paired with   captions has led to models that can transfer well to an array of tasks such as visual question answering, image-text   retrieval and visual commonsense reasoning [6,18,22]. Remarkably, some of these models are also able to perform visual grounding by relying on gradient-based explanations.   While Vision-Language Models (VLMs) take advantage of   the vast amounts of images and text that can be found on   the web, carefully curated data with grounding annotations   in the form of boxes, regions, or segments is considerably more limited. Our work aims to improve the grounding or localization capabilities of vision-language models   further by tuning them under a training objective that encourages their gradient-based explanations to be consistent   with human-provided region-based annotations from visually grounded data when those are available.   A picture of a cathedral next to a park   A picture of a cathedral next to a park   A picture of a cathedral next to a park   Input Image + Text   Regular V-L Model Explanation   Human Explanation   w/ Attention Mask Consistency (Ours)   A picture of a cathedral next to a park   Figure 1. Gradient-based methods can generate heatmaps that explain the match between images and text for a Vision-language   model (VLM). Our work aims to improve their ability to produce   visual groundings by directly optimizing their gradient-based explanations so that they are consistent with human annotations provided for a reduced set of images.   Vision-language transformers extend the success of   masked language modeling (MLM) to multi-modal problems.   In vision-language transformers, objectives such   as image-text matching (ITM), and image-text contrastive   losses (ITC) are used in addition to MLM to exploit commonalities between images and text [6,17,18,22]. We further extend these objectives to include our proposed Attention Mask Consistency (AMC) objective. Our formulation   is based on the observation that gradient-based explanation   maps obtained using methods such as GradCAM [30], can   be used to explain the image-text matching of a VLM. Our   AMC objective explicitly optimizes these explanations during training so that they are consistent with region annotations. Figure 1 illustrates an example input image and text   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   19165   an empty street   GradCAM (\u2207y)   \u03d5v   \u03d5t   y   \u03d5f   M :   \u2112amc   V :   T :   {   }{   }   maximize   minimize   Figure 2. Overview of our method. Among other objectives, standard vision-language models are trained to produce a matching score   y given an input image-text pair (V, T). For inputs containing an extra level of supervision in the form of region annotations (e.g. a   triplet (V, T, M)), where M is a binary mask indicating the regions annotated by a human, we optimize the GradCAM [30] gradient-based   explanations of the model so that the produced explanations are consistent with region annotations using Lamc by maximizing the energy   in the heatmap that falls inside the region annotation and minimizing what falls outside. We accomplish this through soft margin losses as   described in Sec. 3.2.   pair along with a gradient-based explanation obtained from   a VLM model, a region annotation provided by a human,   and an improved gradient-based explanation after the VLM   model was tuned under our proposed objective.   Our   work   builds   particularly   upon   the   ALBEF   model [17] which incorporates a vision-language model   architecture based on transformers [36] and has already   demonstrated off-the-shelf grounding capabilities using   GradCAM. Gradient-based explanations in the form of   heatmaps have been used extensively to explain the areas   of the input images that most impact an output value of   a model.   In our formulation we actively leverage these   heatmaps by designing a loss function that encourages most   of the energy in the heatmaps to fall within the areas of the   input image that most align with human provided region   annotations.   Figure 2 shows a detailed overview of our   method and objective function. Given an input image and   text pair, our goal is to maximize a soft margin between   the energy of the heatmap inside the region annotation and   the energy of the heatmap outside the region annotation.   A soft-margin is important since typical human region   annotations in the form of boxes do not exactly outline   objects of different shapes, and in many cases models   should still be able to ground an input text with multiple   regions across the image.   We compare AMC extensively against other methods   that use the same level of supervision but instead use an   object detector such as Faster-RCNN [10, 11, 17, 23]. Our   method obtains state-of-the-art pointing game accuracy on   both Flickr30k and RefCOCO+. Our contributions can be   summarized as follows: (1) We introduce a new training   objective, AMC, which is effective, simple to implement   and can handle multiple types of region annotations, (2) We   show that AMC can improve the grounding capabilities of   an existing vision-language model \u2013 ALBEF, and (3) the   resulting model is state-of-the-art in two benchmarks for   phrase grounding and referring expression comprehension.   2. Related Work   Vision-Language Representation Learning.   Followed   by the success of pretraining methods in NLP such as   BERT [8], many transformer-based image-text models have   been proposed to leverage benefits of pretraining on largescale unlabeled image-text pairs [13, 18, 20]. While earlier   pretraining methods rely on an object detector to divide an   image into input tokens, some recent works, such as ALBEF [17], use an end-to-end vision transformer [9]. These   pretrained models can then be finetuned to obtain impressive performance in a wide variety of vision-language tasks,   such as image-text retrieval, visual question answering, and   visual commonsense reasoning. While these models can   perform some visual grounding by running them on the outputs of an object detector or using gradient-based explanations, they are not trained to take advantage of grounded   data. Our AMC objective provides this additional capability by leveraging gradient-based explanations that can be   easily obtained for a large variety deep learning models.   Gradient-based Localization.   Localizing the most discriminative areas of an image for a given task has been   widely used as a tool to provide visual explanation about a   model. Class activation maps (CAM) [41] were proposed to   provide weighted feature maps for any networks with minimal modifications to the model. Gradient-weighted Class   Activation Mapping (GradCAM) [30] improves CAM by   directly using gradients to obtain weighted feature maps   19166   without the need for model modifications or retraining;   The attention maps generated by these methods can be directly optimize to guide the model toward solutions that   are more consistent with human-based annotations.   Our   proposed method is also based on GradCAM heatmaps.   However, we use GradCAM during training to guide the   generated heatmaps to achieve better consistency with   known region and phrases that describe them. Recently,   Pham et al [26] explored a similar idea by using segmentation masks to guide attention maps to focus on significant image regions for an attribute prediction task. Selvaraju et al [31] use saliency maps generated using DeepUSPS [24] at training time to guide attention maps in order   to improve self-supervised representation learning. Similarly Pillai et al [27] rely on consistent explanations for   generic representation learning using contrastive objectives.   Our goal in using supervision on top of gradient-based   heatmaps is to directly leverage these heatmaps to evaluate   on visual grounding.   Visual Grounding Methods. Visual Grounding is a task   that requires a model to select the region of an image described by a phrase. Several methods have been proposed   to ground phrases to regions of an image, typically a bounding box [10, 11, 17, 23]. Visual grounding has also been   used to improve performance on downstream tasks such as   VQA [32]. These methods take advantage of object detectors which can provide high quality locations. The recently   proposed GLIP model [19] incorporates an object detection model as part of its grounding objective, effectively   combining vision-language pretraining with bounding box   localization. Our work instead of outputting a box, optimizes its own gradient-based model explanations. Since our   model does not output bounding boxes but heatmaps as an   output it can generate more general groundings for phrases   or objects that can not be mapped to a box such as stuff categories or", "conf": "CVPR", "year": "2023", "index": 371}, {"title": "Multi-Sentence Grounding for Long-term   Instructional Video   Zeqian Li1\u22c6, Qirui Chen1\u22c6, Tengda Han3 , Ya Zhang1,2 ,   Yanfeng Wang1,2 , and Weidi Xie1,2,3   1 Coop. Medianet Innovation Center, Shanghai Jiao Tong University   2 Shanghai AI Laboratory, China   3 Visual Geometry Group, University of Oxford   https://lzq5.github.io/Video-Text-Alignment/", "abstract": ". In this paper, we aim to establish an automatic, scalable   pipeline for denoising the large-scale instructional dataset and construct a   high-quality video-text dataset with multiple descriptive steps supervision,   named HowToStep. We make the following contributions: (i) improving   the quality of sentences in dataset by upgrading ASR systems to reduce   errors from speech recognition and prompting a large language model   to transform noisy ASR transcripts into descriptive steps; (ii) proposing   a Transformer-based architecture with all texts as queries, iteratively   attending to the visual features, to temporally align the generated steps   to corresponding video segments. To measure the quality of our curated   datasets, we train models for the task of multi-sentence grounding on   it, i.e., given a long-form video, and associated multiple sentences, to   determine their corresponding timestamps in the video simultaneously,   as a result, the model shows superior performance on a series of multisentence grounding tasks, surpassing existing state-of-the-art methods   by a significant margin on three public benchmarks, namely, 9.0% on   HT-Step, 5.1% on HTM-Align and 1.9% on CrossTask. All codes, models,   and the resulting dataset have been publicly released.   Keywords: Instructional Video Understanding \u00b7 Video-Text Dataset \u00b7   Multi-Sentence Grounding   1", "content": "The research on visual-language representation learning has recently made great   progress. It is primarily driven by contrastive learning on image-caption pairs   crawled from the Internet at scale [20, 21, 34], and has shown remarkable performance on zero-shot image classification. However, in videos, the time dimension adds extra complexity that requires temporally corresponding captions/descriptions, posing challenges to learning fine-grained representation for   video understanding tasks, such as temporal action localization, visual-language   grounding, and grounded visual question answering.   \u22c6These authors contributed equally to this work.   2   Z. Li et al.   0:10   0:15   0:30   0:32   0:54   1:00   1:06   1:15   1:39   1:42   1:52   1:56   ASR Transcripts   1.   [11s-17s]: add extra virgin olive oil and garlic    [\uf050 well-aligned]    2.   [36s-40s]: thing sauce nice quick sauce [\uf04f   recognition errors]   3.   [50s-56s]: we\u2018ll begin slicing our eggplant \u2026    [\uf04fnot well-aligned (early)]   4.   [63s-65s]: eggplant is full of phytochemicals    [\uf04fnon-alignable]    5.   [71s-74s]: put the oil on eggplant [\uf04fnot wellaligned (delay)]   6.   [75s-78s]: we\u2018re going to place it in the    broiler [\uf04fambiguity]   HowToStep   1.   [10s-17s]: Add tablespoons of extra    virgin olive oil.   2.   [30s-32s]: Add basil leaves.   3.   [53s-61s]: Slice the eggplant into a    quarter of an inch.   4.   [66s-75s]: Coat the eggplant slices with    oil.    5.   [99s-102s]: Cook whole wheat penne    pasta    6.   [112s-116s]: Sprinkle cheese on top of    the pasta.   Wikihow Steps   1.   [None]: Cut the eggplant into 1/2inch thick slices and sprinkle with    salt [\uf050alignable]   2.   [None]: Heat oil in a large pan over    medium-high heat [\uf050alignable]   3.   [None]: Add eggplant and salt and    pepper to saucepan and cook,    stirring \u2026 [\uf04fnon-alignable]   4.   [None]: Pour in chicken broth and    thyme [\uf04fnon-alignable]   5.   [None]: Remove from heat [\uf04fnonalignable]   1   2   3   4   5   6   1   3   5   Fig. 1: A comparison of the proposed HowToStep with annotations on HowTo100M.   Our dataset consists of multiple descriptive steps, with the corresponding temporal   windows. Compared to existing training data derived from ASR transcripts [17] and taskrelated articles of Wikihow [2,10,28], HowToStep data offers the following advantages:   1) Descriptive: clearly describes the procedural action steps in the instructional video;   2) Concise: all sentences can be grounded in the video, without redundancy or noises;   3) Temporally well-aligned: offers precise temporal boundaries for procedural steps.   Instructional videos, e.g., HowTo100M [30], have been widely used for learning   video representations [17,23,28,29], with the textual narrations acquired from   the Automatic Speech Recognition (ASR) system. However, unlike manually   annotated captions, training models with these ASR transcripts naturally incur   three issues: (i) off-the-shelf ASR systems may introduce recognition errors; (ii)   spoken narrations are generally not of descriptive style, thus contain redundancy   or ambiguity, e.g., talking about a specific ingredient or using ambiguous pronouns   (it, them, etc.); (iii) transcripts may not be well aligned with the visual signals,   e.g., greetings from the speaker, associating with inaccurate timestamps, or   describing the action before/after performing it. According to the statistics   from [17], only 30% of the narrations are visually alignable, and only 15% are   naturally well-aligned with correct start/end timestamp, as presented in Fig. 1.   In this paper, we aim to establish an automatic, scalable pipeline for \u2018denoising\u2019   the large-scale instructional dataset, and contribute a \u2018cleaned\u2019 video-text dataset,   termed as HowToStep. Specifically, our curation procedure involves the following   aspects, first, to improve the quality of text descriptions, we replace the original   YouTube transcripts with the ones generated from WhisperX [3], and prompt   the large language models (LLMs) to transform noisy ASR transcripts into   coherent, descriptive steps that are closely related to the video content; second, to   temporally align the texts to the corresponding video timestamp, we adopt a twostage determination procedure, i.e., approximate estimation based on timestamps   from ASR transcripts, followed by training a lightweight Transformer-based   Multi-Sentence Grounding for Long-term Instructional Video   3   model for further refinement, termed as Narrations / Steps to Video Aligner   (NaSVA), where we use multiple sentences as queries, to iteratively attend the   video features, and output the alignability or optimal temporal windows.   As a result, to demonstrate the effectiveness of our dataset curation procedure, we evaluate the model trained on such dataset, namely NaSVA, on   multi-sentence grounding tasks - narrations alignment [17] and procedural steps   grounding [28], both aiming to localize the corresponding temporal segments for   multiple sentences in a video. Contrary to the existing text-to-video grounding   tasks that focus on only a single sentence at a time, for example, temporal   action localization [15], moment retrieval [19] and natural language queries [22],   multi-sentence grounding necessitates the understanding of long instructional   videos, with finer-grained events or actions, while managing multiple interrelated   text queries simultaneously. Our model sets state-of-the-art performance on   both procedural steps grounding and narrations alignment across three public   benchmarks, surpassing existing models by a large margin, specifically, 9.0% on   HT-Step [28], 5.1% on HTM-Align [17] and 1.9% on CrossTask [50].   2   Related Work   Large-Scale Video-Text Datasets. Multi-modal video datasets are crucial   for video understanding tasks. Conventional video-text datasets [7,49] are often   manually labelled, suffer from short video lengths, limited scale, and coarse   label granularity, and prevent the model from learning a generalized video   representation. In the recent literature, scalability becomes an essential factor for   constructing datasets, Youtube-8M [1] collects YouTube videos with metadata   provided by the users, Instagram65M [16] uses the associated hashtags as labels   to supply weak supervision for training. In order to get descriptive sentences with   richer semantics, instructional videos are collected at scale, as they naturally   come with dense narrations, obtained from ASR systems, by far, the largest   video-text dataset is HowTo100M [30]. As an alternative, [4] comprises over two   million videos with weak captions scraped from the internet, while the captions   are manually generated, they are not temporally aligned, and thus are insufficient   for learning fine-grained temporal representation. In this work, we transform   the noisy ASR transcripts into descriptive procedural steps and propose to   train a model to improve the video-text correspondence for instructional videos,   mitigating the flaws of the HowTo100M dataset for visual representation learning.   Language Grounding in Videos. Early efforts in language grounding predominantly concentrate on single sentence grounding tasks such as temporal   action localization [15], moment retrieval [19, 24, 47], and natural language   queries [22, 37], which lead to unprecedented progress in recent years. Given   a temporally untrimmed video and a natural language query, the goal of singlesentence grounding is to determine the start and end times for the described   content in the video. However, due to the limitation of query lengths to single   sentences, the trained models lack an understanding of textual context. Unlike single-sentence grounding, multi-sentence grounding involves simultaneously   4   Z. Li et al.   grounding multiple sentences from different semantic scales within the video. The   model needs to determine the intervals corresponding to each sentence based   on the correlation between multiple sentences, significantly increasing the task\u2019s   complexity. This also enables the model to learn enhanced video-text representations. Initially, [5,14,50] tries to delineate the video segments corresponding to   an action list. Instead of an action list, scripts describing a series of events in the   video are given for transcript alignment [11,33]. The availability of large-scale   video-text datasets such as HowTo100M has prompted many works on joint videotext embedding training. Specifically, TAN [17] investigated directly aligning   contextualized narration representations generated from ASR transcripts to video   segments. Given that the ASR can be rather noisy, DistantSup [23] proposes   using distant supervision from a textual knowledge base, namely Wikihow [18]   to denoise narrations from ASR. More recently, instead of aligning narrations,   VINA [28] proposes to ground procedural steps sourced from instructional articles,   however, as the order of instructions from Wikihow does not necessarily follow   those in the video, grounding procedural steps is thus more challenging. In this   paper, our improved HowToStep dataset enables to train models that tackle the   two problems simultaneously, namely, narration alignment [17] and procedural   step grounding [28], while previous studies have only focused on one aspect.   Dataset Curation with Large Language Models. In the recent literature,   large language models (LLMs) such as GPT [6, 31, 36] and Alpaca [40] have   achieved great success in natural language processing. Constructing multi-modal   datasets while generating pseudo-labels using LLMs becomes an efficient way   to exploit the common sense knowledge in LLMs, and save human efforts for   annotations [25,39]. For instance, VQA-T [45] generates question-answer pairs   for instructional video ASR transcripts by LLMs, which are then coupled with   the related video. The VQA model pre-trained on the generated dataset exhibits   enhanced generalization capabilities in various downstream tasks. Similarly, LLMs   are adopted to automatically derive question-answer pairs at scale for images   or videos, by leveraging the existing image/video captions [8,12]. In addition to   generating a dataset directly, some works use the LLMs to create pseudo-labels   for large-scale video data that are later used for multi-modal vision tasks. For   example, LAVILA [48] first trains a video captioning model on egocentric videos,   and proposes to rephrase the densely generated captions from the model. After   finetuning, the model demonstrates improved performance. As a concurrent   work, [38] adopts LLMs to transform the ASR transcripts of instructional videos,   however, there exists one crucial difference, we transform the transcripts into   concise steps, rather than dense captions as in their work.   3   Method   In this section, our goal is to establish an automatic, scalable pipeline for \u2018denoising\u2019 the large-scale instructional dataset, and contribute a textually descriptive,   and temporally-aligned video-text dataset, which is termed as HowToStep. As   presented in Fig. 2, the entire pipeline can be divided into three parts: (i) we   Multi-Sentence Grounding for Long-term Instructional Video   5   leverage LLM to transform narrations from the ASR transcript into descriptive   procedural steps; (ii) we use the similarity between the original transcript and   generated steps to approximate the start/end timestamp for each step; (iii) we   train a lightweight multi-sentence grounding model on the generated steps with   the approximated timestamp, and then use the trained model to refine the time   range for each generated step (i.e., self-training).   In the following sections, we start with improving the quality of texts by   LLMs in Sec. 3.1. Then, we introduce the multi-sentence grounding network   that enables to align texts to their corresponding video segments in Sec. 3.2.   Afterward, we describe the self-training procedure to refine the temporal ranges   for the generated steps in Sec. 3.3. Lastly, we discuss two tasks that involve   multi-sentence grounding, that can be addressed by our model, and reflect the   quality of our curated dataset, namely, narration alignment and procedural step   localization in Sec. 3.4.   3.1   Noisy ASR Transcripts \u2192Descriptive Procedural Steps   To improve the quality of texts of the ASR transcripts in HowTo100M, we first   replace the original Youtube ASR transcripts with a recent WhisperX [3] as the   speech recognition tool, which can potentially resolve errors in recognition or   punctuation. However, some complex cases such as ambiguous pronouns and   grammatical mistakes can not simply addressed by upgrading the ASR system.   Therefore, we further propose to exploit the strong reasoning ability in large   language models (LLMs), to summarize the procedural action steps from the   noisy ASR transcripts, converting them to descriptive textual explanations for   instructional videos. The details are as follows:   Prompting LLM to Summarize Procedural Steps. We start by splitting   the complete transcript for each video into M segments, each segment Gi consists   of around 10 sentences. Next, we prompt a Llama2-7B [41] to summarize the   transcript into concise steps that describe the main actions presented in the video,   as well as filtering the colloquial sentences in the speech. We refer the readers to   the complete prompt in Appendix.   Formally, let \u0398, \u03ba refer to LLM and the prompt texts, and the steps are   generated separately due to the limits of context length of LLM:     \\ math c a l  {S} =  \\{ \\ hat { G}_ 1,  \\do ts , \\hat {G}_M\\}, \\text {\\hspace {4pt}} \\hat {G}_i = \\Theta (G_i;\\kappa ), \\text {\\hspace {3pt}} \\forall i \\in [1, M] \\label {eq:prompt}    where M is the number of separated ASR transcript segments. \u02c6Gi refers to the   summarized procedure steps for each segment Gi. S denotes the complete sequence   of generated steps for one instructional video. As a result, we have transformed   the ASR transcripts of around 370K videos (a subset of HowTo100M selected   by [17]) into approximately 7M descriptive procedural steps. For comparison,   the Wikihow knowledge base only contains 100k procedural steps from 14K   instructional articles, according to the statistics in [28].   Estimating Timestamp For Procedural Steps. Till this point, we propose to   equip the summarised procedural steps with corresponding video timestamps, by   6   Z. Li et al.   \u2022   turn on fire onto low heat over saucepan   \u2022   add extra virgin olive oil and garlic   \u2022   that have been finely chopped and I want to \u2026   \u2022   Now this is tomato puree.   \u2026\u2026   \u2022   I\u2019m going to start pouring my pasta into my    serving bowl   \u2022   I\u2019m going to sprinkle some cheese on top   MSG   trained on Pseudo-labelled Steps     Llama 2   Noisy ASR Transcript   Descriptive Steps   01. Turn on fire onto low heat.   02. Add extra virgin olive and finely chopped garlic.   03. Add tomato puree.   \u2026\u2026   14. Pour the pasta into a serving bowl.   15. Sprinkle cheese on top.    Pseudo-labelled Steps   006s - 010s: Turn on fire onto low heat.   010s - 017s: Add extra virgin olive and finely     chopped garlic.   017s - 020s: Add tomato puree.   \u2026\u2026   100s - 110s: Pour the pasta into a serving bowl.   115s - 120s: Sprinkle cheese on top.    Pseudo-label Generator   chaining \u2018Step\u2192ASR\u2019 and \u2018ASR\u2192Video\u2019   Stage 1  Approximate estimation    Noisy ASR Transcript   Descriptive Steps   Stage 2  Temporal refinement   010s - 018s: Add extra virgin olive and finely   chopped garlic.   017s - 025s: Add tomato puree.   \u2026\u2026   104s - 112s: Pour the pasta into a serving bowl.   112s - 120s: Sprinkle cheese on top.    HowToStep   \ud83e\udd99   Descriptive Steps   Fig. 2: Schematic illustration of the proposed pipeline to summarizing noisy ASR   transcripts into descriptive steps (left), while determining the start-end timestamp   in the video (right). We utilize the Large Language Model (LLM) to summarize the   narrations from ASR transcripts into descriptive steps. Afterwards, we roughly get the   pseudo-label by chaining the \u2018steps\u2192ASR\u2019 similarity and \u2018ASR\u2192video\u2019 timestamp to   train our multi-sentence grounding network NaSVA in Stage 1. Lastly, we use the   trained model to refine the timestamp of the generated steps in Stage 2, resulting in   an extra training source for multi-sentence grounding, named HowToStep.   mapping them back to the original narrations with sentence similarity. Specifically,   we compute the similarity score between S generated steps and N narrations of   the transcript from each video, getting \u2018steps\u2192transcript\u2019 matrix:     \\ m athbb {T}_{\\ m athca l { S}\\ m athcal {N}} = \\text {softmax}(g(\\mathcal {S}) \\cdot g(\\mathcal {N})^T/\\nu , \\text {dim}=1)    where g(\u00b7) is a pre-trained text encoder, \u03bd is temperature, and TSN \u2208RS\u00d7N is   the textual similarity matrix. The generated steps grounding score matrix can be   computed by chaining the \u2018steps\u2192transcript\u2019 matrix TSN and \u2018transcript\u2192video\u2019   matrix YN V (i.e., ASR timestamps).     \\ m ath b b {Y   }_{ \\ mathcal {S}\\mathcal {V}} = \\mathbb {T}_{\\mathcal {S}\\mathcal {N}} \\cdot \\mathbb {Y}_{\\mathcal {N}\\mathcal {V}}, \\quad \\mathbb {Y}_{\\mathcal {S}\\mathcal {V}} \\in \\mathbb {R}^{S \\times T}    We set the timestamp with maximal score as the centre time ck for each step k,     c _k = \\   m   athop    {\\text {argmax}}_t \\mathbb {Y}_{\\mathcal {S}\\mathcal {V}}^{[k, t]}    and then find start-end timestamp (sk, ek) from ck until the alignment score   (Y[k,sk]   SV   , Y[k,ek]   SV   ) are lower than the percentage \u03b6 of the score of the center time (i.e.,   Y[k,ck]   SV   \u00d7 \u03b6) following [28]. The step whose maximal alignment score Y[k,ck]   SV   is   lower than a threshold \u03f51 will be regarded as unalignable and discarded.   Multi-Sentence Grounding for Long-term Instructional Video   7   Transformer   Encoder   Transformer   Decoder   !\ud835\udd38!\u00d7#   \ud835\udc47video frames   \ud835\udc3etexts   \ud835\udc65!   \ud835\udc67\"   sin/cos positional embedding   learnable positional embedding   \ud835\udc65\"   - add salt and pepper flakes   - put lid on   - that\u2019s nice and firm like this   \u2026   Text    Encoder   Video    Encoder   \ud835\udc67!   key-value   Fig. 3: Schematic visualization of the proposed multi-sentence grounding network   termed NaSVA. The visual features are treated as key-value pairs while textual   features as queries, to predict the alignment score matrix \u02c6A between video and texts.   3.2   Multi-Sentence Grounding Model   At this point, the start-end timestamps of the generated steps are obtained directly   from weakly-aligned transcripts, which are naturally inaccurate, here, we propose   a multi-sentence grounding model, termed as Narrations / Steps to Video Aligner   (NaSVA), and a self-training strategy to refine these timestamps. Specifically,   given an untrimmed long-form instructional video (6.7 minutes on average),   X = {V, J }, where V = {V1, V2, . . . , VT } refers to the frames of the video, and   J = {J1, J2, . . . , JK} denotes the K textual sentences associated with the video.   Our goal is to train a temporal multi-sentence grounding network that takes   one video and multiple texts as inputs, and outputs a textual-visual alignment   score matrix (\u02c6A \u2208RK\u00d7T ) with the binary visual alignability (\u02c6y \u2208RK\u00d72) for each   sentence. Formally, it can be denoted as:     \\{ \\ha t  {y}, \\hat {\\math bb {A}} \\} = \\Psi _{\\text {align}}(\\Phi _{\\text {v-enc}}(\\mathcal {V}), \\Phi _{\\text {t-enc}}(\\mathcal {J}))   (1)   In training, the ground truth label YJ V \u2208{0, 1}K\u00d7T takes value Y[k,t]   J V = 1 only   if k-th text depicts the scene of timestamp t in the video, and zero otherwise.   Overall Architecture. As shown in Fig. 3, we adopt a simple Transformer-based   architecture, where visual features of each 1s clip are individually encoded and   treated as key-value pairs, the textual features of narrations or steps are treated   as queries. The queries iteratively attend visual features with cross attention, and   the alignment scores can be computed by textual-visual similarity, which emits   high scores for any alignable sentences at their corresponding video timestamp.   The following sections describe the full details.   Visual-Textual Features. Given an instructional video with K associated texts,   we extract the visual and textual features with pre-trained backbones,     x _v =  f( \\ma   th c al { V }) \\in \\mathbb {R}^{T\\times C}, \\quad x_j = g(\\mathcal {J})\\in \\mathbb {R}^{K\\times C} \\label {eq:backbone}    8   Z. Li et al.   where xv refers to features of a video sequence, and xj denotes the textual features   associated with the video. C is the dimension of the feature vector. Note that,   the resulting feature dimensions depend on the pre-trained backbone.   We consider three popular pre-trained visual-language models, namely, the   S3D-G backbone trained with MIL-NCE [29,44], CLIP-ViT/L-14 trained with   InfoNCE [13,32,34], and InternVideo-MM-L14 trained with masked video reconstruction and multi-modal contrastive loss [42,43]. The ablation studies about   these pre-trained backbones are presented in Sec. 4.4.   Multi-Sentence Grounding Module. As shown in Fig. 3, after extracting the   visual and textual features independently, we project both features into the same   dimension, fuse multimodal information with Transformer, and then predict the   alignment score matrix between video and texts.   (1) Feature Projection. After computing visual-textual features with pretrained frozen models, we adopt one linear layer to project the features into the   embedding with the same dimension D. In terms of positional encoding, we add   sin/cos positional encoding to the visual features:     h _v = \\ p hi    _v ( x_v)+p _ v,  \\ q uad h_j = \\phi _j(x_j)+\\mathbb {I}_{\\mathcal {J}=\\mathcal {N}}\\cdot p_j \\label {eq:pe}    where hv \u2208RT \u00d7D, hj \u2208RK\u00d7D. \u03d5v, \u03d5j refer to different projection heads for the   features of video and sentences. IJ =N is the indicator function which takes value   1 only when input texts J are ordered narrations N, otherwise zero. pv, pj denote   positional encoding for visual and textual features respectively.   (2) Visual-Textual Feature Fusion. The visual features are processed with a   temporal aggregator, followed by a grounding module, expressed as:     o _v = \\Phi _{\\t   ex t  {temp-agg}}(h_v ), \\quad o_j = \\Psi _{\\text {temp-ground}}(o_v, h_j) \\label {eq:transformer}    where ov \u2208RT \u00d7D, oj \u2208RK\u00d7D, \u03a6temp-agg(\u00b7) refers to a temporal aggregator with   three Transformer Encoder layers. \u03a6temp-ground(\u00b7) denotes a temporal grounding   module, consisting of three Transformer Decoder layers, where visual features   act as key-value pairs and textual features act as queries.   (3) Alignment Prediction. To get the alignment score matrix between video   and texts, we project the encoder and decoder outputs into the same dimension,     z _v = \\ v ar phi    _ v (o_v) \\ in \\mathbb {R}^{T \\times d}, \\quad z_j = \\varphi _j(o_j) \\in \\mathbb {R}^{K \\times d} \\label {eq:proj}    and then compute the alignment score matrix:     \\hat {   \\m   a t hb   b       {A}} ^ {[k, t ]} = \\frac {{z_j^k} \\cdot {z_v^t}^T}{\\left \\lVert z_j \\right \\rVert \\cdot \\left \\lVert z_v \\right \\rVert } \\in [0, 1] \\label {eq:similarity}   where \u02c6A \u2208RK\u00d7T is the predicted alignment matrix. The higher value of \u02c6A[k,t]   means the k-th sentence is more likely to align with the scene of timestamp t.   Multi-Sentence Grounding for Long-term Instructional Video   9   3.3   Self-training to Refine Generated Steps Grounding   In this section, we describe the procedure to refine the start-end timestamp   of generated procedural steps, by first training the proposed multi-sentence   grounding model, and use it to update the labels.   Training with Estimated Timestamps of Procedural Steps. Given the   approximately estimated timestamps for the generated steps in Sec. 3.1, denoted   as YJ V \u2208{0, 1}K\u00d7T , we train the multi-sentence grounding network with a   variant of the InfoNCE loss, following [17,28]:     \\ m   a   t   h   cal    {L   }   =  -\\fra   c  {1}{K}\\sum ^K_   {   k =1}\\log \\frac {\\sum _t \\mathbb {Y}_{\\mathcal {J}\\mathcal {V}}^{[k,t]}\\exp (\\hat {\\mathbb {A}}^{[k,t]}/\\tau )}{\\sum _t\\exp (\\hat {\\mathbb {A}}^{[k, t]}/\\tau )} \\label {eq:infonce}    where \u02c6A \u2208RK\u00d7T is the model\u2019s output alignment matrix, as explained in   Sec. 3.2, \u03c4 is a temperature hyper-parameter, and k, t refer to k-th sentence and   t-th timestamp in the video respectively.   Updating the Timestamps of Procedural Steps. We use the trained model   to do inference on the whole set of procedural steps generated by LLM. Specifically,   when feeding video and a set of descriptive steps as model input, the proposed   NaSVA model outputs the alignment matrix (\u02c6A). We take the timestamp of the   maximal alignment score as the start time for each step, and the duration is a   constant \u25b3sec, following [9,38]. Similarly, the generated step with the maximal   alignment score lower than a threshold \u03f52 will be discarded. Finally, we obtain a   dataset consisting of aligned descriptive steps, named HowToStep. In practice,   we observe that although our NaSVA model is trained on noisy pseudo-labels (i.e.,   relying on weakly-aligned transcripts, and getting steps-transcript similarity from   imperfect pre-trained text encoder), the model tends to learn the alignment   patterns from procedural step to videos before overfitting to the noises, as also   being observed in [46]. Note this refined process (i.e., self-training) can be repeated   multiple rounds. To balance computation cost and performance improvement,   we choose only one round as default. More details for generating the dataset are   presented in ablation studies in Sec. 4.4 and Appendix as well.   Relation to Existing Work. We note that recent works [2,10,28] aim to ground   steps collected from the only task-related articles in the external knowledge   base (i.e., Wikihow) within videos from HowTo100M as shown in Fig. 1 (middle),   while we try to ground the procedural steps summarized from the ASR transcript   of the same video. This motivation has two natural advantages: (i) the procedural   steps generated per task are more diverse than those from Wikihow (e.g., 74   different videos are showing how to make pumpkin puree in HowTo100M, while   only one article of the same task is collected in Wikihow); (ii) the generated steps   within the video are more likely to be alignable to the video itself than those   externally sourced Wikihow steps intrinsically.   3.4   Downstream Applications   To demonstrate the effectiveness of the automatic pipeline for curating highquality video-text datasets at scale, we consider using HowToStep for training   10   Z. Li et al.   add salt and pepper flakes   put lid on   that's nice and firm like this   eggplant is full of phytochemicals   put the oil on eggplant   we're going to place it in the broiler   Ground Truth   Video Frames   Alignment Score   add salt and pepper flakes   put lid on   that's nice and firm like this   eggplant is full of phytochemicals   put the oil on eggplant   we're going to place it in the broiler   Make the vanilla sauce.   Roll the pastry.   Sprinkle the brown sugar over the apples.   Make the vanilla sauce.   Prepare your pan and oven   Place the dumplings on the baking tray.   Slowly pour the sauce over the dumplings.   Ground Truth   Video Frames   Alignment Score   Prepare your pan and oven.   Roll the pastry.   Sprinkle the brown sugar over the apples.   Place the dumplings on the baking tray.   Slowly pour the sauce over the dumplings.   Narration Alignment   Procedural Step Grounding   Fig. 4: Qualitative examples of manually annotated visually-aligned text-to-video   alignment matrix Y \u2208{0, 1}K\u00d7T and the learned text-to-video alignment score matrix   \u02c6A \u2208RK\u00d7T of the model output for samples from HTM-Align (left) and HT-Step (right).   The ground truth timestamps of the example on HT-Step are labelled manually. Note   that the temporal density and order of texts are quite different between the two tasks.   and evaluation on multi-sentence grounding tasks. As shown in Fig. 4, we consider   two variants, depending on the type of given texts J \u2208{N, S} (narrations or   procedural steps), and differentiate the two targeted tasks as follows:   Narration Alignment [17]. Narrations (N) are textual sentences transcribed   from the instructor\u2019s speech via the ASR system (around 100 sentences per   video in HTM-Align [17]), which means these sentences maintain a time order   that can be utilized for visual grounding. Note that, some narrations are not   visually alignable or not well-aligned with visual signals temporally as shown   in Fig. 1 (left). However, the ASR transcripts with timestamps can be used   as weakly-aligned labels YN V for training multi-sentence grounding network as   previous work does [17].   Procedural Step Grounding [28,50]. Procedural Steps (S) are collected from   Wikihow [18], an external knowledge base with instructional articles explaining the   procedures for completing certain tasks (around 10 steps per task in HT-Step [28])   or pre-defined action taxonomy (around 5 steps per video in CrossTask [50]).   Compared with narrations, the procedural steps do not necessarily have consistent   temporal order as that happens in the video, and some steps may not even be   present in the video at all.   4   Experiments   Here, we start by describing the datasets for training and evaluation, then present   the implementation details and results for the multi-sentence grounding tasks.   4.1   Datasets and Metrics   Following the previous work [17,28], we perform dataset curation on a subset   of the HowTo100M dataset, and train our NaSVA model with narrations from   Multi-Sentence Grounding for Long-term Instructional Video   11   transcripts and steps from HowToStep. As for evaluation, we conduct procedural   step grounding on HT-Step [28], narration alignment on HTM-Align [17], and   zero-shot action step localization on CrossTask [50].   HTM-370K (Training). The HowTo100M dataset [30] is a large-scale instructional dataset crawled from YouTube, consisting of approximately 1.2M videos   with ASR transcripts. Following previous work [17,28], we use HTM-370K for   training, it contains videos from the Food & Entertaining categories, consisting   of 32% of the videos of the entire HowTo100M dataset.   HowToStep (Training). As introduced in Sec. 3, we construct this dataset   for training, by transforming the original transcripts of HTM-370K into around   4M ordered instructional steps with post-determined start/end timestamps for   almost 340K videos after filtering.   HTM-Align (Evaluation). This benchmark is proposed by [17] for evaluating   narration alignment. It contains 80 videos from the HTM-370K as a holdout   testing set. The authors have manually labelled the alignability for each narration   and further aligned them to the visual signal with start/end timestamps if   alignable. The metric on this dataset is Recall@1 (R@1), which means if the   maximally matched timestamp for each alignable sentence model predicted   falls into the ground truth temporal window, it is regarded as being successfully   recalled. The recall score is computed as the ratio of successfully recalled sentences   to all the alignable sentences.   HT-Step (Evaluation). This benchmark [28] aims to evaluate the procedural   step grounding. It contains manual annotations for 600 videos, specifically, for   each video, the authors first collect activity steps from the related Wikihow   article using the task name, e.g., Make Pumpkin Puree, and then annotate the   temporal segment for steps alignable with the video. The metric on this dataset   is the same as HTM-Align, namely, R@1.   CrossTask (Evaluation). In addition to benchmarks based on HowTo100M,   we also adopt this established instructional video benchmark for zero-shot step   localization. The CrossTask Dataset [50] contains 4800 videos, which can be   divided into 18 primary tasks and 65 related tasks. The videos in the primary   tasks are annotated as steps with temporal segments from a predefined taxonomy.   The metric is Average Recall@1 (Avg. R@1), which measures the recall over steps   in videos for each task and averages the results. Following previous work [28], we   evaluate on a random set of 1850 videos from the primary tasks.   4.2   Implementation Details   Overall, we investigate the effectiveness of three popular pre-trained visuallanguage models for constructing the whole pipeline, namely, the S3D-G, CLIPViT/L-14, and InternVideo-MM-L14, as described in Sec. 3.2. While exploring   other factors, for example, the effect of ASR transcripts, and the effect of   incorporating descriptive steps during training, we use InternVideo-MM-L/14   by default, unless specified otherwise. At training time, the temperature \u03c4 in   12   Z. Li et al.   Table 1: Comparison with the state-of-the-art for narration alignment on HTMAlign, step grounding on HT-Step, and zero-shot action step localization on CrossTask.   The results of TAN* are reproduced by [28]. \u2018ZS\u2019 refers to zero-shot.   Method   HT-Step   HTM-Align   CrossTask (ZS)   \u2191R@1   \u2191R@1   \u2191Avg. R@1   Zhukov [50]   40.5   UniVL [27]   42.0   TAN* [17]   31.2   47.1   VINA [28]   37.4   66.5   44.8   Ours   46.4   71.6   46.7   our loss is 0.07. We use the AdamW [26] optimizer and train the model with   an initial learning rate 10\u22124 and cosine decay for 12 epochs. When determining   the start/end time for generated steps, the hyper-parameters for the 2-stage   are \u03b6 = 0.7, \u03f51 = 0.20, \u03f52 = 0.8, which are also discussed in the ablation study.   We train one unified model for both narration alignment and procedural step   grounding tasks, by setting the texts of one training batch to be either ordered,   dense narrations or shuffled, sparse procedural steps. Complete implementation   details are included in the Appendix.   4.3   Main Results   Comparison with State-of-the-art. We compare our best model with existing state-of-the-art approaches on three public benchmarks for multi-sentence   grounding tasks. As shown in Tab. 1, on the challenging HT-Step task, that aims   to ground unordered procedural steps in videos, our model achieves 46.4% R@1,   leading to an absolute improvement of 9.0%, over the existing state-of-the-art   (37.4%) achieved by VINA [28]. On HTM-Align [17], which aligns narrations in   the video, our method exceeds the SOTA model by 5.1%. On CrossTask [50],   where we need to align video frames and task-specific steps without finetuning, our method outperforms existing the state-of-the-art approach by 1.9%,   demonstrating the effectiveness of the proposed pipeline for downstream tasks.   4.4   Ablation Study   We explore the effects of multiple design choices in the proposed pipeline and   evaluate them on both narration alignment and procedural step grounding tasks.   Effect of Upgrading ASR System. To start with, we compare the original   transcripts scrawled from YouTube, with that from the recent WhisperX [3,35],   as the weakly-aligned labels for training. Qualitatively, we do observe that WhisperX generates fewer punctuation errors, and gives higher accuracy of temporal   boundaries in the ASR transcripts. As shown in Tab. 4, upgrading the ASR system indeed leads to noticeable performance improvement in narration alignment.   Multi-Sentence Grounding for Long-term Instructional Video   13   Table 2: Ablation of transforming ASR transcripts into descriptive steps   with post-determined timestamps as the extra training source. \u2018W\u2019 denotes   transcripts from WhisperX, and \u2018S\u2019 denotes our proposed HowToStep dataset.   Backbone   Training Text   HT-Step   HTM-Align   \u2191R@1   \u2191R@1   CLIP-ViT/L-14 [34]   W   32.2   58.7   CLIP-ViT/L-14   W + S   42.4   64.9   MIL-NCE S3D-G [29]   W   29.1   59.5   MIL-NCE S3D-G   W + S   40.9   63.0   InternVideo-ViT/L-14 [43] W   34.7   70.0   InternVideo-ViT/L-14   W + S   46.4   71.6   Table 3: Ablation of choices in constructing the pipeline. \u2018Step-Video\u2019 means to   determine the start/end time directly by computing visual-textual similarity, while \u2018StepASR\u2019 means indirectly by chaining \u2018steps\u2192transcript\u2019 similarity and \u2018transcript\u2192video\u2019   timestamp. The texts used for training here are only the generated steps (\u2018S\u2019).   Method   HT-Step   Pesudo-label   \u03f51   Self-training   \u03f52   \u2191R@1   Step-Video   0.20   \u2717   36.5   Step-Video   0.20   \u2713   0.8   36.9   Step-ASR   0.15   \u2717   35.3   Step-ASR   0.20   \u2717   36.0   Step-ASR   0.15   \u2713   0.7   41.4   Step-ASR   0.15   \u2713   0.8   41.7   Step-ASR   0.20   \u2713   0.7   42.3   Step-ASR   0.20   \u2713   0.8   43.7   However, in procedural step grounding, showing a marginal performance decrease,   we conjecture this is because the gap of text style between the train set (dense   speeches) and test set (sparse procedural steps) dominates the performance. In   later sections, we use the WhisperX transcripts by default.   Effect of Upgrading Visual-Textual Backbone. Here, we explore different   visual backbones with the corresponding text encoder. As shown in Tab. 4, S3D-G   exceeds CLIP ViT/L-14 in narration alignment but is inferior to the latter in step   grounding. In general, InternVideo ViT/L-14 shows significant advantages on   both tasks, attributed to the large pre-trained dataset and effective supervision   in video representation learning, which is our default choice in the pipeline.   Effect of the Proposed Dataset. We validate the effectiveness of using this   pipeline to transform noisy ASR transcripts into descriptive steps, with the   post-determined temporal segments as an extra training source for multi-sentence   grounding. As shown in Tab. 2, on all three backbones, the generated dataset is   effective for both narration alignment and step grounding. Notably, the average   14   Z. Li et al.   Table 4: Ablation study for visualtextual backbones and ASR systems.   Here we only use the weakly-aligned ASR   transcripts to train our model.   Backbone   ASR System HT-Step HTM-Align   \u2191R@1   \u2191R@1   CLIP   WhisperX   32.2   58.7   S3D   WhisperX   29.1   59.5   InternVideo   WhisperX   34.7   70.0   InternVideo   Youtube   35.7   61.3   Table 5: Manual check of the   dataset quality. HTM-370k and   HowToStep are the datasets before   and after the processing in our proposed pipeline, respectively.   Dataset   Alignable Well-aligned   % \u2191   % \u2191   HTM-370K   30.1   21.9   HowToStep   60.6   52.5   improvement exceeds 10% in HT-Step implies that the generated dataset are   indeed more descriptive and well-aligned. For narration alignment, our generated   steps add more diversity for training, thus leading to better performance.   Ablation of Options in Constructing the Pipeline. We investigate the   choices and hyper-parameters to generate the extra training dataset (HowToStep)   described in Sec. 3. As shown in Tab. 3, the difference in determining the timestamp of the generated steps by directly computing the video-step similarity matrix,   and indirectly by chaining \u2018steps\u2192transcript\u2019 similarity and \u2018transcript\u2192video\u2019   labels is not obvious for the first stage, while becomes significant when using   the pseudo-label of the first stage for the self-training in the second stage, there   is a large gap between the two methods. We find that using the video-step   similarity matrix directly will make the model learn a trivial solution (i.e., identity mapping), while indirectly obtained pseudo-label can let the model learn   the alignment patterns as analyzed in Sec. 3.3. In addition, we choose the best   thresholds to generate the final dataset according to Tab. 3.   4.5   Manual Check   To evaluate the quality of our proposed dataset, we have randomly sampled 10   videos with a total of 853 sentences for a manual check, focusing on the proportion   of steps in the dataset that are (i) visually alignable, (ii) and well-aligned with   correct temporal boundaries. We compare the results before and after using our   pipeline, as shown in Tab. 5, demonstrating the significant quality improvement.   5   Conclusion   To conclude, we have established an automatic pipeline for constructing a highquality video-text dataset for multi-sentence grounding in large-scale instructional   videos. We have investigated the factors potentially affecting performance, including upgrading the ASR system, transforming the noisy ASR transcripts   into descriptive steps by LLMs as an extra training source, and proposing a   simple Transformer-based model to refine the temporal windows for each step.   When evaluating three public benchmarks of multi-sentence grounding tasks, our   method surpasses the existing state-of-the-art methods by a significant margin.   Multi-Sentence Grounding for Long-term Instructional Video   15   Acknowledgements   This work is supported by National Key R&D Program of China (No.2022ZD0161400).", "conf": "ECCV", "year": "2024", "index": 271}, {"title": "Deconfounded Visual Grounding   Jianqiang Huang1,2, Yu Qin2, Jiaxin Qi1, Qianru Sun3, Hanwang Zhang1   1Nanyang Technological University, Singapore   2Damo Academy, Alibaba Group   3Singapore Management University   jianqiang.jqh@gmail.com, dongdong.qy@alibaba-inc.com, jiaxin003@e.ntu.edu.sg   qianrusun@smu.edu.sg, hanwangzhang@ntu.edu.sg", "abstract": "We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that   the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial languagelocation association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central   regions, due to that most queries about sheep have groundtruth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the   causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to   break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is   unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED),   to remove the confounding bias. Third, we implement RED   as a simple language attention, which can be applied in any   grounding method. On popular benchmarks, RED improves   various state-of-the-art grounding methods by a significant   margin. Code is available at: https://github.com/JianqiangH/   Deconfounded VG.", "content": "Visual Grounding, also known as the task of Referring   Expression Comprehension (Karpathy, Joulin, and Fei-Fei   2014; Karpathy and Fei-Fei 2015; Hu et al. 2016), has   greatly expanded the application domain of visual detection,   from a fixed noun vocabulary, e.g., dog, car, and person, to   free-form natural language on demand, e.g., the dog next to   the person who is driving a car. Regarding the place of candidate proposal generation in the grounding pipeline, there   are two traditional camps: two-stage and one-stage. For the   two-stage, the first detection stage denotes any object detectors (Ren et al. 2015) for extracting candidate regions from   the image, and the second visual reasoning stage is to rank   the candidates and select the top one based on their similarities to the query embedding (Yu et al. 2018; Liu et al.   2019a); for the one-stage, the detection and reasoning is   unified by directly performing the referent detection: generating the referent region proposals with confidence scores   and spatial coordinates for each pixel in the multi-modal   Copyright \u00a9 2022, Association for the Advancement of Artificial   Intelligence (www.aaai.org). All rights reserved.   brown color sheep   looking at camera   ground-truth   x   distribution of sheep   box center in x-axis   p   0   1   0.5   distribution of sheep   box center in y-axis   y   p   0   1   0.4   biased   x   0.5   0.4   y   Figure 1: Two image examples: one with ground truth location of brown color sheep looking at camera; and the other   with a wrong prediction caused by the language bias in the   grounding model. Blue bars denote the distribution of the   center positions of sheep on the y-axis (of the image), and   orange bars for the x-axis (of the image).   feature map, which is usually the concatenation of the visual feature map, location coordinates, and language embeddings (Yang et al. 2019, 2020). However, if the gradients can backpropagate through the visual detector in the   two-stage methods, there is little difference between the two   camps. Without loss of generality, this paper will be focused   on one-stage methods. In particular, the introduction of our   approach will be based on the one-stage model (Yang et al.   2019) with YOLO (Bochkovskiy, Wang, and Liao 2020)   backbone which has been validated of high inference speed   and good generalizability.   By analyzing the failure examples of existing grounding   methods, we find an ever-overlooked bias, as illustrated in   Figure 1: the bounding box prediction tends to be strongly   biased to some particular position, e.g., for the subject sheep,   the prediction is often on the central area because most of the   ground truth locations for sheep related queries are close to   the center of the image as shown in the statistics in Figure 1.   The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)   998   Noun Bias:   books under cat   Mean box    of cat   the horse closest to    the yellow line   Mean box    of horse   Verb Bias:   girl standing with    design on shirt   Mean box of    standing   chair facing the    corner of table and    an umbrella pole   mean area:    9672 \ud835\udc5d\ud835\udc652   biased area:    5115 \ud835\udc5d\ud835\udc652   Preposition Bias :   Figure 2: Three types of language bias observed in the   grounding model (Yang et al. 2019). The noun and verb biases are from the model trained on RefCOCO+ (Yu et al.   2016) and the preposition bias is from RefCOCOg (Mao   et al. 2016). Green boxes denote ground-truth, red boxes denote biased prediction and purple boxes denote language biased location. \u201cMean box\u201d denotes the average region over   all ground truth bounding boxes of a specific language fragment (e.g., horse) in the dataset. For corner of, \u201cmean area\u201d   is the average size of all ground truth boxes and \u201dBiased\u201d   one is the ground truth boxes averaged over samples with a   specific language fragment (e.g., \u201ccorner of\u201d).   More examples are given in Figure 2, where such bias ubiquitously exists in not only nouns, but also verbs and prepositions. For example, for corner of in Figure 2, the predicted   region is biased to be of smaller size (than the size of the   ground truth), since most of corner of samples in the dataset   are smaller. The model simply takes this bias rather than justifies if the content in this region satisfies the query.   One may realize that the above absurd language bias is   due to the confounder, which is a common cause of other   variables, rendering spurious correlations among them, even   if they have no direct causal effects with each other (Pearl,   Glymour, and Jewell 2016). For example, the confounding   effect of the standing bias in Figure 2 may be due to the   common sense that most standing people are the focus of   the photographer. Thus, if we fail to consider such an effect that causes the correlation between standing people and   the center location in training, when the confounder distribution changes in testing, e.g., most people standing aside,   the former correlation in training will be no longer applicable in testing (Pearl and Bareinboim 2014). Readers are   encouraged to think about the confounders in other examples. In fact, similar confounding effect is also common in   other classical vision tasks such as classification (Yue et al.   2020), detection (Tang, Huang, and Zhang 2020), and segmentation (Zhang et al. 2020). However, compared to those   single-modal tasks, the confounder in the visual grounding   task is special. As this task is complex and multi-modal, the   confounder is unobserved and non-enumerative in general.   For example, a general \u201ccommon sense\u201d is elusive and hard   to model.   In this paper, we provide a theoretical ground that addresses why the visual grounding process is confounded by   the unobserved confounder, and how we can overcome it.   Our technical contributions are three-fold:   \u2022 To answer the above question, we frame the visual grounding pipeline into the causal graph. Thanks to the graph, we   can model the causalities between language, vision, and locations, and thus offer the causal reasons why the grounding process is confounded.   \u2022 As the confounder is unobserved and non-enumerative, we   propose a Referring Expression Deconfounder (RED) to estimate the substitute confounder based on the deconfounder   theory (Wang and Blei 2019). In this way, we can mitigate   the confounding bias without any knowledge on the confounder, i.e., by only using the same observational data just   as other baseline methods of visual grounding.   \u2022 We implement RED as a simple and effective language   attention, whose embedding replaces the corresponding   one in any grounding method. Therefore, RED is modelagnostic and thus can help various grounding methods   achieve better performances.   Related Work   Visual Grounding In earlier works for visual grounding   tasks, most of models are trained in two stages (Plummer   et al. 2018; Wang and Specia 2019): its first stage detects   region proposals using an off-the-shelf object detector (Ren   et al. 2015), and the second stage ranks these regions based   on the distances between visual features and language embeddings. Improved methods include using a stronger attention mechanism (Yu et al. 2018; Liu et al. 2019c,b), a better language parsing (Liu et al. 2019a; Niu et al. 2019; Liu   et al. 2020) or a better mechanism of filtering out poor region proposals (Chen et al. 2021). While, it is often suboptimal to train the model in separated stages. Most of recent works (Chen et al. 2018; Liao et al. 2020; Sadhu, Chen,   and Nevatia 2019) are thus proposing to exploit a one-stage   (i.e., end-to-end) training paradigm. Specifically, they fuse   the image features and language embeddings in a dense and   optimizable way, and directly predict the grounding result,   i.e., the bounding box of the query object. The improved   methods (Yang, Li, and Yu 2020; Yang et al. 2020; Shrestha   et al. 2020; Huang et al. 2021) are mainly based on incorporating additional head networks that are used in two-stage   models.   After careful comparison, we find that regardless of the   different types of object detectors (e.g., YOLO and Faster   R-CNN), the two-stage and one-stage methods do not have   significant difference in terms of the learning pipeline, if all   gradients can backpropagate through the backbone of the detector. Without loss of generality, this paper deploys multiple   one-stage methods as baselines to show that our method is   effective and generic.   Causal Inference. Causal inference (Rubin 2019; Pearl,   Glymour, and Jewell 2016; Bareinboim and Pearl 2012)   is often used to mitigate spurious correlations. Its methods include deconfounding (Wang et al. 2020; Zhang et al.   999   Conventional Methods [1, 2, 3]   Referring Expression Deconfounder (Ours)   BERT   decode   encode   \ud835\udc3f2 \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60   CNN   collect   Query   Query   Attention   no bias   detect   brown color    sheep   looking at    camera   location bias    corresponding    to sheep   duplicate   BERT   detect   CNN   brown color    sheep   looking at    camera   biased   The red dotted line links between the feature and the latent    location (with or without bias) represented by the feature   Figure 3: The pipeline of conventional grounding methods vs. our RED. [1, 2, 3] denote the methods in (Yang et al. 2019),   (Yang et al. 2020) and (Huang et al. 2021), respectively. In the left part, green bars denote different region features and blue   bars denote the duplicated language features. In the right part, the gray region denotes the process of dictionary extraction   before training any grounding model. \u201cQuery\u201d denotes all the referring expressions in the current training grounding dataset.   The dashed black box bounding multiple pink bars represents the dictionary we get by clustering all \u201cpink features\u201d extracted   via a trained auto-encoder (in the gray region). The purple bars denote the deconfounded language features derived by our RED.   2020; Yue et al. 2020) and counterfactual inference (Tang,   Huang, and Zhang 2020; Tang et al. 2020; Niu et al. 2020).   The generic way is to disentangle the factors (in the task),   and modeling the existing causal effects (among the factors) on the structural causal graph. In our work, we leverage the method of deconfounding to remove the bias we   found in visual grounding datasets. We understand that the   confounder is often invisible and non-enumerative from the   given dataset. We propose an approach to solve this by generating the proxy of the confounder inspired by a data generation method called Deconfounder (Wang and Blei 2019).   Our approach is different from existing deconfounding   methods. For example, (Wang et al. 2020; Yue et al. 2020;   Zhang et al. 2020) select a specific substitute for confounder   (like object class) and build the confounder set by enumerating its values. (Qi et al. 2020; Yang, Zhang, and Cai 2020b)   use a learnable dictionary to learn the values of the confounder in the main training process. In contrast, we build   the dictionary by Deconfounder (Wang and Blei 2019) first   as an off-the-shelf part, and then in the grounding stage, we   directly use the fixed deconfounder set based on which we   are able to guarantees the confounder set is the cause of the   feature but not vice versa.   Visual Grounding in Causal Graphs   We frame the grounding pipeline summarized in Figure 3   into a structural causal graph (Pearl, Glymour, and Jewell   2016), as shown in Figure 4, where the causalities among   the ingredients, e.g., image, language query, and locations,   can be formulated. The nodes in the graph denote data variables and the directed links denote the causalities between   the nodes. Now, we introduce the graph notations and the   rationale behind its construction at a high-level.   Causal Graph   X \u2192L \u2190R. L is an object location. X \u2192L and R \u2192L   denote that L is directly detected from X according to R.   X \u2190G \u2192R. G is the invisible (unobserved) confounder   whose distribution is dataset-specific. For example, when G   is a dominant visual context (e.g., if there are more samples of horses on grass than horse on road, on grass dominates). G \u2192X indicates that most images picturing horse   and grass (Zhang et al. 2020), and G \u2192R denotes that   most language queries R contain the words about horse and   grass (Zhang, Niu, and Chang 2018).   G \u2192L. This is the ever-overlooked causation for the   grounding: the confounder G directly causes L, because the   ground truth of L is annotated under the same dataset containing the confounder G. For example, under the visual   scene horses on grass, G \u2192L indicates where and how   to put L (e.g., at the middle bottom position, with certain   shape and size), which is the root of the bias in the grounding (Zhang et al. 2020).   Causal Inference Process   Given an image, the causal inference process of the grounding task is to maximize the intervention conditional probability of the object location L according to the language   query R, which is not equal to the observational conditional   probability:   P(L | do(R), X) \u0338= P(L | R, X),   (1)   where the do-operation for R denotes that we want to pursue the causal effect of R to L by intervening R = r (Pearl,   Glymour, and Jewell 2016), i.e., we expect that the model   should predict location L that varies from queries R. The   inequality is because of the confounding effect (Pearl, Glymour, and Jewell 2016): Shown in Figure 4, even though we   conditional on X (i.e., cutting off the backdoor path R \u2190   1000   X   R   L   G   L   R   X   the brown    horse in front   Figure 4: The proposed causal graph and the examples of   corresponding nodes. G: unobserved confounder, X: pixellevel image, R: language query, L: the location for the query.   G \u2192X \u2192L), there is still another one, R \u2190G \u2192L,   confounding R and L. Therefore, this is the key bottleneck   that the grounding pipeline is confounded by language.   Remarks. One may argue that we should also intervene X,   because X leads another symmetric backdoor path like R   and P(L|do(R), do(X)) \u0338= P(L|do(R), X). Although this   is true in general, we argue that such confounding effect does   not only exist in the grounding task, but also ubiquitously exist in any visual recognition tasks (Sch\u00a8olkopf 2019), which   is not the main confounding effect in the language-dominant   task.   To solve the confounding problem in visual grounding,   the general way is to deconfound by using the backdoor adjustment (Pearl, Glymour, and Jewell 2016) through controlling all the values of G:   P(L | do(R), X) = Eg\u223cG[P(L | R, X, g)].   (2)   Yet, as the confounder is unobserved in general, we cannot   enumerate its exact semantic, let alone control it.   Deconfounded Visual Grounding   Deconfounder   Thanks to Deconfounder (Wang and Blei 2019), we can derive a substitute confounder \u02c6G from a data generation model   M even if the confounder is unobserved (invisible). Suppose   we have the model M that can generate R from \u02c6G:   P(R | \u02c6G) =   m   Y   i=1   P(Ri | \u02c6G),   (3)   where Ri denotes the i-th feature of R (e.g., the i-th dimension or i-th word embedding), respectively. M ensures the   independence among the features in R conditional on \u02c6G,   making \u02c6G include all the confounder stratifications g \u223cG.   Here we give a sketch proof: If a confounder stratification   (i.e., a value) g is not included in \u02c6G, according to the causal   graph illustrated in Figure 4, g can simultaneously cause   multiple dimensions in X, R and L, making the different   dimensions in R dependent, which contradicts with Eq. (3).   Please kindly refer to the formal proof in Appendix.   Therefore, if we have a model M, we can sample \u02c6g \u223c   \u02c6G to approximate stratifying the original G. By doing this,   Eq. (2) can be re-written as:   P(L | do(R), X) = E\u02c6g\u223c\u02c6   G[P(L | R, X, \u02c6g)].   (4)   In practice, to avoid expensive network forward pass calculation, thanks to (Xu et al. 2015; Yang, Zhang, and Cai   2020a), we can apply the Normalized Weighted Geometric   Mean to move the outer expectation into the feature level   (The proof can be found in Appendix):   P(L|do(R), X) = E\u02c6g\u223c\u02c6   G[P(L|R, X, \u02c6g)]   \u2248P(L | E\u02c6g\u223c\u02c6   G[R, \u02c6g], X).   (5)   Next, we detail how to obtain the generative model M and   implement the above Eq. (5).   Referring Expression Deconfounder (RED)   Shown in the right part of Figure 3, our proposed approach   includes two steps: training the generative model M (illustrated as the shaded part) and training the deconfounded   grounding model.   In the first step, we propose to use an auto-encoder to implement the generative model M. The auto-encoder encodes   language query R into a hidden embedding, i.e., the substitute confounder \u02c6G (denoted as pink bar), which can be   decoded to the query R\u2032:   Encoder:   \u02c6G = Fenc(R),   Decoder:   R\u2032 = Fdec( \u02c6G),   (6)   where Fenc and Fdec are deterministic neural networks, i.e.,   once R is set to specific values, the values of \u02c6G and R\u2032 will   be collapsed to a certain value with probability 1. Then, we   can use a reconstruction loss Lrecon = d(R, R\u2032) to train   the generative model M, where d(, ) is a distance function   for the features (e.g., we use Euclidean distance after feature pooling). Note that our deconfounder formulation also   supports other non-deterministic generative models such as   VAEs (Kingma and Welling 2013; Nielsen et al. 2020) and   conditional GANs (Douzas and Bacao 2018).   After we derive the generative model M, we can collect   all the samples of substitute confounders \u02c6g \u2208\u02c6G for R, then   cluster them to a dictionary D\u02c6g for \u02c6G (denoted as the stack   of pink bars in Figure 3). The reason of clustering is because   the number of \u02c6g is too large for backdoor adjustment, while   a clustered dictionary can efficiently represent the main elements of \u02c6G.   In the second step, we use the dictionary D\u02c6g to perform   deconfounded training by Eq.(5). We can instantiate the   backdoor adjustment as:   E\u02c6g\u223c\u02c6   G[R, \u02c6g] =   X   \u02c6g\u223cD\u02c6g   f(r, \u02c6g)P(\u02c6g),   (7)   where the right-hand-side is summed over elements in the   dictionary D\u02c6g, f(r, \u02c6g) is a feature fusion function and P(\u02c6g)   is 1/n, which is an empirical prior. We implement f(r, \u02c6g)   1001   Algorithm 1: Visual Grounding with RED   Input: Training images, language queries, and   ground-truth locations D = {(xi, ri, li)}   Output: Deconfounded grounding model   P\u03b8(L = l | do(R = r), X = x) in Eq. (9),   whose illustration is in Figure 3   1 Train the generative model Fenc and Fdec in Eq. (6);   2 Cluster the substitute confounders to derive D\u02c6g;   3 Initialize grounding model parameters \u03b8 randomly;   while not converged do   4   Extract features (x, r) for sample (xi, ri);   5   Calculate deconfounded language feature   r\u2032 = P   \u02c6g f(r, \u02c6g)P(\u02c6g) in Eq. (8);   6   Calculate deconfounded prediction   P(L = l | do(R = r), X = x) in Eq. (9);   7   Update \u03b8 by using the loss in Eq. (10);   as a simple language attention mechanism and derive our   deconfounded language feature r\u2032 by:   r\u2032 =   X   \u02c6g\u223cD\u02c6g f(r, \u02c6g)P(\u02c6g)   =   X   \u02c6g\u223cD\u02c6g Att(Q = r, K = \u02c6g, V = r)P(\u02c6g),   (8)   where Att(Q, K, V ) can be any attention (Vaswani et al.   2017) with the Query-Key-Value convention. In this paper,   we only adopt a simple top-down attention. Recall in Figure 3 to equip our RED with any conventional grounding   method, all we need to do is just to replace the original language embedding feature r (i.e., the light blue bar) with r\u2032   (i.e., the purple bar).   Then, the overall deconfounded visual grounding model   in Eq. (5) can be implemented as:   P(L = l | do(R = r), X = x)   \u2248P(l | (   X   \u02c6g f(r, \u02c6g)P(\u02c6g)) \u2295x) = P(l | r\u2032 \u2295x),   (9)   where \u2295denotes feature concatenation and l is a location   index. In particular, the grounding model P(l | \u00b7) can be   any detection head network that is a softmax layer outputing the probability of the l-th anchor box in the image (Yang   et al. 2019, 2020). Specifically, the reason why we only fuse   r and \u02c6g without x is because the ignorability principle (Imai   and Van Dyk 2004) should be imposed to the variable under   causal intervention, that is, R but not X. Table 3 demonstrates that other fusions that take X as input will hurt the   performance.   So far, we are ready to train the deconfounded grounding   model with the following training losses:   Loverall = \u2212log P(lgt | r\u2032 \u2295x) + Lreg,   (10)   where lgt is the ground-truth location and Lreg is a regression loss for the ground-truth bounding box (Yang et al.   2019; Redmon and Farhadi 2018) or mask (Luo et al. 2020).   The whole implementation is summarized in Algorithm 1.   Experiments   Datasets   To evaluate RED, we conducted extensive experiments on   the following benchmarks. RefCOCO, RefCOCO+ and   RefCOCOg are three visual grounding benchmarks and   their images are from MS-COCO (Lin et al. 2014). Each   bounding box from MS-COCO object detection ground   truth is annotated with several referring expressions. RefCOCO (Yu et al. 2016) has 50,000 bounding boxes with   142,210 referring expressions in 19,994 images and is split   into train/ validation/ testA/ testB with 120,624/ 10,834/   5,657/ 5,095 images, respectively. RefCOCO+ (Mao et al.   2016) has 19,992 images with 141,564 referring expressions and is split into train/ validation/ testA/ testB with   120,191/ 10,758/ 5,726/ 4,889 images, respectively. Note   that, during testing, RefCOCO and RefCOCO+ provide two   splits as testA and testB. Images in testA contain multiple   people and images in testB contain multiple instances of   all other objects. For RefCOCOg, we deploy the popular   data partition called RefCOCOg-umd (Nagaraja, Morariu,   and Davis 2016), and denote the partitions as val-u and   test-u in Table 1. We also conduct experiments on ReferItGame (Kazemzadeh et al. 2014) and Flickr30K Entities (Plummer et al. 2015). More details can be found in the   supplementary materials.   Implementation Details   As our RED is model-agnostic, We followed the same settings of implemented methods to extract the visual representations. As for language representations, before the grounding model training, we used the embedding extracted from   uncased version of BERT (Devlin et al. 2018) to train an   auto-encoder. Then, we used the trained auto-encoder to extract the substitute confounders for all the training samples   R. We deployed the K-Means algorithm to cluster those into   N = 10 clusters forming the confounder dictionary Dg\u2217in   Eq. (7). In the grounding model training stage, we first used   a pre-trained frozen BERT to extract language embeddings   from the query. Note that we applied the same frozen BERT   used in auto-encoder training to make the extracted embeddings consistent with our dictionary. Second, we computed   the deconfounded language embeddings by Eq. (8). Then, it   will be concatenated to the visual features as shown in Figure 3. Limited by the theory of deconfounder, to ensure the   validity of confounder embeddings, we have to use the same   structure (i.e., the same frozen BERT structure) to prevent   the embeddings gap between the substitute confounders and   deconfounded training. As a finetuned BERT is more popular, We will closely follow the breakthroughs of the deconfounder to improve this implementation. More other details   of implementations and training settings can be found in the   supplementary materials.   Quantitative Results   Comparing with the state-of-the-art (SOTA). In Table 1,   we summarize the results of SOTA methods and those w/   and w/o our RED, on RefCOCO-series datasets. We used   4 SOTA methods as our baselines: Yang\u2019s-V1 (Yang et al.   1002   Methods   RefCOCO   RefCOCO+   RefCOCOg   val   testA   testB   val   testA   testB   val-u   test-u   Two-Stage   SOTA   MattNet(Yu et al. 2018)   76.40   80.43   69.28   64.93   70.26   56.00   66.67   67.01   NMT(Liu et al. 2019a)   76.41   81.21   70.09   66.46   72.02   57.52   65.87   66.44   CM-Att(Liu et al. 2019c)   78.35   83.14   71.32   68.09   73.65   58.03   67.99   68.67   DGA (Yang et al. 2019)   78.42   65.53   69.07   51.99   63.28   Ref-NMS (Chen et al. 2021)   80.70   84.0   76.04   68.25   73.68   59.42   70.55   70.62   One-Stage   SOTA   RCCF (Liao et al. 2020)   81.06   71.85   70.35   56.32   65.73   Yang\u2019s-V1 (Yang et al. 2019)   72.54   74.35   68.50   56.81   60.23   49.60   61.33   60.36   Iterative (Sun el al. 2021)   74.27   68.10   71.05   58.25   70.05   PFOS (Suo et al. 2021)   79.50   81.49   77.13   65.76   69.61   60.30   69.06   68.34   LBYL (Huang et al. 2021)   79.67   82.91   74.15   68.64   73.38   59.49   Yang\u2019s V1\u2020 (YOLO-V4)   74.67   75.98   70.76   57.21   61.11   50.93   61.89   61.56   MCN (Luo et al. 2020)   80.08   82.29   74.98   67.16   72.86   57.31   66.46   66.01   MCN (fixed BERT)   78.93   82.21   73.95   65.54   71.29   56.33   65.76   66.20   OURS   Yang\u2019s-V1 +RED   78.04   79.84   74.58   62.82   66.03   56.72   66.40   66.64   Yang\u2019s-V1\u2020+RED   78.76   80.75   76.19   63.85   66.87   57.51   69.46   69.51   MCN (fixed BERT) +RED   79.23   83.22   75.23   66.84   72.47   59.03   67.28   67.02   LBYL +RED   80.97   83.20   77.66   69.48   73.80   62.20   71.11   70.67   Table 1: The performance (Acc@0.5%) compared to the state-of-the-art (SOTA) methods on RefCOCO, RefCOCO+ and   RefCOCOg, where \u2020 denotes applying the visual feature backbone from YOLO-V4, methods with citation denote that the   results are from the cited papers. Bold text denotes the best performance. We will use the same notation in the following tables.   Note that our reproduced performance of MCN is different from the original paper, because we applied fixed BERT instead of   LSTM for language embedding and the reason can be found in \u201cImplementation Details\u201d.   Method   ReferIt Game Flickr30K Entities   ZSGNet (Sadhu el al. 2019)   58.63   63.39   RCCF (Liao et al. 2020)   63.79   Yang\u2019s-V1 (Yang et al. 2019)   60.67   68.71   Yang\u2019s-V2 (Yang et al. 2020)   64.33   69.04   LBYL (Huang et al. 2021)   66.51   Yang\u2019s-V1\u2020 (YOLO-V4)   61.21   70.25   Yang\u2019s-V1\u2020 +RED   64.37   70.50   Yang\u2019s-V2 +RED   66.09   69.76   LBYL +RED   67.27   Table 2: Comparing with SOTA methods on the test sets of   ReferItGame and Flickr30K Entities (Acc@0.5%). It is reasonable that ours gets significantly higher gains on the more   difficult dataset\u2014ReferIt Game. Please refer to \u201cComparing   with the state-of-the-art (SOTA)\u201d.   2019), Yang\u2019s-V2 (Yang et al. 2020), LBYL (Huang et al.   2021), and MCN (Luo et al. 2020). Table 2 shows the results   on the ReferItGame and Flickr30K Entities. From an overall   view, we observe that using our RED method, which means   mitigating the bias in language features, improves the performance of grounding models generally. We elaborate the   improvements in the following two points.   RED achieved more improvements for RefCOCOg. This   is because RefCOCOg have longer queries than other   datasets. Our approach is used to mitigate the bias in language queries. Longer queries contain more bias, so they   can benefit more from our approach. For example, our gain   on ReferItGame is more than that on Flickr30K Entities,   i.e., an increase of 3.16% vs. 0.25%, as most of queries in   Methods   RefCOCO+   RefCOCOg   testA   testB   val-u   test-u   Yang\u2019s-V1\u2020   61.11   50.93   61.89   61.56   +AE(X \u2295R,X\u2032 \u2295R\u2032)   66.74   57.23   69.01   68.55   +AE(R,R\u2032) (our RED)   66.87   57.51   69.46   69.51   +Att(X)+Att(R)   65.57   56.37   67.83   68.11   +Att(R) (our RED)   66.87   57.51   69.46   69.51   Table 3: The ablation study of using different auto-encoders   and different fusion methods. X and R denote image and   language query, respectively. AE(input value, output value)   denotes the auto-encoder in Eq. (6) fed with different   input values for generating corresponding output values.   Att(value) denotes the attention operation in Eq. (8) using   different values of V .   Flickr30K Entities (e.g., a man, a bench) are as short as class   labels, which contain very little language bias.   In addition, we find that RED can improve more on testB   than on testA or val on RefCOCO and RefCOCO+. The reason is that the distribution of testB data is more discrepant   than that of training data. It means the testing on testB suffers more from the bias in the grounding model.   RED improved Yang\u2019s-V1 (or -V1\u2020) the most. The improvement of RED is brought by applying the deconfounding operation\u2014do(R) (and optionally using do(X)). While,   those methods with complex co-attention operations and   additional visual controlling, e.g., MCN and LBYL, may   downplay the effectiveness of deconfounding. In specific,   MCN uses extra mask supervision to enforce model looking   at the shapes rather than only the bounding boxes. Because   1003   56   58   60   62   64   66   68   70   72   Acc@0.5%   baseline   N=5   N=10   N=20   dist=cosine   validation   test   Figure 5: Results using different values of cluster number   N and distances for clustering from RefCOCOg. Note that   the skyblue, pink, yellow and green columns use Euclidean   distance (default), and the blue with N = 10 (default).   language confounding effect only influences the location L,   the additional shape in the objective function will not be confounded. Therefore, the whole training process of MCN will   take less language confounding effects. LBYL splits the image features into four parts combining with queries respectively. This is to intervene X by the value of each part of X,   playing the similar role of do(X). Therefore, plugging our   RED brings only a partial margin of improvement.   Justification for do(R) and attention for R. We use   AE(X \u2295R,X\u2032 \u2295R\u2032) to evaluate the performance of language and vision deconfounder for do(R, X), where \u2295denotes concatenation operation. Specifically, before training   grounding models, we reconstruct the embeddings of R and   X simultaneously, and drive the dictionary by clustering   the language and vision confounder using the same way in   \u201cImplementation Details\u201d. As shown in Table 3, we find   RED can achieve higher improvements than language and   vision deconfounder. The reason may be the embeddings of   X need to change during the grounding training, which hurts   the consistency of the dictionary of vision deconfounder.   Then, we perform attention on both X, R (i.e.,+Att(X) +   Att(R)) by the substitute confounder, where the attention   for X is implemented as a normal channel attention following SENet. We found the performance drop in this ablation.   This generally conforms to our statement for Eq (9): To pursue the causal effect from R to L, we need to use the confounder of R and conduct intervention on R.   Hyperparameter selection. We explore different numbers   of clustering centers and two clustering distance metrics. As   shown in Figure 5, all the clustering numbers N from 5 to 20   show significant improvements over the baseline. After N   exceeding 10, the performance won\u2019t show further improvement, thus we set N = 10. In the comparison of clustering criterion, Euclidean distance shows constant superiority,   which is reasonable because of the usage of L2 loss in the   auto-encoder.   Time Consumption. As a significant advantage of the onestage grounding model is its speed, we test the overhead of   (a) cat   looking    at you   (b) man    holding   cup   (c) blank computer screen   (d) bench   blurred in the    foreground   Figure 6: Qualitative examples. The left two examples show   that RED mitigates the language bias in the results of   Yang\u2019s-V1\u2020, where the red boxes denote the biased predictions of Yang\u2019s-V1\u2020, blue boxes denote prediction of Yang\u2019sV1\u2020+RED and greens ones are ground-truth boxes. The right   two examples show our failure cases.   our RED in the inference stage to ensure it will not hurt the   speed. Under fair settings, we test the speed of Yang\u2019s-V1   and Yang\u2019s-V1+RED on a single Tesla V100. The results   are 24.56ms and 26.53ms per sample, respectively, and the   overhead is only 8%, which is reasonable.   Qualitative Results   Language Bias Corrections. The qualitative results illustrated in the left half in Figure 6 show that our RED can help   models to remove the spurious correlations. In (a) and (b),   the location bias is drawn at the left-bottom corner which   misleads the grounding models to choose red boxes. After   applying our RED, we find that the model can counter such   bias (i.e., predict blue boxes near to the ground truth).   Failure Cases. We still find some failure cases shown in   the right in Figure 6. In (d), for example, both the red and   blue boxes (predicted by baseline and +RED) tend to select the colorful computer screen, which is more common   in the dataset. On the contrary, bench is the salient subject   compared to the complicated description. That\u2019s why both   models tend to choose the middle bench.   Conclusions   We investigated the confounding effect that exists ubiquitously in visual grounding models, where the conventional   methods may learn spurious correlations between certain   language pattern and object location. As the confounder is   unobserved in general, we proposed a Referring Expression   Deconfounder (RED) approach that seeks a substitute one   from observational data. The implementation result of RED   is just a simple language attention feature that replaces the   language embeddings used in any grounding method. By   doing this, RED improves various strong baselines consistently. Our future plans include: 1) We will use other generative models to implement RED (Nielsen et al. 2020). 2)   As hacking the unobserved confounder is ill-posed in general (D\u2019Amour 2019), we will try to introduce the large-scale   vision-language pre-training priors (Lu et al. 2020, 2019)   into the deconfounder.   1004   Acknowledgements   This research is supported, in part, by the Alibaba-NTU Singapore Joint Research Institute (JRI). Any opinions, findings   and conclusions or recommendations expressed in this material are those of the authors(s) and do not reflect the views   of funding agencies.", "conf": "AAAI", "year": "2022", "index": 593}, {"title": "KOSMOS-2: Grounding Multimodal Large Language   Models to the World   Zhiliang Peng\u2217, Wenhui Wang\u2217, Li Dong\u2217, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei\u2020   Microsoft Research   https://aka.ms/GeneralAI", "abstract": "We introduce KOSMOS-2, a Multimodal Large Language Model (MLLM), enabling   new capabilities of perceiving object descriptions (e.g., bounding boxes) and   grounding text to the visual world. Specifically, we represent refer expressions   as links in Markdown, i.e., \u201c[text span](bounding boxes)\u201d, where object   descriptions are sequences of location tokens. Together with multimodal corpora,   we construct large-scale data of grounded image-text pairs (called GRIT) to train   the model. In addition to the existing capabilities of MLLMs (e.g., perceiving   general modalities, following instructions, and performing in-context learning),   KOSMOS-2 integrates the grounding capability into downstream applications. We   evaluate KOSMOS-2 on a wide range of tasks, including (i) multimodal grounding,   such as referring expression comprehension, and phrase grounding, (ii) multimodal   referring, such as referring expression generation, (iii) perception-language tasks,   and (iv) language understanding and generation. This work lays out the foundation   for the development of Embodiment AI and sheds light on the big convergence of   language, multimodal perception, action, and world modeling, which is a key step   toward artificial general intelligence. Code and pretrained models are available at   https://aka.ms/kosmos-2.   Grounding   Kosmos-2: Multimodal Large Language Model   [a campfire](<loc4> <loc1007>)   [It](<loc44> <loc863>) sits next to    Referring   Figure 1: KOSMOS-2 is a multimodal large language model that has new capabilities of multimodal   grounding and referring. KOSMOS-2 can understand multimodal input, follow instructions, perceive   object descriptions (e.g., bounding boxes), and ground language to the visual world.   \u2217Equal contribution. \u2020 Corresponding author.   arXiv:2306.14824v3  [cs.CL]  13 Jul 2023   Input Prompt   the left eye of emoji   Question: How many cows are    here? Answer:    Question: What does the    sign say? Answer:   Completion   two cows present in the image.   The sign says \"Welcome to    Carnaby Street.\"   Input Prompt   Question: What is the biggest    difference betwen bottle-1 and    bottle-2? Answer:   Question: Why is this animal    unusual? Answer:   (1)   (2)   (3)   Completion   The biggest difference between    bottle-1 and bottle-2 is the label.   This animal is unique because it is a    turtle. Turtles are known for their slow    and steady pace, whereas rabbits are    known to be agile and quick. It is not    common for a turtle to be faster than a    rabbit, especially in terms of speed.   a buoy attached to    the boat.   (4)   (5)   (6)   Question: What is it? Answer:   Describe this image in detail:   Completion   A snowman sits next to a campfire in    the snow. He is wearing a hat, scarf,    and mittens. There are several pots   nearby, likely containing a hot meal    for the snowman. The scene is    serene and peaceful, with the    snowman looking at the fire with a    warm smile.   (7)   Input Prompt   Figure 2: Selected examples generated from KOSMOS-2. The examples include (1) visual grounding,   (2)-(3) grounded question answering, (4)-(6) multimodal referring via bounding boxes, and (7)   grounded image captioning.   2   1", "content": "Multimodal Large Language Models (MLLMs) [HSD+22, ADL+22, HDW+23, DXS+23, Ope23]   have successfully played a role as a general-purpose interface across a wide range of tasks, such   as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including   texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot   settings.   In this work, we unlock the grounding capability for multimodal large language models. Grounding   capability can provide a more convenient and efficient human-AI interaction for vision-language   tasks. It enables the user to point to the object or region in the image directly rather than input   detailed text descriptions to refer to it, the model can understand that image region with its spatial   locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding   boxes), which can support more vision-language tasks such as referring expression comprehension.   Visual answers are more accurate and resolve the coreference ambiguity compared with text-only   responses. In addition, grounding capability can link noun phrases and referring expressions in the   generated free-form text response to the image regions, providing more accurate, informational, and   comprehensive answers.   We introduce KOSMOS-2, a multimodal large language model with grounding capability built upon   KOSMOS-1. KOSMOS-2 is a Transformer-based causal language model and is trained using the   next-word prediction task. In order to unlock the grounding capability, we construct a web-scale   dataset of grounded image-text pairs, and combine it with the multimodal corpora in KOSMOS-1   to train the model. The grounded image-text pairs are built upon a subset of image-text pairs from   LAION-2B [SBV+22] and COYO-700M [BPK+22]. We construct a pipeline to extract and link   the text spans (i.e., noun phrases and referring expressions) in the caption to the spatial locations   (e.g., bounding boxes) of its corresponding objects or regions in the image. We convert the spatial   coordinates of the bounding boxes to a sequence of location tokens, which is then appended after its   respective text spans. The data format serves as a \u201chyperlink\u201d to connect the objects or regions of the   image to the caption.   Experimental results demonstrate that KOSMOS-2 not only achieves competitive performance on language and vision-language tasks evaluated in KOSMOS-1, but also achieves impressive performance   on grounding tasks (phrase grounding and referring expression comprehension) and referring tasks   (referring expression generation). As shown in Figure 2, integrating the grounding capability enables   KOSMOS-2 to be used for more downstream tasks, such as grounded image captioning, and grounded   visual question answering.   2   Construction of Web-Scale Grounded Image-Text Pairs (GRIT)   We introduce GRIT2, a large-scale dataset of Grounded Image-Text pairs, which is created based on   image-text pairs from a subset of COYO-700M [BPK+22] and LAION-2B [SBV+22]). We construct   a pipeline to extract and link text spans (i.e., noun phrases and referring expressions) in the caption   to their corresponding image regions. The pipeline mainly consists of two steps: generating nounchunk-bounding-box pairs and producing referring-expression-bounding-box pairs. We describe   these steps in detail below:   Step-1: Generating noun-chunk-bounding-box pairs   Given an image-text pair, we first extract   noun chunks from the caption and associate them with image regions using a pretrained detector. As   illustrated in Figure 3, we use spaCy [HMVLB20] to parse the caption (\u201ca dog in a field of flowers\")   and extract all noun chunks (\u201ca dog\u201d, \u201ca field\u201d and \u201cflowers\u201d). We eliminate certain abstract noun   phrases that are challenging to recognize in the image, such as \u201ctime\u201d, \u201clove\u201d, and \u201cfreedom\u201d, to   reduce potential noise. Subsequently, we input the image and noun chunks extracted from the caption   into a pretrained grounding model (e.g., GLIP [LZZ+22]) to obtain the associated bounding boxes.   Non-maximum suppression algorithm is applied to remove bounding boxes that have a high overlap   with others, even if they are not for the same noun chunk. We keep noun-chunk-bounding-box pairs   with predicted confidence scores higher than 0.65. If no bounding boxes are retained, we discard the   corresponding image-caption pair.   2A subset of GRIT can be downloaded at https://aka.ms/kosmos-2.   3   a dog in a    field of flowers   Identify    noun chunks   Detection &   Post-process   a dog:    [290,371,605,750]   a field:    [0,264,919,921]   Step-1: Creating noun    chunk - bounding box pairs   a dog   a field   flowers   Expand   noun chunks   Sentence dependency relations   a dog in a field of flowers   a field of flowers   flowers   Drop    substrings   Compose   a dog in a field of    flowers:    [290,371,605,750]   Step-2: Producing referring    expression - bounding box pairs   Keep \u201ca dog in a field of flowers\u201d   Drop \u201ca field of flowers\u201d   Drop \u201cflowers\u201d   Figure 3: The pipeline of constructing web-scale grounded image-text pairs.   Dataset   Images   Objects   Text Spans   Avg Expression Length   Flickr Entities [PWC+15]   31,783   275,775   513,644   RefCOCOg [MHT+15]   26,711   54,822   85,474   8.43   RefCOCO [YPY+16]   19,994   50,000   142,209   3.61   RefCOCO+ [YPY+16]   19,992   49,856   141,564   3.53   Visual Genome [KZG+16]   108,077   4,102,818   GRIT (Ours)   90,614,680   137,349,210   114,978,233   4.7   Table 1: Comparison GRIT with existing visual grounding datasets.   Step-2: Producing referring-expression-bounding-box pairs   In order to endow the model with   the ability to ground complex linguistic descriptions, we expand noun chunks to referring expressions.   Specifically, we use spaCy to obtain dependency relations of the sentence. We then expand a noun   chunk into a referring expression by recursively traversing its children in the dependency tree and   concatenating children tokens with the noun chunk. We do not expand noun chunks with conjuncts.   For noun chunks without children tokens, we keep them for the next process. In the example shown   in Figure 3, the noun chunk \u2018a dog\u2019 can be expanded to \u201ca dog in a field of flowers\u201d, and the noun   chunk \u2018a field\u2019 can be expanded to \u201ca field of flowers\u201d.   Furthermore, we only retain referring expressions or noun chunks that are not contained by others.   As shown in Figure 3, we keep the referring expression \u201ca dog in a field of flowers\u201d and drop \u201ca field   of flowers\u201d (as it is entailed by \u201ca dog in a field of flowers\u201d) and \u2018flowers\u2019. We assign the bounding   box of the noun chunk (\u2018a dog\u2019) to the corresponding generated referring expression (\u201ca dog in a   field of flowers\u201d).   In the end, we obtain approximately 91M images, 115M text spans, and 137M associated bounding   boxes. We compare GRIT with existing publicly accessible visual grounding datasets in Table 1.   Data samples of GRIT are shown in the Appendix.   3   KOSMOS-2: A Grounded Multimodal Large Language Model   KOSMOS-2 is a grounded multimodal large language model, which integrates grounding and referring   capabilities compared with KOSMOS-1. The model can accept image regions selected by the user   using bounding boxes as input, provide visual answers (i.e., bounding boxes), and ground the text   output to the visual world. KOSMOS-2 adopts the same model architecture and training objective   as KOSMOS-1. We add grounded image-text pairs into the training data to endow the model with   grounding and referring capabilities. For a text span (such as noun phrase and referring expression)   and its corresponding bounding boxes in a grounded image-text pair, We discretize continuous   coordinates of bounding boxes into a sequence of location tokens to encode with text tokens in a   unified way. Then we link the location tokens and their corresponding text span via a \u201chyperlink\u201d data   4   format. The model is trained to establish a mapping between image regions and their corresponding   location tokens and connect the image regions with their associated text spans.   3.1   Grounded Input Representations   Given a text span and its associated bounding boxes in a grounded image-text pair, we first convert the   continuous coordinates of bounding boxes into a sequence of discrete location tokens [CSL+21]. For   an image with width W and height H, we evenly divide both the width and height into P segments   each. P \u00d7 P bins are obtained and each bin consists of (W/P) \u00d7 (H/P) pixels. For each bin, we use   a location token to represent the coordinates within that bin. We use the coordinates of the center   pixel of each bin to determine bounding boxes on the image. In total, P \u00d7 P location tokens are   introduced, and these tokens are added to word vocabulary to enable unified modeling with texts.   The bounding box can be represented using its top-left point (x1, y1) and bottom-right point   (x2, y2). We discretize the top-left and bottom-right corner points to location tokens, respectively.   We concatenate the top-left location token <loc1>, the bottom-right location token   <loc2>, and special boundary tokens <box> and </box>, to represent a single bounding box:   \u201c<box><loc1><loc2></box>\u201d.   If the text span is associated with multiple bounding boxes,   we use a special token <delim> to concatenate the location tokens of these bounding boxes:   \u201c<box><loci   1><loci   2><delim>...<locj   1><locj   2></box>\u201d.   Then we arrange the text span and its associated location tokens in a format resembling a \u201chyperlink\u201d   in markdown. For the text span with a single bounding box, the resulted sequence is \u201c<p> text span   </p><box><loc1><loc2></box>\u201d, where <p> and </p> are special tokens indicating the beginning   and end of the text span. The data format tells the model that image regions within the bounding box   are associated with the text span.   For the example shown in Figure 1, the input representation is:   <s> <image> Image Embedding </image> <grounding> <p> It </p><box><loc44><loc863></box>   seats next to <p> a campfire </p><box><loc4><loc1007></box> </s>   where <s> and </s> indicate start- and end-of-sequence, and <image> and </image> represent   the beginning and end of encoded image embeddings. <grounding> is a special token to tell the   model ground the text output to the visual world. We map input text tokens and location tokens to   embeddings via a lookup table. Following KOSMOS-1, a vision encoder and a resampler module are   used to obtain image embeddings for input images.   For language-only data, cross-modal paired data (i.e., image-text pairs), and interleaved multimodal   data, we use the same input representations as of KOSMOS-1.   3.2   Grounded Multimodal Large Language Models   Based on KOSMOS-1, KOSMOS-2 enhances multimodal large language models by incorporating   grounding and referring capabilities. KOSMOS-2 also uses a Transformer-based causal language   model as the backbone and is trained with the next-token prediction task.   In addition to multimodal corpora used in KOSMOS-1 (including text corpora, image-caption pairs,   and interleaved image-text data), we add grounded image-text pairs into training. The training loss   only considers discrete tokens, such as text tokens and location tokens. The model can learn to locate   and understand image regions by their location tokens and the whole image, associate text spans to   image regions, and output bounding boxes of the image region using location tokens.   KOSMOS-2 shows new capabilities of grounding and referring. The referring capability enables us to   point out image regions with bounding boxes. KOSMOS-2 can understand the image regions users   refer to by the coordinates of bounding boxes. The referring capability provides a new interaction   method. Different from previous MLLMs [ADL+22, HSD+22, HDW+23], which can only provide   text output, KOSMOS-2 can provide visual answers (i.e., bounding boxes) and ground text output to   the image. The grounding capability enables the model to provide more accurate, informative, and   comprehensive responses. In addition to vision, language, and vision-language tasks evaluated in   5   KOSMOS-1, the model can be used for more downstream tasks, such as grounded image-captioning,   grounded VQA, referring expression comprehension and generation.   3.3   Model Training   Training Setup   We train the model on newly added grounded image-text pairs, monomodal text   corpora, image-caption pairs, and interleaved image-text data. Our training process involves a   batch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original   and grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2   for 60k steps, equivalent to around 25 billion tokens. The AdamW optimizer is employed with   \u03b2 = (0.9, 0.98). We set the weight decay to 0.01 and the dropout rate to 0.1. The learning rate   increases to 2e-4 during the first 375 warm-up steps and linearly decays to zero. We train the model   on 256 V100 GPUs and the training takes approximately one day to complete. In order to tell the   model when to ground text output to the visual world, we prepend the \u2018<grounding>\u2019 token to the   grounded caption during training.   Following KOSMOS-1, the vision encoder has 24 layers with 1,024 hidden size and 4,096 FFN   intermediate size. The multimodal large language model component is a 24-layer MAGNETO   Transformer [WMH+22, MWH+22] with 2,048 hidden dimensions, 32 attention heads, and 8,192   FFN intermediate size. The total number of trainable parameters amounts to approximately 1.6B.   The image resolution is set to 224\u00d7224 and the patch size is 14\u00d714. We divide the width and height   of the image into 32 bins, with each bin consisting of 7\u00d77 pixels. A total of 32\u00d732 location tokens   are added to the vocabulary. KOSMOS-2 uses the weights of KOSMOS-1 for initialization, the newly   added word embeddings of location tokens are initialized randomly. We update all the parameters   during training and instruction tuning.   Instruction Tuning   After the model is trained, we perform instruct tuning to better align   KOSMOS-2 with human instructions. we combine vision-language instruction dataset (i.e., LLaVAInstruct [LLWL23]) and language-only instruction datasets (i.e., Unnatural Instructions [HSLS22]   and FLANv2 [LHV+23]) with the training data to tune the model. In addition, we construct grounded   instruction data by utilizing the pairs of bounding boxes and expressions (i.e., noun phrases, and   referring expressions) in GRIT. Given an expression-bounding-box pair, we use \u201c<p> expression   </p>\u201d as the input instruction, and prompt the model to generate the corresponding location tokens of   the bounding boxes. We also use the prompt like \u201c<p> It </p><box><loc1><loc2></box> is\u201d to   ask the model to generate expressions according to its bounding boxes. Table B in Appendix presents   more templates.   4   Evaluation   We first evaluate KOSMOS-2 on multimodal grounding and multimodal referring tasks to assess the   new capabilities, and then test the model on language and perception-language tasks evaluated in   KOSMOS-1.   \u2022 Multimodal grounding   \u2013 Phrase grounding   \u2013 Referring expression comprehension   \u2022 Multimodal referring   \u2013 Referring expression generation   \u2022 Perception-language tasks   \u2013 Image captioning   \u2013 Visual question answering   \u2022 Language tasks   \u2013 Language understanding   \u2013 Language generation   6   Grounded MLLM   A man in a blue hard    hat and <p> orange    safety vest </p>   <box> <loc165> <loc360> </box>   (1) Phrase grounding   (2) Referring expression comprehension   Grounded MLLM   <box> <loc68> <loc425> </box>   <p> A man in a blue    hard hat and orange    safety vest </p>   Figure 4: Input format of evaluation on (1) phrase grounding and (2) referring expression comprehension.   4.1   Multimodal Grounding   In order to evaluate the ability of multimodal grounding, we test KOSMOS-2 on widely used phrase   grounding and referring expression comprehension tasks in a generation manner. Phrase grounding   task requires the model to predict a set of bounding boxes based on one or more given phrases that   maybe interrelated within a single caption. Referring expression comprehension task encourages the   model to locate the object described in a text referring expression within a given image.   By testing KOSMOS-2 on these two tasks, we can assess how well the model performs in grounding   text descriptions to the visual world, which is crucial for developing advanced AI systems capable of   handling complex multimodal tasks.   For both phrase grounding and referring expression comprehension tasks, KOSMOS-2 is required   to generate location tokens which are then converted to bounding boxes for evaluation. The input   format is \u201c<s><image> Image Embedding </image><grounding>...\u201d, where \u201c<grounding>\u201d is   used to prompt the model to generate locations tokens.   4.1.1   Phrase Grounding   We evaluate phrase grounding task on Flickr30k Entities [PWC+15] val and test splits. In order to   reduce ambiguity, we do not prompt the model with individual phrases; instead, we use the current   phrase along with the preceding words as input where preceding words serve as context: \u201c ... <p>   {phrase} </p>\u201d. For the example shown in Figure 4(1), the model needs to predict the locations of   phrases \u201cA man\u201d, \u201ca blue hard hat\u201d, \u201corange safety vest\u201d and \u201can intersection\u201d in the caption \u201cA man   in a blue hard hat and orange safety vest stands in an intersection.\u201d. To generate the location tokens   for the phrase \u201cA man\u201d that is the beginning of the caption, the prompt is \u201c<p>A man</p>\u201d. For the   phrase \u201corange safety vest\u201d, the prompt is \u201cA man in a blue hard hat and <p>orange safety vest</p>\u201d.   When multiple men are in the image, the context \u201cA man in a blue hard hat and\u201d explicitly helps the   model locate the object to reduce ambiguity.   We obtain the location tokens in \u201c<box>...</box>\u201d from the model response and then covert it into   bounding boxes. The generated bounding box is correct if its intersection over union (IoU) with the   ground-truth bounding box is greater than 0.5. If KOSMOS-2 generates a location sequence that can   not be converted correctly (e.g., \u201c<box><loc1></box>\u201d), we treat it as a negative sample. We use   ANY-BOX protocol in MDETR [KSL+21]. We report the R@1, R@5, and R@10 metrics, where   R@1/5/10 means calculating the recall using the top 1/5/10 generated bounding boxes. If there are   fewer than 5 or 10 bounding boxes generated by KOSMOS-2, we use all available bounding boxes for   the calculation.   Results   Table 2 presents results on Flickr30k Entities [PWC+15] val and test splits. KOSMOS-2   achieves impressive zero-shot performance and outperforms GRILL [JMC+23], which relies on   an attached detector, by a large margin. Moreover, our model outperforms traditional finetuned   7   Model   Zero-shot   Val Split   Test Split   R@1   R@5   R@10   R@1   R@5   R@10   VisualBert [LYY+19]   \u2717   70.4   84.5   86.3   71.3   85.0   86.5   MDETR [KSL+21]   \u2717   83.6   93.4   95.1   84.3   93.9   95.8   GLIP [LZZ+22]   \u2717   86.7   96.4   97.9   87.1   96.9   98.1   FIBER [DKG+22]   \u2717   87.1   96.1   97.4   87.4   96.4   97.6   GRILL [JMC+23]   \u2713   18.9   53.4   70.3   KOSMOS-2   \u2713   77.8   79.2   79.3   78.7   80.1   80.1   Table 2: Phrase grounding results on Flickr30k Entities. We report the R@1, R@5, and R@10   metrics, where R@1/5/10 means calculating the recall using the top 1/5/10 generated bounding boxes.   Model   ZeroRefCOCO   RefCOCO+   RefCOCOg   shot   val   testA   testB   val   testA   testB   val   test   UNITER [CLY+19]   \u2717   81.41   87.04   74.17   75.90   81.45   66.70   74.86   75.77   MDETR [KSL+21]   \u2717   87.51   90.40   82.67   81.13   85.52   72.96   83.35   83.31   OFA [WYM+22]   \u2717   90.05   92.93   85.26   84.49   90.10   77.77   84.54   85.20   FIBER [DKG+22]   \u2717   90.68   92.59   87.26   85.74   90.13   79.38   87.11   87.32   VisionLLM [WCC+23]   \u2717   86.7   GRILL [JMC+23]   \u2713   47.5   KOSMOS-2   \u2713   52.32   57.42   47.26   45.48   50.73   42.24   60.57   61.65   Table 3: Referring expression comprehension results on RefCOCO, RefCOCO+ and RefCOCOg.   We report the accuracy metric for all methods.   VisualBert [LYY+19] model by 7.4% R@1 on both val and test splits. In contrast to other models,   KOSMOS-2 does not involve prior designs (e.g., object queries or proposals), leading to similar results   among R@1, R@5, and R@10. These results demonstrate that KOSMOS-2 can generate high-quality   locations without the need for post-processing redundant locations. This capability highlights the   effectiveness of our model in handling phrase grounding tasks.   4.1.2   Referring Expression Comprehension   We assess the referring expression comprehension task using three well-established datasets: RefCOCO [YPY+16], RefCOCO+ [YPY+16] and RefCOCOg [MHT+15]. Both RefCOCO and   RefCOCO+ were generated through a two-player game, with RefCOCO+ specifically designed   to exclude spatial relations, such as \u201con the left\u201d. RefCOCOg incorporates spatial relations and   features longer expressions on average. Different from phrase grounding on Flickr30k entities, we   measure this task by using referring expression as the input: \u201c<p> referring expression </p>\u201d. For   the example shown in Figure 4(2), the input sequence is \u201c<p>A man in a blue hard hat and orange   safety vest</p>\u201d. Similarly, the predicted bounding box is considered correct only if its IOU with   the ground-truth bounding box is greater than 0.5. The failed decoded sequence is also treated as a   negative sample. We use the first generated bounding box for the query expression to measure the   accuracy.   Results   Table 3 reports referring comprehension results on RefCOCO [YPY+16], RefCOCO+ [YPY+16] and RefCOCOg [MHT+15]. KOSMOS-2 also obtains promising zero-shot   performance on the comprehension task, significantly outperforming previous zero-shot models on   RefCOCOg benchmark. However, compared to previous finetuned works, KOSMOS-2 achieves   slightly lower performance on RefCOCO and RefCOCO+ than on RefCOCOg. This discrepancy can   be attributed to the data distribution present in RefCOCO and RefCOCO+, where they tend to use a   shorter referring expression (e.g., \u201cleft bottom\u201d) during the two-player game. Hence, one of our future   goals is to enhance MLLMs\u2019 ability to accurately understand more types of human expressions.   8   Grounded MLLM   <p> It </p> <box>    <loc627> <loc895>    </box> is    the front most cow to the    right of other cows.   (1) Zero-shot evaluation   (2) Few-shot evaluation   Grounded MLLM   <p> It </p> <box>    <loc627> <loc895>    </box> is    the front most cow to the    right of other cows.   <p> It </p>    <box> <loc261>    <loc1011>    </box> is the    giraffe in the    middle.   Figure 5: The input format of referring expression generation evaluation under (1) zero-shot and (2)   few-shot settings. The bounding boxes shown in the image are for visualization purposes.   4.2   Multimodal Referring   In addition to multimodal grounding tasks, we evaluate the model\u2019s ability to understand image   regions or objects users refer to via inputting bounding boxes. Compared with previous multimodal   LLMs that can only refer image regions or objects to the model via detailed text descriptions, directly   referring to image regions using its bounding boxes is more effective and reduces ambiguity.   We evaluate the model on the referring expression generation task, which aims to generate unambiguous text descriptions of specific objects or regions within the bounding box. We employ the widely   used RefCOCOg dataset [MHT+15] to evaluate the model\u2019s performance under both zero-shot and   few-shot settings, showcasing its adaptability in different scenarios.   4.2.1   Evaluation Setup   The model is tasked with generating an associated text description for an object or region given its   location tokens of the bounding boxes (e.g., \u201c<box><loc1><loc2></box>\u201d). Benefiting from the   unified input format, we use \u201c<p> It </p><box><loc1><loc2></box> is\u201d as prompt to encourage   the model to predict its text description. Figure 5 (1) and (2) demonstrate the input format for   zero-shot and few-shot referring expression generation, respectively. Following previous works, we   report results using METEOR and CIDEr metrics. The image resolution is 224\u00d7224. Greedy search   is used for decoding.   4.2.2   Results   Table 4 presents the zero-shot and few-shot results of referring expression generation on RefCOCOg.   We compare KOSMOS-2 with a finetuned listener-speaker model, which introduces an added rewardbased module (SLR). Our model obtains impressive zero-shot performance on referring expression   generation, and even outperforms finetuned SLR by 1.1 CIDEr scores. Moreover, when prompted   with fewshot demonstrations, KOSMOS-2 shows further improvements, highlighting its in-context   learning ability.   Model   Setting   RefCOCOg   Meteor   CIDEr   SLR[YTBB17]   Finetuning   15.4   59.2   SLR+Rerank[YTBB17]   Finetuning   15.9   66.2   KOSMOS-2   Zero-shot   12.2   60.3   Few-shot (k = 2)   13.8   62.2   Few-shot (k = 4)   14.1   62.3   Table 4: Results of referring expression generation on RefCOCOg.   9   4.3   Perception-Language Tasks   In addition to multimodal grounding and referring tasks, we also evaluate KOSMOS-2 on the visionlanguage tasks following KOSMOS-1. In particular, we perform zero-shot evaluations on two popular   tasks, including image captioning and visual question answering. Image captioning requires the   model to generate a text description of the given image, whereas visual question answering seeks   to answer a natural language question based on an image. In order to have a fair comparison with   KOSMOS-1, we report results without instruction tuning.   4.3.1   Evaluation Setup   For image captioning, we evaluate the model on the widely used Flickr30k Karpathy split test set.   We employ beam search for caption generation, with a beam size of 5. We report results using   CIDEr [VLZP15] metrics evaluated by COCOEvalCap3. We use the prompt \u201cAn image of\u201d to   generate the image description.   For visual question-answering, we evaluate zero-shot performance on the test-dev set of VQAv2.   Greedy search is used for decoding. We report VQA scores obtained from VQAv2 evaluation server4.   \u201cQuestion: {question} Answer: {answer}\u201d is used as the prompt for the dataset. The image resolution   is 224\u00d7224 for both two tasks.   4.3.2   Results   We present the zero-shot performance on Flickr30k and VQAv2 in Table 5. KOSMOS-2 exhibites   comparable overall performance to the KOSMOS-1, showing a slight improvement on Flickr30k   while experiencing a marginal decrease on VQA. While KOSMOS-2 introduces new capabilities of   grounding and referring, the model still achieves competitive performance on perception-language   tasks.   Model   Flickr30k   VQAv2   CIDEr   VQA acc.   FewVLM [JCS+22]   31.0   METALM [HSD+22]   43.4   41.1   Flamingo-3B [ADL+22]   60.6   49.2   Flamingo-9B [ADL+22]   61.5   51.8   KOSMOS-1   65.2   46.7   KOSMOS-2   66.7   45.6   Table 5: Zero-shot image captioning results on Flickr30k test set and zero-shot visual question   answering results on VQAv2 test-dev set. We report results of KOSMOS-2 and KOSMOS-1 without   instruction tuning.   4.4   Language Tasks   We evaluate KOSMOS-2 on eight language tasks, such as cloze and completion tasks (StoryCloze,   HellaSwag), Winograd-style tasks (Winograd, Winogrande), commonsense reasoning (PIQA), and   three SuperGLUE benchmark [WPN+19] datasets (BoolQ, CB, and COPA). We report the zeroshot results in Table 6. Compared with KOSMOS-1, KOSMOS-2 achieves similar performance on   StoryCloze, HellaSwag, Winograd, Winogrande, and PIQA, experiences a decrease in performance   on CB, but shows improvement on BoolQ and COPA. In summary, KOSMOS-2 demonstrates the   acquisition of new capabilities while experiencing comparable performance on language tasks. This   illustrates the potential of the model in balancing and expanding its skills across different domains.   3https://github.com/salaniz/pycocoevalcap   4https://eval.ai/challenge/830/overview   10   Model   Story   Cloze   Hella   Swag   Winograd   Winogrande   PIQA   BoolQ   CB   COPA   LLM   72.9   50.4   71.6   56.7   73.2   56.4   39.3   68.0   KOSMOS-1   72.1   50.0   69.8   54.8   72.9   56.4   44.6   63.0   KOSMOS-2   72.0   49.4   69.1   55.6   72.9   62.0   30.4   67.0   Table 6: Zero-shot performance comparisons of language tasks between KOSMOS-2, KOSMOS-1   and LLM. LLM uses the same text data and training setup to reimplement a language model as   KOSMOS-1. We report results of KOSMOS-2 and KOSMOS-1 without instruction tuning. Results of   KOSMOS-1 and the LLM baseline are from [HDW+23].   5   Conclusion   We present KOSMOS-2, a multimodal large language modal, that can ground to the visual world.   Specifically, we pre-train KOSMOS-2 by augmenting the multimodal corpora used in KOSMOS-1   with GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created by extracting   and associating noun phrases and referring expressions in the caption to the objects or regions in   the scene. KOSMOS-2 enables new capabilities of perceiving image regions and grounding text   output to the visual world, which makes grounding as a foundation capability of MLLMs in many   downstream applications. Experimental results demonstrate that KOSMOS-2 achieves impressive   results on language and vision-language tasks evaluated in KOSMOS-1, grounding tasks including   phrase grounding and referring expression comprehension, and referring tasks such as referring   expression generation.   Acknowledgement   Some examples (such as Figure 1) are taken from the WHOOPS corpus [BGBH+23].   Ethics Statement   The model presented in this paper is intended for academic and research purposes. The utilization   of the model to create unsuitable material is strictly forbidden and not endorsed by this work. The   accountability for any improper or unacceptable application of the model rests exclusively with the   individuals who generated such content. We also put Microsoft AI Principles5 into practice when   developing the models.", "conf": "ICLR", "year": "2024", "index": 221}, {"title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics   Volume 1: Long Papers, pages 8707 - 8717   May 22-27, 2022 c\u20dd2022 Association for Computational Linguistics   End-to-End Modeling via Information Tree for One-Shot Natural   Language Spatial Video Grounding   Mengze Li1, Tianbao Wang1, Haoyu Zhang1, Shengyu Zhang1, Zhou Zhao2,3\u2217,   Jiaxu Miao1\u2217, Wenqiao Zhang1\u2217, Wenming Tan4, Jin Wang4, Peng Wang5,   Shiliang Pu4, Fei Wu2,3\u2217   1Zhejiang University 2Shanghai Institute for Advanced Study of Zhejiang University   3Shanghai AI Laboratory 4Hikvision Research Institute   5Northwestern Polytechnical University", "abstract": "Natural language spatial video grounding aims   to detect the relevant objects in video frames   with descriptive sentences as the query.   In   spite of the great advances, most existing methods rely on dense video frame annotations,   which require a tremendous amount of human effort.   To achieve effective grounding   under a limited annotation budget, we investigate one-shot video grounding, and learn to   ground natural language in all video frames   with solely one frame labeled, in an end-toend manner. One major challenge of end-toend one-shot video grounding is the existence   of videos frames that are either irrelevant to   the language query or the labeled frames. Another challenge relates to the limited supervision, which might result in ineffective representation learning.   To address these challenges, we designed an end-to-end model via   Information Tree for One-Shot video grounding (IT-OS). Its key module, the information   tree, can eliminate the interference of irrelevant frames based on branch search and branch   cropping techniques. In addition, several selfsupervised tasks are proposed based on the information tree to improve the representation   learning under insuf\ufb01cient labeling.   Experiments on the benchmark dataset demonstrate   the effectiveness of our model.   1", "content": "Natural language spatial video grounding is a vital task for video-text understanding (Luo and   Shakhnarovich, 2017; Zhou et al., 2019; Hu et al.,   2019; Zhang et al., 2020b; Li et al., 2021), which   aims to detect the objects described by the natural   First author.   mengzeli@zju.edu.cn   \u2217Corresponding author.   zhaozhou@zju.edu.cn   jiaxu.miao@yahoo   wenqiaozhang@zju.edu.cn   wufei@zju.edu.cn   Figure 1: An example of spatially grounding natural   language in video frames.   language query from each video frame, as shown   in Figure 1. There is a substantial and rapidlygrowing research literature studying this problem   with dense annotations (Li et al., 2017; Yamaguchi   et al., 2017; Sadhu et al., 2020), where each frame   that contains objects relevant to the language query   will be manually labeled with bounding boxes. Obviously, such annotations require tremendous human effort and can hardly be satis\ufb01ed in real-world   scenarios. Recently, some works have investigated   weakly-supervised video grounding with solely the   video-text correspondence rather than object-text   annotations (Huang et al., 2018; Chen et al., 2019a;   Shi et al., 2019; Chen et al., 2019b; Zhou et al.,   2018). However, the performance is less satis\ufb01ed   with such weak supervision. In practice, we are   more likely to have a limited annotation budget   rather than full annotation or no annotation. In   addition, as humans, after experiencing the language query and one frame object paired together   for the \ufb01rst time, we have the ability to generalize   this \ufb01nding and identify objects from more frames.   Towards this end, we investigate another practical   problem setting, i.e., one-shot spatial video grounding, with solely one relevant frame in the video   labeled with bounding boxes per video.   Existing methods that are devised for supervised   video grounding are not directly applicable to this   novel setting. We summarize several critical challenges:   \u2022 On the one hand, most of them incorporate a   multi-stage training process, i.e., \ufb01rstly training   a clip localization module, and training an object   8707   localization module in the second stage. However, in one-shot spatial video grounding, there   are no temporal annotations, which indicate the   start/end time of the relevant clip, to train the   clip localization module. Moreover, many of   them extract video features in a pre-processed   manner using feature extractor or object detector pretrained on large-scale datasets. However,   independent modeling limits the cooperation of   different modules, especially when the labels are   few. Therefore, it is in urgent need to derive   an end-to-end training framework for one-shot   spatial video grounding.   \u2022 On the other hand, there are video frames that are   either irrelevant to the natural language query or   the labeled frames. These irrelevant frames might   increase the computation complexity of end-toend training, and bring confounding between the   frame label and (irrelevant) visual features.   \u2022 Lastly, with fewer supervision signals, deep representation learning might become error-prone   or easily under-\ufb01tting, especially for end-to-end   training.   To address these challenges, we devise an endto-end model via the Information Tree for the One   Shot natural language spatial video grounding (ITOS). Different from previous works, we design a   novel tree structure to shield off the one-shot learning from frames that are irrelevant to either the   language query or the labeled frame. We devise   several self-supervised tasks based on the tree structure to strengthen the representation learning under   limited supervision signals. Speci\ufb01cally, the calculation processes of the key module, information   tree, contains four steps: (1) To construct the information tree, we view video frame features as   nodes, and then compress the adjacent nodes to   non-leaf nodes based on the visual similarity of   themselves and the semantic similarity with the   language query; (2) We search the information tree   and select branch paths that are consistently relevant to the language query both in the abstractive   non-leaf node level and in the \ufb01ne-grained leaf   node level; (3) We drop I) the leaf nodes that do   not belong the same semantic unit with the labeled   node; and II) the non-leaf nodes on the low relevance branch paths. We also down-weight the   importance of the leaf nodes that belong to the   same semantic unit with the labeled node but are   on the low relevance paths; (4) Finally, we input   the extracted and weighted information to the transformer, and conduct training with the one-shot label   and self-supervised tasks, including masked feature   prediction and video-text matching. We note that   both the information tree and the transformer are   jointly trained in an end-to-end manner.   We conduct experiments on two benchmark   datasets, which demonstrate the effectiveness of   IT-OS over state-of-the-arts. Extensive analysis   including ablation studies and case studies jointly   demonstrate the merits of IT-OS on one-shot video   grounding. Our contributions can be summarized   as follows:   \u2022 To the best of our knowledge, we take the initiative to investigate one-shot natural language   spatial video grounding. We design an end-toend model named IT-OS via information tree to   address the challenges brought by limited labels.   \u2022 By leveraging the language query, several novel   modules on the information tree, such as tree   construction, branch search, and branch cropping, are proposed.   Moreover, to strengthen   the deep representation learning under limited   supervision signals, we introduce several selfsupervised tasks based on the information tree.   \u2022 We experiment with our IT-OS model on two   benchmark datasets. Comparisons with the stateof-the-art and extensive model analysis jointly   demonstrate the effectiveness of IT-OS.   2   Related works   Natural Language Video Grounding.   Among   numerous   multimedia   understanding   applications (Zhang et al., 2020a,c, 2021d,c, 2020d; Kai   et al., 2021; Zhang et al., 2020e), natural language   video grounding has attracted the attention of more   and more researchers recently. There are mainly   three branches, temporal grounding[(Ross et al.,   2018; Lu et al., 2019; Zhang et al., 2019; Lin et al.,   2020a,b; Zhang et al., 2021a; Li et al., 2022; Gao   et al., 2021; Yang et al., 2021)], spatio-temporal   grounding[(Tang et al., 2021; Zhang et al., 2020f,g;   Su et al., 2021)], and spatial grounding. We focus   on the last one.   Deep neural network has convincingly demonstrated high capability in many domains (Wu et al.,   2020, 2022; Guo et al., 2021; Li et al., 2020b,c,a),   especially for video related tasks (Miao et al., 2021;   Miao et al.; Xiao et al., 2020, 2021), like video   8708   grounding. For example,(Li et al., 2017) use the   neural network to detect language query related objects in the \ufb01rst frame and track the detected object   in the whole video. Compared to it, (Yamaguchi   et al., 2017) and (Vasudevan et al., 2018) go further.   They extract all the object proposals through the   pretrained detector, and choose the right proposal   described in the text.   Supervised training for the natural language   video object detection needs high labeling costs.   To reduce it, some researchers pay attention to   weakly-supervised learning fashion using multiple instances learning(MIL) method (Huang et al.,   2018; Chen et al., 2019a; Shi et al., 2019; Chen   et al., 2019b; Zhou et al., 2018; Wang et al.,   2021a)transfers contextualized knowledge in crossmodal alignment to release the unstable training   problem in MIL. Based on contrastive learning   (Zhang et al., 2022), (Da et al., 2021) proposes an   AsyNCE loss to disentangle false-positive frames   in MIL, which allows for mitigating the uncertainty   of from negative instance-sentence pairs. Weakly   supervised false-positive identi\ufb01cation based on   contrastive learning has witnessed success in some   other domains (Zhang et al., 2021b; Yao et al.,   2022)   One-shot Learning for Videos. One-shot learning has been applied in some other video tasks.   (Yang et al., 2018) proposes a meta-learning-based   approach to perform one-shot action localization   by capturing task-speci\ufb01c prior knowledge. (Wu   et al., 2018) investigates the one-shot video person   re-identi\ufb01cation task by progressively improving   the discriminative capability of CNN via stepwise   learning. Different from these works, (Caelles et al.,   2017) and (Meinhardt and Leal-Taix\u00e9, 2020) de\ufb01ne   the one-shot learning as only one frame being labeled per video. Speci\ufb01cally, (Caelles et al., 2017)   use a fully convolutional neural network architecture to solve the one-shot video segmentation task.   (Meinhardt and Leal-Taix\u00e9, 2020) decouple the detection task, and uses the modi\ufb01ed Mask-RCNN   to predict local segmentation masks. Following   this setting, we investigate one-shot natural language spatial video grounding, and devise a novel   information-tree based end-to-end framework for   the task.   3   Method   3.1   Model Overview   Problem Formulation.   Given a video V   =   {vi}i=1,2,...,I and a natural language query C, spatial video grounding aims to localize the querydescribed object from all the objects Oi   =   {oi   j}j=1,2,...,J for each frame. I denotes the frame   number of the video, and the J is the object number   in the video. In one-shot spatial video grounding,   solely one frame vi in video V is labeled with the   region boxes of the target objects Oi.   Pipeline of IT-OS. As shown in Figure 2, there   are mainly four steps involved in the end-to-end   modeling of IT-OS:   \u2022 Firstly, we extract the features from the input   video and the input caption. Speci\ufb01cally, for the   video, we use ResNet-101(He et al., 2016) as the   image encoder to extract the frame feature maps;   for the language query, we employ a language   model Roberta(Liu et al., 2019). Both the vision   encoder and the language encoder are jointly optimized with the whole network.   \u2022 Secondly, we build the information tree to get   the representation of the video. The information   tree is built upon the frame feature maps, which   are the leaf nodes. Leaf nodes will be further   merged based on the relevance between nodenode and node-query to have non-leaf and root   nodes. Nodes on unnecessary branches will be   deleted conditioned on the language query.   \u2022 Thirdly, we utilize the transformer encoder to   reason on the remaining nodes and language features. Upon the transformer, we devise two selfsupervised tasks, i.e., masked feature modeling,   and video-text matching, which enhances the representation learning under limited labels.   Prediction and Training. We follow the common   prediction and training protocol of visual transformers used in other object detection models (Wang   et al., 2021b). We input the embedding parameters   Ede and the multi-model features Fde generated   by the transformer encoder into the transformer   decoder D. Then, the decoder D outputs possible   prediction region features for each frame. For each   possible region, a possibility P and a bounding box   B are generated.   P, B = D(Fde, Ede),   (1)   8709   \u2026 \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   Tree Construction   Merge or not?   Multi-steps   Branch Search & Cropping   Cropped   Selected   query-related   video frames   query-unrelated   down-weighted   language query tokens   + Position/Type Embeddings   Taks1: Maksed Prediction   Taks2: Video-Text Matching   Taks3:    One-shot Grounding   [Mask]   [Mask]   [Mask]   [Mask]   [CLS]   Information Tree   Multi-layer Transformer Encoder   ResNet-101   Labeled Frame   90%   10%   IsMatch?   Figure 2: The overall schema of the proposed end-to-end one-shot video grounding via information tree (IT-OS),   which contains query-guided tree construction, query-based branch search & cropping, and a transformer encoder   enhanced by self-supervised tasks.   We choose the box B with the highest possibility   value P for each frame as the target box.   During the training process, we \ufb01rst calculate the   possible prediction regions. Then, we match the   possible regions with the target boxes, and choose   the best match for each frame. Finally, use the   match to train our IT-OS model.   3.2   Information Tree Module   In this section, we will elaborate the information   tree modules in detail. We will illustrate how to   construct the information tree, how to extract critical information from it and how to design the selfsupervised learning based on the tree. To ease the   illustration, we take the 6 frames as an example,   and show the process in Figure 2.   3.2.1   Tree Construction   Given the frame features generated by the CNN,   we build the information tree by merging adjacent   frame features in the speci\ufb01ed order. Speci\ufb01cally,   the frame features output by the image encoder are   the leaf nodes N = {ni}2M   i=1. A sliding window   of size 2 and step 2 is applied on these nodes and   nodes in the window are evaluated to be merged or   not.   We calculate the semantic relevance difference   between each node pair with the language query,   and get the visual relevance between the nodes   in each pair. For the visual relevance calculation,   we max-pool the feature maps of the i node pair to   have the feature vector f2i\u22121   v   and f2i   v . And then, we   compute the cosine similarity ri   vv between f2i\u22121   v   and f2i   v to be the visual relevance. Next, we calculate the semantic relevance r2i\u22121   tv   and r2i   tv between   the text feature ft and the nodes of i node pair:   r2i\u22121   tv   = \u03c3((wt \u2217ft) \u2217(wv \u2217f2i\u22121   v   )T ),   (2)   r2i   tv = \u03c3((wt \u2217ft) \u2217(wv \u2217f2i   v )T ),   (3)   where the wt and wv are learnable parameters, and   \u03c3 is the sigmoid activation function.   The semantic relevance difference di   tv between   the ith paired nodes is:   di   tv = |r2i\u22121   tv   \u2212r2i   tv| + \u03b3 \u2217ri   vv,   (4)   where the \u03b3 is the hyperparameter.   8710   With the relevant difference value, we rank the   node pairs and pick out the top \u03bb. The \u03bb is a hyperparameter, which can be set as a constant or a   percentage. We merge the node pairs:   nnew = wmg \u2217(n2i\u22121 + n2i) + bmg,   (5)   where the wmg and bmg are trainable. Finally, The   new node nnew replace the old nodes n2i\u22121 and   n2i in the queue. Repeat the process until there is   only one node in the queue. Saving all nodes in   the process and the composite relationship between   nodes generated in the merging process, we get the   information tree.   3.2.2   Branch Search   We use a branch to denote a subtree. To \ufb01lter   critical local and global information, we perform   branch search and selection.   We \ufb01rstly select   branches that contain leaf nodes less than \u03b4max   and more than \u03b4min. \u03b4max and \u03b4min are hyperparameters. We calculate the semantic relevance of   branches\u2019 root nodes and the language query based   on Equation 2.   Training. During training, we directly select the   branch that contains the labeled leaf node and the   root node with the highest semantic relevance. This   selection improves the training ef\ufb01ciency.   Inference.   During inference, all frames should   be processed. We conduct an iterative search with   multiple search steps. For each step, we select the   branch with the highest semantic relevance and   remove the selected branch from the information   tree. After the search, we have multiple selected   branches and each branch will be forwarded to the   following processes.   3.2.3   Branch Cropping   Note that not all the non-leaf nodes in the selected   branches are closely related to the input caption.   We remove non-leaf nodes that are with semantic   relevance less than \u2206, which is a hyperparameter.   Their descendant non-leaf nodes are also removed.   To reserve enough frame nodes for training, we do   not remove the descendant leaf nodes. Instead, we   down-weight them with \u03bb = 0.5. For other leaf   nodes, \u03bb = 1. The remaining leaf nodes and nonleaf nodes represent the critical local information   and the global information, respectively. We multiply the feature of node i and the node\u2019s semantic   relevance ri   tv:   fi   vnew = fi   v \u2217ri   tv \u2217\u03bb,   (6)   where fi   vnew is the feature vector input into the   transformer. As such, Equation 6 considers both   local relevance rtv and global relevance \u03bb with the   language query.   3.2.4   Self-supervised Tasks   We leverage a transformer encoder for these extracted information and the language query. As   shown in the Figure 2, we design two selfsupervised tasks as: 1) predicting the masked text   features, and masked local/global video information; 2) judging whether the text and the video   match. For the transformer, the input tokens Fin   consist of the local information, the global information and the text features, which are three types   of tokens. We further introduce 2-D position embedding for video tokens and type embedding for   all tokens, which are added to the tokens\u2019 features.   Then, the features Fin are input into the transformer encoder E. After encoding, the fusion features Fout are output:   Fout = E(Fin).   (7)   We predict the original features for masked language tokens and masked video tokens (leaf/nonleaf nodes in the selected branch) using multilayer   perceptrons.   \u02c6fi   in = MLPt(fi   out),   \u02c6fj   in = MLPv(fj   out),   (8)   where the MLPt and MLPv are the multilayer perceptrons for text and video features, respectively.   We view masked token modeling as feature regression and adopt L2 distance as the loss function.   In addition, there will be a mismatched language   query at the rate of 50%. We propose to predict   whether the video and language are matched, i.e.,   whether the video contains the event described by   the language query, based on the output representation of token [CLS]. When the video and the   language are not matched, we will not train the   model with the one-shot label.   4   Experiments   4.1   Experimental Setup   Datasets We consider two video grounding benchmarks for evaluation: (1) VidSTG (Zhang et al.,   2020g) is a large-scale benchmark dataset for   video grounding, which is constructed based on   VidOR (Shang et al., 2019) dataset. VidSTG contains 10, 000 videos and 99, 943 sentences with   8711   Method   Declarative Sentence Grounding   Interrogative Sentence Grounding   0.4   0.5   0.6   Avg   0.4   0.5   0.6   Avg   GroundeR   24.56   18.22   13.73   18.85   25.28   18.87   14.39   19.52   STPR   25.68   20.07   14.64   19.89   27.09   21.04   16.00   21.38   STGRN   27.57   20.91   16.25   21.50   28.51   21.89   17.20   22.47   VOGnet   32.08   24.38   19.91   25.75   33.08   25.54   20.85   26.72   OMRN   34.43   27.57   21.91   27.96   35.69   28.74   23.03   29.14   VOGnet*   36.42   29.37   21.95   29.25   36.98   28.35   22.57   29.30   OMRN*   39.54   30.02   22.34   30.64   38.89   30.53   24.10   31.17   IT-OS   46.75   35.81   23.23   35.26   46.16   34.55   25.19   35.30   Table 1: Compared with baselines on VidSTVG. It is worth noting that all methods are trained using the one-shot   learning. The \u2217represents the baselines use the MDETR as the object detector backbone, which is the same as the   IT-OS.   Method   0.4   0.5   0.6   Avg   GroundeR   32.09   27.80   24.25   28.05   STPR   33.40   28.92   25.37   29.23   STGRN   35.45   30.41   26.31   30.72   VOGnet   38.81   32.65   26.87   32.78   OMRN   40.11   34.51   28.36   34.35   VOGnet*   41.23   35.82   29.48   35.51   OMRN*   45.52   37.69   30.41   37.87   IT-OS   51.87   42.91   33.58   42.79   Table 2:   Compared with baselines on VID-sentence.   All methods are trained using one-shot learning. The \u2217   represents the MDETR is applied to these baselines as   the object detector backbone.   55, 135 interrogative sentences and 44, 808 declarative sentences.   These sentences describe 79   types of objects appearing in the videos.   We   follow the of\ufb01cial dataset split of (Zhang et al.,   2020g). (2) VID-sentence (Chen et al., 2019b) is   another widely used video grounding benchmark   constructed based on the VID (Russakovsky et al.,   2015) dataset. There are 30 categories and 7, 654   video clips in this dataset. We report the results   of all methods on the validation set for the VIDsentence dataset. We obtain similar observations   and conclusions on the test set.   Implementation Detail For video preprocessing,   we random resize the frames, and set the max size   is 640\u2217640. The other data augmentation methods,   such as random horizontal \ufb02ip and random size   cropping are used at the same time. During training, the learning rate is by default 0.00005, and   decays by a factor of 10 for every 35 epochs. The   batch size is 1 and the maximum training epoch   is 100. We implement IT-OS in Pytorch and train   it on a Linux server. For model hyperparameters,   we set \u03bb = 60%, and \u2206= 0.7. Most of the natural language spatial video grounding models use   the pretrained detection model as the backbone.   Thus, like them, we choose the of\ufb01cial pretrained   MDETR (Kamath et al., 2021) as the parameter   basis for target detection of our IT-OS.   Evaluation Metrics We follow the evaluation protocol of (Chen et al., 2019b). Speci\ufb01cally, we   compute the Intersection over Union (IoU) metric for the predicted spatial bounding box and the   ground-truth per frame. The prediction for a video   is considered as \"accurate\" if the average IoU of all   frames exceeds a threshold \u03b1. The \u03b1 is set to 0.4,   0.5, and 0.6 during testing.   Baselines Since existing video grounding methods   are not directly applicable to the one-shot setting,   we extend several state-of-the-arts as the baselines.   Speci\ufb01cally, to have a comprehensive comparison, we consider 1)fully supervised models, including VOGnet (Sadhu et al., 2020), OMRN (Zhang   et al., 2020f) and STGRN (Zhang et al., 2020g);   and 2) other widely known methods, including   video person grounding STPR (Yamaguchi et al.,   2017), and visual grounding method, GroundeR   (Rohrbach et al., 2016).   4.2   Performance Comparison   The experimental results for one-shot video grounding on VidSTVG and VID-sentence datasets are   shown in Table 1 and 2, respectively. According to   the results, we have the following observations:   \u2022 Not surprisingly, although extended to the video   grounding setting, baselines that belong to other   domains, including video person grounding   STPR and visual grounding GroundeR, achieve   8712   inferior results on video grounding benchmarks.   They lack domain-speci\ufb01c knowledge and might   fail to effectively model the spatial-temporal relationships of videos and language queries.   \u2022 IT-OS consistently achieves the best performance   on two benchmarks and multiple experimental   settings with a large margin improvement. Remarkably, IT-OS boosts the performance (Avg)   of the previous state-of-the-art OMRN from   nearly 28.0/29.1/34.4 to 35.3/35.3/42.8 on   VidSTVG and VID-sentence, respectively. It   demonstrates the superiority of IT-OS on oneshot video grounding.   \u2022 The baselines are implemented with the backbones used in their original papers, which   are different from ours.   To further disentangle the sources of performance improvement,   we re-implement the best-performing baselines   (VOGnet*, and OMRN*) with the same object detection backbone, MDETR, as IT-OS. Although there is performance improvement with   the new backbone, the best-performing baseline   OMRN*, still underperforms IT-OS by over 4   points for the average accuracy on all datasets.   It further reveals the effectiveness of our novel   model designs eliminating interference with different pre-training parameters. We attribute the   improvement to the end-to-end modeling, where   different modules can simultaneously bene\ufb01t   from each other. In addition, the proposed information tree alleviates the negative effects of irrelevant frames, and effectively models the interactions between the video global/local information   and the language query. Several self-supervised   learning tasks based on the information tree enhance the representation learning under limited   one-shot labels.   4.3   Comparison with Fully Supervised   Methods   We are interested in 1) how different baselines perform under fully supervised settings; 2) how oneshot IT-OS perform compared to these baselines.   Towards this end, we train multiple baselines and   IT-OS with all labels on the VID-sentence dataset.   The experiment results are shown in Table 3. From   the table, we have the following \ufb01ndings:   \u2022 Remarkably, the performance gap between oneshot IT-OS and the fully supervised OMRN is   Method   0.4   0.5   0.6   Avg   GroundeR   42.72   33.77   27.05   34.51   STPR   47.95   36.19   30.41   38.18   STGRN   49.25   44.03   34.89   42.72   VOGnet   53.17   43.47   33.77   43.47   OMRN   55.22   46.64   37.50   46.45   IT-OS (OS)   51.87   42.91   33.58   42.79   Table 3:   Compared with the baselines on VIDsentence. The baselines are trained using fully supervised learning. The OS represents the IT-OS trained   under the one-shot settings.   less than 4%. Such a minor gap demonstrates   the effectiveness of IT-OS on learning with limited annotations. This is signi\ufb01cant and practical   merit since we are more likely to have a limited   annotation budget in real-world applications.   \u2022 Surprisingly, one-shot IT-OS can still outperform some weak baselines such as GroundeR   and STPR. These results reveal the necessity of   end-to-end modeling for video grounding.   4.4   Ablation Study   We are interested in how different building blocks   contribute to the effectiveness of IT-OS. To this end,   we surgically remove several components from   IT-OS and construct different architectures. The   investigated components include information tree   (\u0393tree), the branch cropping (\u0393crop), and the selfsupervised training (\u0393self). It is worth noting that   the other components cannot be deleted independently except the branch cropping. Thus, we don\u2019t   conduct an ablation study for them. Results on   VidSTG and VID-sentence datasets are shown in   Table 4 and Table 5, respectively. There are several   observations:   \u2022 Overall, removing any component incurs a performance drop, demonstrating the necessity and effectiveness of the information tree, branch search   & cropping, and self-supervised training.   \u2022 Stacking multiple components outperform the   architecture with a single component. This result   reveals that the proposed components can bene\ufb01t   from each other in end-to-end training and jointly   boost one-shot video grounding.   4.5   Case Study   We conduct a case study to visually reveal the ability of the IT-OS in detail. Speci\ufb01cally, we random   8713   Declarative Sentence Grounding   Interrogative Sentence Grounding   \u0393self   \u0393tree   \u0393crop   0.4   0.5   0.6   Avg   0.4   0.5   0.6   Avg   39.00   30.52   17.61   29.05   38.78   28.75   19.67   29.07   \u2713   40.52   32.32   18.83   30.56   40.82   31.44   20.66   30.97   \u2713   42.34   32.65   20.35   31.78   42.26   32.02   21.89   32.06   \u2713   \u2713   44.16   33.38   21.11   32.89   44.55   33.78   23.19   33.84   \u2713   \u2713   44.77   34.62   22.93   34.11   44.30   33.23   24.17   33.90   \u2713   \u2713   \u2713   46.75   35.81   23.23   35.26   46.16   34.55   25.19   35.30   Table 4: Ablation study on VidSTG dataset.   \u0393self \u0393tree \u0393crop   0.4   0.5   0.6   Avg   44.40 35.07 27.24 35.57   \u2713   46.64 36.38 28.54 37.19   \u2713   47.95 38.99 29.85 38.93   \u2713   \u2713   49.44 40.30 31.16 40.30   \u2713   \u2713   50.19 40.49 32.46 41.04   \u2713   \u2713   \u2713   51.87 42.91 33.58 42.79   Table 5: Ablation study on VID-sentence dataset.   sample 3 videos from the datasets, and sample 6   frames from each video to visualize.   We compare our IT-OS model with the baseline method, OMRN, and the fundamental ablation   model of the IT-OS, which is removed from the   self-supervised module and the information tree.   As shown in Figure 3, we have the following key   \ufb01ndings: (1) The IT-OS detects the more accurate one from all objects of the video than the best   performing previous method. It demonstrates the   better representation extraction and analysis capabilities of our model. (2) Even if the target object   is selected correctly, the IT-OS localizes a more   precise spatial area compared with the previous   two stages method. The results re\ufb02ect the endto-end model, IT-OS, has more accurate domain   knowledge through training the whole model on   the target dataset. (3) After adding the information tree and the self-supervised module, the IT-OS   outputs more precise bounding boxes. It reveals   that combining the two modules introduce stronger   supervision signals for model training so that the   model has stronger detection ability.   5   Conclusion   In this paper, we introduce the one-shot learning   into the natural language spatial video grounding   task to reduce the labeling cost. To achieve the   goal, the main point is to make full use of only   one frame label for each video. The invalid frames   Figure 3: Examples of the detection result visualization.   The IT-OS(Base) represents the IT-OS model without   the self supervised module and the informaiton tree.   The GT represents the target labels.   unrelated to the input text and target objects bring   confounding to the one-shot training process. We   design an end-to-end model (IT-OS) via the information tree to avoid it. Speci\ufb01cally, the information   tree module merges frames with similar semantics   into one node. Then, by searching the tree and   cropping the invalid nodes, we can get the complete and valid semantic unit of the video. Finally,   two self-supervised tasks are used to make up the   insuf\ufb01cient supervision.   8714   Acknowledgements   This work is supported in part by the National   Natural Science Foundation of China (Grant   No.62037001, No.61836002, No.62072397). This   work is also partially funded by Hangzhou Hikvision Digital Technology.", "conf": "ACL", "year": "2022", "index": 331}, {"title": "SeqTR: A Simple yet Universal Network for   Visual Grounding   Chaoyang Zhu1, Yiyi Zhou1, Yunhang Shen3, Gen Luo1, Xingjia Pan3,   Mingbao Lin3, Chao Chen3, Liujuan Cao1\u2217, Xiaoshuai Sun1,4, Rongrong Ji1,2,4   1MAC Lab, Department of Artificial Intelligence, School of Informatics, Xiamen   University.   2Institute of Energy Research, Jiangxi Academy of Sciences.   3Tencent Youtu Lab.   4Institute of Artificial Intelligence, Xiamen University.   cyzhu@stu.xmu.edu.cn, zhouyiyi@xmu.edu.cn, shenyunhang01@gmail.com,   luogen@stu.xmu.edu.cn, xjia.pan@gmail.com, linmb001@outlook.com,   aaronccchen@tencent.com, {caoliujuan,xssun,rrji}@xmu.edu.cn", "abstract": ". In this paper, we propose a simple yet universal network   termed SeqTR for visual grounding tasks, e.g., phrase localization, referring expression comprehension (REC) and segmentation (RES). The   canonical paradigms for visual grounding often require substantial expertise in designing network architectures and loss functions, making them   hard to generalize across tasks. To simplify and unify the modeling, we   cast visual grounding as a point prediction problem conditioned on image and text inputs, where either the bounding box or binary mask   is represented as a sequence of discrete coordinate tokens. Under this   paradigm, visual grounding tasks are unified in our SeqTR network without task-specific branches or heads, e.g., the convolutional mask decoder   for RES, which greatly reduces the complexity of multi-task modeling.   In addition, SeqTR also shares the same optimization objective for all   tasks with a simple cross-entropy loss, further reducing the complexity   of deploying hand-crafted loss functions. Experiments on five benchmark   datasets demonstrate that the proposed SeqTR outperforms (or is on par   with) the existing state-of-the-arts, proving that a simple yet universal   approach for visual grounding is indeed feasible. Source code is available   at https://github.com/sean-zhuh/SeqTR.   Keywords: Visual Grounding, Transformer   1", "content": "Visual grounding [55,34,36,21,52] has emerged as a core problem in visionlanguage research, as both comprehensive intra-modality understanding and   accurate one-to-one inter-modality correspondence establishment are required.   According to the manner of grounding, it can be divided into two groups, i.e.,   phrase localization or referring expression comprehension (REC) at bounding   \u2217Corresponding author.   2   C. Zhu et al.   Natural language query: A jockey in red and white riding a horse.   Bounding box: x!=87, y!=2, x\"=500, y\"=492.     Binary mask: x!=253, y!=2, x\"=123, y\"=23, \u2026 x#=376, y#=23.   Fig. 1. Illustration of the serialization of grounding information. Our model directly   generates the sequence of points representing the bounding box or binary mask.   box level [54,29,27,58,50,25,32,49,44,16,6,59,24], and referring expression segmentation (RES) at pixel level [54,51,2,15,18,28,32,10,48,31,19,8,24].   To accomplish the accurate vision-language alignment, existing approaches   often require substantial prior knowledge and expertise in designing network architectures and loss functions. For instance, MAttNet [54] decomposes language   expressions into subject, location, and relationship phrases, and designs three corresponding attention modules to compute matching score individually. Despite   being faster, one-stage models also require the complex language-guided multimodal fusion and reasoning modules [33,16,25,49,32], or sophisticated crossmodal alignment via various attention mechanisms [10,31,51,15,28,32,8]. Loss   functions in existing methods are also complex and tailored to each individual grounding task, such as GIoU loss [41], set-based matching loss [1], focal   loss [26], dice loss [35], and contrastive alignment loss [20]. Under a multi-task   setting, coefficients among different losses also need to be carefully tuned to   accommodate different tasks [32,24]. Despite great progress, these highly customized approaches still suffer from the limited generalization ability.   Recent endeavors [6,8,20,24] in visual grounding shift to simplifying network architectures via Transformers [45]. Concretely, the multi-modal fusion   and reasoning modules are replaced by a simple stack of transformer encoder   layers [6,20,8]. However, the loss function used in these transformer-based methods is still highly customized for each individual task [26,35,20,41,1]. Moreover,   these approaches still require task-specific branches or heads [32,24], i.e., the   bounding box regressor and convolutional mask decoder.   In this paper, we take a step forward in simplifying the modeling of visual   grounding tasks via a simple yet universal network termed SeqTR. Specifically,   inspired by the recently proposed Pix2Seq [3], we first reformulate visual grounding as a point prediction problem conditioned on image and text inputs, where   the grounding information, e.g., the bounding box, is serialized into a sequence   of discrete coordinate tokens. Under this paradigm, different grounding tasks   can be universally accomplished in the proposed SeqTR with a standard transformer encoder-decoder architecture [45]. In SeqTR, the encoder serves to update   the multi-modal feature representations, while the decoder directly predicts the   discrete coordinate tokens of the grounding information in an auto-regressive   manner. In terms of optimization, SeqTR only uses a simple cross-entropy loss   for all grounding tasks, requiring no further prior knowledge or expertise. OverSeqTR: A Simple yet Universal Network for Visual Grounding   3   all, the proposed SeqTR greatly reduces the difficulty and complexity of both   architecture design and optimization for visual grounding.   Notably, the proposed SeqTR is not just a simple multi-modal extension of   Pix2Seq for the challenging open-ended visual grounding tasks. In addition to   bridging the gap between object detection and visual grounding, we also apply   the sequential modeling to RES via an innovative mask contour sampling scheme.   As shown in Fig. 1, SeqTR transforms the pixel-wise binary mask into a sequence   of N points by performing clockwise sampling on the mask contour. In this case,   RES, as a language-guided segmentation task, can be seamlessly integrated into   the proposed SeqTR network without the additional convolutional mask decoder,   demonstrating the high generalization ability of SeqTR across grounding tasks.   The proposed SeqTR achieves or is on par with the state-of-the-art performance on five benchmark datasets, i.e., RefCOCO [55], RefCOCO+ [55],   RefCOCOg [34,36], ReferItGame [21], and Flickr30K Entities [37]. SeqTR also   outperforms a set of large-scale BERT-style models [30,43,4,20] with much less   pre-training expenditure. Main contributions are summarized as follows:   \u2013 We reformulate visual grounding tasks as a point prediction problem, and   present a novel and general network, termed SeqTR, which unifies different   grounding tasks in one model with the same cross-entropy loss.   \u2013 The proposed SeqTR is simple yet universal, and can be seamlessly extended   to the referring expression segmentation task via an innovative mask contour   sampling scheme without network architecture modifications.   \u2013 We achieve or maintain on par with the state-of-the-art performance on five   visual grounding benchmark datasets, and also outperform a set of largescale pre-trained models with much less expenditure.   2   Related Work   2.1   Referring Expression Comprehension   Early practitioners [14,57,60,54,29,47,13,27] tackle referring expression comprehension (REC) following a two-stage pipeline, where region proposals [40] are   first extracted then ranked according to their similarity scores with the language query. Another line of work [58,50,25,32,49,44,16,6,59], being simpler and   faster, advocates one-stage pipeline based on dense anchors [40]. RealGIN [58]   proposes adaptive feature selection and global attentive reasoning unit to handle   the diversity and complexity of language expressions. ReSC [49] recursively constructs sub-queries to predict the parameters of the normalization layers in the   visual encoder, which is used to scale and shift visual features. LBYL [16] designs landmark feature convolution to encode the contextual information. Recent   works [6,59,20,8,24] resort to Transformer-like structure [45] to perform multimodal fusion. MDETR [20] further demonstrates that Transformer is efficient   when pre-trained on a large corpus of data. Compared with existing approaches,   our work is simple in both the architecture and loss function, which has little   requirement of task priors and expert engineering.   4   C. Zhu et al.   2.2   Referring Expression Segmentation   Compared to REC, referring expression segmentation (RES) grounds language   query at a fine-granularity i.e., the precise pixel-wise binary mask. Typical solutions are to design various attention mechanisms to perform cross-modal alignment [31,32,8,51,10,2,15,17,28,18]. EFN [10] transforms the visual encoder into a   multi-modal feature extractor with asymmetric co-attention, which fuses multimodal information at the feature learning stage. CGAN [31] performs cascaded   attention reasoning with instance-level attention loss to supervise attention modeling at each stage. LTS [19] first performs relavance filtering to locate the referent, and uses this visual object prior to perform dilated convolution for the   final segmentation mask. VLT [8] produces a set of queries representing different   understandings of the language expression and proposes a query balance module   to focus on the most reasonable and suitable query, which is then used to decode   the mask via a mask decoder. In this work, we are the first to regard RES as a   point prediction problem, thus the proposed SeqTR can be seamlessly extended   to RES without any network architecture modifications.   2.3   Multi-task Visual Grounding   Multi-task visual grounding aims to jointly address REC and RES. Prior art   MCN [32] constrains the REC and RES branches to attend to the same region by applying consistent energy maximization. In this way, REC can help   RES better localize the referent, and RES can help REC achieve superior crossmodal alignment. RefTR [24] tackles multi-task visual grounding by sharing the   same transformer architecture, but it requires an additional convolutional mask   decoder for RES. In contrast, the proposed SeqTR is universal across different   grounding tasks without additional branch or head. Under the point prediction   paradigm, SeqTR can segment the referent without the aid from REC branch.   3   Method   In this section, we introduce our simple yet universal SeqTR network for visual   grounding, of which structure is depicted in Fig. 2. The objective function is   detailed in Sec. 3.1. Sequence construction from grounding information is elaborated in Sec. 3.2. The architecture and inference are presented in Sec. 3.3.   3.1   Problem Definition   Unlike existing visual grounding models [32,6,10,19,8], SeqTR aims to predict   the discrete coordinate tokens of the grounding information, e.g., the bounding   box or binary mask. To this end, we define the optimization objective under the   point prediction paradigm as:     \\   la   b   el    {e q:l oss} \\ma thcal {L} = -\\sum _{i=1}^{2N}w_{i}\\log P(T_{i}|F_m, S_{1:i-1}),    (1)   SeqTR: A Simple yet Universal Network for Visual Grounding   5   Fusion   \u22c5\u22c5\u22c5   \ud835\udc39! \u2208\ud835\udc45(#\u2217%) \u00d7 )   \ud835\udc53* \u2208\ud835\udc45)   Transformer   Encoder   \ud835\udc39+   \u22c5\u22c5\u22c5   \u22c5\u22c5\u22c5   Transformer    Decoder   x! y!   x\" y\"   TASK   Input sequence   \u22c5\u22c5\u22c5   x! y!   x\" y\" EOS   Target sequence   \u22c5\u22c5\u22c5   Fig. 2. Overview of the proposed SeqTR network, of which all components, i.e., multimodal fusion, cross-modal interaction, and loss function, are standard operations and   shared across grounding tasks.   where S and T are the input and target sequences for decoder as shown in Fig. 2.   Fm \u2208R(H\u2217W )\u00d7C is the multi-modal features detailed in Sec. 3.3. A per-token   weight wi is used to scale the loss. Note that the input sequence S1:i\u22121 only   contains the preceding coordinate tokens when predicting the i-th one. It can be   implemented by putting a causal mask [38] on attention weights to only attend   to previous coordinate tokens.   We construct the input sequence by prepending a [TASK] token before the   sequence of points {xi, yi}N   i=1, and the target sequence is the one appended   with an [EOS] token. These two special tokens indicate the start or end of the   sequence, which are learnable embeddings. [TASK] token also indicates which   grounding task the model performs on. To achieve multi-task visual grounding,   we can equip each task with the corresponding [TASK] token randomly initialized   with different parameters, showing great simplicity and generalization ability.   Under our point prediction reformulation, the simple cross-entropy loss conditioned on multi-modal features and preceding discrete coordinate tokens can   be directly shared across tasks, avoiding the complex deployment of hand-crafted   loss functions and loss coefficient tuning [35,26,1,20,41].   3.2   Sequence Construction from Grounding Information   A key design in SeqTR is to serialize and quantize the grounding information,   e.g., the bounding box or binary mask, into a sequence of discrete coordinate   tokens, which enables different grounding tasks to be universally addressed in   one network architecture with the same objective.   We first review the serialization and quantization of the bounding box introduced in Pix2Seq [3]. Given a sequence of floating points { \u02dcxi, \u02dcyi}N   i=1 representing   the top-left and bottom-right corner points of the bounding box (N is 2), these   floating coordinates are quantized into integer bins by     \\ label {eq   : q uan   ti z ation}  x_   i  = \\text {round}(\\frac {\\tilde {x_i}}{w} * M),\\qquad y_i = \\text {round}(\\frac {\\tilde {y_i}}{h} * M),    (2)   where each coordinate is normalized by image width w and height h, and M   is the number of quantization bins. We refer readers to Pix2Seq [3] for more   6   C. Zhu et al.   (a)    (b)    (c)    (d)    (e)    (f)    Fig. 3. Visualization of different sampling strategies. (a-b) are the original image and   ground-truth. (c-d) are the sampled points and reassembled mask of center-based sampling, respectively, while (e-f) are the ones of uniform sampling.   discretization details. In practice, we construct a shared embedding vocabulary   E \u2208RM\u00d7C for both x-axis and y-axis.   While bounding boxes can be naturally determined by two of its corner points   and serialized into a sequence as in Eq. 2, binary masks can not. A binary mask   consists of infinite points, of which both quantities and positions impact the   details of the mask significantly, thus the above serialization and quantization   for bounding boxes is not directly applicable to binary masks.   To address this issue, we propose an innovative mask contour sampling scheme   for the sequence construction from binary masks. As shown in Fig. 3, we sample   N points clockwise from the consecutive mask contour of the referred object,   then, the sequence of sampled points can be quantized via Eq. 2. Following   sampling strategies are experimented:   \u2013 Center-based sampling. Starting from the mass center of the binary mask,   N rays are emitted with the same angle interval. The intersection points   between these rays and the mask contour are clockwise sampled.   \u2013 Uniform sampling. We uniformly sample N points clockwise on top of the   mask contour, which is much simpler compared to the first strategy.   Compared to the center-based sampling, uniform sampling distributes the sampled points along the mask contour more evenly, and can better represent the   irregular mask especially when the outline between two adjacent sampled points   is tortuous. As shown in Fig. 3, center-based sampling loses the fine details of the   zebra legs, while uniform sampling preserves the mask contour more precisely.   In practice, the proposed sampling scheme slightly restricts the performance   upper-bound of RES, e.g., uniformly sampling 36 points from ground-truth   masks will achieve 95.63 mIoU on RefCOCO validation set. Considering current state-of-the-art performance, such a defect is still acceptable. Besides, even   if we take as ground-truth the precise binary mask, the upper-bound still will   not reach 100 mIoU since down-sampling operations are often necessary.   Both center-based and uniform sampling use deterministic (clockwise) ordering in the sequence of points for the binary mask, however, a binary mask is   only determined by points\u2019 positions instead of the ordering. Hence we randomly   shuffle points\u2019 order, which enables the model to learn which point to predict   next. In Sec. 4.5, we thoroughly study the proposed sampling scheme.   SeqTR: A Simple yet Universal Network for Visual Grounding   7   3.3   Architecture   Language Encoder. To demonstrate the efficacy of SeqTR, we do not opt for   the pre-trained language encoders such as BERT [7], hereby the language encoder   is a one layer bidirectional GRU [5]. We concatenate both unidirectional hidden   states ht = [\u2212\u2192   ht; \u2190\u2212   ht] at each step t to form word features {ht}T   t=1.   Visual Encoder. The multi-scale features of the visual encoder are unidirectionally down-sampled from the finest to coarsest spatial resolution, and flattened to generate visual features Fv \u2208R(H\u2217W )\u00d7C as input to the fusion module.   H and W are 32 times smaller of the original image size. In contrast to previous   work, we only use the coarsest scale visual features instead of the finest ones for   RES task [32,19,10,31], as we do not predict the binary mask pixel-wisely, which   reduces the memory footprint during training.   Fusion. Different from Pix2Seq [3], which only perceives the pixel inputs, we devise a simple yet efficient fusion module to align vision and language modalities.   Given visual features Fv and word features {ht}T   t=1, we first construct language   feature fl \u2208RC by max pooling word features along the channel dimension.   We use Hadamard product between Fv and fl without the linear projection to   produce the multi-modal features Fm \u2208R(H\u2217W )\u00d7C to transformer encoder:     \\l a bel {eq : fusion} F_{m,i} = \\sigma (F_{v,i}) \\odot \\sigma (f_l),    (3)   where \u03c3 is tanh function. Note that we do not concatenate word features and   visual features then use the transformer encoder to perform fusion as in [6,20,8],   because that the complexity will quadratically increase.   Transformer and Predictor. The standard transformer encoder updates the   feature representations of multi-modal features Fm, while the decoder predicts   the target sequence in an auto-regressive manner. The hidden dimension of transformer is set to 256, the expansion rate in feed forward network (FFN) is 4,   and the number of encoder and decoder layers are 6 and 3, respectively. This   results in the transformer being extremely compact. Since the transformer is   permutation-invariant, the Fm and the input sequence are added with sine and   learned positional encoding [45], respectively. To predict the coordinate tokens,   an MLP with a final softmax function is used.   Inference. During inference, coordinates are generated in an auto-regressive   manner, each coordinate is the argmax-ed index of the probabilities over the   vocabulary E, and mapped back to the original image scale via the inversion of   Eq. 2. We predict exactly 4 discrete coordinate tokens for REC, while leaving the   decision of when to prediction to [EOS] token for RES. The predicted sequence   is assembled to form the bounding box or binary mask for evaluation.   4   Experiments   4.1   Datasets   RefCOCO/RefCOCO+/RefCOCOg. RefCOCO [55] contains 142,210 referring expressions, 50,000 referred objects, and 19,994 images. Referring expressions in testA set mostly describe people, while the ones in testB set mainly   8   C. Zhu et al.   describe objects except people. Similarly, RefCOCO+ [55] contains 141,564 expressions, 49,856 referred objects, and 19,992 images. Compared to RefCOCO,   referring expressions of RefCOCO+ describe more about attributes of the referent, e.g., color, shape, digits, and avoid using words of absolute spatial location.   RefCOCOg [34,36] has two types of partition strategy, i.e., the google split [34]   and umd split [36]. Both splits have 95,010 referring expressions, 49,822 referred   objects, and 25,799 images. We use the validation set as the test set following   [10,48,16,49] for umd split. The language length of RefCOCOg is 8.4 words on   average while that of RefCOCO and RefCOCO+ are only 3.6 and 3.5 words.   ReferItGame [21] contains 120,072 referring expressions and 99,220 referents   for 19,997 images collected from the SAIAPR-12 [9] dataset. We use the cleaned   berkeley split to partition the dataset, which consists of 54,127, 5,842, and 60,103   referring expressions in train, validation, and test set, respectively.   Flickr30K. Language queries in Flickr30K Entities [37] are short region phrases   instead of sentences which may contain multiple objects. It contains 31,783 images with 427K referred entities in train, validation, and test set.   Pre-training dataset. Following [20], we merge region descriptions from Visual   Genome (VG) [23] dataset, annotations from RefCOCO [55], RefCOCO+ [55],   RefCOCOg [34,36], and ReferItGame [21] datasets, and Flickr entities [37]. This   results in approximately 6.1M distinct language expressions and 174k images in   train set, which are less than 200k images as in [20].   4.2   Evaluation Metrics   For REC and phrase localization, we evaluate the performance using Precision@0.5. The prediction is deemed correct if its intersection over union (IoU)   with ground-truth box is larger than 0.5. For RES, we use mIoU as the evaluation metric. Precision at 0.5, 0.7, and 0.9 thresholds are also used for ablation.   4.3   Implementation Details   We train SeqTR 60 epochs for REC and phrase localization, and 90 epochs for   RES with batch size 128. The Adam [22] optimizer with an initial learning rate   5e-4 is used, which decays the learning rate 10 times after 50 epochs and 75   epochs for the detection and segmentation grounding tasks, respectively. Following standard practices [6,32,19,8], image size is resized to 640 \u00d7 640, and the   length of language expression is trimmed at 15 for RefCOCO/+ and 20 for RefCOCOg. For ablation, we train SeqTR 30 epochs unless otherwise stated. During   pre-training, SeqTR is trained 15 epochs and fine-tuned another 5 epochs. The   number of quantization bins is set to 1000. We use DarkNet-53 [39] as the visual   encoder. More details are provided in the appendix.   4.4   Comparisons with State-of-the-Arts   In this section, we compare the proposed SeqTR with the state-of-the-art methods on five benchmark datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, ReferItGame, and Flickr30K Entities. Tab. 1 and Tab. 3 show the performance on   SeqTR: A Simple yet Universal Network for Visual Grounding   9   Table 1. Comparison with the state-of-the-arts on the REC task. Visual encoders of   models with \u2020 is trained without excluding val/test images of the three datasets. RN101   refers to ResNet101 [12] and DN53 denotes DarkNet53 [39].   Models   Visual   Encoder   RefCOCO   RefCOCO+   RefCOCOg   Time   val   testA   testB   val   testA   testB   val-g   val-u   test-u   (ms)   Two-stage   CMN [14]   VGG16   71.03   65.77   54.32   47.76   57.47   VC [57]   VGG16   73.33   67.44   58.40   53.18   62.30   ParalAttn [60]   VGG16   75.31   65.52   61.34   50.86   58.03   MAttNet [54]   RN101   76.40   80.43   69.28   64.93   70.26   56.00   66.58   67.27   320   CM-Att-Erase [29]   RN101   78.35   83.14   71.32   68.09   73.65   58.03   67.99   68.67   DGA [47]   VGG16   78.42   65.53   69.07   51.99   63.28   341   RvG-Tree [13]   RN101   75.06   78.61   69.85   63.51   67.45   56.66   66.95   66.51   NMTree [27]   RN101   76.41   81.21   70.09   66.46   72.02   57.52   64.62   65.87   66.44   One-stage   RealGIN [58]   DN53   77.25   78.70   72.10   62.78   67.17   54.21   62.75   62.33   35   FAOA\u2020 [50]   DN53   71.15   74.88   66.32   56.86   61.89   49.46   59.44   58.90   39   RCCF [25]   DLA34   81.06   71.85   70.35   56.32   65.73   25   MCN [32]   DN53   80.08   82.29   74.98   67.16   72.86   57.31   66.46   66.01   56   ReSC\u2020   L [49]   DN53   77.63   80.45   72.30   63.59   68.36   56.81   63.12   67.30   67.20   36   Iter-Shrinking [44]   RN101   74.27   68.10   71.05   58.25   70.05   LBYL\u2020 [16]   DN53   79.67   82.91   74.15   68.64   73.38   59.49   62.70   30   TransVG [6]   RN101   81.02   82.72   78.35   64.82   70.70   56.94   67.02   68.67   67.73   62   TRAR\u2020 [59]   DN53   81.40   78.60   69.10   56.10   68.90   68.30   SeqTR (ours)   DN53   81.23   85.00   76.08   68.82   75.37   58.78   71.35   71.58   50   SeqTR\u2020 (ours)   DN53   83.72 86.51 81.24 71.45 76.26 64.88 71.50 74.86 74.21   50   REC and RES tasks. Tab. 4 reports the result of SeqTR pre-trained on the   large corpus of data. The performance on ReferItGame and Flickr30K Entities   datasets are given in Tab. 2.   The performance of SeqTR on REC and phrase localization tasks is illustrated in Tab. 1 and Tab. 2. From Tab. 1, our model performs better than   two-stage models, especially MAttNet [54] while being 6 times faster. We also   surpass one-stage models that exploit prior and expert knowledge, with +2-7%   absolute improvement over LBYL [16] and ReSC [49]. Despite we predict discrete coordinate tokens in an auto-regressive manner, the inference speed1 of   SeqTR is only 50ms, which is real-time and comparable with one-stage models.   For transformer-based models, SeqTR surpasses TransVG [6] and TRAR [59]   with up to 6.27% absolute performance improvement. Our SeqTR achieves new   state-of-the-art performance with a simple architecture and loss function on   the RefCOCO [55], RefCOCO+ [55], and RefCOCOg [34,36] datasets. On the   ReferItGame and Flickr30K Entities datasets which mostly contain short noun   phrases, the performance boosts to 69.66 and 81.23 with a large margin over   previous one-stage methods [50,42,25,49] and is comparable with current stateof-the-art methods [6,24].   1Tested on GTX 1080 Ti GPU, batch size is 1.   10   C. Zhu et al.   Table 2. Comparison with the state-of-the-art models on the test set of Flickr30K   Entities [37] and ReferItGame [21] datasets.   Models   Visual   Encoder   ReferItGame   Flickr30k   Time   test   test   (ms)   Two-stage   MAttNet [54]   RN101   29.04   320   SimilarityNet [46]   RN101   34.54   60.89   184   DDPN [56]   RN101   63.00   73.30   One-stage   FAOA [50]   DN53   60.67   68.71   23   ZSGNet [42]   RN50   58.63   63.39   RCCF [25]   DLA34   63.79   25   ReSCL [49]   DN53   64.60   69.28   36   TransVG [6]   RN101   70.73   79.10   62   RefTR [24]   RN101   71.42   78.66   40   SeqTR (ours)   DN53   69.66   81.23   50   SeqTR can be seamlessly extended to RES without any network architecture   modifications since we reformulate the task as a point prediction problem. As   shown in Tab. 3, we outperform various models with sophisticated cross-modal   alignment and reasoning mechanisms [19,31,10,32,51,17,28]. SeqTR is on par   with current state-of-the-art VLT [8] which selectively aggregates responses from   the diversified queries, whereas we directly produce the corresponding segmentation mask and establish one-to-one correspondence. When initialized with the   pre-trained parameters using the large corpus of data, the performance boosts up   to 10.78% absolute improvement, proving that a simple yet universal approach   for visual grounding is indeed feasible.   From Tab. 4, when pre-trained on the large corpus of text-image pairs, SeqTR   is more data-efficient than the current state-of-the-art [20]. Our transformer architecture only contains 7.9M parameters which is twice as few as MDETR [20],   while the performance is superior especially on the RefCOCOg dataset with up   to 2.48% improvement.   4.5   Ablation Studies   To give a comprehensive understanding of SeqTR, we discuss ablative studies on   the validation set of the RefCOCO [55], RefCOCO+ [55], and RefCOCOg [36]   datasets in this section.   Construction of language feature. Language feature in Sec. 3.3 can be constructed by either max/mean pooling of word features or directly using the final   hidden state of bi-GRU. As shown in the upper part of Tab. 5, max pooling   performs best, and is the default construction throughout this paper.   Token weight. If previously predicted points are inaccurate, model can not   recover from the wrong predictions since the inference is sequential. Hence we   increase a few former token weights to penalize more on the first several predicted   SeqTR: A Simple yet Universal Network for Visual Grounding   11   Table 3. Comparison with the state-of-the-arts on the RES task. Model with \u2217is   pre-trained on the large corpus of data.   Models   Visual   Encoder   RefCOCO   RefCOCO+   RefCOCOg   val   testA   testB   val   testA   testB   val-g   val-u   test-u   MAttNet [54]   RN101   56.51   62.37   51.70   46.67   52.39   40.08   47.64   48.61   CMSA [51]   RN101   58.32   60.61   55.09   43.76   47.60   37.89   39.98   STEP [2]   RN101   60.04   63.46   57.97   48.19   52.33   40.41   46.40   BRINet [15]   RN101   60.98   62.99   59.21   48.17   52.32   42.11   48.04   CMPC [17]   RN101   61.36   64.53   59.64   49.56   53.44   43.23   49.05   LSCM [18]   RN101   61.47   64.99   59.55   49.34   53.12   43.50   48.05   CMPC+ [28]   RN101   62.47   65.08   60.82   50.25   54.04   43.47   49.89   MCN [32]   DN53   62.44   64.20   59.71   50.62   54.99   44.69   49.22   49.40   EFN [10]   WRN101   62.76   65.69   59.67   51.50   55.24   43.01   51.93   BUSNet [48]   RN101   63.27   66.41   61.39   51.76   56.87   44.13   50.56   CGAN [31]   DN53   64.86   68.04   62.07   51.03   55.51   44.06   46.54   51.01   51.69   LTS [19]   DN53   65.43   67.76   63.08   54.21   58.32   48.02   54.40   54.25   VLT [8]   DN56   65.65   68.29   62.73   55.50   59.20   49.36   49.76   52.99   56.65   SeqTR (ours)   DN53   67.26   69.79   64.12   54.14   58.93   48.19   55.67   55.64   SeqTR\u2217(ours)   DN53   71.70   73.31   69.82   63.04   66.73   58.97   64.69   65.74   Table 4. Comparison with pre-trained models on RefCOCO [55], RefCOCO+ [55], and   RefCOCOg [36] datasets. We only count the parameters of transformer architecture.   Models   Visual   Encoder   Params Pre-train   images   RefCOCO   RefCOCO+   RefCOCOg   (M)   val   testA testB   val   testA testB   val-u test-u   ViLBERT [30]   RN101   3.3M   72.34 78.52 62.61   VL-BERTL [43]   RN101   3.3M   72.59 78.57 62.30   UNITERL [4]   RN101   4.6M   81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77   VILLAL [11]   RN101   4.6M   82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71   ERNIE-ViLL [53] RN101   4.3M   75.95 82.07 66.88   MDETR [20]   RN101   17.36   200K   86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89   RefTR [24]   RN101   17.86   100K   85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01   SeqTR (ours)   DN53   7.90   174K   87.00 90.15 83.59 78.69 84.51 71.87 82.69 83.37   discrete coordinate tokens. As shown in the lower part of Tab. 5, increasing the   weight of first token is better than increasing the latter tokens, and setting the   1st token weight to 1.5 and subsequent tokens to 1 gives the best performance.   We set wi = 1, \u2200i for RES task.   Sampling scheme. We verify the upper bound as the mIoU of the assembled   mask from the sampled points and original ground-truth. From Fig. 4 (a), we can   see that the mIoU approaches nearly 100 when the number of sampled points   increases, i.e., 95.57 for uniform sampling, and 91.58 for center-based sampling.   Therefore, though the upper bound is limited theoretically, in practice, the research effort might be better spent on improving the real-world performance. In   terms of sampling strategies, from Fig. 4 (a) and Fig. 4 (c-e), uniform sampling   is consistently better than center-based sampling in terms of both the upper   bound and the performance, which preserves more details of the mask illustrated in Fig. 3. The number of sampled points controls the trade-off between   12   C. Zhu et al.   Table 5. Ablation experiments on the construction of language feature and token   weight. The first token is the [TASK] token, while subsequent tokens are discrete coordinate tokens, i.e., (x1, y1, x2, y2).   Language feature   Token weight   RefCOCO   RefCOCO+   RefCOCOg   1st   2nd   3rd   4th   5th   val   val   val-u   mean pooling   1   1   1   1   1   79.73   67.12   68.97   max pooling   80.07   68.31   69.95   final hidden state   79.85   67.46   69.93   max pooling   1   1   1   1   1   80.07   68.31   69.95   1.5   1   1   1   1   80.10   68.63   70.05   2   1   1   1   1   80.19   68.33   70.01   3   1   1   1   1   80.08   67.81   69.45   1   2   2   1   1   79.70   67.22   69.51   2   2   2   1   1   80.16   67.83   69.45   (c) RefCOCO   (d) RefCOCO+   (e) RefCOCOg   (a)   (b)   Fig. 4. Ablative experiments on RES task. (a) The upper bound is averaged over   validation sets (the fluctuation is within 0.2). (b) Shuffling percentage refers to the   fraction of shuffled sequences within a batch, uniform sampling strategy is used. (c-e)   depict the impact of sampling strategies and the number of sampled points.   the inference speed and performance, from Fig. 4 (c-e), we can see that 18 and   12 points are the best for RefCOCO and RefCOCO+/RefCOCOg datasets.   Shuffling percentage. We train SeqTR 60 epochs instead of 30 as we empirically found that point shuffling takes a longer time to converge, since the groundtruth is different for each coordinate token at each forward pass. Fig. 4 (b) shows   that no shuffle and 0.2 are best for RefCOCO and RefCOCO+/RefCOCOg. As   the number of shuffled sequences increases, the performance drops slightly, and   we observe that SeqTR is under-fitting since the mIoU during training is lower   than the one without shuffling.   Multi-task training. Previous multi-task visual grounding approaches require   REC to help RES locate the referent. In contrast, SeqTR is capable to locate   SeqTR: A Simple yet Universal Network for Visual Grounding   13   Table 6. Ablation study of multi-task training. IE is the inconsistency error [32] to   measure the prediction conflict between REC and RES, \u2193denotes the lower is better.   Dataset   Multi-task training   REC   RES   mIoU   IE\u2193   Prec@0.5   Prec@0.5   Prec@0.7   Prec@0.9   RefCOCO   %   80.38   78.03   63.35   9.75   64.20   13.93   \"   79.65   77.24   60.29   7.23   62.93   5.86   RefCOCO+   %   67.98   65.11   48.27   5.19   52.22   22.22   \"   68.79   66.67   51.02   5.46   53.65   4.85   RefCOCOg   %   69.63   65.20   46.23   5.31   53.25   22.65   \"   70.29   65.20   46.05   5.15   53.25   8.25   the referent at pixel level without the aid from REC. We train SeqTR 60 epochs   and test whether multi-task supervision can bring further improvement. For the   input sequence construction of multi-task grounding, please see the supplementary material. From Tab. 6, we can see that multi-task supervision even slightly   degenerates the performance compared to the single-task variant. Though the   inconsistency error significantly decreases, the location ability of RES measured   by Prec@0.5, 0.7, and 0.9 stays the same, suggesting that the sampled points   are independent between the sequence of the bounding box and binary mask.   4.6   Qualitative Results   We visualize the cross attention map averaged over decoder layers and attention   heads in Fig. 5. At each prediction step, SeqTR generates a coordinate token   given previous output tokens. Under this setting, a clear pattern emerges, i.e.,   attends to the left side of the referent when predicting x1, the top side of the   referent when predicting y1, and so on. This axial attention is sensitive to the   boundary of the referent, thus can more precisely ground the referred object.   The predicted masks are visualized in Fig. 6. SeqTR can well comprehends attributive words and absolute or relative spatial relations, and the predicted mask   aligns with the irregular outlines of the referred object such as \u201cleft cow\u201d. More   qualitative results are given in the appendix.   5   Conclusions   In this paper we reformulate visual grounding tasks as a point prediction problem and present an innovative and general network termed SeqTR. Based on   the standard transformer encoder-decoder architecture and cross-entropy loss,   SeqTR unifies different visual grounding tasks under the same point prediction   paradigm without any modifications. Experimental results demonstrate that SeqTR can well ground language query onto the corresponding region, suggesting   that a simple yet universal approach for visual grounding is indeed feasible.   14   C. Zhu et al.   48   a man in a green    polo shirt   a man in white    clothes standing   x!   y!   x\"   y\"   Fig. 5. Visualization of normalized cross attention map in transformer decoder. From   left to right column, we generate (x1, y1, x2, y2) in sequential order.   Blue recliner    on right   Little kid in black    on far right   Left person   First motorcycle    on the left   Left cow   Mandarine orange    segments on left side    of place   Couch on    bottom   Red bus   Car closest to us   Bike left side   Fig. 6. Example mask predictions by SeqTR on the validation set of RefCOCO dataset,   best viewed in color.   Acknowledgements. This work was supported by the National Science Fund   for Distinguished Young Scholars (No. 62025603), the National Natural Science   Foundation of China (No. U21B2037, No. 62176222, No. 62176223, No. 62176226,   No. 62072386, No. 62072387, No. 62072389, and No. 62002305), Guangdong   Basic and Applied Basic Research Foundation (No. 2019B1515120049), and the   Natural Science Foundation of Fujian Province of China (No. 2021J01002).   SeqTR: A Simple yet Universal Network for Visual Grounding   15", "conf": "ECCV", "year": "2022", "index": 151}, {"title": "Four Ways to Improve Verbo-visual Fusion for   Dense 3D Visual Grounding   Ozan Unal1,2 , Christos Sakaridis1, Suman Saha1,3, and Luc Van Gool1,4,5   1ETH Zurich, 2Huawei Technologies, 3PSI, 4KU Leuven, 5INSAIT   {ozan.unal, csakarid, suman.saha, vangool}@vision.ee.ethz.ch", "abstract": ". 3D visual grounding is the task of localizing the object in a   3D scene which is referred by a description in natural language. With   a wide range of applications ranging from autonomous indoor robotics   to AR/VR, the task has recently risen in popularity. A common formulation to tackle 3D visual grounding is grounding-by-detection, where   localization is done via bounding boxes. However, for real-life applications that require physical interactions, a bounding box insufficiently   describes the geometry of an object. We therefore tackle the problem of   dense 3D visual grounding, i.e. referral-based 3D instance segmentation.   We propose a dense 3D grounding network ConcreteNet, featuring four   novel stand-alone modules that aim to improve grounding performance   for challenging repetitive instances, i.e. instances with distractors of the   same semantic class. First, we introduce a bottom-up attentive fusion   module that aims to disambiguate inter-instance relational cues, next,   we construct a contrastive training scheme to induce separation in the   latent space, we then resolve view-dependent utterances via a learned   global camera token, and finally we employ multi-view ensembling to   improve referred mask quality. ConcreteNet ranks 1st on the challenging   ScanRefer online benchmark and has won the ICCV 3rd Workshop on   Language for 3D Scenes \u201c3D Object Localization\u201d challenge. Our code is   available at ouenal.github.io/concretenet/.   Keywords: 3D visual grounding \u00b7 verbo-visual fusion   1", "content": "of the cornerstone datasets ScanRefer [3],   Nr3D and Sr3D [1]. These datasets are based on the 3D ScanNet [8] dataset and   additionally contain either free-form [3] or contrastive [1] lingual descriptions,   each of which refers to a single 3D object in the scene which must be recognized.   This subtle difference in the construction of descriptions between ScanRefer on   the one side and Nr3D and Sr3D on the other induces two respective variants   of 3D visual grounding. The former variant consists of the 3D localization of   the referred object directly from the point cloud [3], while the latter additionally   provides as input the ground-truth 3D bounding boxes of all objects in the scene   which belong to the class of the referred object [1]. We refer to the former variant as referral-based 3D object localization and to the latter as referral-based   3D object identification. Referral-based 3D object localization is arguably more   challenging, as it requires (i) detecting multiple candidate objects from several   classes, including classes besides that of the referred object, and (ii) discriminating between the referred object and all other candidate objects. As our goal is to   achieve an end-to-end solution to verbo-visual fusion, we focus on this variant.   State-of-the-art methods [3, 5, 38] focus on grounding descriptions to 3D   bounding boxes of referred objects. While such detection-level grounding has   vast potential for real-world applications, ranging from autonomous robots to   AR/VR, the level of geometric detail which is provided by 3D bounding box detections remains limited. As an example, autonomous robots may have to grasp   or avoid objects. Therefore, delivering a detailed, pointwise mask is more beneficial for downstream tasks than just having the axis-aligned bounding boxes.   We illustrate this in Fig. 1.   Considering the above-mentioned goals, this paper tackles the underexplored   problem of dense 3D visual grounding [14, 35]. We propose a novel dense 3D   visual grounding network (ConcreteNet), where we adopt the commonly used   grounding-by-selection strategy. In grounding-by-selection, a visual backbone   first produces 3D instance candidates with a point cloud as input. After that,   a verbo-visual fusion module selects the correct candidate based on the natural   language description. With this framework, we observe that 3D instance segmentation yields more robust and tighter predictions compared to 3D object   detection, but suffers from reduced separability in the latent space between instances of the same semantic class. This results in a significant increase in false   positive rates for the higher level 3D visual grounding task, most notably observed for repetitive instances, i.e. instances that are not semantically unique in a   scene. To combat this effect, we propose four novel ways to improve verbo-visual   fusion for dense 3D visual grounding.   First, we observe that in cases where referrals may be construed as valid for   multiple instances, locality rules. In other words, due to our limited attention   Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding   3   spans, we humans mainly consider nearby objects when referring to an instance.   To disambiguate inter-instance relational cues, we propose a bottom-up attentive   fusion module (BAF) that induces this locality during verbo-visual fusion via   spherical masking with an increasing radius, allowing only neighboring objects   to attend to each other.   Second, we form a general solution to the instance separability issue within   the latent verbo-visual space by constructing a contrastive training scheme to   alleviate ambiguities between embeddings of repetitive instances. Specifically,   we pull sentence embeddings and matching instance vectors towards each other,   while contrasting non-matching pairs.   Next, we tackle the issue of view-dependent referrals. Unlike in 2D, 3D scenes   do not inherently possess a directional right or left side, or a room does not have a   clear back or front. However, often our perception is unequivocally guided by our   personal perspectives, and thus such view-dependent descriptions are unavoidable in any real-world situation. While we can empathize with the speaker and   rationalize the possible viewpoint associated with an utterance, this trait does   not come naturally to machines. We therefore propose to introduce a learned   global camera token (GCT) that can be directly supervised via the camera positions used during annotation to help resolve view-dependent prompts.   Finally, we improve the quality of our predicted referred instance mask   through ensembling over multiple views of a single point cloud scene by reducing the epistemic uncertainty. To this end, we develop a two-stage ensemble   algorithm for dense 3D visual grounding that first determines the correct referred object from all predictions of the individual viewpoints and then refines   the aggregated instance mask.   In summary, our contributions are as follows:   \u2013 We present ConcreteNet, a kernel-based 3D visual grounding network that   predicts 3D instance masks as opposed to 3D bounding boxes to aid realworld applications that require a fine geometric understanding of an object.   \u2013 We introduce a bottom-up attentive fusion module (BAF) to disambiguate   inter-instance relational referrals through spherical neighborhood masking.   \u2013 We construct a contrastive learning scheme to induce further separation in   the latent representation to aid repetitive instance grounding.   \u2013 We learn a global camera token (GCT) to resolve view-dependent prompts.   \u2013 We propose multi-view ensembling to improve referred mask quality.   With all four proposed improvements, we rank 1st in the challenging ScanRefer [3] online benchmark.   2   Related Work   3D visual grounding is a prominent 3D task in the area of vision and language   and constitutes the 3D version of the more extensively studied task of 2D visual   grounding [18,22,24], which aims to ground a verbal description for an image to   the specific object this description refers to. Respectively, 3D visual grounding   4   O. Unal et al.   methods [10, 21, 25] accept a 3D scene in the form of a point cloud as visual   input and need to ground the accompanying description to the referred object   in 3D. Closely related 3D vision-language tasks are 3D dense captioning [4, 6]   and grounding spatial relations (instead of individual objects) in 3D [11].   An early, seminal work in 3D visual grounding [19] employed an MRF   to densely ground lingual descriptions of 3D scenes from the NYU Depth-v2   dataset [29], which reasons jointly about the 3D scene and its description. Attention modules were proposed in [38] both for leveraging context in the object   proposal module and for the verbo-visual fusion module. We build our attentive verbo-visual fusion module on top of the one used in [38], but we propose   four novel ways to improve this fusion. In particular, we improve (i) the internal fusion mechanism by enforcing progressive context aggregation across object   candidates via masked attention, (ii) the supervision of the fusion outputs via   a cross-modal contrastive loss which uniquely attracts the visual embedding of   the referred object to its verbal embedding, (iii) the sensitivity of object embeddings to the viewpoint from which the verbal description is generated by   including a dedicated, learned global camera token in our attentive fusion and   finally (iv) the referred mask predictions by lowering the epistemic uncertainty   through multi-view ensembling.   Previous 3D grounding works have explored local attention [5] and viewpoint   dependency [15,27], similarly to us. 3DVG-T [38] utilizes relative Euclidean distance for relational encoding. While this helps capture object-to-object interactions, the model still relies on global attention for information routing. Extending 3DVG-T, 3DJCG [2] introduces an additional spatial distance matrix,   computed from the centers of the initial object proposals. While this matrix acts   as a relation encoder on the attention maps, the model still relies on a global attention scheme where all object tokens can exchange information. By contrast,   we induce hard locality through spherically masked attention in a bottom-up   manner. Rendering attention local, i.e., only allowing object-to-object message   passing between neighbors, helps the model better disambiguate inter-object relations and improve the 3DG performance for the multiple cases. Chen et al. [5]   provide language embeddings of multiple granularities as inputs to verbo-visual   fusion, which includes a module implementing local attention via partitioning   the 3D volume of the scene into coarse voxels and restricting attention across   different visual embeddings within each voxel. By contrast, our attentive fusion implements locality in an isotropic fashion, using spherical attention masks   centered at the centroid of the respective object. MVT [15] addresses the insensitivity of vision-based object embeddings to the description viewpoint in a   data-driven fashion by applying rotation augmentations to the input 3D scene.   Instead of proliferating the already sizable 3D inputs of the grounding model, we   inject viewpoint sensitivity in our verbo-visual attentive fusion by including an   additional, camera viewpoint token in the visual tokens of our attention, which   induces a comparatively negligible computational overhead. The empirical findings of [27] support the positive effect of even approximate viewpoint information   on referral-based 3D object identification accuracy on Nr3D scenes with viewFour Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding   5   dependent descriptions. This information is provided in [27] by canonicalizing   the yaw of the scenes with respect to their descriptions. We instead learn the   viewpoints of ScanRefer data from exact annotations as part of our verbo-visual   fusion, leveraging the abundant viewpoint-dependent descriptions in these data.   Dense 3D visual grounding or referral-based 3D instance segmentation   presents the additional challenge of precisely segmenting the 3D points belonging to the referred object from points belonging to other objects or to the background, and it is far less explored than standard 3D detection-level grounding. In Huang et al. [14], verbo-visual fusion between instance embeddings and   word embeddings is performed with a graph neural network. The attention-based   verbo-visual fusion of [35] only accepts the global sentence embedding as input,   which does not allow the instance embeddings to attend to individual words that   may carry more specific information about geometry and appearance. Semantic   instance-specific features produced in [35] in the process of extracting the candidate instances are discarded in the subsequent extraction of instance embeddings   for grounding, whereas we learn these semantic features end-to-end, optimizing   them both for the generation of instance embeddings that are discriminative for   grounding and for segmentation accuracy.   Verbo-visual contrast has been shown to provide a strong alternative to traditional categorical visual supervision for learning discriminative 2D visual representations. In particular, CLIP [26] learns a multi-modal, verbo-visual embedding space by contrasting 2D visual embeddings to language embeddings based   on the co-occurrence of respective inputs from the two modalities. Our proposed   contrastive loss for 3D visual grounding also applies verbo-visual contrast between the embedding of the verbal description and the visual 3D object-level   embeddings, which effectively pushes the embedding of the referred object away   from the embeddings of other objects and thus aids classification. Another work   that leverages verbo-visual contrast in a 3D task is [28], which contrasts learned   3D semantic segmentation features with features from a pre-trained CLIP model   based on the class of the respective 3D and verbal inputs.   Multi-view in 3D visual grounding is leveraged in MVT [15] which aggregates the features from multiple views to reduce dependence on specific views,   and in ViewRefer [12] that also utilizes multi-view text input. Multi3DRefer [37]   generates multi-view 2D images from 3D objects and employs a CLIP image encoder to inject rich features into object candidates. Compared to the aforementioned works, our multi-view ensembling (MVE) directly operates on selected   referred objects rather than each individual predicted object, i.e. acts as late   multi-view aggregation rather than early multi-view fusion. Thus not only does   MVE reduce the epistemic uncertainty within the selection (similar to previous   works) but also within the final mask prediction. In other words, our method   does not fully rely on an initial object proposal to determine the final mask, but   further uses the multi-view information to construct a refined mask.   3   Method   Our architecture comprises three core modules: a visual encoder, a verbal encoder and a verbo-visual fusion module. In Sec. 3.1, we introduce the 3D visual   6   O. Unal et al.   Fig. 2: Illustration of our ConcreteNet dense 3D visual grounding pipeline (left). Given   a point cloud and a natural language prompt, we first generate instance candidates   (blue) and word embeddings (pink). We then fuse these to densely ground the verbal   description to the 3D scene. We improve performance by localizing attention via a   bottom-up attentive fusion module (right), utilizing contrastive learning to promote   better feature separability, and learning the camera position to disambiguate viewdependent descriptions. Our final prediction is generated by merging the token of the   best-fitting instance with its predicted mask.   backbone which generates 3D instance candidates along with the ensuing masks.   In Sec. 3.2, we outline how we encode the language queries into high-dimensional   word embeddings, and in Sec. 3.3, we present our verbo-visual fusion module for   grounding a description in 3D space by fusing the word embeddings and instance   embeddings to predict the referred instance mask. The overall pipeline is seen   in Fig. 2 - left.   3.1   Kernel-Based 3D Instance Segmentation   3D point clouds are large, unordered data structures. Commonly, dense tasks   such as instance segmentation require a dense representation, thus high-level   feature information needs to be captured in relatively high resolution [17, 31].   Following attentive verbo-visual fusion approaches [5, 38], interactions between   language cues and a sizeable number of points result in a considerable amount   of computing and memory requirements. By contrast, kernel-based instance segmentation models condense instance information within a single scene into a   sparse representation in the form of instance-aware kernels [13, 33]. These kernels are then used to scan the whole scene to reconstruct instance masks via dot   product or dynamic convolution. Our kernel-based 3D instance segmentation   pipeline is illustrated in Fig. 2 - blue.   Formally, following recent literature [31], we first extract features f3D \u2208   RN\u00d7D for all N 3D points using a sparse convolutional UNet backbone [17].   The resulting features are then used to predict an auxiliary semantic prediction   s \u2208RN\u00d7C, where C is the number of classes, and the offsets x = p \u2212o \u2208RN\u00d73   from points p to their instance centroids o. We supervise via:     \\m a thcal {   L}_{\\text {sem}} = H(\\vect {s}, \\hat {\\vect {s}}) \\textrm {\\phantom {sp} and \\phantom {sp}} \\mathcal {L}_{\\text {off}} = L_1(\\vect {x}, \\hat {\\vect {x}}), d   (1)   Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding   7   with H and L1 denoting the cross-entropy and L1 losses respectively, and   \u02c6\u00b7   denoting ground-truth values. For the offset loss Loff, we ignore points that do   not belong to an associated instance.   Candidate generation. To generate instance candidates from pointwise features, we closely follow DKNet [33]. We generate a sharp centroid map h by   concatenating f3D and o and processing the joint information via an MLP and   an ensuing softmax operation. The centroid maps are supervised via geometryadaptive Gaussian kernels applied to ground-truth heatmaps:     \\m a   t   hc   al {L}_{   \\   t   ext    {cen   }} = \\f r ac    {   1}{\\sum    _   {   i=1}^N  \\mathbbm {1}(\\vect {p}_i)}\\sum _{i=1}^N \\mathbbm {1}(\\vect {p}_i) \\, \\left |h_i - \\exp \\left (-\\frac {\\gamma {\\|\\vect {x}_i\\|}^2}{b_i^2}\\right )\\right |,    (2)   with bi denoting the length of the axis-aligned box, \u03b3 the temperature hyperparameter, and 1(pi) an indicator function that returns 1 if and only if pi belongs   to an instance. We then generate candidates from predicted heatmaps via local normalized non-maximum suppression (LN-NMS), with duplicate proposals   being aggregated based on their context. The aggregation is supervised via a   ground-truth merging map \u02c6a:     \\m a thcal  {L}_{\\text {agg}} = H_b(a, \\hat a),    (3)   with Hb denoting the binary cross-entropy loss.   An MLP processes f3D to produce pointwise instance features g \u2208RN\u00d7L.   The generated candidate masks are then used to average-pool instance features   g across each mask in order to generate the instance embeddings ei \u2208RI\u00d7L that   are input to the subsequent verbo-visual fusion module, where I is the number of   candidates. For further details regarding the heatmap generation and proposal   aggregation, we refer the readers to Wu et al. [33].   The resulting total loss to supervise candidate generation is:     \\m a thca l  {L} _ {\\te x t {can}} = \\mathcal {L}_{\\text {sem}} + \\mathcal {L}_{\\text {off}} + \\mathcal {L}_{\\text {cen}} + \\mathcal {L}_{\\text {agg}}.    (4)   Mask generation. To generate dense instance masks, we first remap each instance candidate onto its respective kernel parameters via an MLP, and following   DyCo3d [13], we generate the final instance masks z \u2208{0, 1}N with the use of   dynamic convolutions. The masks that have ground-truth counterparts (IoU \u2265   0.25) can then be supervised via:     \\ma t hcal {L} _ {\\text {mask}} = H_b(\\vect {z}, \\hat {\\vect {z}}) + \\text {DICE}(\\vect {z}, \\hat {\\vect {z}}),    (5)   with DICE denoting the Dice loss.   3.2   Encoding Language Cues   In recent years, NLP has boomed with the success of large pre-trained transformer models that perform exceptionally well on a wide range of different   tasks [9, 20, 30]. Such models are trained with large-scale datasets that allow   8   O. Unal et al.   them to capture context and intent within natural language prompts. While   most early work in 3D visual grounding employed the more traditional GloVE   embeddings [23] followed by a GRU [3,5,38], recent works have switched focus   towards transformer-based verbal encoding strategies [16, 27]. In this work, to   extract the initial word embeddings, we utilize a pre-trained transformer architecture, namely MPNet [30]. We then project the encoded tokens to L dimensions   via by a single linear layer to form the word embeddings ew \u2208RW \u00d7L, with W   denoting the number of tokens. The generated word embeddings are then used   as input for the subsequent verbo-visual fusion module.   3.3   Verbo-visual Fusion   Commonly, 3D visual grounding is achieved through grounding-by-selection,   wherein given a set of visual candidates, a verbal cue is used to select the referred   object [3,5,38]. Formally, a fusion module consumes both visual and verbal information to output a probability distribution over predetermined candidates in   order to determine the likelihood that an object is referred by the description,   the argmax of which is taken as the prediction. While early approaches have   considered a simple MLP to fuse the two modalities [3], recent methods are constructed using the popular transformer architecture [5,38], taking advantage of   the expressibility of the attention mechanism given by:     \\ label {eq:at   t ent i on} \\vect {f}_l = \\textrm {softmax}(\\vect {q}_l \\vect {k}_l^T)\\vect {v}_l + \\vect {f}_{l-1},    (6)   with the queries q extracted from object features and key-value pairs k and v   interchangeably from object and word features.   The naive approach when tackling 3D visual grounding in a dense setting is   to follow a similar pipeline. Given instance candidates ei (Sec. 3.1) and word embeddings ew (Sec. 3.2), a transformer decoder fuses the multi-modal information   and ultimately selects the referred mask. However, while instance segmentation   has shown to perform on par, if not better than 3D object detection for 3D indoor   localization1, dense kernel-based models show limited separability in the latent   space between instances of the same semantic class. When it comes to 3DVG,   this results in a significant performance drop when facing utterances that refer to   repetitive instances, i.e., instances that are not semantically unique in a scene. To   combat this, we propose four modules that (i) aim to disambiguate inter-instance   relational cues, (ii) aid training to induce better separability in the latent representation, (iii) infer the sensor position to resolve view-dependent descriptions,   and (iv) ensemble across multiple viewpoints to improve mask quality.   Bottom-up attentive fusion (BAF). Natural language prompts may aim to   localize a repetitive object by establishing its relation to another object (e.g.   \u201cThe chair next to the cabinet.\u201d). We observe that due to human nature, as   our attention is limited, verbal relations are often formed between neighboring   instances. However, such localized information routing is difficult to learn for   1 Classifying each point yields a more robust solution compared to the localization of   8 corner points in complete free 3D space.   Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding   9   global attention schemes. Inspired by the effective windowing in NLP and 2D   applications [7,36], to explicitly induce this locality condition in our model, we   develop a bottom-up attentive fusion module (BAF) built on a masked selfattention mechanism. An illustration of BAF can be seen in Fig. 2 - right.   In BAF, the word embeddings first get processed via a vanilla transformer   encoder block for further abstraction. Next, the spatial and contextual information between candidate instances are routed via a localized self-attention layer.   Formally, we restate the mechanism of Eq. 6 for the localized self-attention layer:     \\ vect {f}_l  = \\t   e xtr m  {softmax}(\\vect {M}_l + \\vect {q}_l \\vect {k}_l^T)\\vect {v}_l + \\vect {f}_{l-1}.    (7)   The mask takes the value Ml(i, j) when computing the attention vector between   the i\u2019th and j\u2019th instance candidates:     \\la be l       {e   q: mask i ng} \\ ve   ct    {M}_l(i,j) = \\begin {cases} 0, & \\textrm {if } || \\vect {o}_i - \\vect {o}_j || < r_l \\\\ -\\infty ,\\quad & \\textrm {otherwise} \\end {cases}    (8)   with rl denoting the radius of the spherical masking set per layer. The spherical   3D masking operation limits the attention to neighboring instances, which helps   when grounding cues contain inter-instance relations.   Via a cross-attention and feed-forward layer, we fuse the instance tokens   with word embeddings. To construct the final instance embeddings, we apply   the transformer decoder block l times, each time with an increasing masking   radius rl (hence bottom-up), and fuse the resulting features across different   layers via an MLP to capture relational cues from different neighborhood scales.   We attach a classification head that maps the language-aware instance tokens   onto confidence scores u \u2208RI. We supervise the selection via cross-entropy:     \\m a thca l {L}_{sel} = H(\\vect {u}, \\hat {\\vect {u}}),    (9)   with \u02c6u denoting the ground-truth index computed by applying the Hungarian   algorithm on the instance predictions and referred ground-truth mask. The final   dense visual grounding prediction is then obtained via:     \\ vect  { z }^* = \\vect {z}^{(i)} \\, | \\, i = \\argmax \\vect {u}.    (10)   Inducing separation via contrastive learning. With the aim of alleviating ambiguities between repetitive instance mappings, we propose employing a   verbo-visual contrastive learning scheme to induce better separation between   language embeddings and language-aware instance embeddings in the latent   space. Firstly, we form sentence embeddings by applying masked averaging to   the learned word embeddings (es = ew). In the contrastive loss formulation,   matched sentence embeddings and instance vectors are treated as positive samples and thus are pulled towards each other, while the pairings with remaining   instances are treated as negative samples and are pushed away from one another.   Formally, this can be expressed as:     \\mathc al { L }_{   \\text {co n}}(\\vect       { e_s}, \\ve ct {e_i}) = - \\log \\frac {\\exp (d(\\vect {e_s}, \\vect {e_i}_{,k+}) / \\tau )}{\\sum _{k} \\exp (d(\\vect {e_s}, \\vect {e_i}_{,k}) / \\tau )},    (11)   10   O. Unal et al.   with \u03c4 denoting the temperature, es and ei,k referring to the sentence embedding and k\u2019th instance candidate embeddings respectively. d(\u00b7) is chosen as the   cosine similarity and k+ denotes the index of the instance that matches the   reference cue. The contrastive loss provides an easy-to-apply, general solution   to aid referral-based localization within the multiple subset, i.e., the subset of   repetitive instance referrals.   Global camera token (GCT). As individuals, we observe the world from our   own personal perspective. It is often this perspective that we tap into to describe   our surroundings, which results in view-dependent", "conf": "ECCV", "year": "2024", "index": 340}, {"title": "A Controllable Model of Grounded Response Generation   Zeqiu Wu,1 Michel Galley,2 Chris Brockett,2 Yizhe Zhang,2 Xiang Gao,2 Chris Quirk,2   Rik Koncel-Kedziorski,1 Jianfeng Gao,2 Hannaneh Hajishirzi,1, 3 Mari Ostendorf,1 Bill Dolan 2   1University of Washington, Seattle, WA, USA   2Microsoft Research, Redmond, WA, USA   3Allen Institute for AI, Seattle, WA, USA   zeqiuwu1@uw.edu, mgalley@microsoft.com", "abstract": "Current end-to-end neural conversation models inherently   lack the \ufb02exibility to impose semantic control in the response   generation process, often resulting in uninteresting responses.   Attempts to boost informativeness alone come at the expense   of factual accuracy, as attested by pretrained language models\u2019 propensity to \u201challucinate\u201d facts. While this may be mitigated by access to background knowledge, there is scant   guarantee of relevance and informativeness in generated responses. We propose a framework that we call controllable   grounded response generation (CGRG), in which lexical control phrases are either provided by a user or automatically   extracted by a control phrase predictor from dialogue context and grounding knowledge. Quantitative and qualitative   results show that, using this framework, a transformer based   model with a novel inductive attention mechanism, trained on   a conversation-like Reddit dataset, outperforms strong generation baselines.   1", "content": "End-to-end neural models for open-domain response generation (Shang, Lu, and Li 2015; Sordoni et al. 2015; Vinyals   and Le 2015; Gao, Galley, and Li 2019) are capable of generating conversational responses that are both \ufb02uent and contextually appropriate. Although the earliest neural generation models were characterized by bland and evasive responses (Li et al. 2016a), surprisingly human-like conversations can be generated using recent diversity-enhancing   strategies (Holtzman et al. 2020; Gao et al. 2019a) and massive GPT-2 style models (Radford et al. 2019; Zhang et al.   2020).1 While blandness may no longer present a challenge,   the downside has been a propensity towards \u201challucinated\u201d   or \u201cfake\u201d output (Zellers et al. 2019) of the kind illustrated   in scenario I in Figure 1.   Grounded response generation (Ghazvininejad et al.   2018; Dinan et al. 2019; Qin et al. 2019) approaches can   inhibit hallucination of facts. Yet grounding alone (e.g, the   Copyright \u00a9 2021, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   1For a related task (document creation), 72% of human judges   found GPT-2 credible vs. 83% for New York Times articles: https:   //openai.com/blog/gpt-2-6-month-follow-up/   A: You should check out the movie La La Land.   B: Tell me more about it.    Damien    Chazelle   \u2026  musical film    directed by    Damien  Chazelle    \u2026 Ryan Gosling    as a jazz pianist ...   La La Land is a 2016 American romantic comedy-drama    musical film written and directed by Damien Chazelle. It    stars Ryan Gosling as a jazz pianist and Emma Stone as    an aspiring actress, who meet and fall in love while    pursuing their dreams in Los Angeles. Having been fond    of musicals during his time as a drummer, Chazelle first    conceptualized the film alongside Justin Hurwitz while    attending Harvard University together. Moving to Los    Angeles in 2010, Chazelle wrote the screenplay but did    not find a studio willing to finance the production without    changes to his design. Following the success of his 2014    film Whiplash, the project was picked up by Summit    Entertainment. Filming took place in Los Angeles from    August to September 2015, with the film's score    composed by Hurwitz and the dance choreography by    Mandy Moore. La La Land premiered at the    73rd\u2026\u2026\u2026\u2026\u2026\u2026\u2026.   A: I\u2019m not sure    \u2026 Might be a    superhero    movie ...   A: It is a 2016    movie ...   A: It \u2026 stars    Damien    Chazelle.   A: It is a    musical film    directed by    Damien    Chazelle and    Ryan Gosling    is also in it!   I: No control,    No grounding    II: Grounding only    III: Control only   IV: Both Control    and Grounding    Figure 1: Generated responses tend to be generic or factually incorrect without grounding or control. Adding grounding improves information reliability but may lead to vague   responses. Adding control boosts response speci\ufb01city, but   using both leads to contentful and reliable responses.   Wikipedia page about La La Land in scenario II of Figure 1) without control and semantic targeting may induce   output that is accurate but vague or irrelevant. Controllable   text generation (Hokamp and Liu 2017; Keskar et al. 2019;   Tang et al. 2019; See et al. 2019) provides a level of semantic control that can guide the decoder towards relevant   output, but in the absence of grounding the model is prevented from associating control phrases with correct facts.   We posit that both grounding knowledge and lexical control   are essential to generating reliable information. We therefore introduce a generation framework called controllable   grounded response generation that incorporates both components. Lexical controls not only enforce response speci\ufb01city,   but \ufb01lter lengthy, irrelevant or incoherent groundings.   We consider two scenarios for lexical control of conversational text generation. Control can come from a human user,   as in applications where an editorial assistant helps a person   write a message. Figure 2 depicts a person typing keywords   to indicate their semantic intent, while the machine helps   construct the response to be sent out. Alternatively, control   could be predicted in a fully automated system.   The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)   14085   Damien       A: You should check out the    movie La La Land .   B: What\u2019s about it?   A is typing ...   you may type ...   It\u2019s a musical film directed by Damien    Chazelle and Ryan Gosling is in it!   A drummer, Damien Chazelle wrote    the screenplay for the movie in 2010.   La La Land was praised for Damien    Chazelle\u2019s screenplay and direction.   Figure 2: The machine acts as a response editorial assistant   that suggests candidate responses for the user A according   to the conversation history, the user\u2019s partial input (Damien)   and grounding knowledge.   This work makes the following contributions: (1) We propose a novel framework called controllable grounded response generation (CGRG) that generates a response from   the dialogue context, lexical control phrases and groundings. To the best of our knowledge, this is the \ufb01rst work   to integrate both control and grounding into response generation, and to explore how they can be mutually bene\ufb01cial. (2) We combine recent success in transformer-based   generation models and a novel inductive attention mechanism to this problem setting. (3) We show through qualitative and quantitative evaluations that CGRG outperforms   strong baselines where: a) the control phrases are provided   by a (simulated) user, and b) automatically extracted by a   control phrase prediction model.   2   Approach   We formalize the problem as follows: given dialogue context X, p lexical control phrases C = (C1, \u00b7 \u00b7 \u00b7 , Cp) and q   sentences of grounding G = (G1, \u00b7 \u00b7 \u00b7 , Gq), generate a response R = (r1, \u00b7 \u00b7 \u00b7 , rm) that contains semantic information guided by C. Control phrases can be either directly   provided by a user or automatically derived from a control   phrase predictor. The CGRG framework assumes we have a   grounded conversational dataset, such as in (Qin et al. 2019).   We assume that each data instance consists of a dialogue   context, grounding knowledge and a reference response. To   analyze this framework, we de\ufb01ne a control mechanism that   de\ufb01nes one or more control phrases for each instance. The   controls are lexical phrases that are relevant to both the target response and some part of the grounding knowledge.   To leverage the recent success in transformer-based generation models 2 within CGRG, we concatenate X, C and GC   to be our input sequence, as shown in Figure 3 (left). Then   we have the model predict the next response word given the   2Although our model can be generalized to any attention-based   model, we illustrate our method with the basic auto-regressive generation model like GPT-2.   concatenated input sequence (denoted as S) and the previous   response tokens in R. GC is the subset of G that is relevant   to C. For example, in this work, we denote the grounding   sentences that contain any phrase in C as GC. To differentiate the input elements, we insert an end-of-text token \u27e8eos\u27e9   at the end of each dialogue utterance in X, a \u27e8c\u27e9token at the   end of each control phrase in C and a \u27e8s\u27e9token at the end of   each sentence in GC.   We \ufb01rst concatenate the input sequence S and the response sequence R into a long text. We denote the source   sequence as S = (w1, \u00b7 \u00b7 \u00b7 , wn), which is used to generate   target sentence R. The conditional probability of P(R|S)   can be written as the product of conditional probabilities:   p(R|S) =   m+1   Y   k=1   p(rk|w1, \u00b7 \u00b7 \u00b7 , wn, r1, \u00b7 \u00b7 \u00b7 , rk\u22121)   where rm+1 is the additional end-of-text token.   2.1   Inductive Attention   As shown in Figure 3 (left), a standard transformer-based   generation model takes consecutive text sequences as input   to train a language model. In our setting, we have input elements X, C, GC in a segmented format. Simply concatenating all these input elements can induce noise, as segments   may have differential relevance, and we consider attention   links between such segments to be uninformative.   We remove potentially uninformative attention links for   each data example by injecting pre-established structural information between C and GC. For example, in Figure 3   (right), say that C consists of C1, C2, C3, and GC consists   of G1 and G2. If we know C1 is only found in G1, then we   only want to keep the attention link between C1 and G1, and   not between C1 and any of the other grounded sentences.   Since GC is a set of segmented sentences from G, we remove all cross-sentence links within GC tokens. Similarly,   we remove all links between non-identical phrases (e.g., tokens in C1 do not attend to tokens in C2). Thus, the attention   links for each data example are pre-determined by structural   information between C and GC. To implement this, in each   transformer layer, we manipulate attention masks by setting   undesired attention links to be 0. The others remain at 1.   We refer to this pre-calculated attention as inductive attention. Each response token still attends to all input tokens and   other response tokens on its left.   We denote the start and end positions of a control phrase   Ci \u2208C in S by cs   i and ce   i, and those of a grounding sentence   Gj \u2208GC by gs   j and ge   j. GCi denotes the set of grounding   sentences in GC that contain Ci. Then we calculate the attention mask M as follows:   Ma,b =   \uf8f1   \uf8f4   \uf8f4   \uf8f4   \uf8f4   \uf8f4   \uf8f2   \uf8f4   \uf8f4   \uf8f4   \uf8f4   \uf8f4   \uf8f3   0   if a < b   0   if a \u2208[cs   i, ce   i], b \u2208[cs   i\u2032, ce   i\u2032], i \u0338= i\u2032   0   if a \u2208[gs   j, ge   j], b \u2208[gs   j\u2032, ge   j\u2032], j \u0338= j\u2032   0   if a \u2208[cs   i, ce   i], b \u2208[gs   j, ge   j], Gj \u0338\u2208GCi   1   otherwise   Then for each transformer head, we have the stacked matrices Q, K and V to represent each example sequence (concatenated S and T) as in (Vaswani et al. 2017). We calculate   14086   No Inductive Attention   With Inductive Attention   Figure 3: Without Inductive Attention, the model considers all possible forward attentions, which can overwhelm the model   when the context contains context (X), grounding (G), and constraints (C). In contrast, Inductive Attention uses attentions that   are relevant to the constraints. Each dashed arrow applies to all tokens in the corresponding X or C phrase or G grounding.   the attention as follows (d is the model dimension):   Attention(Q, K, V ) = softmax(M \u25e6QKT   \u221a   d   )V   2.2   Control Phrase Selection   User Control.   Our user control is de\ufb01ned by lexical   phrase(s). Since it is costly to have humans annotate the   control phrases associated with an existing set of human   comment-response pairs, we use lexical matching to simulate the human-controlled scenario. Speci\ufb01cally, we de\ufb01ne   control phrases C to be informative n-grams (n \u22645) that   appear in both the grounding document and the reference   response, where informativeness is de\ufb01ned based on a document frequency threshold. When two n-grams are identical   except for an added function word or punctuation, we use   only the shorter version. In addition, we remove the matched   n-grams that appear in dialogue context, with the goal of focusing on providing new information. We provide human   veri\ufb01cation details for such control phrases in Section 3.   Automatic Control Phrase Predicting.   For the fully automatic scenario, we experiment with two control phrase   predictors. We denote these as \u02dcC to differentiate from the   user-provided C. The \ufb01rst predictor uses a simple retrievalbased strategy. Given a dialogue context, we rank sentences   in G by IDF-weighted word overlaps with X and select the   two most frequent n-grams in the top 50 sentences as \u02dcC. In   order to reduce the search space, we use noun phrases only.   The second predictor leverages the BERT QA system,   which is \ufb01ne-tuned using the dialogue context X as the   query, G as the document and the control phrases in C as   answers. Then we use the \ufb01ne-tuned model to predict answers on test examples. We obtain the top two answers as   predicted control phrases \u02dcC. For both predictors, we drop   the second phrase if the string fully overlaps with the \ufb01rst.   3   Controllable Conversation Dataset   As a social media aggregator, Reddit is effectively a dataset   of multiple domains. We start with the grounded Reddit conversation dataset described in Qin et al. (2019). This dataset   features Reddit conversations about web pages such as news   stories and Wikipedia articles, and covers 178 subreddit topics ranging from news/technology to literature/music and   multiple writing styles.   We want to focus on contexts where controllable generation is useful. Thus, we keep only responses where at least   one matched phrase can be found in the grounding document. Strict lexical matching between target response and   grounding assures that the retained examples have a high   ratio of grounding utilization. The number of utterances of   train, dev and test are 390K, 6.7K and 21K, respectively.   The average length of all reference responses is 26.5, which   is about 40% longer than in the full dataset due to the focus   on controllability. The average numbers of phrases in C for   train, dev and test set are 1.32, 1.27 and 1.38 respectively.   The average numbers of sentences in GC for train, dev and   test set are 4.37, 4.32 and 4.25 respectively.   To verify that the simulated user control phrases are appropriate for the responses, we use crowd-sourced workers   to annotate whether the extracted control phrases are central   to the reference response, given the dialogue context. For   each response, we had 3 judges to enter a score on a scale   of 1 (completely unrelated) to 6 (very central), where 5 was   \u201csomewhat central\u201d and 4 was \u201cneutral.\u201d In 2000 annotated   examples, the median score was 4.33 and 67.4% of examples had a score over 4. Inter-rater agreement was \u201cfair\u201d with   Krippendorff\u2019s alpha coef\ufb01cient at 0.32.3   4   Experimental Setup   4.1   Training and Inference Setup   In our experiments, all transformer-based models have both   type and positional embedding for each input token. We treat   X, each sentence in GC, each phrase in C and response R   as separate segments. We set the maximum number of sentences in GC to be 20 and maximum number of phrases in C   3This dataset is a \ufb01ltered version of (Qin et al. 2019)\u2019s public dataset. To further facilitate reproducibility, we release our data   preparation and modeling code at https://github.com/ellenmellon/   CGRG.   14087   to be 10, then we have \u201c0\u201d for X; \u201c1-20\u201d for GC; \u201c21-30\u201d   for C and \u201c31\u201d for R tokens as type embedding. For each   segment, we have the position embedding for each token as   its position in that segment.   We use GPT-2 as the basic transformer-based generation   model in our experiments for drawing fair comparison with   the DialoGPT (Zhang et al. 2020) architecture, a state-ofthe-art conversational response generation model trained on   147M Reddit comment chains on the basis of GPT-2. We use   the small version of GPT-2 with 117M parameters, with the   maximum length of the input or target response sequence   to be 512. We use BPE tokenization, following GPT-2. We   \ufb01netune our model and all other GPT-2-based baselines (including DialoGPT) on our controllable and grounded Reddit dataset on top of the original DialoGPT. None of their   Reddit training or validation examples overlap with our test   examples. We use batch size 32. Learning rate (1e-5) and   warmup steps (1600) are tuned on the dev set perplexity,   with all other parameters being the same as DialoGPT 4.   Each training process is run on 2 Tesla K-80 nodes.   We use greedy search as the decoding strategy for all   GPT-2 and GPT2IA setups, except for a single experiment   setting where grid beam search (GBS) (Hokamp and Liu   2017) is applied for comparison with lexically constrained   decoding. We also compare our methods with GBS to investigate whether it helps to encode the constraints into the   hidden state during both training and inference, as GBS uses   lexical constraints only during inference.   4.2   Evaluated Systems   Models   Although our designed model can be applied to   any transformer-based generation model, we use GPT-2 as   our base model for experiments. We use the following models for experiments: (a) GPT-2, which has the same architecture as DialoGPT (Zhang et al. 2020), under the input   setting X (see below) or the \ufb01rst line in both Table 1 and   Table 3; (b) GPT2IA model with inductive attention; (c)   GPT-2 + GBS that applies the attended GPT-2 model, while   control phrases C are given to the model at decoding time;5   and (d) CMR (Qin et al. 2019), which is a previous state-ofthe-art grounded response generation model on the Reddit   dataset that combines a MRC model and an LSTM decoder.   Input Settings   We evaluate the above models according to   the following settings to analyze how control and grounding   help improve the response generation performance:   \u2022 X: This is the standard setting for non-controllable response generation, where only the dialogue context is   given. We conduct experiments for the GPT-2 generation   model. Note that GPT-2 in this setting is the same as the   DialoGPT architecture.   4All model con\ufb01gurations in Section 4.2 share the same hyperparameter values (only tuned 3 times on top of the original values   from DialoGPT) except CMR, which has the same parameters in   the original paper.   5Such constrained decoding is based on grid beam search   (GBS) introduced in Hokamp and Liu (2017), where lexical control   phrases are added in decoding only, without involving training.   \u2022 X+G: This is the standard setting for grounded response   generation. We compare two models: CMR and GPT-2.   GPT-2 for this setting concatenates X and G as its input.   As both models have an input sequence length limit, only   a random subset of grounding is fed into each model.   \u2022 X+C: This is the controllable response generation setting without grounding. We conduct GPT-2 experiments   by concatenating X and C.   \u2022 X+GC: This setting measures how the grounding only   relevant to C can help with response generation, without   explicitly providing C. We conduct experiments for GPT2, by concatenating X and GC as the input.   \u2022 X+C+GC: This setting measures how grounded control   can help with response generation. We conduct experiments for GPT-2 and GPT2IA, by concatenating X, GC   and C as the input.   \u2022 X+C+G: This setting is for comparison against existing constrained generation methods like grid beam search   (GBS) introduced in Hokamp and Liu (2017), where lexical control phrases are added in decoding only without   involving training. We conduct experiments for GPT-2   where X and G are the only encoded inputs and C is only   applied in decoding with GBS.   4.3   Automatic Evaluation   Previous work (Li et al. 2016b; Sun and Nenkova 2019) has   shown that automatic metrics for generation can be unreliable and have low absolute values, so we rely on human evaluation for our main conclusions. However, due to the high   cost of human evaluation, automatic evaluation metrics can   be useful for hyper-parameter tuning and model selection.   For automatic evaluation, we measure the relevance of the   generated responses with three metrics: BLEU-4 (Papineni   et al. 2002), NIST-4 (Doddington 2002) (a variant of BLEU   that weights n-gram matches by their information gain, penalizing uninformative n-grams), and diversity of bigrams in   generated responses (Div-2, the ratio between the number of   distinct vs. total bigrams). The human evaluation is used to   verify the improvements of the best case systems as determined by the automatic metrics.   We experiment with both user-controllable and fully automatic response generation, with simulated user-selected   and predicted lexical control phrases, respectively. As different reference responses correspond to different \u201cgold\u201d control phrases, we use single-reference evaluation for the usercontrollable setting. Predicted control phrases are independent of reference responses, so we use multi-reference evaluation. Comments in Reddit discussions are often associated   with multiple responses, which provide a multi-reference   test set. For each metric, we report the highest score among   up to 5 alternative human", "conf": "AAAI", "year": "2021", "index": 481}, {"title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7339\u20137353   August 11-16, 2024 \u00a92024 Association for Computational Linguistics   Generate-then-Ground in Retrieval-Augmented Generation   for Multi-hop Question Answering   Zhengliang Shi1 Shuo Zhang2 Weiwei Sun1 Shen Gao3   Pengjie Ren1 Zhumin Chen1 Zhaochun Ren4\u2217   1Shandong University, Qingdao, China   2Bloomberg, London, United Kingdom   3University of Electronic Science and Technology of China, Chengdu, China   4Leiden University, Leiden, The Netherlands   shizhl@mail.sdu.edu.cn   szhang611@bloomberg.net   sunnweiwei@gmail.com   z.ren@liacs.leidenuniv.nl", "abstract": "Multi-Hop Question Answering (MHQA) tasks   present a significant challenge for large   language models (LLMs) due to the intensive   knowledge required. Current solutions, like   Retrieval-Augmented Generation, typically   retrieve potential documents from an external   corpus   to   read   an   answer.   However,   the performance of this retrieve-then-read   paradigm is constrained by the retriever   and the inevitable noise in the retrieved   documents.   To mitigate these challenges,   we introduce a novel generate-then-ground   (GenGround) framework,   synergizing the   parametric knowledge of LLMs and external   documents to solve a multi-hop question.   GenGround empowers LLMs to alternate two   phases until the final answer is derived: (1)   formulate a simpler, single-hop question and   directly generate the answer; (2) ground the   question-answer pair in retrieved documents,   amending any wrong predictions in the answer.   We also propose an instructional grounding   distillation method to generalize our method   into smaller models. Extensive experiments   conducted on four datasets illustrate the   superiority of our method.   1", "content": "Multi-Hop   Question   Answering   (MHQA)   tasks (Yang et al., 2018) require multi-hop   reasoning using intensive knowledge to derive   the answer (Xu et al., 2024). It has been widely   employed in various practical scenarios and   domains (Mavi et al., 2022).   To answer a   multi-hop question, most prior work integrates   large language models (LLMs) with information   retrieval techniques, following a retrieve-then-read   paradigm (Shao et al., 2023).   As illustrated   in Figure 1, the initial step employs the LLMs   to   break   down   the   complex   question   and   formulate a series of simpler, single questions   \u2217Corresponding author.   Question: What was the name of the theatrical program founded    by Joseph Papp?    Retrieve-then-Read   Retrieve   Generate-then-Ground (ours)   Read   Docs   Reason   Retrieve   Generate   Ground   Docs   42.43   13.4   44.43   71.65   47.27   20.24   46.87   78.12   0   20   40   60   80   HotpotQA   MusiqueQA   2WikiQA   StrategyQA   Retrieve-then-read   Generate-then-ground (ours)   Accuracy   question   sub-question   answer   Finish   question   sub-question   answer   revised    answer   Finish   Figure 1: The top block depicts the comparison with the   commonly-used retrieve-then-read paradigm in MHQA   task. The bottom block provides the performance of our   method and baselines in four MHQA benchmarks.   in a step-by-step manner.   For each step, the   LLMs are guided to derive an answer from the   relevant documents, which are retrieved using the   formulated question (Yao et al., 2022).   Despite the progress of the retrieve-then-read   paradigm, it faces two challenges in practice. First,   its effectiveness is constrained by the performance   of the retriever (Yu et al., 2022; Xu et al.,   2024). Given that the answer can only be derived   from the retrieved documents, the inherent world   knowledge of LLMs is overlooked. This limitation   is particularly magnified in multi-hop QA tasks that   frequently require complex logical reasoning (Mavi   et al., 2022).   The retrievers may struggle to   retrieve all necessary documents to answer the   question, leading to a performance decline using   this paradigm (Yu et al., 2022; Abdallah and   Jatowt, 2023). Second, the retrieved documents   7339   inevitably contain irrelevant or plausible-looking   statements (Gao et al., 2023b; Jiang et al., 2023b).   Directly incorporating them into the chain-ofreasoning of LLMs may mislead the LLMs to   generate incorrect or irrelevant responses (Adlakha   et al., 2024; Thakur et al., 2023b).   Therefore,   developing an adaptive method for utilizing the   retrieved documents remains an active research   area.   Inspired by the extensive knowledge and   powerful deduction capability of LLMs, e.g.,   ChatGPT or GPT-4, we propose to address these   challenges with a novel generate-then-ground   (GenGround) method, as shown in Figure 1. This   approach diverges from the retrieve-then-read   paradigm by first allowing the LLMs to generate an   immediate answer, and then grounding this answer   in evidence to revise it. In this work, we focus on   the following research questions: (1) RQ1: How   does our method synergize the world knowledge of   LLMs and retrieved documents to answer a multihop question? (2) RQ2: For LLMs with different   scales of parameters, how do we generalize our   method?   To address RQ1, we enable LLMs to alternate   between answer deduction and instructional   knowledge grounding.   In the deduction phase,   LLMs form sub-questions from the input question   and context. The LLMs then produce an immediate   answer for each sub-question. To prevent nonfactual hallucination by LLMs (Zhang et al., 2023;   Gao et al., 2023a), we guide LLMs to revise the   answer in the grounding phase, using documents   retrieved from an external corpus like Wikipedia.   The LLMs ground the question-answer pair in   evidence by citing relevant content and correcting   errors. This revised answer is used for the next   iteration\u2019s sub-question, continuing till the final   hop. We also introduce a batch grounding strategy   for efficient document use.   To address RQ2, we propose an instructional   grounding distillation method.   Despite LLMs   like ChatGPT performing well with our method,   smaller models may struggle with instructionfollowing in the grounding phase. Thus, we use 50k   single-hop questions from the Natural Questions   (NQ) dataset (Kwiatkowski et al., 2019), each   with ground-truth and noise documents. We guide   ChatGPT to generate and adjust an answer for each   question, then distill ChatGPT\u2019s process into a   student model using instruction tuning.   Extensive experiments on four commonly-used   MHQA datasets demonstrate superior performance   over strong baselines (e.g., ReAct and DSPy),   achieving the best performance overall.   We   also observe that our instructional grounding   distillation empowers the smaller model with   strong performance.   To sum up, our main contributions are as   follows:   (1) We propose a novel generatethen-ground framework for retrieval-argument   generation technique in multi-hop question tasks,   which effectively synergizes the knowledge of   LLMs and retrieved documents. (2) We introduce   an instructional grounding distillation method,   enabling a smaller model with the generatethen-ground framework.   (3) Experiments on   four datasets are conducted to demonstrate the   superiority of our method.   2   Related work   2.1   Multi-hop Question Answering   Multi-Hop Question Answering (MHQA) tasks   focus on answering questions that require gathering   information from multiple sources and conducting   multi-step reasoning to arrive at a comprehensive   answer (Zhang et al., 2024; Li and Du, 2023).   Some works utilize the knowledge deduction   capability (Wei et al., 2022) of LLMs to decompose   the input question into single-hop questions and   then solve them step-by-step (Wang et al., 2022a,   2023). And many techniques (Yao et al., 2023;   Besta et al., 2023) are proposed to improve the   reasoning ability of LLMs. However, the LLMs   suffer from generating non-factual statements. As   an intuitive solution, many recent works integrate   retrieval into the chain of thought reasoning   process (Yao et al., 2022; Schick et al., 2023),   prompting the LLMs to generate the answer using   retrieved documents.   Although promising, the   inevitable noise in retrieved documents could   mislead the LLMs to a wrong reasoning direction   and derive a wrong answer (Xu et al., 2024). In   this work, we propose an instructional knowledge   grounding method that enables LLMs to find the   most relevant evidence from the document list,   thereby reducing the effect of the noise.   2.2   Retrieval-Augmented Generation   Retrieval-augmented generation (RAG) has been   proven a promising technique to improve the   performance of LLMs in knowledge-intensive NLP   tasks (Zhu et al., 2023; Yu et al., 2023), which   7340   Answer    Deduction   Revise   Retriever   Grounding   (a) Answer Deduction   (b) Instructional    knowledge Grounding   evidence   Question: What was the original name of the theatrical program    that was founded by Joseph Papp?    immediate answer   doc   doc   doc   revised    answer   sub-question   ith iteration   i+1th iteration   Figure 2: The architecture of the proposed generatethen-ground framework.   enhances LLMs with retrievers to access external   knowledge.   Existing RAG methods typically   follow a retrieval-then-read pipeline (Ma et al.,   2023; Feng et al., 2023; Gao et al., 2023b). Given   a query, a retriever is first employed to retrieve   the relevant document and a reader is then used to   predict the answer on the condition of retrieved   documents (Khattab et al., 2023).   Despite the   advancement of this paradigm, it is limited by the   accuracy of retrievers (Yu et al., 2022; Zhang et al.,   2024).   Recent   works   mitigate   this   problem   by   incorporating the world knowledge of LLMs (Gao   et al., 2023a; Chen et al., 2023), where they   utilize   the   LLM   as   a   knowledge   base   to   generate contextual documents and then read   the answer from both retrieved and generated   documents (Abdallah and Jatowt, 2023). However,   the potential knowledge conflict between the   two knowledge sources is ignored, which may   hallucinate the LLMs (Xie et al., 2023; Thakur   et al., 2023a; Mallen et al., 2023). In our work, we   propose a generate-then-ground framework, more   effectively incorporating the parametric knowledge   of LLMs and external documents.   3   Generate-then-Ground with LLMs   This section provides a detailed explanation of   GenGround. As depicted in Figure 2, GenGround   empowers the LLMs to alternate between two   phases over multiple iterations until the final   answer a, is derived.   These phases include   answer deduction (Section 3.1) and instructional   knowledge grounding (Section 3.2). During each   iteration, the former guides the LLMs to generate   Please decompose a multi-hop question into sub-questions and    answer the sub-questions step by step.    Starting below, you should interleave Deduce and Answer until    deriving at the final answer.    - Deduce: deduce the current context and then formulate a subquestion   - Answer:  answer the deduced question   Here are some examples: {examples}   (a) Answer Generation   (b) Instructional Knowledge Grounding   Here is an answer to the question. Please cite evidence from the    documents list to revise the answer. You should encapsulate the    evidence using \u2019\u2019<ref></ref>\u2019\u2019, and the revised answer using    \u2018\u2019<revise> </revise>\u2018\u2019.   If no evidence can be found, just give \u2019\u2019<ref> Empty </ref>\u2018\u2019.   <revise> {Revised answer} </revise>   <ref> Evidence </ref>   {question}   {answer}   Figure 3: The instruction for the answer deduction (a)   and instructional knowledge grounding(b) phases in our   framework. The pink and yellow blacks indicate the   input while the gray blocks indicate the output.   a simpler, single-hop answer. The latter phase   addresses LLMs\u2019 non-factual hallucination (Zhang   et al., 2023) by prompting them to ground the   question-answer pair in evidence and correct wrong   predictions. The revised answer and sub-question   are then integrated into the LLMs\u2019 context for the   following iteration\u2019s prediction.   3.1   Answer Deduction   The answer deduction phase aims to utilize the   world knowledge of LLMs stored within their   parameters \u03b8.   Given the complex reasoning   involved in multi-hop questions (Tang et al., 2021;   Li and Du, 2023), it can be challenging to generate   an accurate answer directly. As a result, we guide   the LLM, denoted as M\u03b8, to break down a complex   question Q into single-hop questions with a fine   granularity. Formally, for i-th iteration, we denote   the current context as H = {(qj, \u02dcji)|j < i},   which comprises the accumulation of previous   deduced sub-questions, q<i, and revised answers,   \u02dca<i. The context H, along with the input question   Q, is then fed into the LLMs to generate a subquestion, qi, that defines the specific information to   be retrieved. Subsequently, we prompt the LLMs,   M\u03b8, to directly generate an answer, ai, for the   formulated question, qi. This can be formulated as   follows:   qi, ai = M\u03b8(IA, Q, Hi).   (1)   7341   doc5   doc6   doc7   doc10   doc4   question   answer   LLM   doc1   doc2   doc3   <ref> Empty </ref>   \u2026   Evidence   Earlier stop   Revised answer   No evidence   Current batch   Figure 4: Demonstration of our batch grounding strategy   with the batch size of 3 and retrieved documents amount   of 10, where the LLMs ground the input questionanswer pair into the second batch.   Here, the IA represents the instruction shown in   Figure 3, which includes the task demonstration   and in-context learning examples.   3.2   Instructional Knowledge Grounding   Given that LLMs may generate non-factual   statements or \u201challucinations\u201d, we further guide   the LLMs to revise the generated answer, ai, with   the support of retrieved documents. Specifically,   in the i-th iteration, we initially utilize a retriever   (e.g., Google, BM25, or dense retrieval model) to   retrieve relevant documents, Di, using the deduced   sub-question qi (Equation 1):   D = Retrieval(qi).   (2)   We then guide the LLMs,   M\u03b8, to ground   the question-answer pair, (qi, ai), in established   evidence by citing the most relevant content, \u02dcd,   from the retrieved documents, Di. Subsequently,   we revise the answer ai:   \u02dcai = M\u03b8(IG, Q, qi, ai).   (3)   As depicted in Figure 3(b), IG represents our   grounding instruction in a zero-shot setting, while   \u02dcai is the revision trajectory, which includes the   cited evidence and the revised answer. The revision   trajectory \u02dcai, along with the question qi, are then   combined to build the context, Hi+1 = Hi \u222a   {(qi, \u02dcai)}, for the LLMs in the subsequent iteration.   If no relevant content can be cited (e.g., the citation   is Empty), we keep the generated answer as the   revised answer without any changes.   3.3   Batch Knowledge Ground   Since retrieved documents are typically lengthy   and contain inevitable noise (Xu et al., 2024),   the LLMs are susceptible to being misled by   plausible-looking statements during the grounding   phase (Sun et al., 2023a; Thakur et al., 2023a).   Therefore, we propose a simple yet efficient batch   grounding strategy. Suppose the batch size is b.   We first utilize the LLMs to revise the generated   answer ai using the (1, b)-th documents. If relevant   evidence can be cited to revise the answer, we end   our grounding phase for the current iteration and   move to the next iteration. Otherwise, we prompt   the LLMs to generate an \u201cEmpty\u201d signal and then   access the (b + 1, 2b)-th documents sequentially.   This process continues until the relevant evidence   can be found to support our grounding phase.   Figure 4 shows a concrete example with ten   retrieved documents. If no relevant document can   be found, we directly output the generated answer   as a backup.   4   Generalization with Grounding   Distillation   While LLMs like ChatGPT are skilled and adept   at following instructions, they are often considered   black boxes (Qin et al., 2023; Gao et al., 2024) and   their extensive parameters can increase latency and   inference cost in real-world applications (Sun et al.,   2023b). Thus, we aim to adapt our framework to   smaller, open source models with fewer parameters.   Initial experiments show these smaller models   struggle to cite relevant evidence during the   knowledge grounding phase. To overcome this,   we introduce Instructional Grounding Distillation   (IDG), which distills the output trajectory of   ChatGPT into a smaller student model.   4.1   Synthesize the Training Dataset   The instructional grounding distillation collects   the trajectory of LLMs, i.e., ChatGPT, during the   instructional knowledge grounding (Section 3.2).   This trajectory is then used as the training   dataset to distill the grounding capability into a   student model. To achieve this, we first sample   50k questions from the Natural Questions (NQ)   dataset (Kwiatkowski et al., 2019). Each question   q is paired with a corresponding ground-truth   document \u02dcd and the noise documents D.   The   questions in the NQ dataset are of high quality   and single-hop, making them inherently similar to   the setting of our instruction knowledge grounding   (Section 3.2). Next, we supplement each question   with an immediate answer a and a detailed revision   trajectory \u02dca. Specifically, the immediate answer a   is generated directly by feeding the question q into   a smaller model, such as Mistral-7B (Jiang et al.,   2023a). The revision trajectory \u02dca is generated by   ChatGPT with the assistance of the ground truth   7342   Statistic   # The data scale   45,710   # The average length of input instruction   70.87   # The average length of output   683.21   # The average number of ground truth documents   1.00   # The average length of ground truth documents   117.57   Table 1: The statistics of our synthetic dataset in the   instructional grounding distillation method.   document. Various heuristic methods are also used   to filter low-quality output (see Appendix III for   more details). The statistics of out synthetic dataset   is provided in Table 1.   4.2   Training Objective   Formally, for each question q in our synthetic   dataset, we train the model to cite the relevant   content from a document list and revise any   incorrect predictions in the immediate answer a   following the instruction IG. Using the collected   revision trajectory \u02dca, we apply the standard   language modeling loss to optimize the student   model:   LG = \u2212log P\u03b8(\u02dca|IG)   = \u2212   |\u02dca|   X   t=1   log P\u03b8(\u02dcat|\u02dca(<t), IG, { \u02dcd} \u222aD).   (4)   Here, the \u02dcd indicates the ground-truth document,   and D indicates the noise documents.   5   Experimental Setup   5.1   Datasets   In line with previous research (Lewis et al., 2020;   Xu et al., 2024), we conduct experiments on   four common-used MHQA benchmarks, namely   HotpotQA (HQA) (Yang et al., 2018), MuSiQue   (MQA) (Trivedi et al., 2022), 2Wikimultihopqa   (WQA) (Ho et al., 2020),   and StrategyQA   (SQA) (Geva et al., 2021). StrategyQA is derived   from BIG-bench1. For the remaining benchmarks,   we randomly sample 1.4k questions adhering   to RetGen (Shao et al., 2023) and Verify-andEdit (Zhao et al., 2023).   5.2   Baselines   We compare our method with Generation w/   Retrieval and Generation w/o Retrieval methods,   1https://github.com/google/BIG-bench   respectively. The w/ indicates with while the w/   indicates without.   The Generation w/o Retrieval methods utilize   the parametric knowledge of LLMs to answer   questions. This includes (1) CoT (Wei et al., 2022),   which prompts the LLMs to a series of intermediate   reasoning steps when answering a question; (2)   CoT-SC (Wang et al., 2022b), which samples a   diverse set of reasoning paths and then selects   the most consistent answer; and (3) GenRead (Yu   et al., 2022), which generates the answer by reading   from the documents generated by LLMs.   The   Generation w/ Retrieval methods augment LLMs   with retrievers to access external knowledge when   answering questions, including: (1) ReAct (Yao   et al., 2022), which interleaves question generation,   document retrieval, and knowledge incorporation   to answer a question; (2) GRG (Abdallah and   Jatowt, 2023), reading an answer from both   retrieved documents and contextual documents   generated by LLMs; (3) RetGen (Shao et al.,   2023), which enhances LLMs with an iterative   retrieval-generation synergy strategy to answer   a multi-hop question.   (4) DSP (Khattab et al.,   2022), a programming framework empowered by   LLMs; and (5) SearChain (Xu et al., 2024), which   dynamically interacts with the retriever to verify   and correct the generated answers.   Considering   the   complexity   of   multi-hop   questions, we enhance GenRead and GRG with   the chain-of-thought technique (w/ decomposition),   dividing the input question into sub-questions and   using GenRead (or GRG) for each sub-question.   5.3   Evaluation Metrics   Following previous studies (Xu et al., 2024; Ren   et al., 2023), we use Accuracy (Acc) and F1   metrics for evaluation.   The accuracy metric   checks if the ground truth answer is in the   generated answer, which is also named coverEM. The F1 score is used to measure the overlap   between the generated answer and the ground truth   answer, which represents the harmonic mean of   precision and recall.   Recall is determined by   considering the number of overlaps with the correct   answer tokens, while precision is determined   by considering the number of overlaps with   all generated tokens.   We also assess semantic   accuracy (Acc\u2020) using gpt-3.5-turbo-instruct2 for   a more thorough evaluation, which prompts the   2https://openai.com/   7343   Methods   HotpotQA   MuSiQue   2Wikimultihopqa   StrategyQA   F1   Acc   Acc\u2020   F1   Acc   Acc\u2020   F1   Acc   Acc\u2020   Acc   Generate w/o Retrieval   CoT (Wei et al., 2022)   35.28 30.79 37.07 23.35 13.21 17.85 35.41 32.46 34.52   67.83   CoT-SC (Wang et al., 2022b)   42.25 38.68 39.07 15.61 10.02 12.42 40.37 36.57 38.59   70.84   GenRead (Yu et al., 2022)   35.21 36.81 37.54   9.77   9.29   10.32 23.13 20.62 28.31   67.13   GenRead w/ decomposition   42.28 43.32 45.31 20.13 17.58 20.62 41.19 41.63 43.24   68.13   Generate w/ Retrieval   VE (Zhao et al., 2023)   29.64 22.64 24.64   6.5   11.14 15.57 13.76 31.57 32.64   63.07   ReAct (Yao et al., 2022)   40.70 33.10 37.12 15.34 17.32 19.32 35.50 30.10 33.41   68.37   GRG w/ decomposition   50.21 45.18 50.80 24.87 17.91 22.33 40.42 40.48 43.05   75.21   RetGen (Shao et al., 2023)   28.30 41.04 44.10 21.04 17.69 20.19 36.00 42.17 45.21   73.42   SearChain (Xu et al., 2024)   46.76 48.12   17.07 20.45   42.14 46.27   76.95   DSPy (Khattab et al., 2023)   47.80 42.43 50.07 20.11 13.40 17.40 44.77 43.43 45.43   71.78   GenGround (Ours)   52.26 47.27 55.73 27.36 20.24 24.77 50.21 45.61 48.58   77.12   Table 2: Evaluation results on multi-hop question answering datasets. Acc\u2020 indicates the semantic accuracy of   model outputs evaluated with gpt-3.5-turbo-instruct with the same prompt. Since the SearChain prompts the   LLM to generate a long-form answer while the ground truth answer in our dataset is short-form, we only evaluate it   with the Acc and Acc\u2020 metrics.   Method   HQA   MQA   WQA   Average \u2206   Retrieval \u2192ColBERTv2   Ours (IDG)   42.08   14.37   32.69   Ours (Vanilla)   38.31   11.34   29.45   3.35\u2193   DSPy   36.41   7.42   28.31   5.67\u2193   GRG w/ dp   32.56   9.34   25.63   7.20\u2193   RetGen   26.52   10.12   24.13   9.45\u2193   SearChain   24.62   9.46   26.53   9.51\u2193   Table 3: Accuracy (Acc) on three datasets with Mistral7B as backbone. The w/ dq indicates decomposition.   The Vanilla and IDG indicate enable our framework by   prompting and our grounding distillation, respectively.   Ablation   HQA   SQA   F1   Acc   Acc\u2020   Acc   w/o deduction   42.65 \u21939 41.08\u21936 43.14 \u219312 66.51 \u219310   w/o grounding   45.14\u21937   41.35\u21934   43.23\u21935   72.34\u21935   w/o batch   47.27\u21935   45.03\u21932   51.19\u21934   71.72\u21935   Table 4: Evaluation results of our ablation study on two   MHQA benchmarks.   LLMs to evaluate the correctness of the generated   answer taking the ground truth answer as reference.   In this work, we implement the Acc\u2020 using gpt-3.5turbo-instruct and the full prompt for our evaluation   can be found in Appendix I.   To counter the potential bias of automatic   metrics   (Shi   et   al.,   2023),   we   conduct   a   human evaluation, with three educated individuals   assessing the correctness of 120 randomly sampled   cases from four benchmarks on a three-scale rating.   5.4   Implementation Details   We utilize OpenAI\u2019s gpt-3.5-turbo as the backbone   for our framework and all baselines, with the   decoding temperature set to 0 for deterministic   generation, and batch size set to 3 in our   batch grounding strategy.   The open source   model, Mistral-7B (Jiang et al., 2023a), is also   employed for comparison.   We mainly use   ColBERTv2 (Santhanam et al., 2021) as the   retriever, retrieving top-10 documents for each   question.   Alternately, BM25 (Robertson et al.,   2009), Google Search3 serve as retrievers in   our analysis experiment.   Following previous   work (Xu et al., 2024), we use Wikipedia 2017   for HotpotQA, and a large-scale passage collection   built on Wikipedia 2018 for other open-domain QA   benchmarks.   Our instructional grounding distillation trains   Mistral-7B with 50k synthetic examples.   We   optimize the model using the deepspeed ZeRO   strategy (Rasley et al., 2020) with the learning rate   of 5e\u22125 and the weight decay coefficient of 0.01.   The training of our model can be done within 18   hours with 3 NVIDIA A100-PCIE-80GB GPUs.   6   Experimental Results   6.1   Experimental Results   Overall performance.   Table 2 presents the   experiment results.   Our framework surpasses   3https://serper.dev/   7344   Method   HQA   MQA   WQA   Average \u2206\u2193   Retriever \u2192BM25   Ours   42.21   18.32   40.32   DSPy   40.86   15.32   30.85   5.27\u2193   GRG w/ dq   41.31   15.62   38.84   2.36\u2193   RetGen   39.12   8.41   35.83   6.50\u2193   SearChain   39.57   14.93   37.41   3.65\u2193   Retriever \u2192Google Search   Ours   48.95   21.54   46.87   DSPy   46.86   20.71   39.92   3.29\u2193   GRG w/ dq   42.57   18.41   43.21   4.39\u2193   RetGen   42.82   14.27   44.31   5.32\u2193   SearChain   44.35   19.76   44.39   2.95\u2193   Table 5: Accuracy (Acc) on three datasets using BM25   and Google Search as retrievers, respectively. The w/   dq is short for without decomposition.   others on all four datasets and all metrics.   Specifically,   on the HotpotQA dataset,   our   GenGround   achieves   Acc=47.27,   F1=52.26,   and Acc\u2020=55.73, considerably improving over   the Generate w/ Retriever baselines.   It also   significantly   outperforms   retrieval-then-read   baselines like DSPy and SearChain, with a 4-6   point increase in accuracy metrics across all   datasets. The similar improvement is observed in   our human evaluation results (see Appendix IV   for more detials). These results indicate that our   method effectively utilizes world knowledge and   LLMs\u2019 deductive abilities to answer questions.   Results with the smaller model.   We further   evaluate our method by swapping the backbone   LLMs with the open source model, i.e., Mistral7B, and repeating the experiment under the same   conditions. As shown in Table 3, we implement   our methods in two ways with the Mistral-7B: (1)   directly prompting (vanilla); (2) tuning with our   proposed instructional grounding distillation (IGD).   We obverse that directly prompting Mistral-7B with   our method yields better performance compared   with baselines.   The instructional grounding   distillation further improves overall performance   significantly, e.g., pushing the Acc to 42.08 in the   HotpotQA dataset (9.84% relative improvement)   and 14.37 in the MusiqueQA dataset (26.4%   relative improvement).   Results with different retrievers.   We further   evaluate the performance of our framework by   using different retrievers in various retrieval   scenarios.   As shown in Table 5, we replace   ColBERTv2 with BM25 and Google Search, using   ChatGPT as a backbone LLM in all instances.   Our method demonstrates the best performance   regardless of the retriever used, indicating its   adaptability in both low recall (BM25) and high   recall (Google Search) scenarios. This could be due   to our answer deduction phase, which uses LLMs\u2019   parametric knowledge to supplement the retrieved   knowledge. Moreover, our instructional knowledge   grounding phase effectively incorporates the   retrieved document by citing the most relevant   evidence, mitigating the negative impact of noisy   documents.   6.2   Ablation Study   We employ the following modifications and repeat   the experiment in the same setting as Table 2.   w/o deduction. We remove the answer deduction   phase mentioned in Section 3.1, prompting the   LLMs to directly generate an answer for a multihop question and revise it.   As illustrated in   Table 4, we observe 6 and 10 absolute decreases   in HQA and SQA datasets in terms of the Acc   metric, respectively. These results demonstrate the   necessity of deducing the intricate knowledge in   our answer deduction phase.   w/o grounding.   We replace the instructional   knowledge   grounding   phase   mentioned   in   Section 3.2) with directly generating an answer   using retrieved documents As shown in Table 4,   the F1 and Acc metrics have a significant   decline. A potential reason is that the LLMs may   hallucinate the plausible-looking statement in the   retrieved documents. Our instructional knowledge   grounding method further instructs the LLMs to   find the most relevant evidence.   w/o batch.   We remove the batch grounding   strategy in Section 2. As shown in Table 4, the   F1 decreases from 52.26 to 47.27 and the Acc   decreases from 47.27 to 45.03. These comparisons   indicate that the LLMs struggle to generate correct   answers when the reference document list is   lengthy with irrelevant information.   6.3   Case Study   We conduct several case studies and find that   GenGround is more effective at generating high   quality answers to a question. Details can be found   in Appendix II.   7345   Failure rate   Success rate   \u274c   ground   generate   question   answer   revised answer   question   answer   revised answer   question   answer   revised answer   question   answer   revised answer   \u274c   \u274c   Error rate   \u274c   5.6%   28.7%   24.5%   41.2%   Figure 5: The fine-granularity correctness analysis of   our answer deduction and knowledge grounding phases.   7   Analysis and Discussion   7.1   Result Consistency and Stability   We further explore the consistency and stability   of our framework.   Specifically, we repeat the   experiment with the same setting as Table 2 in   the HotpotQA dataset. The statistical significance   of differences observed between the performance   of two runs is tested using a two-tailed paired ttest. We find no significant difference between the   results of two randomly conducted experiments   (significance level \u03b1 = 0.05).   We further   explore the fine-granularity stability for our answer   deduction and instruction knowledge grounding   phases. Specifically, we compute the Rough score   for the trajectory of our method in two repeated   runs. The Rough-1, 2, and L are 81.33, 53.7, and   79.7, which shows the high lexicon similarity for   the output, indicating the stability of our method.   7.2   Knowledge Incorporation   Our method combines the knowledge in LLMs\u2019   parameters and external documents to answer a   question. We explore the synergistic integration of   these two distinct knowledge sources. Specifically,   we calculate the following three metrics:   (1)   Success rate: the rate at which LLMs either directly   generate a correct answer or accurately revise an   incorrectly generated answer; (2) Failure rate:   LLMs generate a wrong answer and fail to correct   it; (3) Error rate: LLMs generate a correct answer   but incorrectly revise it.   Our method addresses multi-hop questions stepby-step. As existing datasets lack the ground truth   answer for immediate answering trajectory, we   invite three annotators to evaluate 100 randomly   sampled cases from the Hotpot QA dataset in   Table 2.   As illustrated in Figure 5, the overall success   rate is 53.2%, with LLMs directly answering   28.7% of questions correctly.   For 24.5% of   the questions though, LLMs initially generate   non-factual statements, and then use external   documents for revision. These results underscore   the importance of incorporating both knowledge   sources.   During our grounding phase, LLMs   may be misled by plausible-looking statements in   the retrieved documents. Therefore, we further   calculate the error rate, which assesses how often   LLMs are incorrect after revisions. We find that   the error rate is only 5.6%, indicating that LLMs   usually use the retrieved documents effectively.   7.3   Qualitative Analysis for Efficiency   The intensive parameters of LLMs typically raise   concern about inference cost. Thus, we compare   token consumption with GRG w/ decomposition   and RetGen, using the HotpotQA dataset in   Table 2.   We show the frequency histogram   for the number of consumed tokens in different   methods in Figure 6.   Though our framework   achieves better performance, we observe that our   method spends fewer tokens compared with strong   baselines RetGen and GRG w/ decomposition. The   potential reason is that our framework benefits from   the deduction capability of LLMs to decompose a   multi-hop question into simpler sub-questions and   generate an answer directly, leading to a shorter   reasoning trajectory.   We also train Mistral-7B using different amounts   of randomly sampled examples to investigate the   impact of data scale on the effectiveness of our   instructional grounding distillation (IGD). We   notice a slight decrease in performance as the   amount of data reduces.   For instance, when   training with 45k examples, our method achieves   Acc=42.08; but with 20k examples, it achieves   Acc=40.12. All results are averaged over three   runs. We also observe that our IGD allows Mistral7B to achieve performance comparable to, and   sometimes even better than, strong baselines using   ChatGPT as the backbone, such as ReAct.   8   Conclusion   We present a generate-then-ground (GenGround)   framework for multi-hop question answering   tasks, synergizing the parametric knowledge of   LLMs and external documents to solve a multihop question. Given a multi-hop question, our   GenGround enable LLMs to alternate two phases   until the final answer is derived: (1) formulate a   7346   0   3000   6000   9000   Token consumption   0.0   0.6   1.2   1.8   Distribution   1e 3   : 3541.6   : 7806.4   : 8917.8   Ours   GRG w/ decomposition   RetGen   Figure 6: The efficiency analysis for different methods,   where we count the number of consumed tokens and   compute the average consumption \u00b5.   simpler, single-hop question and directly generate   the answer; (2) ground the question-answer pair   in retrieved documents, amending any wrong   predictions in the answer.   To generalize our   framework into smaller models, we also propose   an instructional grounding distillation method.   Extensive experiments conducted on four datasets   illustrate the superiority of our framework.   Limitations   Despite the promising results demonstrated in   this paper, there are several limitations to our   framework.   \u2022 The first step of our framework involves   generating an initial answer, which may   be highly dependent on the task at hand.   For different tasks, the model may struggle   to generate a meaningful or useful initial   answer,   limiting its applicability across   diverse problem domains.   \u2022 Our approach assumes that complex questions   can be broken down into simpler ones.   However, the task of decomposing complex   questions is itself a challenging problem and   has not been fully explored in our current   framework.   \u2022 Our   approach   assumes   that   external   documents can be used to correct initially   non-factual statements generated by the   model.   However, if these sources do not   contain   the   necessary   information   for   correction, or if they contain misinformation,   our framework\u2019s effectiveness could be   compromised.   Ethics Statement   The research conducted in this paper centers   around the development of a generate-then-ground   framework for multi-hop question answering. Our   framework enables Language Learning Models   (LLMs) to alternately deduce answers and utilize   established evidence to revise those answers across   several iterations to solve multi-hop questions.   In the process of conducting this research,   we have adhered to ethical standards to ensure   the integrity and validity of our work.   All   the questions used in this study were obtained   from existing benchmarks, thus ensuring a high   level of transparency and reproducibility in our   experimental procedure.   Furthermore, to support our retrieval system, we   used an open source corpus, specifically, Wikipedia.   This ensures that our research utilizes publicly   accessible and freely available data, minimizing   potential bias and promoting fairness.   We have made every effort to ensure that our   research does not harm individuals or groups, nor   does it involve any form of deception or potential   misuse of information.   7347", "conf": "ACL", "year": "2024", "index": 526}, {"title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal   Grounding   Hao Zhou1, Chongyang Zhang1,2 *, Yan Luo1, Yanjun Chen1, Chuanping Hu1,3   1School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University   2MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China   3Zhengzhou University, Zhengzhou, China   {zhouhao 0039,sunny zhang,luoyan bb,erinchen}@sjtu.edu.cn, cphu@vip.sina.com", "abstract": "Temporal grounding aims to localize temporal boundaries within untrimmed videos by language queries, but it   faces the challenge of two types of inevitable human uncertainties: query uncertainty and label uncertainty. The   two uncertainties stem from human subjectivity, leading to   limited generalization ability of temporal grounding.   In   this work, we propose a novel DeNet (Decoupling and Debias) to embrace human uncertainty: Decoupling \u2014 We explicitly disentangle each query into a relation feature and   a modi\ufb01ed feature. The relation feature, which is mainly   based on skeleton-like words (including nouns and verbs),   aims to extract basic and consistent information in the presence of query uncertainty. Meanwhile, modi\ufb01ed feature assigned with style-like words (including adjectives, adverbs,   etc) represents the subjective information, and thus brings   personalized predictions; De-bias \u2014 We propose a de-bias   mechanism to generate diverse predictions, aim to alleviate the bias caused by single-style annotations in the presence of label uncertainty. Moreover, we put forward new   multi-label metrics to diversify the performance evaluation.   Extensive experiments show that our approach is more effective and robust than state-of-the-arts on Charades-STA   and ActivityNet Captions datasets.   1.", "content": "As the increasing demand for video understanding, many   related works have drawn increasing attention, e.g. action recognition [31, 37, 23] and temporal action detection [50, 20]. These tasks rely on trimmed videos or prede\ufb01ned action categories, yet most videos are untrimmed   and associated with open-world language descriptions in   real scenarios. Temporal grounding task aims to localize   corresponding temporal boundaries in an untrimmed video   *This is the corresponding author.   Query A: the person    washes their hands    at the kitchen sink.   Query B: a person    is washing their    hands in the sink.   Query C: a person    washes hands in the    kitchen sink.   Time   1   2   3   4   5   1   2   3   4   5   temporal annotations   Figure 1. Example of temporal grounding task with two types   of uncertainties. Query uncertainty: For one same event, there   are different language expressions. Label uncertainty: Given one   same query and video, different annotators may provide a variety   of temporal boundaries.   by a language query. Thus, models need to understand both   \ufb01ne-grained video content and complex language queries.   Recently, this task has also shown its potential in a wide   range of applications, e.g. video captioning [26, 41, 5],   video object segmentation [7, 13] and video question answering [19, 14, 35].   We observe there lies inherent uncertainty in temporal   grounding task and classify it into two types: 1) One is   query uncertainty stemming from different expressions for   one same event. As shown in Figure 1, three queries are   attached to the same moment. Previous approaches usually   leverage LSTM-based [45, 47] networks to encode entire   language as a deterministic vector. However, the variety of   expressions makes it challenging to extract discriminative   semantic features, sometimes leading to quite different predictions for the same event. 2) The other is label uncertainty   representing subjective boundaries for one same event. As   shown in Figure 1, for the same query A and video, temporal boundaries annotated by different people exist disagree8445   ment. Due to the expensive cost of multiple-labeling, most   of previous models [24, 28] are optimized using single-style   annotations (which means each sample is labeled by one annotator), whereas the inherent uncertainty of event localization [29] is ignored. As a result, models may learn singlestyle prediction bias from training datasets, leading to limited generalization performances.   Considering the fact that uncertainty can cover a broad   range of human perspectives, it should be embraced to promote robust temporal grounding. Furthermore, we argue   single-annotation, single prediction is not reasonable in the   presence of uncertainty, and diversity of predictions is an   effective way to alleviate the bias caused by single-style annotations. Therefore, the key challenge is how to obtain diverse predictions. Inspired by linguistic knowledge, we \ufb01nd   consistent discriminative information lies in a skeleton-like   relation phrase (including nouns and verbs), and query uncertainty mainly exists in a style-like modi\ufb01ed phrase (including adjectives, adverbs, etc). On one hand, the relation phrase is bene\ufb01cial to robust temporal grounding. On   the other hand, the modi\ufb01ed phrase may be largely associated with human p", "conf": "CVPR", "year": "2021", "index": 151}, {"title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset   Chengjian Feng1   Yujie Zhong1   Zequn Jie1,\u2020   Weidi Xie2,\u2020   Lin Ma1   1 Meituan Inc.   2 CMIC, Shanghai Jiao Tong University   https://fcjian.github.io/InstaGen   Figure 1. (a) The synthetic images generated from Stable Diffusion and our proposed InstaGen, which can serve as a dataset synthesizer   for sourcing photo-realistic images and instance bounding boxes at scale. (b) On open-vocabulary detection, training on synthetic images   demonstrates significant improvement over CLIP-based methods on novel categories. (c) Training on the synthetic images generated from   InstaGen also enhances the detection performance in close-set scenario, particularly in data-sparse circumstances.", "abstract": "In this paper, we present a novel paradigm to enhance   the ability of object detector, e.g., expanding categories   or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically,   we integrate an instance-level grounding head into a pretrained, generative diffusion model, to augment it with the   ability of localising instances in the generated images. The   grounding head is trained to align the text embedding of   category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object   detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough   experiments to show that, this enhanced version of diffusion   model, termed as InstaGen, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over   existing state-of-the-art methods in open-vocabulary (+4.5   AP) and data-sparse (+1.2 \u223c5.2 AP) scenarios.   1.", "content": "Object detection has been extensively studied in the field   of computer vision, focusing on the localization and categorization of objects within images [3, 5, 12, 26, 27]. The   common practise is to train the detectors on large-scale im\u2020: corresponding author.   age datasets, such as MS-COCO [20] and Object365 [30],   where objects are exhaustively annotated with bounding   boxes and corresponding category labels. However, the procedure for collecting images and annotations is often laborious and time-consuming, limiting the datasets\u2019 scalability.   In the recent literature, text-to-image diffusion models   have demonstrated remarkable success in generating highquality images [28, 29], that unlocks the possibility of training vision systems with synthetic images. In general, existing text-to-image diffusion models are capable of synthesizing images based on some free-form text prompt, as   shown in the first row of Figure 1a. Despite being photorealistic, such synthesized images can not support training   sophisticated systems, that normally requires the inclusion   of instance-level annotations, e.g., bounding boxes for object detection in our case. In this paper, we investigate a   novel paradigm of dataset synthesis for training object detector, i.e., augmenting the text-to-image diffusion model to   generate instance-level bounding boxes along with images.   To begin with, we build an image synthesizer by finetuning the diffusion model on existing detection dataset.   This is driven by the observation that off-the-shelf diffusion models often generate images with only one or two objects on simplistic background, training detectors on such   images may thus lead to reduced robustness in complex   real-world scenarios. Specifically, we exploit the existing   detection dataset, and subsequently fine-tune the diffusion   model with the image-caption pairs, constructed by taking   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   14121   random image crops, and composing the category name of   the objects in the crop. As illustrated in the second row of   the Figure 1a, once finetuned, the image synthesizer now   enables to produce images with multiple objects and intricate contexts, thereby providing a more accurate simulation   of real-world detection scenarios.   To generate bounding boxes for objects within synthetic   images, we propose an instance grounding module that establishes the correlation between the regional visual features from diffusion model and the text embedding of category names, and infers the coordinates for the objects\u2019   bounding boxes. Specifically, we adopt a two-step training   strategies, firstly, we train the grounding module on synthetic images, with the supervision from an off-the-shelf   object detector, which has been trained on a set of base categories; secondly, we utilize the trained grounding head to   generate pseudo labels for a larger set of categories, including those not seen in existing detection dataset, and selftrain the grounding module.   Once finished training, the   grounding module will be able to identify the objects of arbitrary category and their bounding boxes in the synthetic   image, by simply providing the name in free-form language.   To summarize, we explore a novel approach to enhance   object detection capabilities, such as expanding detectable   categories and improving overall detection performance,   by training on synthetic dataset generated from diffusion   model. We make the following contribution: (i) We develop   an image synthesizer by fine-tuning the diffusion model,   with image-caption pairs derived from existing object detection datasets, our synthesizer can generate images with   multiple objects and complex contexts, offering a more realistic simulation for real-world detection scenarios. (ii) We   introduce a data synthesis framework for detection, termed   as InstaGen.   This is achieved through a novel grounding module that enables to generate labels and bounding   boxes for objects in synthetic images. (iii) We train standard   object detectors on the combination of real and synthetic   dataset, and demonstrate superior performance over existing state-of-the-art detectors across various benchmarks, including open-vocabulary detection (increasing Average Precision [AP] by +4.5), data-sparse detection (enhancing AP   by +1.2 to +5.2), and cross-dataset transfer (boosting AP by   +0.5 to +1.1).   2. Related Work   Object Detection.   Object detection aims to simultaneously predict the category and corresponding bounding box   for the objects in the images.   Generally, object detectors [3, 4, 6, 26, 27] are trained on a substantial amount of   training data with bounding box annotations and can only   recognize a predetermined set of categories present in the   training data.   In the recent literature, to further expand   the ability of object detector, open-vocabulary object detection (OVD) has been widely researched, for example, OVRCNN [37] introduces the concept of OVD and pre-trains a   vision-language model with image-caption pairs. The subsequent works make use of the robust multi-modal representation of CLIP [24], and transfer its knowledge to object   detectors through knowledge distillation [9, 36], exploiting   extra data [5, 41] and text prompt tuning [2, 5]. In this paper,   we propose to expand the ability of object detectors, e.g.,   expanding categories or improving detection performance,   by training on synthetic dataset.   Generative Models. Image generation has been considered as a task of interest in computer vision for decades. In   the recent literature, significant progress has been made, for   example, the generative adversarial networks (GANs) [8],   variational autoencoders (VAEs) [15], flow-based models [14], and autoregressive models (ARMs) [32]. More   recently, there has been a growing research interest in diffusion probabilistic models (DPMs), which have shown great   promise in generating high-quality images across diverse   datasets. For examples, GLIDE [23] utilizes a pre-trained   language model and a cascaded diffusion structure for textto-image generation. DALL-E 2 [25] is trained to generate images by inverting the CLIP image space, while Imagen [29] explores the advantages of using pre-trained language models. Stable Diffusion [28] proposes the diffusion   process in VAE latent spaces rather than pixel spaces, effectively reducing resource consumption. In general, the rapid   development of generative models opens the possibility for   training large models with synthetic dataset.   3. Methodology   In this section, we present details for constructing a dataset   synthesizer, that enables to generate photo-realistic images   with bounding boxes for each object instance, and train an   object detector on the combined real and synthetic datasets.   3.1. Problem Formulation   Given a detection dataset of real images with manual annotations, i.e., Dreal = {(x1, B1, Y1), . . . , (xN, BN, YN)},   where Bi = {b1, . . . , bm|bj \u2208R2\u00d72} denotes the set of   box coordinates for the annotated instances in one image,   and Yi = {y1, . . . , ym|yj \u2208RCbase} refers to the categories   of the instances. Our goal is thus to exploit the given real   dataset (Dreal), to steer a generative diffusion model into   dataset synthesizer, that enables to augment the existing detection dataset, i.e., Dfinal = Dreal +Dsyn. As a result, detectors trained on the combined dataset demonstrate enhanced   ability, i.e., extending the detection categories or improving   the detection performance.   In the following sections, we first describe the procedure   for constructing an image synthesizer, that can generate images suitable for training object detector (Section 3.2). To   14122   (a) Fine-tuning diffusion model on detection dataset.   (b) Supervised training and self-training for grounding head (i.e. student).   Figure 2. Illustration of the process for finetuning diffusion model and training the grounding head: (a) stable diffusion model is fine-tuned   on the detection dataset on base categories. (b) The grounding head is trained on synthetic images, with supervised learning on base   categories and self-training on novel categories.   simultaneously generate the images and object bounding   boxes, we propose a novel instance-level grounding module, which aligns the text embedding of category name with   the regional visual features from image synthesizer, and infers the coordinates for the objects in synthetic images. To   further improve the alignment towards objects of arbitrary   category, we adopt self-training to tune the grounding module on object categories not existing in Dreal (Section 3.3).   As a result, the proposed model, termed as InstaGen, can   automatically generate images along with bounding boxes   for object instances, and construct synthetic dataset (Dsyn)   at scale, leading to improved ability when training detectors   on it (Section 3.4).   3.2. Image Synthesizer for Object Detection   Here, we build our image synthesizer based on an off-theshelf stable diffusion model (SDM [28]). Despite of its impressive ability in generating photo-realistic images, it often outputs images with only one or two objects on simplistic background with the text prompts, for example, \u2018a   photograph of a [category1 name] and a [category2 name]\u2019,   as demonstrated in Figure 4b. As a result, object detectors trained on such images may exhibit reduced robustness   when dealing with complex real-world scenarios. To bridge   such domain gap, we propose to construct the image synthesizer by fine-tuning the SDM with an existing real-world   detection dataset (Dreal).   Fine-tuning procedure. To fine-tune the stable diffusion   model (SDM), one approach is to na\u00a8\u0131vely use the sample   from detection dataset, for example, randomly pick an image and construct the text prompt with all categories in the   image. However, as the image often contains multiple objects, such approach renders significant difficulty for finetuning the SDM, especially for small or occluded objects.   We adopt a mild strategy by taking random crops from the   images, and construct the text prompt with categories in the   image crops, as shown in Figure 2a. If an image crop contains multiple objects of the same category, we only use this   category name once in the text prompt.   Fine-tuning loss. We use the sampled image crop and constructed text prompt to fine-tune SDM with a squared error   loss on the predicted noise term as follows:     \\mathcal  {L}_{\\ text {fin   e   -tu n e}} = \\m athbb       {   E}_{z, \\epsilon \\sim \\mathcal {N}(0,1),t,y} \\Big [||\\epsilon - \\epsilon _{\\theta }(z^{t},t,y)||_{2}^{2} \\Big ],    (1)   where z denotes a latent vector mapped from the input image with VAE, t denotes the denoising step, uniformly sampled from {1, . . . , T}, T refers to the length of the diffusion   Markov chain, and \u03f5\u03b8 refers to the estimated noise from   SDM with parameters \u03b8 being updated. We have experimentally verified the necessity of this fine-tuning step, as   shown in Table 4.   3.3. Dataset Synthesizer for Object Detection   In this section, we present details for steering the image   synthesizer into dataset synthesizer for object detection,   which enables to simultaneously generate images and object bounding boxes. Specifically, we propose an instancelevel grounding module that aligns the text embedding of   object category, with the regional visual feature of the   diffusion model, and infers the coordinates for bounding   boxes, effectively augmenting the image synthesizer with   instance grounding, as shown in Figure 3. To further improve the alignment in large visual diversity, we propose a   self-training scheme that enables the grounding module to   generalise towards arbitrary categories, including those not   exist in real detection dataset (Dreal). As a result, our data   synthesizer, termed as InstaGen, can be used to construct   synthetic dataset for training object detectors.   3.3.1   Instance Grounding on Base Categories   To localise the object instances in synthetic images, we introduce an open-vocabulary grounding module, that aims   to simultaneously generate image (x) and the corresponding instance-level bounding boxes (B) based on a set of   14123   Figure 3. Illustration of the dataset generation process in InstaGen. The data generation process consists of two steps: (i) Image collection:   given a text prompt, SDM generates images with the objects described in the text prompt; (ii) Annotation generation: the instance-level   grounding head aligns the category embedding with the visual feature region of SDM, generating the corresponding object bounding-boxes.   categories (Y), i.e., {x, B, Y} = \u03a6InstaGen(\u03f5, Y), where   \u03f5 \u223cN(0, I) denotes the sampled noise.   To this end, we propose an instance grounding head, as   shown in Figure 3, it takes the intermediate representation   from image synthesizer and the text embedding of category   as inputs, then predicts the corresponding object bounding   boxes, i.e., {Bi, Yi} = \u03a6g-head(Fi, \u03a6t-enc(g(Yi))), where   Fi = {f 1   i , . . . , f n   i } refers to the multi-scale dense features   from the image synthesizer at time step t = 1, g(\u00b7) denotes   a template that decorates each of the visual categories in the   text prompt, e.g., \u2018a photograph of [category1 name] and   [category2 name]\u2019, \u03a6t-enc(\u00b7) denotes the text encoder.   Inspired by GroundingDINO [22], our grounding head   \u03a6g-head(\u00b7) mainly contains four components: (i) a channelcompression layer, implemented with a 3\u00d73 convolution,   for reducing the dimensionality of the visual features; (ii) a   feature enhancer, consisting of six feature enhancer layers,   to fuse the visual and text features. Each layer employs a deformable self-attention to enhance image features, a vanilla   self-attention for text feature enhancers, an image-to-text   cross-attention and a text-to-image cross-attention for feature fusion; (iii) a language-guided query selection module   for query initialization. This module predicts top-N anchor   boxes based on the similarity between text features and image features. Following DINO [38], it adopts a mixed query   selection where the positional queries are initialized with   the anchor boxes and the content queries remain learnable;   (iv) a cross-modality decoder for classification and box refinement. It comprises six decoder layers, with each layer   utilizing a self-attention mechanism for query interaction,   an image cross-attention layer for combining image features, and a text cross-attention layer for combining text   features. Finally, we apply the dot product between each   query and the text features, followed by a Sigmoid function   to predict the classification score \u02c6s for each category. Additionally, the object queries are passed through a Multi-Layer   Perceptron (MLP) to predict the object bounding boxes \u02c6b, as   shown in Figure 3. We train the grounding head by aligning the category embedding with the regional visual features from diffusion model, as detailed below. Once trained,   the grounding head is open-vocabulary, i.e., given any categories (even beyond the training categories), the grounding head can generate the corresponding bounding-boxes   for the object instances.   Training triplets of base categories. Following [18], we   apply an automatic pipeline to construct the {visual feature,   bounding-box, text prompt} triplets, with an object detector trained on base categories from a given dataset (Dreal).   In specific, assuming there exists a set of base categories   {c1   base, . . . , cN   base}, e.g., the classes in MS-COCO [20]. We   first select a random number of base categories to construct   a text prompt, e.g., \u2018a photograph of [base category1] and   [base category2]\u2019, and generate both the visual features and   images with our image synthesizer. Then we take an offthe-shelf object detector, for example, pre-trained Mask RCNN [12], to run the inference procedure on the synthetic   images, and infer the bounding boxes of the selected categories. To acquire the confident bounding-boxes for training, we use a score threshold \u03b1 to filter out the boundingboxes with low confidence (an ablation study on the selection of the score threshold has been conducted in Section 4.5). As a result, an infinite number of training triplets   for the given base categories can be constructed by repeating the above operation.   14124   (a) Stable Diffusion + Grounding head w/ Supervised training.   (b) Stable Diffusion + Grounding head w/   Supervised- and Self-training.   (c) Stable Diffusion w/ Fine-tuning + Grounding   head w/ Supervised- and Self-training.   Figure 4. Visualization of the synthetic images and bounding-boxes generated from different models. The bounding-boxes with green   denote the objects from base categories, while the ones with red denote the objects from novel categories.   Training loss. We use the constructed training triplets to   train the grounding head:     \\la b   e   l    {e   q:base-gro und i ng} \\mathcal {L}_ {\\text {base}} = \\sum \\limits _{i=1}^N[\\mathcal {L}_{\\text {cls}}(\\hat {s}_{i}, c_{i}) + \\mathds {1}_{\\{c_i\\neq \\varnothing \\}} \\mathcal {L}_{\\text {box}}(\\hat {b}_i, b_{i})], \\vspace {-3pt}    (2)   where the ith prediction (\u02c6si, \u02c6bi) from the N object queries   is assigned to a ground-truth (ci, bi) or \u2205(no object) with   bipartite matching. Lcls and Lbox denote the classification   loss (e.g. Focal loss) and box regression loss (e.g. L1 loss   and GIoU loss), respectively.   3.3.2   Instance Grounding on Novel Categories   Till here, we have obtained a diffusion model with openvocabulary grounding, which has been only trained with   base categories. In this section, we propose to further leverage the synthetic training triplets from a wider range of   categories to enhance the alignment for novel/unseen categories. Specifically, as shown in Figure 2b, we describe a   framework that generates the training triplets for novel categories using the grounded diffusion model, and then selftrain the grounding head.   Training triplets of novel categories. We design the text   prompts of novel categories, e.g., \u2018a photograph of [novel   category1] and [novel category2]\u2019, and pass them through   our proposed image synthesizer, to generate the visual features.   To acquire the corresponding bounding-boxes for   novel categories, we propose a self-training scheme that   takes the above grounding head as the student, and apply a   mean teacher (an exponential moving average (EMA) of the   student model) to create pseudo labels for update. In contrast to the widely adopted self-training scheme that takes   the image as input, the student and teacher in our case only   take the visual features as input, thus cannot apply data augmentation as for images. Instead, we insert dropout module   within each feature enhancer layer and decoder layer in the   student. During training, we run inference (without dropout   module) with teacher model on the visual features to produce bounding boxes, and then use a score threshold \u03b2 to   filter out those with low confidence, and use the remaining training triplets (Fi,\u02c6bi, ynovel   i   ) to train the student, i.e.,   grounding head.   Training loss. Now, we can also train the grounding head   on the mined triplets of novel categories (that are unseen in   the existing real dataset) with the training loss Lnovel defined   similar to Eq. 2. Thus, the total training loss for training the   grounding head can be: Lgrounding = Lbase + Lnovel.   3.4. Training Detector with Synthetic Dataset   In this section, we augment the real dataset (Dreal), with synthetic dataset (Dsyn), and train popular object detectors, for   example, Faster R-CNN [27] with the standard training loss:     \\m a thca l { L }_{\\ tex t  {de t}}  = \\m athcal {L}_{\\text {rpn\\_cls}} + \\mathcal {L}_{\\text {rpn\\_box}} + \\mathcal {L}_{\\text {det\\_cls}} + \\mathcal {L}_{\\text {det\\_box}},    (3)   where Lrpn cls, Lrpn box are the classification and box regression losses of region proposal network, and Ldet cls, Ldet box   are the classification and box regression losses of the detection head. Generally speaking, the synthetic dataset enables   to improve the detector\u2019s ability from two aspects: (i) expanding the original data with more categories, (ii) improve   the detection performance by increasing data diversity.   Expanding detection categories. The grounding head is   designed to be open-vocabulary, that enables to generate object bounding boxes for novel categories, even though it is   trained with a specific set of base categories. This feature   enables InstaGen to construct a detection dataset for any   category. Figure 4 demonstrates several synthetic images   and object bounding boxes for novel categories, i.e., the object with red bounding box. We evaluate the effectiveness of   training on synthetic dataset through experiments on openvocabulary detection benchmark. For more details, please   refer to Figure 1b and Section 4.2.   Increasing data diversity.   The base diffusion model is   trained on a large corpus of image-caption pairs, that enables to generate diverse images. Taking advantage of such   capabilities, InstaGen is capable of generating dataset with   diverse images and box annotations, which can expand the   14125   Method   Supervision   Detector   Backbone   AP50box   all   AP50box   base   AP50box   novel   Detic [41]   CLIP   Faster R-CNN   R50   45.0   47.1   27.8   PromptDet [5]   CLIP   Faster R-CNN   R50   50.6   26.6   BARON [34]   CLIP   Faster R-CNN   R50   53.5   60.4   34.0   OADP [33]   CLIP   Faster R-CNN   R50   47.2   53.3   30.0   ViLD [9]   CLIP   Mask R-CNN   R50   51.3   59.5   27.6   F-VLM [16]   CLIP   Mask R-CNN   R50   39.6   28.0   RO-ViT [13]   CLIP   Mask R-CNN   ViT-B [1]   41.5   30.2   VLDet [19]   CLIP   CenterNet2 [40]   R50   45.8   50.6   32.0   CxORA [35]   CLIP   DAB-DETR [21]   R50   35.4   35.5   35.1   DK-DETR [17]   CLIP   Deformable DETR [42]   R50   61.1   32.3   EdaDet [31]   CLIP   Deformable DETR [42]   R50   52.5   57.7   37.8   InstaGen   Stable Diffusion   Faster R-CNN   R50   52.3   55.8   42.3   Table 1. Results on open-vocabulary COCO benchmark. AP50box   novel is the main metric for evaluation. Our detector, trained on synthetic   dataset from InstaGen, significantly outperforms state-of-the-art CLIP-based approaches on novel categories.   original dataset, i.e., increase the data diversity and improve   detection performance, particularly in data-sparse scenarios. We conducted experiments with varying proportions   of COCO [20] images as available real data, and show the   effectiveness of training on synthetic dataset when the number of real-world images is limited. We refer the readers for   more details in Section 4.3, and results in Figure 1c.   4. Experiment   In this section, we use the proposed InstaGen to construct   synthetic dataset for training object detectors, i.e., generating images with the corresponding bounding boxes. Specifically, we present the implementation details in Section 4.1.   To evaluate the effectiveness of the synthetic dataset for   training object detector, we consider three protocols: openvocabulary object detection (Section 4.2), data-sparse object detection (Section 4.3) and cross-dataset object detection (Section 4.4). Lastly, we conduct ablation studies on   the effectiveness of the proposed components and the selection of hyper-parameters (Section 4.5).   4.1. Implementation details   Network architecture. We build image synthesizer from   the pre-trained Stable Diffusion v1.4 [28], and use the CLIP   text encoder [24] to get text embedding for the category   name. The channel compression layer maps the dimension   of visual features to 256, which is implemented with a 3\u00d73   convolution. For simplicity, the feature enhancer, languageguided query selection module and cross-modality decoder   are designed to the same structure as the ones in [22]. The   number of the object queries is set to 900.   Constructing image synthesizer. In our experiments, we   first fine-tune the stable diffusion model on a real detection   dataset, e.g., the images of base categories. During training,   the text encoder of CLIP is kept frozen, while the remaining   components are trained for 6 epochs with a batch size of 16   and a learning rate of 1e-4.   Instance grounding module. We start by constructing the   training triplets using base categories i.e., the categories   present in the existing dataset. The text prompt for each   triplet is constructed by randomly selecting one or two categories. The regional visual features are taken from the image synthesizer time step t = 1, and the oracle ground-truth   bounding boxes are obtained using a Mask R-CNN model   trained on base categories, as explained in Section 3.3.1.   Subsequently, we train the instance grounding module   with these training triplets for 6 epochs, with a batch size   of 32. In the 6th epoch, we transfer the weights from the   student model to the teacher model, and proceed to train the   student for an additional 6 epochs. During this training, the   student receives supervised training on the base categories   and engages in self-training on novel categories, and the   teacher model is updated using exponential moving average   (EMA) with a momentum of 0.999. The initial learning rate   is set to 1e-4 and is subsequently reduced by a factor of 10   at the 11th epoch, and the score thresholds \u03b1 and \u03b2 are set   to 0.8 and 0.4, respectively.   Training object detector on combined dataset. In our experiment, we train an object detector (Faster R-CNN [27])   with ResNet-50 [11] as backbone, on a combination of the   existing real dataset and the synthetic dataset. Specifically,   for synthetic dataset, we randomly select one or two categories at each iteration, construct the text prompts, and   feed them as input to generates images along with the corresponding bounding boxes with \u03b2 of 0.4. Following the   standard implementation [27], the detector is trained for 12   epochs (1\u00d7 learning schedule) unless specified. The initial   learning rate is set to 0.01 and then reduced by a factor of   10 at the 8th and the 11th epochs.   4.2. Open-vocabulary object detection   Experimental setup. Following the previous works [5, 39],   we conduct experiments on the open-vocabulary COCO   benchmark, where 48 classes are treated as base categories,   14126   InstaGen 10% 25% 50% 75% 100%   \u2717   23.3   29.5   34.1   36.1   37.5   \u2713   28.5   32.6   35.8   37.3   38.5   Table 2. Results on data-sparse object detection. We employ Faster R-CNN with the   ResNet-50 backbone as the default object   detector and evaluate its performance using   the AP metric on MS COCO benchmark.   Please refer to the text for more details.   Method   Supervision   Detector   Extra Data   Object365   LVIS   Gao et al. [7]   CLIP   CenterNet2   \u2713   6.9   8.0   VL-PLM [39]   CLIP   Mask R-CNN   \u2713   10.9   22.2   InstaGen   Stable Diffusion   Faster R-CNN   \u2717   11.4   23.3   Table 3. Results on generalizing COCO-base to Object365 and LVIS. All detectors utilize   the ResNet-50 backbone. The evaluation protocol follows [7] and reports AP50. Extra   data refers to an additional dataset that encompasses objects from the categories within the   target dataset. In both experiments, the extra data consists of all the images from COCO,   which has covered the majority of categories in Object365 and LVIS.   G-head   ST   FT   AP50box   all   AP50box   base   AP50box   novel   \u2713   50.6   55.3   37.1   \u2713   \u2713   51.1   55.0   40.3   \u2713   \u2713   \u2713   52.3   55.8   42.3   Table 4. The effectiveness of the proposed components. G-head,   ST and FT refer to the grounding head, self-training the grounding   head and fine-tuning SDM, respectively.   and 17 classes as the novel categories. More results for   LVIS can be found in the supplementary material. To   train the grounding head, we employ 1250 synthetic images   per category per training epoch. While for training the object detector, we use 3000 synthetic images per category,   along with the original real dataset for base categories. The   object detector is trained with input size of 800 \u00d7 800 and   scale jitter. The performance is measured by COCO Average Precision at an Intersection over Union of 0.5 (AP50).   Comparison to SOTA. As shown in Table 1, we evaluate the performance by comparing with existing CLIPbased open-vocabulary object detectors. It is clear that our   detector trained on synthetic dataset from InstaGen outperforms existing state-of-the-art approaches significantly,   i.e., around +5AP improvement over the second best. In   essence, through the utilization of our proposed openvocabulary grounding head, InstaGen is able to generate   detection data for novel categories, enabling the detector to   attain exceptional performance. To the best of our knowledge, this is the first work that applies generative diffusion   model for dataset synthesis, to tackle open-vocabulary object detection, and showcase its superiority in this task.   4.3. Data-sparse object detection   Experimental setup. Here, we evaluate the effectiveness   of synthetic dataset in data-spare scenario, by varying the   amount of real data. We randomly select subsets comprising 10%, 25%, 50%, 75% and 100% of the COCO training   set, this covers all COCO categories. These subsets are used   to fine-tune stable diffusion model for constructing image   synthesizer, and train a Mask R-CNN for generating oracle   ground-truth bounding boxes in synthetic images. We employ 1250 synthetic images per category to train a Faster RCNN in conjunction with the corresponding COCO subset.   The performance is measured by Average Precision [20].   Comparison to baseline. As shown in Table 2, the Faster   R-CNN trained with synthetic images achieves consistent   improvement across various real training data budgets. Notably, as the availability of real data becomes sparse, synthetic dataset plays even more important role for performance improvement, for instance, it improves the detector   by +5.2 AP (23.3\u219228.5 AP) when only 10% real COCO   training subset is available.   4.4. Cross-dataset object detection   Experimental setup.   In this section, we assess the effectiveness of synthetic data on a more challenging task,   namely cross-dataset object detection. Following [39], we   evaluate the COCO-trained model on two unseen datasets:   Object365 [30] and LVIS [10]. Specifically, we consider   the 48 classes in the open-vocabulary COCO benchmark   as the source dataset, while Object365 (with 365 classes)   and LVIS (with 1203 classes) serve as the target dataset.   When training the instance grounding module, we acquire   1250 synthetic images for base categories from the source   dataset, and 100 synthetic images for the category from the   target dataset at each training iteration. In the case of training the object detector, we employ 500 synthetic images per   category from the target dataset for each training iteration.   The detector is trained with input size of 1024 \u00d7 1024 and   scale jitter [39].   Comparison to SOTA. The results presented in Table 3   demonstrate that the proposed InstaGen achieves superior performance in generalization from COCO-base to Object365 and LVIS, when compared to CLIP-based methods   such as [7, 39]. It is worth noting that CLIP-based methods   require the generation of pseudo-labels for the categories   from the target dataset on COCO images, and subsequently   train the detector using these images. These methods necessitate a dataset that includes objects belonging to the categories of the target dataset. In contrast, InstaGen possesses   the ability to generate images featuring objects of any category without the need for additional datasets, thereby enhancing its versatility across various scenarios.   14127   #Images AP50box   all   AP50box   base AP50box   novel   1000   51.6   55.9   39.7   2000   51.7   55.4   41.1   3000   52.3   55.8   42.3   Table 5. Number of generated images.   \u03b1   AP50box   all   AP50box   base   AP50box   novel   0.7   51.3   55.1   40.6   0.8   52.3   55.8   42.3   0.9   51.8   55.6   41.1   Table 6. \u03b1 for bounding-box filtration.   \u03b2   AP50box   all   AP50box   base   AP50box   novel   0.3   46.4   53.3   26.9   0.4   52.3   55.8   42.3   0.5   51.2   55.4   39.2   Table 7. \u03b2 for bounding-box filtration.   4.5. Ablation study   To understand the effectiveness of the proposed components, we perform thorough ablation studies on the openvocabulary COCO benchmark [20], investigating the effect of fine-tuning stable diffusion model, training instance   grounding module, self-training on novel categories. Additionally, we investigate other hyper-parameters by comparing the effectiveness of synthetic images and different score   thresholds for base and novel categories.   Fine-tuning diffusion model.   We assess the effectiveness of fine-tuning stable diffusion model, and its impact   for synthesizing images for training object detector. Figure 4c illustrates that InstaGen is capable of generating   images with more intricate contexts, featuring multiple objects, small objects, and occluded objects. Subsequently, we   employed these generated images to train Faster R-CNN for   object detection. The results are presented in Table 4, showing that image synthesizer from fine-tuning stable diffusion   model delivers improvement detection performance by 2.0   AP (from 40.3 to 42.3 AP).   Instance grounding module. To demonstrate the effectiveness of the grounding head in open-vocabulary scenario, we   exclusively train it on base categories. Visualization examples of the generated images are presented in Figure 4a.   These examples demonstrate that the trained grounding   head is also capable of predicting bounding boxes for instances from novel categories. Leveraging these generated   images to train the object detector leads to a 37.1 AP on   novel categories, surpassing or rivaling all existing state-ofthe-art methods, as shown in Table 1 and Table 4.   Self-training scheme. We evaluate the performance after   self-training the grounding head with novel categories. As   shown in Table 4, training Faster R-CNN with the generated   images of novel categories, leads to a noticeable enhancement in detection performance, increasing from 37.1 to 40.3   AP. Qualitatively, it also demonstrates enhanced recall for   novel objects after self-training, as shown in Figure 4b.   Number of synthetic images. We investigate the performance variation while increasing the number of the generated images per category for detector training. As shown in   Table 5, when increasing the number of generated images   from 1000 to 3000, the detector\u2019s performance tends to be   increasing monotonically, from 39.7 to 42.3 AP on novel   categories, showing the scalability of the proposed training   mechanism.   Score thresholds for bounding box filtration. We compare the performance with different score thresholds \u03b1 and   \u03b2 for filtering bounding boxes on base categories and novel   categories, respectively. From the experiment results in Table 6, we observe that the performance is not sensitive to the   value of \u03b1, and \u03b1 = 0.8 yields the best performance. The   experimental results using different \u03b2 are presented in Table 7. With a low score threshold (\u03b1 = 0.3), there are still   numerous inaccurate bounding boxes remaining, resulting   in an AP of 26.9 for novel categories. by increasing \u03b2 to   0.4, numerous inaccurate bounding boxes are filtered out,   resulting in optimal performance. Hence, we set \u03b1 = 0.8   and \u03b2 = 0.4 in our experiments.   5. Limitation   Using synthetic or artificially generated data in training AI   algorithms is a burgeoning practice with significant potential. It can address data scarcity, privacy, and bias issues.   However, there remains two limitations for training object   detectors with synthetic data, (i) synthetic datasets commonly focus on clean, isolated object instances, which limits the exposure of the detector to the complexities and contextual diversity of real-world scenes, such as occlusions,   clutter, varied environmental factors, deformation, therefore, models trained on synthetic data struggle to adapt to   real-world conditions, affecting their overall robustness and   accuracy, (ii) existing diffusion-based generative model also   suffers from long-tail issue, that means the generative model   struggles to generate images for objects of rare categories,   resulting in imbalanced class representation during training   and reduced detector performance for less common objects.   6. Conclusion   This paper proposes a dataset synthesis pipeline, termed   as InstaGen, that enables to generate images with object bounding boxes for arbitrary categories, acting as a   annotation-free approach for constructing large-scale synthetic dataset to train object detector. We have conducted   thorough experiments to show the effectiveness of training on synthetic data, on improving detection performance,   or expanding the number of detection categories. Significant improvements have been shown in various detection   scenarios, including open-vocabulary (+4.5 AP) and datasparse (+1.2 \u223c5.2 AP) detection.   Acknowledgement. WX is supported by National Key R&D Program of China (No. 2022ZD0161400).   14128", "conf": "CVPR", "year": "2024", "index": 502}, {"title": "Learning to Ground Instructional Articles in Videos through Narrations   Effrosyni Mavroudi*, Triantafyllos Afouras*, Lorenzo Torresani   Meta AI   {emavroudi, afourast, torresani}@meta.com", "abstract": "In this paper we present an approach for localizing steps   of procedural activities in narrated how-to videos. To deal   with the scarcity of labeled data at scale, we source the   step descriptions from a language knowledge base (wikiHow) containing instructional articles for a large variety of   procedural tasks. Without any form of manual supervision,   our model learns to temporally ground the steps of procedural articles in how-to videos by matching three modalities: frames, narrations, and step descriptions. Specifically,   our method aligns steps to video by fusing information from   two distinct pathways: i) direct alignment of step descriptions to frames, ii) indirect alignment obtained by composing steps-to-narrations with narrations-to-video correspondences. Notably, our approach performs global temporal   grounding of all steps in an article at once by exploiting   order information, and is trained with step pseudo-labels   which are iteratively refined and aggressively filtered. In   order to validate our model we introduce a new benchmark \u2013 HT-Step \u2013 obtained by manually annotating a 124hour subset of HowTo100M1 with steps sourced from wikiHow articles. Experiments on this benchmark as well as   zero-shot evaluations on CrossTask demonstrate that our   multi-modality alignment yields dramatic gains over several baselines and prior works. Finally, we show that our   inner module for matching narration-to-video outperforms   by a large margin the state of the art on the HTM-Align   narration-video alignment benchmark.   1.", "content": "Instructional videos have emerged as a popular means   for people to learn new skills and improve their abilities in   executing complex procedural activities, such as cooking   a recipe, performing home improvements, or fixing things.   In addition to being useful teaching materials for humans,   how-to videos are a promising medium for learning by ma*Equal contribution   1A   test   server   is   accessible   at   https://eval.ai/web/   challenges/challenge-page/2082.   Narrated     Instructional Videos   ~370k   Training    Resources     (No manual    annotations)   VINA   Video Frames   1. Wash the pumpkins   2. Cut the pumpkins   3. Scoop out the seeds   10. Puree pumpkin flesh   \u201cFirst thing you\u2019ll need to do    is cut off the stem.\u201d   \u201cThis requires a sharp knife    and some muscle\u201d   \u201cThanks for watching\u201d   Steps-to-Video Alignment (Direct)   Steps-to-Narrations-to-Video Alignment (Indirect)   NV   SN   SV   Steps   Narrations   Video Frames   Instructional Articles   ~14k   Narrations (ASR)   Instruction Steps   1. Using a meat pounder or sharp knife, pound    or cut the chicken into 1/4\" thick cutlets.    Chicken piccata should be thin \u2026    2. Coat the chicken with salt and pepper. Use as    much as you desire, making sure the entire pieces    have at least ..    3. Dredge the chicken in flour, coating    completely. Dredging means coating it, and it    should be a nice \u2026    4. In a thick-bottomed pan on medium, heat up    2 tablespoons butter and 2 tablespoons olive oil.    You could use all of one or the other, but this    mixture actually has a purpose \u2014 the two work    together to keep   Make Chicken    1. Heat 2 tbsp. of vegetable oil in a pan over    medium heat. Give the oil a minute to heat up.    2. Add 1 chopped onion to the oil. Saut\u00e9 the    onion until it's golden. Then, remove the pan from    the heat.    3. Add 1 tbsp. of paprika to the onion. Stir to    incorporate the ingredients.    4. Add 1 lb. of cubed beef, a pinch of salt, and 3    tbsp. of water to the pan. Stir the ingredients    again to incorporate them.    5. Heat the pan over medium heat again. Stir the    ingredients again.   Make Hungarian    Goulash   1. Place the Italian sausage and crushed red    pepper into a large saucepan. Start cooking the    sausage over low heat.     2. Saut\u00e9 the bacon, onions and garlic. Use the    same pan you used for the sausage, without    washing it.Cook these over low-medium heat.    3. Add the water and chicken bouillon. You    will need to heat this until it boils.Increase the    heat to medium.Cook the soup until it gets to a    full,     4. Add the sliced potatoes. These will need to    cook until they are soft and tender.At medium    heat, cook the soup with the added potatoes.   Make Zuppa    1. Make the filling. You will make two fillings:    one comprised of meat, and the other of    cheese.Meat filling     2. Roast the chile peppers. Sometimes, you    can buy the peppers already roasted, especially    in areas where \u2026    3. Prepare the batter. This can be done while    the chiles are roasting. Place a ..    4. Cut an opening in the side of the chile.    Make it large enough that stuffing can be put in,    but not so large ..    5. Stuff the chiles. Be careful because the soft,    roasted chiles are easily torn. Begin with the    meat stuffing\u2026    Make Chiles    1. Make mashed potatoes. Chop the potatoes into    chunks, boil until mashable, and then mash away.    Add a quarter or a half a stick of butter     2. Chop and saut\u00e9 a large onion with cumin    until soft and sweet.     3. Hard boil the eggs. Peel and chop into small    pieces.    4. Pan fry two or two and a half pounds of    ground beef. Drain when almost cooked. Add    onion, paprika, salt and pepper to taste.    5. Grease a nine-by-nine-inch (23cm x 23cm)    oven dish. Good trick: If you have the paper from    a stick of butter, don't throw it out. Rub the buttery    side along the dish to\u2026   Make Pastel De    Figure 1: Our proposed Video, Instructions, and Narrations   Aligner (VINA) learns to simultaneously ground narrations   and instruction steps in how-to videos from an uncurated   set of narrated videos and a separate knowledge base of instructional articles, without any manual annotations. This is   contrary to prior work that learns how to align a video with   a single sequence of sentences by leveraging ground-truth   pairs of video-text sequences, e.g., a video and its narrations [22], or a video and an annotated, ordered list of steps   demonstrated in it [16].   chines, as they provide revealing visual demonstrations of   complex activities and show elaborate human-object interactions in a variety of domains. Motivated by this observation, in this work we look at the task of temporally localizing the steps of procedural activities in instructional videos.   This problem is foundational to the broader goal of humanprocedure understanding and advances on this task promise   to enable breakthrough applications, such as AI-powered   skill coaching and human-to-robot imitation learning.   Prior work has tackled procedural step localization by   leveraging either (a) fully-annotated datasets where the task   This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   15201   shown in the video is given (video-level labeling) and manually annotated temporal segments are provided for each step   (segment-level labeling) [59] or (b) weakly-annotated training sets where the task and the order in which the steps appear in the video is given [79]. However, due to the inherent   manual cost involved in collecting step annotations, these   works have relied on datasets that are small-scale both in   the number of tasks (e.g., at most few hundreds [79]) and in   the number of video samples (e.g., 12k videos [59]). These   limitations affect both the generality and the complexity of   the models that can be trained on these benchmarks. In   this paper, we therefore pose the following question: can   we leverage large-scale, unlabeled video datasets to train a   model that can ground procedural steps in how-to videos?   To answer this question, we propose a novel training   framework for weakly-supervised step grounding that utilizes two freely available sources of information: (a) instructional articles which define ordered lists of steps for   a wide variety of tasks (e.g., from wikiHow) and (b) narrations which provide instance-specific rich commentaries   of the execution of the task in the video, e.g., from ASR   transcriptions. Our work treats the former as an abstraction   of the latter and uses the video-specific narrations to support   the grounding of the article steps. Specifically, during training, our method leverages narrations as an auxiliary signal   to (i) identify the task shown in the video, (ii) temporally   ground the article steps that are visually-demonstrated and   (iii) filter out steps that are not executed in the given instance. To further motivate this mechanism, let us look at   the example in Figure 1. The narrations help disambiguate   the task (make a pumpkin puree), enabling the automatic   retrieval of relevant instructional articles for the video. Furthermore, the narrations can be matched to steps described   in the articles to roughly localize the steps that are represented in the video. In this example, the timestamp of \u201cFirst   thing you\u2019ll need to do is cut off the stem\u201d provides a loose   temporal prior for the matching step \u201cCut the pumpkins.\u201d   On the other hand, steps that do not have any matching narrations (e.g., \u201cWash the pumpkins\u201d) are unlikely to be represented in the video and thus can be rejected. Based on this   intuition, we propose a procedure that learns to align steps   to video by fusing information from two pathways. The first   is an indirect pathway inferring step-frame alignments by   composing step-to-narration assignments with narration-toframe correspondences. The second is a direct pathway that   learns associations between step descriptions and frames by   leveraging information from all videos having steps in common.   In our experiments we demonstrate that our multimodality alignment leads to significant performance gains   over several baselines, including single-pathway temporal   grounding, as well adaptations of prior works to our problem. During inference, the direct pathway can be used by   itself to temporally ground steps in absence of transcribed   narrations. When narrations are available at test time, our   method improves further the accuracy of temporal grounding by fusing the inference outputs of the two pathways.   To summarize, our work makes the following contributions: 1) we learn to align steps to frames in how-to videos,   using only weak supervision in the form of noisy ASR narrations and instructional articles; 2) we propose a novel approach for joint dense temporal grounding of instructional   steps and video narrations; 3) we introduce a new benchmark for evaluating instructional step grounding which we   will make available to the community; 4) we demonstrate   state-of the art results on multiple benchmarks for both step   as well as narration grounding.   2. Related Work   Procedural step recognition.   Going beyond classifying a   trimmed segment video to a procedural step [9, 32, 41, 76],   there has been active research in untrimmed video segmentation [10, 59, 72, 77], step anticipation [40, 47, 48], unsupervised step discovery [18, 19], procedure planning [6,   12, 74] and step description [68, 72].   In this work we   focus on parsing untrimmed procedural videos into a sequence of step segments.   Prior approaches can be categorized into two groups: action localization-based methods [3, 29, 50, 59] use fixed taxonomies of step classes and   are not able to recognize unseen steps, while groundingbased methods [2, 5, 21], including ours, operate in an   open-world setting, treating the problem as video-text alignment.   Grounding-based approaches can be further subdivided into single step grounding, where single steps are   queried over the whole video[27, 57] and dense temporal   grounding methods [14, 22] where the objective is to jointly   ground a sequence of steps or whole article into the video.   Chen et al.   [14] use video-level instructional step labels   for (weak) supervision of a model that grounds instructional   articles to videos. Their approach attempts to localize steps   without using any narration information during training. Instead, we argue that narrations provide a much richer source   of supervision for training step grounding models, while essentially coming for free.   Weakly-supervised approaches   Existing methods also   vary by the level of supervision used during training. One   option is leveraging fully-annotated datasets with known   temporal segments for each step [10, 26, 29, 50, 53, 60,   71, 80]. Other works use weakly-annotated training sets   where the task and the order in which the steps appear   in the video are known [7, 8, 11, 16, 20, 28, 45, 79], or   only the task and potential steps are known [44], or only   loose association between video and instructional articles is   given [14, 17]. More related to our work are approaches that   are trained using weak supervision coming for free from instructional video narrations [1, 20, 36, 49, 75]. Narrations   15202   and their timestamps have been used to discover procedural steps in videos and then align them with visual clusters [1, 51], to obtain temporal constraints for the location   of steps in videos [20, 79], and to roughly localized steps by   leveraging their textual similarity with ASR narrations [36].   In this work we utilize narrations, and in particular the estimated video-narration alignment, via multi-task learning   and complementary inference pathways [73] for training a   step grounding model.   Instructional articles have been recently used for learning strong video representations [32, 40] by replacing noisy   ASR narrations with steps mined from instructional articles   using their semantic similarity. Although we also associate   steps from wikiHow articles to video frames through the use   of narrations, our work differs from prior work in several   aspects: we align steps to video by a global procedure that   takes into account all ordered steps in the article (inspired   by dense temporal grounding methods [5]) and temporally   grounds them in the whole video, instead of matching individual video clips to an orderless collection of steps; our   step grounding uses video in addition to steps and narrations   while the method proposed in Lin et al. [32] relies purely   on text-matching narrations to step descriptions; finally, the   works differ in objectives with our aim being step grounding   in long how-to videos rather than learning video-clip representations. In summary, our model is trained by combining   supervision signals from video narrations and instructional   articles, and can be applied for aligning procedural steps to   videos with and without narrations.   Video-Text alignment   The availability of large-scale   video-text datasets such as HowTo100M has prompted   many works on joint video-language embedding training [10, 24, 32, 33, 37, 46, 52, 70]. A form of contrastive   loss is often adopted for bringing together the representations of the two modalities [4, 34, 35, 37, 42, 43, 66, 69],   while masked objectives are also gaining popularity [15, 23,   31, 35, 56, 57, 58, 61, 78]. Some works perform end-to-end   representation learning [37, 38], while others freeze representation and focus on longer-term temporal modelling,   which aims to capture context [66]. Recently Han et al. investigated directly aligning contextualized narration representations to video frames [22]. We build our method off of   this approach \u2013 we note however that our objective is complementary: rather than aligning a video\u2019s narrations as an   end-goal, we use this functionality to ground a set of independent steps sourced from instructional articles; in that   process we show that the synergy that develops while training jointly on the two tasks results in improved performance   for both.   3. Narration-Aided Step Grounding   We first present our architecture for joint narration and   step grounding (Sec. 3.2), followed by learning objectives   (Sec. 3.3.1) and pseudo-labeling strategy (Sec. 3.3.3); we   discuss inferring the video task in (Section 3.3.2).   3.1. Problem Formulation   Let (V, N) be a video-narration pair, consisting of T   video frames and a sequence of N narrations. Also, let S   be an ordered list of S steps from an instructional article for   a candidate task \u03c4. Our objective is to ground each step of   S to the video, conditioned on the other steps and the ASR   transcript2. In particular, the desired output of our model   is an alignment matrix Y SV \u2208{0, 1}S\u00d7T , where Yst = 1   only if frame t is depicting the s-th step of task \u03c4, and zero   otherwise. Note that some steps might not be represented in   the video.   3.2. Joint Narration and Article Step Grounder   As shown in Figure 2, our proposed VINA model follows the popular paradigm of leveraging Transformers for   modeling multimodal interactions [62, 67].   Unimodal Encoders. Before feeding the video, narrations   and article steps to our model, we preprocess them to extract a sequence of tokens. Given a video-narration pair (V,   N) we extract visual features, Xv \u2208RT \u00d7Dv and narration   features Xn \u2208RN\u00d7Dn using standard backbone networks   (e.g., a frozen S3D [65] network for visual features, and   pooled Word2Vec [39] embeddings for narration features).   Similarly, we encode the sequence of steps in a sequence   of features Xs \u2208RS\u00d7Ds. The features of each modality m are embedded into a common embedding space of   dimensionality D using a Unimodal Encoder that consists   of a modality-specific MLP network, and then learnable,   modality-specific positional embeddings P m are added to   them:   Hm = MLP(Xm; \u03b8m) + P m,   (1)   where m \u2208{v, n, s} denotes the modality.   Multimodal Encoder. The outputs of the Unimodal Encoders are concatenated into a sequence of tokens: H =   [Hv; Hn; Hs] \u2208R(T +N+S)\u00d7D and fed to the Multimodal   Encoder, which is a standard Transformer with multiple layers of multi-head self-attention:   Z = Transformer(H) \u2208R(T +N+S)\u00d7D.   (2)   The contextualized embeddings Z = [Zv; Zn; Zs] computed by the Multimodal Encoder capture interactions   within each modality (e.g., temporal relationships within   the video and context among steps of an article) and   across modalities.   We can then compute cosine similarity matrices between all pairs of modalities: narrationsto-video ANV \u2208RN\u00d7T , steps-to-video ASV \u2208RS\u00d7T ,   2ASR transcripts are assumed to be always available for training and   optionally during inference.   15203    Instruction steps   Multimodal Transformer    ASR narrations   \u201cSo let\u2019s get these chicken    pieces in there.\u201d   \u201cThanks for watching\u201d   ASNV   VINA   Video encoder   ASR encoder   1. Mix the milk and \u2026    6. Place cooked chicken \u2026   Steps-Video   Alignment   LSV   2. Mix the dry ingredients \u2026   Teacher VINA   Steps encoder   +   +   +   Video frames   Positional    Encodings   PS   ZS   Contextual    Embeddings   Unimodal    Encoders   LNV   ASR timestamps   Narrations-Video   \u0391lignment   NVSA   Steps-Narrations   \u0391lignment   Steps-NarrationsVideo   \u0391lignment   ASV   ANV   ASN   Video frames   : Steps-Video Alignment   ASV   1. Mix the milk and seasoning in a large bowl   3. Heat the oil to a medium heat   2. Mix the dry [\u2026] the chicken till lightly coated   \u201cSo we\u2019re gonna    start here with two    cups of buttermilk\u201d   \u201cAnd then    we\u2019ll get    on to the    marinating\u201d   \u201cI\u2019m using    my 13-inch    jumbo wok\u201d   \u201cYou want to make    sure that we\u2019re    coating all those    chicken pieces\u201d   : Steps-to-Narrations Alignment   ASN   1   2   1   2   3   3   ...   Figure 2: (left) Schematic illustration of our system. First, it extracts representations for each modality (video, ASR narrations, task steps) with three Unimodal Encoders. These representations are fed to a Transformer-based Multimodal Encoder   for capturing interactions among video frames as well as among the textual modalities and the video. The contextualized   embeddings for video frames, narrations and steps are used to compute correspondences between all pairs of modalities.   Step grounding is achieved by fusing the output of two pathways: a direct pathway aligning steps to video (ASV ) and an   indirect pathway that composes steps-to-narration (ASN) with narration-to-video (ANV ) alignments to produce a second   step-to-video alignment (ASNV ). We train our model using a teacher-student strategy with iteratively refined and filtered   step pseudo-labels. (right) Qualitative examples of our learned steps-to-video alignment and steps-to-narrations alignment   matrices for a video snippet.   and steps-to-narrations ASN   \u2208RS\u00d7N.   For example,   the narrations-to-video similarity matrix ANV is obtained   by simply computing the cosine similarity between each   frame embedding and each narration embedding: ANV   nt   =   zn   n   \u22a4zv   t / (||zn   n|| ||zv   t ||).   Narration-aided Step Grounding. A straightforward inference path for temporally grounding the steps in the video   is directly through the ASV similarity matrix, which captures the similarity of each video frame with each instructional step. However, this alignment does not explicitly take   into account the narrations of the video (only implicitly,   through the Multimodal Transformer). We observe that an   alternative way to ground steps in a video is to first identify narrations in the ASR transcript that are relevant to the   step and then exploit the similarity of those narrations with   video frames to get a loose prior over the step location.   This is computed by combining the information captured   in the steps-to-narrations and narrations-to-video alignment   matrices ASN and ANV :   ASNV = \u02dcASNANV \u2208RS\u00d7T ,   (3)   where \u02dcASN is the predicted steps-to-narrations alignment   matrix ASN after being normalized with a softmax function   with temperature \u03be: \u02dcASN   sn =   exp (Asn/\u03be)   PN   j=1 exp(Asj/\u03be).   The resulting ASV and ASNV alignment matrices provide two complementary inference paths to align steps   to video frames. The mutual agreement between the direct steps-to-video alignment provided by ASV and indirect, narration-based steps-to-video alignment provided by   ASNV can be used to better ground steps. Intuitively, if a   frame is both very similar to a step in the joint embedding   space learned by the Multimodal Transformer, and also very   similar to a narration that is relevant to the step, then it is   more likely to be indeed relevant to the step. Hence, we   fuse the ASV and ASNV alignment matrices to a matrix   AF = (ASV + ASNV )/2.   3.3. Weakly-Supervised Training from Narrated Instructional Videos   Next, we discuss how to supervise the VINA model in   order to learn steps-to-video alignment and narrations-tovideo alignment. We first present the training objective assuming that the ground-truth temporal segments for each   narration and step in the video are given. Then we describe   our approach for obtaining automatic pseudo-labels for the   temporal segments.   15204   3.3.1   Learning on Labeled Data   Let B = {Vi, Ni, Si, Y NV   i   , Y SV   i   }B   i=1 denote a set of training tuples, each comprising a video-narration pair, an ordered list of relevant task steps, and the target videonarrations and video-steps alignment matrices, we train the   VINA model by optimising the following objective:   L = 1   B   h B   X   i=1   \u03bbNV H(Y NV   i   , ANV   i   ) + \u03bbSV H(Y SV   i   , ASV   i   )   i   ,   (4)   where H(\u00b7, \u00b7) is the modified InfoNCE loss used by [22] for   aligning video with text using noisy ground-truth temporal   segments:   H(Y, A) = \u22121   K   K   X   k=1   log   P   t Yt,k exp (At,k/\u03b7)   P   t exp (At,k/\u03b7)   ,   (5)   where \u03b7 is a temperature constant. Note that although we   do not explicitly supervise the steps-narrations alignment   ASN, meaningful alignments emerge during training due to   the joint grounding of narrations and steps to the same video   samples, as seen in Figure 2. Note that although we do   not directly supervise the steps-to-narrations alignment, our   model is able to learn meaningful correspondences, which   go beyond simple pairwise textual matching.   3.3.2   Pairing Videos with Articles   We assume access to a set of instructional articles A =   {Sj, \u03c4j}W   j=1, where \u03c4j denotes the article title and Sj the   associated set of steps. To assign a set of steps to a given   video from our training set B we need to associate it with an   article from A. If our video dataset provides metadata (e.g.,   a task id for every video), then this can be used to obtain the   association \u2013 although there is no guarantee that this will result in the best article-match for the video (see discussion in   supplementary materials for more analysis). If such metadata is not available, we can predict a task id, using the similarity between the narration and the titles of the available   articles. To that end we use an off-the-shelf language model   (e.g. MPNet [55]) to compute semantic embeddings of the   ASR captions of every video and the title of each article   \u03c4j \u2208W. For every video Vi we then calculate the semantic similarity between all the N captions in Ni and all task   titles \u03c4j \u2208W, and assign N votes; the vote of every caption goes to the task that best matches it. Finally the video   is assigned the task with the most votes. Alternatively, in   order to obtain multiple sets of steps for a video, we rank   the tasks by the number of votes.   3.3.3   Narration-Aided Pseudo-Labeling   Once a task \u03c4j has been associated with a video, we have   access to a list of steps Sj from the article of the task. However, whether these steps appear in the video and their temporal location remain unknown. Inspired by self-labeling   approaches from the SSL literature [30, 54, 64], we follow a teacher-student approach where a teacher version of   our models generates pseudo-labeled temporal segments for   training the student. For every step represented by a row   in the learned steps-to-video alignment matrix we obtain a   pseudo-ground truth segment by finding the maximal activation (peak) and expanding a temporal segment on both   sides until the activation falls below an adaptive threshold   \u03b6 (e.g., 70% of the peak). To avoid training with unreliable   pseudo-labels, we filter out pseudo-labels with low confidence: if the peak activation is below a fixed threshold \u03b3,   the alignment of that step is treated as unreliable for pseudolabeling, and is altogether ignored.   Training curriculum. For the first Eb epochs we perform   burn-in training of the student model on fixed pseudo-labels   generated by feeding the video and the list of steps Sj to   TAN [22], an off-the-shelf model pre-trained on the task   of video-text alignment. Afterwards, we switch to using   pseudo-labels generated from the teacher, where the teacher   is initialized by duplicating the burn-in student model and   then updated every \u03bd epochs. During both stages, we utilize   the original ASR timestamps for supervising the video-tonarrations alignment.   4. Experiments   4.1. Datasets and Metrics   We train our models on narrated videos from the   HowTo100M dataset by leveraging the dataset release of   wikiHow instructional articles [25], without using any form   of manual annotations. In order to evaluate the effectiveness of our method, we evaluate: step grounding on HTStep (a new benchmark, described below), narration alignment on HTM-Align [22], and zero-shot step localization   on CrossTask [79]. A statistics summary of all the datasets   used for training and evaluation is provided in Table 1.   HowTo100M (Training). The HowTo100M dataset [38]   contains instructional videos from YouTube.   Following   Han et al. [22], we use the Food & Entertainment subset   containing approximately 370K videos, where each video   is complemented by the \u201csentencified\u201d ASR transcription of   its audio narration.   wikiHow (Training). We train using 14,541 cooking tasks   from the wikiHow-Dataset [25]. For each task, we generate   an ordered list of steps by extracting the step headlines.   CrossTask (Evaluation). We use this established instructional video benchmark for zero-shot grounding, i.e., by   15205   Dataset   Step Annot. # Videos # Activities # Steps # Segments   HowTo100M [38]   \u2717   1.2M   25k   HTM-Align [22]   \u2717   80   80   CrossTask [79]   \u2713   4.8k (2.8k)   83 (18)   133   20.9k   HT-Step (val)   \u2713   600   120   1,204   3,441   HT-Step (test)   \u2713   600   120   1,242   3,631   wikiHow   14k   100k   Table 1: Summary statistics for the datasets in our work.   For CrossTask, the statistics for primary activities only are   shown in parentheses.   directly evaluating on CrossTask our model learned from   HowTo100M. Following common practices, we use two   evaluation protocols: the first one \u2013 step localization \u2013 aims   at predicting a single timestamp for each occurring step in   videos from 18 primary tasks [79]. Performance is evaluated by computing the recall (denoted as Avg. R@1) of   the most confident prediction for each task and averaging   the results over all query steps in a video, where R@1   measures whether the predicted timestamp for a step falls   within the ground truth boundaries. We report average results over 20 random sets of 1850 videos [79]. The second task \u2013 article grounding \u2013 requires predicting temporal   segments for each step of an instructional article describing   the task represented in the video. We use the mapping between CrossTask and simplified wikiHow article steps provided in Chen et al. [14] and report results on 2407 videos   of 15 primary tasks obtained excluding three primary tasks   following the protocol of [14] (see supplementary materials for details). Performance for this task is measured with   Recall@K at different IoU thresholds [14].   HTM-Align (Evaluation). This benchmark is used to evaluate our model on narration grounding.   It contains 80   videos where the ASR transcriptions have been manually   aligned temporally with the video. We report the R@1 metric [22], which evaluates whether the model can correctly   localize the narrations that are alignable with the video.   HT-Step (Evaluation). To evaluate the effectiveness of our   model in grounding article steps, we introduce an evaluation benchmark consisting of 1200 HowTo100M narrated   videos spanning a total of 216 unique tasks, with each video   manually annotated with temporal segments for each occurring step. For each video, annotators were provided with the   task name (e.g., Make Pumpkin Puree) and candidate recipe   steps from the corresponding wikiHow article. We refer the   reader to supplementary materials for details about the data   annotation. We split the annotated videos into a validation   and a test set, each containing 600 videos, with 5 videos   per task. We ensure that our validation set does not contain   videos from HTM-Align.   4.2. Implementation Details   As video encoder we adopt the S3D [65] backbone pretrained with the MIL-NCE objective on HowTo100M [37].   Following previous work [22, 66], we keep this module   frozen and use it to extract clip-level features (one feature per second for video decoded at 16 fps). For extracting context-aware features for each sentence (step or narration), we follow the Bag-of-word (BoW) approach based   on Word2Vec embeddings [39]. Our methods hyperparameters were selected on the HT-Step validation set and are:   \u03bbSV = \u03bbNV = 1, temperatures \u03b7, \u03be = 0.07, and threshold \u03b3 = 0.65. We train our model for 12 epochs, with   3 epochs burn-in and then we update the teacher every 3   epochs. Pseudo-labels are obtained based on the steps-tovideo alignment matrix. To obtain temporal segment detections from the step-to-video alignment output of our model   (e.g. for evaluating on the CrossTask article grounding setting) we use a simple 1D blob detector [63]. Unless otherwise specified, we use the fused alignment matrix for step   grounding when narrations are available during inference   time. More details are included in supplementary materials.   Method   Train. Inp.   HT-Step \u2191R@1   HTM-Align \u2191R@1   w/o nar.   w/ nar.   CLIP (ViT-B/32) [43]   23.4   MIL-NCE [37]   N   30.7   34.2   TAN (Joint+Dual, S2) [22]   N   49.4   TAN* (Joint, S1, LC) [22]   N   31.2   47.1   TAN* (Joint, S1, PE+LC) [22]   N   7.9   63.0   Ours   N+S   35.6 \u00b1 0.4 37.4 \u00b1 0.4   66.5 \u00b1 0.9   Table 2: Comparison with state-of-the-art methods for   step and narration grounding. We report results on the   HT-Step and HTM-Align test sets, respectively.   TAN*   refers to our improved baselines of [22]. S1 and S2 refer to the training stages followed in [22]. PE denotes the   addition of positional encoding to the output of the narration encoder. LC denotes long context, i.e., our improved   TAN* baseline using 1024 seconds of context as opposed   to 64 for TAN. Previous best results are shown underlined.   Our VINA results are reported after 5 random runs. VINA   clearly outperforms all previous work \u2013 as well as our improved TAN baselines \u2013 by large margins on both narration   alignment and step grounding.   4.3. Results   4.3.1   Comparison with the State of the Art   Weakly-Supervised Narration and Step Grounding. Table 2 compares the step and narration grounding performance of our method with recent state-of-the-art video-text   alignment methods trained on HowTo100M using ASR narrations: MIL-NCE [37] and TAN [22]. When using them   for narration alignment, we feed them with ASR as input.   15206   But we also evaluate them as strong baselines for zeroshot step grounding by feeding them with the sequence of   steps as input. Our model achieves 66.5% R@1 on narration alignment on HTM-Align, leading to an absolute   improvement of 17.1% over the previously reported stateof-the-art (49.4%). Notably, on HTM-Align our method   surpasses TAN* (Joint, S1, PE, LC) which is a new version of TAN [22] implemented by us and much stronger in   video-narration alignment. Our re-implementation uses positional encodings for ASR narrations, is trained on longform videos (up to 17 minutes) only with original ASR   timestamps, while TAN was trained on 1 min video-clips   with refined narration timestamps and used a fusion of   two models during inference (Joint+Dual).   Our method   also outperforms all baselines for step grounding on HTStep even when seeing only steps during inference, while   being trained with (video, narrations, steps) triplets.   It   also outperforms TAN* (Joint, S1, LC), which is a second   re-implementation of TAN designed for maximum performance on the task of step grounding. Additionally, VINA   is able to use ASR transcripts of videos during test time, if   available, to further boost the performance.   Method   \u2191Avg. R@1 (%)   Supervised   TempCLR [70]   52.5   Zero-Shot   Zhukov [79]   22.4   HT100M [38]   33.6   VideoCLIP [66]   33.9   MCN [13]   35.1   DWSA [51]   35.3   MIL-NCE [37]   40.5   VT-TWINS* [24]   40.7   UniVL [33]   42.0   Ours w/o nar.   44.1   Ours w/ nar.   44.8   Table 3:   Comparison with state-of-the-art methods   for zero-shot action step localization on the CrossTask   dataset.   The performance of the state-of-the-art fullysupervised method (TempCLR [70]) is reported as an upperbound to the zero-shot approaches. * denotes results reported on different test splits, and hence not directly comparable with the rest. Our model outperforms all previous   works by a clear margin (2.1% absolute improvement over   the previous best result on the standard split). When providing narrations as additional inputs during inference (only   text, not the timings), we obtain a further 0.7% boost.   Step localization on CrossTask.   In Table 3 we compare   our model against the state-of-the art in step localization on   the CrossTask benchmark. Our approach sets a new stateof-the-art for zero-shot step localization on this challenging   benchmark. Importantly, most approaches are evaluated on   this dataset by feeding their predicted steps-to-frames alignment matrix to a dynamic programming algorithm which   finds the optimal assignment of each step with exactly one   short clip assuming a canonical, fixed ordering of steps for   each task. In contrast, our method, which is naturally aware   of context and ordering by densely grounding steps, can   outperform prior results without imposing any constraints   during inference.   Model   \u2191R@50(IOU)   \u2191R@100(IOU)   0.1   0.3   0.5   0.1   0.3   0.5   MIL-NCE-max [37]   33.5   12.0   4.9   39.7   14.3   5.9   MIL-NCE-avg [37]   42.9   24.3   12.9   56.8   32.1   17.0   WSAG [14]   40.1   23.1   10.1   54.3   31.3   14.0   Ours   87.1   59.0   30.0   90.6   61.1   30.9   Table 4: Comparison with state-of-the-art approaches   for article grounding on the CrossTask dataset.   Article grounding on CrossTask.   VINA is robust to the   type of language in which task steps are described. It can   handle both atomic phrases (as demonstrated by our results on step localization on CrossTask), but also rich, natural language step descriptions, as evidenced by performance on HT-Step. To further demonstrate this, we compare against the state-of-the art on the article grounding task   of CrossTask in Table 4. Our model outperforms all previous works by a large margin. We emphasize the performance improvement we obtain compared to WSAG, which   highlights the importance of exploiting the narration information for training.   4.3.2   Ablation Studies   We perform ablations to assess the impact of the various   design choices in our method by measuring step grounding performance on the HT-Step validation set and videonarration alignment performance on HTM-Align.   Effect of weak supervision from instructional articles.   Row 3 in Table 5 shows the step grounding results obtained   from an instance of our model that includes only the direct video-step alignment pathway and that is trained just   on wikiHow steps (without narrations) using the fixed step   pseudo-labels from TAN* [22] without any form of iterative   pseudo-labeling (row 1). Remarkably, this variant improves   by 3.3% over the step-grounding performance of TAN*.   When we let this variant update the step pseudo-labels (row   4), the recall improves further (5.1% over TAN*). These   results provide evidence of the strong benefits of utilizing   instructional articles for the learning of step grounding.   15207   Method   Train. Inp. Iter. Pseudo. HT-Step \u2191R@1 HTM-Align   w/o nar. w/ nar.   Baseline/Initial Step Pseudo-labels   (1) TAN Joint S1   N   30.7   47.1   Single-Task Training   (2) Ours   N   63.2   (3) Ours   S   34.0   (4) Ours   S   \u2713   35.8   Multi-Task Training   (5) Ours   N+S   34.3   36.1   64.8   (6) Ours   N+S   \u2713   36.9   39.1   67.0   Table 5: Ablation of main components of our framework. We study the contribution of (a) multi-task training for narration and step grounding, (b) iterative step   pseudo-labeling (Iter. Pseudo), and (c) narration-aware step   grounding (w/ nar.). We report results on the HT-Step val   set for STG and HTM-Align for NG. We compare training only with narrations (N), only with wikiHow steps (S),   and training with narrations-steps sequence pairs (N+S). We   also compare the performance with and without providing   narrations during inference.   Effect of multimodal training and inference.   Training   our model with multi-modal textual inputs (steps and narrations), we observe an improvement of 1.6% in narration   grounding (row 5 of Table 5) compared to its single-task   counterpart (row 2). However the gain in step grounding is   marginal when seeing only video frames during inference   (w/o nar., 34% in row 3 vs 34.3% in row 5). Our conjecture   is that the missing modality (narrations) leads to some drop   in performance. Providing both steps and narrations during   inference leads to a stronger step grounding performance,   which surpasses the TAN* baseline by 5.4% (30.7 \u219236.1).   Effect of iterative pseudo-labeling.   By comparing row 5   to row 6 of Table 5 we observe a clear boost in performance   on both step grounding and narration alignment. This is a   clear indication of the gains produced by iteratively refining the pseudo-labels using our model as a teacher during   training.   Alignment   S \u2192V   S \u2192N   N \u2192V   HT-Step \u2191R@1   S \u2192V   learned   34.3   S \u2192N \u2192V   learned   learned   30.5   S \u2192N \u2192V   learned   ASR   27.9   S \u2192N \u2192V   MPNet [55]   ASR   19.0   Fused   learned   learned   learned   36.1   Table 6: Impact of the alignment matrix used during inference with narrations. The same model is used for all   results (corresponding to row 5 in Table 5).   Impact of pathways during inference.   In Table 6 we   study the effects of using different pathways and alignment information during inference. All results are produced   Figure 3: Step grounding performance and minimum retained step ratio as a function of the filtering threshold. Aggressive unreliable pseudo-label filtering with high confidence thresholds (large maximum step discard ratio) leads   to improved performance.   from the same model trained for joint narration and step   grounding with fixed pseudo-labels from TAN (row 5 in Table 5). Grounding steps using the indirect steps-to-video   alignment only lags by 3.8% behind the direct steps-tovideo alignment that directly computes the similarity between steps and video frames (30.5% vs 34.3%). Their fusion outperforms their individual grounding performance.   This suggests that they capture complementary information.   We also explore substituting our learned steps-to-narrations   alignment with an alignment computed with an off-the-shelf   language model. This significantly degrades performance   (19.0) showing that our joint steps and narrations grounding model learns relationships between steps and narrations that go beyond textual similarity between pairs of sentences.   Similarly, substituting our learned narrations-tovideo alignment with an alignment based on the original   ASR timestamps reduces performance by 2.6% (see additional details in supplementary materials).   Impact of pseudo-label filtering threshold.   In Figure 3   we study the effect of the filtering threshold used in the iterative pseudo-labeling stage. We can observe that using aggressive filtering (i.e., high thresholds translating to a high   maximum percentage of pseudo-labels that are discarded) is   key to observing gains from iterative pseudo-labeling compared to training with fixed pseudo-labels from TAN. Intuitively, a large percentage of steps described in wikiHow   articles are not represented in the given instructional video   due to task mismatch, variations in recipe execution, and   some steps being optional. Therefore, starting with a small   subset of reliable pseudo-labels can facilitate step grounding.   Task selection.   In Table 7 we investigate different strategies to select the wikiHow articles during training. This   selection determines the set of steps to be grounded. We   evaluate two strategies for video-task association from narrations and compare them with using the task id provided in   15208   Figure 4: Visualization of sample frames per step, ground-truth segments (colored bars) and a single predicted timestamp   per step (dashed lines) for the groundable steps of three HT-step validation videos demonstrating (a) a Mini-Stromboli recipe,   (b) a Make Tabouli recipe, and (c) a Bake a Sweet Potato Pie recipe. Best viewed in color.   Task ID selection   HT-Step \u2191R@1   HT100M metadata   34.3   Top-1 prediction   34.7   Random / top-5 pred   34.3   Table 7: Sensitivity to task id selection. We assess how the   performance of our method changes when using different   strategies to associate videos with articles. We experiment   with using the task ids available from HT100M, as well as   the the two predictive strategies presented in Section 3.3.2.   We conclude that our method is robust to the task selection,   and the task labels are not necessary for training.   the HowTo100M metadata for each video3. We see that our   automatic selection approaches yield results on par with or   even slightly better than those based on metadata.   4.3.3   Qualitative Results   Qualitative grounding results obtained with the proposed   VINA model are presented in Figure 4. These examples   showcase that our model can successfully retrieve complex   and fine-grained steps, such as combining multiple ingredients in a mixing bowl or tapping a filled pie pan for the   Bake a Sweet Potato Pie recipe. Failure cases include discriminating between steps that involve the same ingredients,   such as mixing the milk and eggs versus adding the mixture   to the existing mix.   5. Conclusion   We have presented a method for learning how to temporally ground sequences of steps in instructional videos,   3During inference, metadata task ids are used in all of our HowTo100M   experiments in order to evaluate against the ground-truth step annotations.   without any manual supervision.   Our proposed method   exploits the weak supervision naturally provided in such   videos through their narrations, and solves for joint alignment of narrations and steps, while fusing two complementary pathways for step-to-video alignment. We demonstrated strong quantitative performance, surpassing the   state-of-the-art on multiple benchmarks for both narration   and step grounding.   Acknowledgements. We thank Huiyu Wang, Yale Song,   Mandy Toh, and Tengda Han for helpful discussions.", "conf": "ICCV", "year": "2023", "index": 157}, {"title": "Classi\ufb01cation-Then-Grounding:   Reformulating Video Scene Graphs as Temporal Bipartite Graphs   Kaifeng Gao\u2020, Long Chen\u2021\u2217, Yulei Niu\u2021, Jian Shao\u2020, Jun Xiao\u2020   \u2020Zhejiang University,   \u2021Columbia University   {kite phone,jshao,junx}@zju.edu.cn   {zjuchenlong,yn.yuleiniu}@gmail.com", "abstract": "Today\u2019s VidSGG models are all proposal-based methods,   i.e., they \ufb01rst generate numerous paired subject-object snippets as proposals, and then conduct predicate classi\ufb01cation   for each proposal. In this paper, we argue that this prevalent   proposal-based framework has three inherent drawbacks:   1) The ground-truth predicate labels for proposals are partially correct. 2) They break the high-order relations among   different predicate instances of a same subject-object pair.   3) VidSGG performance is upper-bounded by the quality of   the proposals. To this end, we propose a new classi\ufb01cationthen-grounding framework for VidSGG, which can avoid   all the three overlooked drawbacks. Meanwhile, under this   framework, we reformulate the video scene graphs as temporal bipartite graphs, where the entities and predicates are   two types of nodes with time slots, and the edges denote different semantic roles between these nodes. This formulation   takes full advantage of our new framework. Accordingly, we   further propose a novel BIpartite Graph based SGG model:   BIG. It consists of a classi\ufb01cation stage and a grounding   stage, where the former aims to classify the categories of   all the nodes and the edges, and the latter tries to localize   the temporal location of each relation instance. Extensive   ablations on two VidSGG datasets have attested to the effectiveness of our framework and BIG. Code is available at   https://github.com/Dawn-LX/VidSGG-BIG.   1.", "content": "To bridge the gap between vision and other modalities   (e.g., language), a surge of interests in our community start   to convert the vision data into graph-structured representations, called scene graphs [20]. Scene graphs are visuallygrounded graphs, where the nodes and edges represent object instances (or entities) and their pairwise visual relations   (predicates), respectively. Due to the inherent interpretabil\u2217Long Chen is the corresponding author. This work started when LC at   Zhejiang University, and YN at Nanyang Technological University.   \u2026   behind   towards   in front of   away   (9.71, 16.62)   9.21         10.21        12.01         16.62       17.22     18.82        22.42 time /s   behind   towards   (9.21, 11.21)   Proposal   Generation   behind   towards   in front of   away   Classification   dog (subject)   child (object)   \u2026   (10.10, 13.10)   (9.90, 11.80)   (9.21, 11.21)   (9.71, 16.62)   (17.22, 22.42)   (18.82, 22.42)   with respect to video range   in front of, away   (17.50, 22.50)   Classification   a   Grounding   with respect to video range   b   (a)   (b)   ground-truth   Figure 1. (a): The pipeline of proposal-based framework. Given   a video, it \ufb01rst generates numerous proposals (with different time   slots), and then conducts predicate classi\ufb01cation for each proposal.   (b): The pipeline of classi\ufb01cation-then-grounding framework. It   \ufb01rst conducts predicate classi\ufb01cation based on the whole tracklet   pair, and then grounds all the predicted relation instances.   ity, scene graphs have been widely used in numerous downstream tasks to help boost model performance, e.g., captioning [6,10,47], grounding [8,23], and QA [9,11,19,26].   Video Scene Graph Generation (VidSGG) has achieved   signi\ufb01cant progress over the recent years. Currently, almost   all existing VidSGG models are proposal-based1. Speci\ufb01cally, they can be categorized into two groups: 1) Segmentproposal based: They \ufb01rst cut the video into short segments   and detect object tracklets in each segment to compose segment proposals, then classify predicates in each proposal   and merge all predicted relation triplets (i.e., \u27e8subject,   predicate, object\u27e9) among adjacent segments [28,32,   33]. However, they fail to exploit the long-term context in   the video (or tracklets) due to the limits of short segments.   2) Tracklet-proposal based: They directly detect tracklets in   1We use proposals to represent paired subject-object tracklet segments.   19497   the whole video and generate tracklet proposals by slidingwindows [22] or con\ufb01dence splitting [13], and then conduct   predicate classi\ufb01cation for each proposal.   Although these proposal-based methods have dominated   the performance on VidSGG datasets, it is worth noting that   this prevalent framework has three inherent drawbacks:   1. The ground-truth predicate labels for proposals are   partially correct. By \u201cpartially\u201d, we mean that the groundtruth predicate labels sometimes are WRONG. Speci\ufb01cally,   following the IoU-based strategy in object detection, existing proposal-based models all assign predicate labels to proposals based on the volume IoU (vIoU). This strategy naturally discards some \u201cground-truth\u201d predicates if their vIoUs   are less than the threshold. As shown in Figure 1(a), two relations behind and towards happen simultaneously on   multiple frames inside both proposala and proposalb, but   the assigned predicate label for proposala is only behind   (and towards for proposalb)2. Meanwhile, once a predicate label is assigned to the proposal, they assume this relation should last for the whole proposal (i.e., it happens in all   the frames of the proposal). Obviously, one of the negative   impacts of this issue is that the ground-truth labels for two   highly-overlapped proposals (proposala/b) may be totally   different, and this inconsistency hurts the model training.   2. They break the high-order relations among different   predicate instances of a same subject-object pair. Due to   the nature of videos, there are always multiple relations happening between a same subject-object pair, and these relations can serve as critical context (or inductive bias) to bene\ufb01t the predictions of other relations. For example, behind,   towards, and away always happen sequentially between   dog and child. Instead, proposal-based methods explicitly break these high-order relations by pre-cutting tracklets,   and classify predicates independently in each proposal3.   3. VidSGG performance is upper-bounded by the quality of the proposals. The VidSGG performance is sensitive   to the heuristic rules for proposal generation (e.g., the sizes   or number of proposals). Meanwhile, to achieve higher recalls, they always generate excessive proposals, which signi\ufb01cantly increases the computation complexity.   In this paper, we propose a classi\ufb01cation-then-grounding   framework for VidSGG, which can avoid all the mentioned   drawbacks in proposal-based methods. Speci\ufb01cally, we \ufb01rst   conduct predicate classi\ufb01cation based on the whole tracklets, and then ground each predicted predicate instance (Figure 1(b)). Compared to proposal-based methods, we regard   all the relations happen between the two tracklets as groundtruth predicate labels (e.g., behind, towards, away, and   in-front-of are all ground-truth predicates for dog and   2For proposala, its vIoU with predicate towards < 0.5 and its vIoU   with predicate behind > 0.5. The situation is opposite for proposalb.   3Although a few proposal-based models start to resort to some context   modeling techniques to remedy this weakness, we claim that the proposalbased framework itself overlooks and breaks these high-order relations.   1.53          7.80              11.14          18.42          21.55 time /s   child   dog   in front of   towards   next-to   away   (1.53, 21.55)   (1.53, 7.80)   (18.42, 21.55)   (7.80, 11.04)   (7.34, 9.67)   (20.22, 21.55)   dog(sub.)   child(obj.)   Temporal Bipartite Graph   towards   away   towards   subject   object   (1.53, 21.55)   (1.53, 21.55)   in front of   child(sub.)   dog(obj.)   next-to   next-to   Figure 2. Left: A video example and its ground-truth visual relation triplets. Right: The corresponding temporal bipartite graph.   Comparisons with existing formulation are left in appendix.   child). Our framework not only provides more accurate   ground-truth predicate labels, but also preserves the ability   to utilize high-order relations among predicates. Moreover,   it avoids super\ufb02uous proposals and heuristic rules.   Under this framework, we propose to reformulate video   scene graphs as temporal bipartite graphs, where the entities   and predicates are two types of nodes with time slots, and   the edges denote different semantic roles (i.e., subject   and object) between these nodes (Figure 2). Each entity   node is an object tracklet, and its time slot is the temporal   range of this tracklet. Each predicate node is a set of relation instances between two entities with the same predicate   category, where each time slot denotes the temporal range   of each relation instance (e.g., predicate node towards in   Figure 2 has two time slots). Thus, each entity node can be   linked with multiple predicate nodes to represent multiple   relations involved, and each predicate node can be linked   with at most one entity node for each role. This formulation   can not only be easily extended to more general relations   with more semantic roles [50], but also avoid exhaustively   enumerating all entity pairs for predicate prediction.   Accordingly, we propose a BIpartite Graph based model   BIG, which consists of a classi\ufb01cation stage and a grounding stage. Speci\ufb01cally, the former aims to classify the categories of all the nodes and edges, and the latter tries to localize the temporal location of each relation instance. For the   classi\ufb01cation stage, it is a Transformer-based model, where   the inputs for the encoder and decoder are tracklet features   and learnable predicate embeddings, respectively. To distinguish different semantic roles, we also propose a role-aware   cross-attention which introduces role-wise distinctions into   predicate embeddings. For the grounding stage, we regard   the triplet categories of each predicate node as a language   query (e.g., \u27e8dog, towards, child\u27e9in Figure 2), and   ground this language query in the video. Since each relation   category may happen multiple times between two tracklets,   we design a multi-instance grounding head at this stage.   We evaluate models on two challenging VidSGG benchmarks: VidVRD [32] and VidOR [30]. Extensive ablations   and results have demonstrated the effectiveness of our new   19498   classi\ufb01cation-then-grounding framework and BIG model.   In summary, we make three contributions in this paper:   1. We propose a new classi\ufb01cation-then-grounding framework for VidSGG. It avoids three inherent drawbacks of   the existing proposal-based framework.   2. We reformulate video scene graphs as temporal bipartite   graphs, and take full advantage of the new framework.   3. We propose a novel model BIG, which achieves state-ofthe-art performance on two VidSGG datasets.   2. Related Works   Video Scene Graph Generation. Today\u2019s VidSGG models   are all proposal-based. They usually focus on designing: 1)   more effective context fusing mechanism among segment or   tracklet proposals, e.g., GCNs or CRFs [22,28,37], compositional relation encoding [13], or structure context aggregation [36]; or 2) stronger relation association approaches,   such as MHA [33] or online association [28]. In contrast,   we are the \ufb01rst to avoid the proposal generation step and   solve VidSGG task in a new classi\ufb01cation-then-grounding   manner. Meanwhile, we propose a temporal bipartite graph   formulation by extending the image bipartite graph [50] into   the video domain, i.e., assigning predicate nodes with time   slots. Accordingly, we propose a novel BIG model.   Transformer Structure for SGG. Transformer structures [38, 39] regain vision community attention after the   pioneering work DETR [4], which regards the object detection task as a set prediction problem. Inspired from DETR,   several recent works start to use Transformer models for image scene graph generation [12,15,35,53]. Similarly, these   models utilize a set of learnable embeddings as the input of   the decoder, and predict the triplets based on the encoded   global object features. Inspired from these works, we also   adopt the Transformer structure in our classi\ufb01cation stage,   and design a role-aware cross-attention module to explicitly   model different edges of the temporal bipartite graph.   Video Grounding. It aims to localize the video segment depicted by a language query [16,49]. Existing models can be   roughly grouped into: 1) Anchor-based [1,3,41,44,46,52]:   They match all moment proposals to the language query and   select the one with the highest matching score as the prediction. 2) Anchor-free [7,24,42,45,48]: They directly predict   the probability of being a boundary for each frame, or directly regress the temporal locations of the target moment.   In this paper, we convert the grounding stage as a video   grounding problem, and build on top of a SOTA model DEBUG [24] by extending it into multiple segment outputs.   3. Approach   In this paper, we reformulate video scene graphs as temporal bipartite graphs. Given an entity category set Ce and   predicate category set Cp, a temporal bipartite graph is formally de\ufb01ned as G = (Ne, Np, E), where Ne, Np, and E   denote the set of entity nodes, predicate nodes and edges, respectively. For each entity node ei \u2208Ne, it associates with   an entity class ce   i \u2208Ce and a time slot (se   i, ee   i). Similarly,   for each predicate node pj \u2208Np, it associates with a predicate class cp   j \u2208Cp and a set of time slots {(sp   j,k, ep   j,k)}Kj   k=1.   This multiple time slots setting implies that each predicate   node has Kj instances with the same category (happens Kj   times) in the same subject-object pair. E \u2286Ne \u00d7Np \u00d7Cr is   a set of mapping that maps an entity-predicate pair to a semantic role, i.e., Ne \u00d7 Np \u2192Cr, where Cr = {subject,   object} is a semantic role set. The size of Ne and Np are   denoted as n and m, respectively.   Under this new bipartite graph formulation, we propose a   novel VidSGG model: BIG. The overview pipeline of BIG   is illustrated in Figure 3, which consists of two stages: classi\ufb01cation stage (Sec. 3.1) and grounding stage (Sec. 3.2).   3.1. Classi\ufb01cation Stage   3.1.1   Overview   The classi\ufb01cation stage aims to classify the categories of all   the nodes (i.e., entity and predicate), and the edges between   them (i.e., the semantic roles). As shown in Figure 3(a), the   classi\ufb01cation stage consists of four parts: a tracklet detector,   an encoder, a decoder, and a classi\ufb01cation head.   Tracklet Detector. Given a video, we use a pretrained   tracklet detector to detect all tracklets in the video (denoted   as entity set Ne), and corresponding spatial-temporal locations, categories, and features. Speci\ufb01cally, for each entity   ei \u2208Ne with length li (the number of frames), it is characterized by the bounding box coordinates bi \u2208Rli\u00d74, object   category ce   i \u2208Ce, and a time slot (se   i, ee   i). We \ufb01x all the detection results (i.e., {bi} and {ce   i}) as the \ufb01nal predictions.   The tracklet feature fi for each entity ei is a combination   of appearance feature and spatial feature. The appearance   feature f a   i \u2208Rli\u00d7da is extracted at each frame based on the   box locations by using RoIAlign [29]. The spatial feature   f s   i \u2208Rli\u00d78 is the concatenation of all box coordinates bi   and offsets \u2206bi, where \u2206bi,j is the box coordinate offsets   of two consecutive frames, i.e., \u2206bi,j = bi,j+1\u2212bi,j. Then,   the tracklet feature fi \u2208Rli\u00d7de for entity ei is   fi = Conv [MLPa(f a   i ); MLPs(f s   i )] ,   (1)   where MLPa and MLPs are two learnable MLPs, [; ] is a concatenate operation, and Conv is a 1D convolutional layer.   Encoder. Given entity features {fi}, the encoder aims to   encode global context among all entities. Thus, we utilize   the vanilla Transformer encoder [38] as our encoder, where   each layer consists of a multi-head self-attention (MHSA) and   a feed-forward network (FFN). Since the sizes of the entity   features are different, we \ufb01rst utilize a pooling operation to   transform each feature fi \u2208Rli\u00d7de to a \ufb01xed size feature   f \u2032   i \u2208Rl\u00d7de, and use a MLP to mapping it into a vector hi \u2208   19499   Feature Encoder   Transformer   Encoder   Relation    Decoder   \u2026   \u2026   person   car   car   sit-above   ride   move-left   motorcycle   stop-behind   move-front   \u2026   Tracklet Detector   Classification Head   <motorcycle, move-front, car>   \u2026   \u2026   predicate queries   video feature   multi-instance grounding   Prediction Head   temporal overlapping   +   (a) Classification Stage   Feature Extractor   (b) Grounding Stage   (c) Video Scene Graph   \u2026   NMS   \u0de9\ud835\udc6f\ud835\udc6f   \u0de9\ud835\udc78\ud835\udc78   \ud835\udc6f\ud835\udc6f   \ud835\udc78\ud835\udc78   \ud835\udc6d\ud835\udc6d   \ud835\udc7a\ud835\udc7a\ud835\udc57\ud835\udc57   (\ud835\udc60\ud835\udc60\ud835\udc57\ud835\udc57, \ud835\udc52\ud835\udc52\ud835\udc57\ud835\udc57)   {(\ud835\udc60\ud835\udc60\ud835\udc57\ud835\udc57   \ud835\udc5d\ud835\udc5d, \ud835\udc52\ud835\udc52\ud835\udc57\ud835\udc57   \ud835\udc5d\ud835\udc5d)}   Figure 3. The overall pipeline of the proposed BIG model, which consists of a classi\ufb01cation stage (a) and a grounding stage (b).   Rde. Then, we stack all entity features {hi} into a matrix   H \u2208Rn\u00d7de, and feed the H into the encoder. The outputs   of the encoder are contextualized features f   H \u2208Rn\u00d7de.   Decoder. The decoder is designed to predict the edges   of the graph, and derive enhanced predicate representations   for the following predicate classi\ufb01cation. The inputs for the   decoder is a \ufb01xed-size set of m predicate queries with corresponding learnable embeddings Q \u2208Rm\u00d7dq. Each query   is responsible for a predicate node in the bipartite graph.   We built on top of the Transformer decoder and replace the   original cross-attention with a Role-aware Cross-Attention   (RaCA). Therefore, each decoder layer is summarized as:   Q\u2032   (i) = LNorm(Q(i) + MHSA(Q(i))),   \u00afQ\u2032   (i) = RaCA(Q\u2032   (i), f   H, f   H),   Q\u2032\u2032   (i) = LNorm(Q\u2032   (i) + \u00afQ\u2032   (i)),   Q(i+1) = LNorm(Q\u2032\u2032   (i) + FFN(Q\u2032\u2032   (i))),   (2)   where LNorm is the layer normalization [2], Q(i) is the input query embeddings of i-th decoder layer. The output of   the last deocder layer is denoted as e   Q, i.e., the enchanced   query embeddings. Meanwhile, the cross-attention matrix   (inside RaCA module) of the last decoder layer is denoted   as e   A, which can be regarded as a soft edge linkage of the bipartite graph. More details and discussions about the RaCA   module (vs. original cross-attention) are in Sec. 3.1.2.   Classi\ufb01cation Head. Given the query embeddings e   Q   and cross-attention matrix e   A, the classi\ufb01cation head aims to   classify the category of each query (i.e., predicate node). As   shown in Figure 4, e   A has two channels which correspond to   two different semantic roles in the bipartite graph. Based on   e   A, we \ufb01rst derive the predicted subject and object for each   predicate node pj by selecting the entity with the highest   attention score in each channel, of which the indices are   denoted as js and jo, respectively. Then, the classi\ufb01cation   feature f p   j for predicate pj is a concatenation of three types   \u0de9\ud835\udc68\ud835\udc68   \u00d7   \ud835\udc7d\ud835\udc7d   \ud835\udc39\ud835\udc39\ud835\udc60\ud835\udc60   \ud835\udc39\ud835\udc39\ud835\udc5c\ud835\udc5c   +   \ud835\udc5b\ud835\udc5b\u00d7 \ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc52   2 \u00d7 \ud835\udc5a\ud835\udc5a\u00d7 \ud835\udc5b\ud835\udc5b   \u0d25\ud835\udc78\ud835\udc78\u2032   \ud835\udc5a\ud835\udc5a\u00d7 \ud835\udc51\ud835\udc51\ud835\udc5e\ud835\udc5e   \u00d7\u00d7   \ud835\udc52\ud835\udc52\ud835\udc57\ud835\udc57\ud835\udc60\ud835\udc60   \ud835\udc52\ud835\udc52\ud835\udc57\ud835\udc57\ud835\udc5c\ud835\udc5c   \u00d7   \ud835\udc5d\ud835\udc5d\ud835\udc57\ud835\udc57   {\ud835\udc5d\ud835\udc5d\ud835\udc57\ud835\udc57}   \ud835\udc57\ud835\udc57\ud835\udc60\ud835\udc60   \ud835\udc57\ud835\udc57\ud835\udc5c\ud835\udc5c   entity-wise normalize   role-wise normalize   \ud835\udc7e\ud835\udc7e\ud835\udc5f\ud835\udc5f   \ud835\udc44\ud835\udc44   \ud835\udc7e\ud835\udc7e\ud835\udc5f\ud835\udc5f\ud835\udc3e\ud835\udc3e   \ud835\udc72\ud835\udc72   \ud835\udc78\ud835\udc78\ud835\udc78   \ud835\udc5b\ud835\udc5b\u00d7 \ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc52   \ud835\udc5a\ud835\udc5a\u00d7 \ud835\udc51\ud835\udc51\ud835\udc5e\ud835\udc5e   {\ud835\udc52\ud835\udc52\ud835\udc56\ud835\udc56}   \u2026   \u2026   \u2026   \u2026   \u2026   Figure 4. Illustration of the Role-aware Cross-Attention module.   of features: query embedding eqj, subject/object entity features hjs and hjo, and word embeddings of subject/object   entity categories, i.e., f p   j = [eqj; hjs; hjo; \u03a0(ce   js); \u03a0(ce   jo)],   where \u03a0(ce   i) \u2208Rdw is the GloVe embedding [27] of object   category ce   i. Finally, the predicate category is classi\ufb01ed by:   P(cp   j) = Softmax(MLPp(f p   j ) + bce   js,ce   jo ),   (3)   where MLPp is a MLP, and b\u2217,\u2217is the statistical prior of the   relation triplet categories from the training set [36,51].   3.1.2   Role-aware Cross-Attention (RaCA)   As shown in Figure 4, the RaCA module aims to aggregate   entity features from different semantic roles into query embeddings based on the cross-attention matrix. To distinguish   different semantic roles (i.e., subject or object), we   perform cross-attention for each semantic role separately,   and then fuse these role-wise features with two non-linear   transformations. Speci\ufb01cally, let K = V = f   H \u2208Rn\u00d7de   be the key and value matrix which are the output of the   encoder, and Q\u2032 \u2208Rm\u00d7dq be the query matrix which are   the output from the \ufb01rst subnet in each decoder layer4 (cf.   Eq. (2)). RaCA constructs a two-channel attention matrix   A \u2208R2\u00d7m\u00d7n and each channel of A is calculated as:   Ar = (Q\u2032W Q   r )(KW K   r )T/   p   de,   (4)   4For brevity, we omit subscripts i in this subsection, e.g., Q\u2032   (i) \u2192Q\u2032.   19500   \u00d7 4   1D Conv   Multimodal   Feature   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc3e\ud835\udc3e   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 2\ud835\udc3e\ud835\udc3e   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc51\ud835\udc51   \ud835\udc47\ud835\udc47\u00d7 \ud835\udc3e\ud835\udc3e   \ud835\udc74\ud835\udc74   classification   regression   confidence   \u00d7 4   1D Conv   \u00d7 4   1D Conv   for each   instance   left  right   foreground/background   centerness   bins   \u00d7 1   \u00d7 1   \u00d7 1   bin_1   bin_2   in-front-of   in-front-of   dog   child   \u2026   \u2026   \u2026   bin_K   (a)   (b)   x K   Figure 5. (a): Illustration of label assignment in the multi-instance   grounding. (b): The overview of multi-instance grounding head.   where W Q   r , W K   r are learnable weights and r \u2208{1, 2} represent the subject and object channel, respectively. In our   formulation, since we assume each predicate query can only   link to one entity in each role and each entity-predicate pair   has one type of semantic roles at most. Thus, we normalize   A along both the entity axis and the role axis, i.e.,   e   Ar,j,i =   exp(Ar,j,i)   Pn   i\u2032=1 exp(Ar,j,i\u2032) \u00d7   exp(Ar,j,i)   P2   r\u2032=1 exp(Ar\u2032,j,i)   . (5)   Then, we use two role-speci\ufb01c non-linear MLPs (F\u2217) to introduce role-wise distinctions into query embeddings,   \u00afQ\u2032 = Fs( e   A1V ) + Fo( e   A2V ), F\u2217: Rde 7\u2192Rdq,   (6)   where \u00afQ\u2032 is the output of the RaCA module (cf. Eq. (2)),   which aggregates role-aware information from each entity.   Discussions. Compared to the plain cross-attention in original Transformer [38], RaCA explicitly learns the adjacency   matrix of the bipartite graph based on role-wise normalization (cf. Eq. (5)) and role-aware non-linear mappings (F\u2217in   Eq. (6)). Otherwise the adjacency matrix (or edge linkage)   can not be modeled by the plain cross-attention module.   3.2. Grounding Stage   The grounding stage aims to localize the temporal location of each predicted predicate node. So far, for each   predicate node pj, the classi\ufb01cation stage have predicted its   category cp   j and two linked entity tracklets: subject ejs and   object ejo. In this stage, we regard the predicate localization   as a video grounding problem [16]. Speci\ufb01cally, we treat   the triplet categories sequence (ce   js, cp   j, ce   jo) (e.g., \u27e8person,   ride, motorcycle\u27e9in Figure 3(a)) as a language query,   and extend an existing video grounding model DEBUG [24]   for multi-instance predicate localization. As shown in Figure 3(b), this stage consists of three parts: a feature extractor, a feature encoder, and a multi-instance grounding head.   Feature Extractor. For the given video, we use a pretrained I3D [5] network to extract frame-level visual feature   F \u2208RT \u00d7dv, where T is the number of whole video frames.   For query (ce   js, cp   j, ce   jo) (refer to predicate node pj), we initialize the query feature Sj = [\u03a0(ce   js), \u03a0(cp   j), \u03a0(ce   jo)], i.e.,   the GloVe embeddings of the triplet categories. Meanwhile,   since each predicate only happen in the overlapping time of   its subject and object, we use the temporal boundaries of   this overlapping time as a prior feature to enhance Sj, i.e.,   eSj = MLPw(Sj) + MLPt([sj, ej]), eSj \u2208R3\u00d7dw,   (7)   where [sj, ej] \u2208R2 are the overlapping boundaries of the   subject and object linked to predicate node pj. Note that   only those predicate nodes referring to overlapped subjectobject tracklets are used in the grounding stage. Visual features F is shared for all the queries with features { eSj}m   j=1.   Feature Encoder. This encoder aims to model the interaction between the video feature F and all query features   { eSj}. Speci\ufb01cally, we use the same feature encoder as DEBUG [24], which contains two parallel embedding encoders   and a multi-modal attention layer. The output of the feature   encoder is a fused multimodal feature M \u2208RT \u00d7d. We   refer readers to the DEBUG [24] paper for more details.   Multi-instance Grounding Head. Different from the   existing video grounding task where each query only refers   to a single segment, in VidSGG, a predicate category can   happen multiple times between a same subject-object pair,   i.e., each language query may refer to multiple segments   (cf. Figure 2). Since the number of time slots varies widely   in different predicate nodes, it is dif\ufb01cult to directly predict   a variable number of temporal segments for each query. Instead, we set K bins for each language query. As shown   in Figure 5(a), in the training stage, we divide the whole   normalized video length evenly into K intervals, referring   to K bins. Then, each bin is assigned with the target time   slots centered in its interval5. In the test stage, all time slots   predictions are processed by NMS to reduce false positives.   Finally, the NMS operation results in Kj time slots for the   triplet query (cs   j, cp   j, co   j), denoted as {(sp   j,k, ep   j,k)}Kj   k=1.   Following DEBUG [24], we design three branches network for grounding: a classi\ufb01cation subnet, a boundary regression subnet, and a con\ufb01dence subnet (cf. Figure 5(b)).   In particular, we extend the output channels of the last convlayer to K for the classi\ufb01cation and con\ufb01dence branch, and   2K for the regression branch (corresponding to K bins).   3.3. Training Objectives   Classi\ufb01cation Stage. Since we \ufb01x all the tracklets from   the detection backbone as the \ufb01nal entity nodes predictions,   we only consider the training losses for classi\ufb01cation of the   5Although multiple targets may still fall into a same bin, such samples   only account for a small proportion (details are in the appendix).   19501   edges and the predicate nodes. Let \u02c6   Np = {\u02c6pj}m   j=1 be the   predicted set of m predicate nodes, and N \u2217   p be the groundtruth predicate set of size m padded with \u2205(background).   We adopt a one-to-one label assignment by \ufb01nding a bipartite matching between \u02c6   Np and N \u2217   p . Speci\ufb01cally, we search   for a permutation of m elements \u02c6\u03c3 by optimizing the cost:   \u02c6\u03c3 = arg min   \u03c3   Pm   j=1Lmatch(p\u2217   j, \u02c6p\u03c3(j)).   (8)   This matching problem can be computed ef\ufb01ciently with the   Hungarian algorithm [25], following prior work DETR [4].   The matching cost considers both predicate classi\ufb01cation   and edge prediction. Since all the entity nodes are \ufb01xed, the   cost can be considered at the view of predicate nodes, and   entity nodes are assigned to their ground-truths in advance   (which is based on vIoU and the criterion is similar to that in   Faster R-CNN [29]). Thus, each predicate node can be described by its category and the two edges to subject/object.   We denote p\u2217   j = (cp\u2217   j , a\u2217   j), where cp\u2217   j   is the predicate category (which may be \u2205) and a\u2217   j \u2208{0, 1}2\u00d7n is the j-th   row of A\u2217(ground-truth adjacent matrix) for two channels.   Note that a\u2217   j,r,i = 0 when the i-th entity has no ground-truth   to match. For the predicted predicate with index \u03c3(j), the   corresponding edges are described by \u02c6a\u03c3(j) \u2208R2\u00d7n, which   is the \u03c3(j)-th row of the predicted \u02c6   A for two channels. With   the above notations, the matching cost is de\ufb01ned as:   Lmatch(p\u2217   j, \u02c6p\u03c3(j)) = \u22121{cp\u2217   j \u0338=\u2205} log P(\u02c6cp   \u03c3(j) = cp\u2217   j )   + 1{cp\u2217   j \u0338=\u2205}\u03bbattLatt(a\u2217   j, \u02c6a\u03c3(j)),   (9)   where \u03bbatt is hyperparameter, and Latt is de\ufb01ned as a binarycross entropy (BCE) loss, 1{\u00b7} is an indicator function. After obtaining \u02c6\u03c3, the loss Lc for classi\ufb01cation stage consists   of the matching loss between (p\u2217   j, \u02c6p\u02c6\u03c3(j)) pairs, and the background classi\ufb01cation loss for other predicate nodes, i.e.,   Lc = P   jLmatch(p\u2217   j, \u02c6p\u02c6\u03c3(j)) \u2212P   cp\u2217   j   =\u2205log P(\u02c6cp   \u02c6\u03c3(j) = \u2205). (10)   Grounding Stage. The grounding stage is trained separately from the classi\ufb01cation stage, and we use ground-truth   triplet categories for training. Following DEBUG [24], the   training objectives consist of three losses for three respective branches. The total loss is averaged among all K bins.   4. Experiments   4.1. Datasets and Evaluation Metrics   Datasets. We evaluated BIG on two benchmarks: 1)   VidVRD [32]: It consists of 1,000 videos, which covers   35 object categories and 132 predicate categories. We used   the of\ufb01cial splits: 800 videos for training and 200 videos   for test. 2) VidOR [30]: It consists of 10,000 videos, which   covers 80 object categories and 50 predicate categories. We   used of\ufb01cial splits: 7,000 videos for training, 835 videos for   validation, and 2,165 videos for test. Since the annotations   of the test set are not released, we only evaluated the val set.   Evaluation Metrics. We evaluated BIG on two tasks: 1)   Relation Detection (RelDet): It detects a set of visual relation triplets, and corresponding tracklets of subject and object. A detected triplet is considered to be correct if there is   a same triplet tagged in ground-truth, and both subject and   object tracklets have a suf\ufb01cient vIoU (e.g., 0.5) with the   ground-truth. We used mAP and Recall@K (R@K, K=50,   100) as metrics for RelDet. 2) Relation Tagging (RelTag):   It only focuses on the precision of visual relation triplets   and ignores the localization results of tracklets. For RelTag,   we used Precision@K (P@K, K=1,5,10) as metrics.   4.2. Implementation Details   Tracklet Detector. We utilized the video object detector MEGA [14, 17] with backbone ResNet-101 [18] to obtain initial frame-level detection results, and adopted deepSORT [40] to generate object tracklets.   Adapting BIG to VidVRD. For each relation triplet in   the training set of VidVRD [32], we noticed that only a portion of ground-truth segments is annotated as foreground,   which makes the annotated temporal boundaries unreliable   for training. Therefore, we only used the classi\ufb01cation stage   of BIG for VidVRD, termed BIG-C. Consequently, the time   slot for each predicate pj is calculated as the overlap of its   subject and object, i.e., (se   js, ee   js) \u2229(se   jo, ee   jo), and Kj = 1.   More implementation details are left in the appendix.   4.3. Ablation Studies   Effectiveness of Classi\ufb01cation-Then-Grounding. We   designed a baseline model to show the effectiveness of this   framework and the two stages (classi\ufb01cation & grounding).   Speci\ufb01cally, it directly classi\ufb01es predicate categories of all   tracklet pairs through multi-label classi\ufb01cation, based on   feature f \u2032p   j   = [hjs; hjo; \u03a0(ce   js); \u03a0(ce   jo)] (cf. f p   j in Eq.(3)),   namely Base-C. Then, we apply the grounding stage to the   Base-C, termed Base. All results are in Table 2. From this   table, we can observe that even without the BIG model, the   simple classi\ufb01cation-then-grounding baseline (Base) still   outperforms SOTA proposal-based model Sun et al. [34].   Furthermore, we reported the number of average relation   candidates for the grounding stage (#Cand.) in Table 2 to   demonstrate the effectiveness of each stage. For the classi\ufb01cation stage, by comparing BIG-C with Base-C, we can observe that BIG-C outperforms Base-C on all metrics, especially with a large margin on RelTag while having fewer average relation candidates (135.4 vs. 482.1), which demonstrates the superiority of the encoder-decoder pipeline under   the temporal bipartite graph formulation. For the grounding   stage, we can observe that it can consistently improve detection mAP and recall for both two backbones (Base-C and   BIG-C). The improvements of RelTag are slight because it   19502   Models   Features   RelDet   RelTag   Visual   Motion   mAP   R@50 R@100   P@1   P@5   P@10   VidVRD [32]MM\u201917   iDT   \u2713   8.58   5.54   6.37   43.00 28.90   20.80   GSTEG [37]CVPR\u201919   iDT   \u2713   9.52   7.05   8.67   51.50 39.50   28.23   VRD-GCN [28]MM\u201919   iDT   \u2713   16.26   8.07   9.33   57.50 41.00 28.5 0   MHA [33]MM\u201920   iDT   \u2713   19.03   9.53   10.38   57.50 41.40   29.45   IVRD [21]MM\u201921   RoI   \u2713   22.97 12.40   14.46   68.83 49.87   35.57   VidVRD-II [31]MM\u201921   RoI   \u2713   29.37 19.63   22.92   70.40 53.88   40.16   Liu et al. [22]CVPR\u201920   RoI+I3D\u2020   \u2713   18.38 11.21   13.69   60.00 43.10   32.24   Chen et al. [13]ICCV\u201921   RoI+I3D   \u2713   20.08 13.73   16.88   62.50 49.20   38.45   Liu et al. [22]CVPR\u201920   RoI\u2020   14.01   8.47   11.00   56.50 36.70   26.60   TRACE [36]ICCV\u201921   RoI   15.06   7.67   10.32   \u2014   \u2014   \u2014   BIG-C (Ours)   RoI\u2020   17.56   9.59   10.92   56.50 44.30   32.35   Liu et al. [22]CVPR\u201920   RoI+I3D\u2020   14.81   9.14   11.39   55.50 38.90   28.90   TRACE [36]ICCV\u201921   RoI+I3D   17.57   9.08   11.15   61.00 45.30   33.50   BIG-C (Ours)   RoI+I3D\u2020   17.67   9.63   11.29   56.00 43.80   32.85   BIG-C (Ours)   RoI\u2021   26.08 14.10   16.25   73.00 55.10   40.00   Table 1. Performance (%) on VidVRD of SOTA methods. Visual: \u2020 means that these models use   the same tracklets and features as Liu et al. [22], and \u2021 means that these models use tracklets and   features generated by MEGA. Motion: It refers to the relative motion feature of entity pairs [31].   adult   dog   next-to   watch   behind   caress   (7.11,  12.71)   (18.89, 26.49)   (29.76, 39.64)   (29.23, 40.07)   (0.0, 2.30)   (0.93, 40.57)   dog   away   in-front-of   (0.0, 2.30)   (0.0, 2.30)   0.93s   1.94s   7.34s   13.25s   18.89s   37.07s   child   dog   away   above   (0.0,  3.44)   (10.13, 13.25)   (0.0, 26.93)   (0.0, 3.87)   (3.3, 20.39)   (0.0, 35.47)   in-front-of   in-front-of   stool   0.0s   3.44s   9.51s   11.48s   21.35s   .   (0.0, 35.47)   (0.0, 35.47)   (0.0, 26.93)   (0.0, 2.30)   (0.0, 40.57)   (0.93, 40.57)   Figure 6. Qualitative results on VidOR.   Models   RelDet (%)   RelTag (%) #Cand.   mAP R@50 R@100   P@1   P@5   Sun et al. [34] 6.56   6.89   8.83   51.20 40.73   \u2014   Base-C   7.05   7.17   9.19   59.01 47.07   482.1   Base   7.19   7.32   9.50   59.49 47.28   482.1   BIG-C   8.29   7.92   9.65   64.42 51.70   135.4   BIG   8.54   8.03   10.04   64.42 51.80   135.4   Table 2. Ablations on effectiveness of different stages on VidOR.   doesn\u2019t consider the locations of relation triplets.   Number of Predicate Queries. We compared BIG-C   with different number of predicate queries (m) in Table 4.   It can be observed that more predicate queries always improve the \ufb01nal VidSGG performance, but also result in more   computations (e.g., #Cand.). To trade-off effectiveness and   ef\ufb01ciency, we set m = 192 for all the following experiments.   Ablations on the RaCA Module. We analyzed the impact of role-wise normalization (R-norm) and the two rolespeci\ufb01c mappings (F\u2217) in the RaCA module on BIG-C.   From the results in Table 5, we can observe that both the Rnorm and F\u2217are important for the role-aware information   encoding. Particularly, when both two techniques are used,   the model achieves the best results, especially on P@1.   Ablations on the Multi-instance Grounding. We further investigated the in\ufb02uence of different number of bins in   the multi-instance grounding. Since each predicate category   of a same subject-object may have multiple instances, we   regarded relation triplets with the same subject-object pair   and predicate category as a sample. Each ground-truth sample can be partially hit with a fraction recall (fR), which is   calculated as the fraction of hit relation triplets of each sample. For more precise, we evaluated fR@K for ground-truth   samples with a single instance (fRS) and multiple instances   (fRM), separately. From the results in Table 6, we can observe that: 1) With the increase of K, corresponding fR@K   increases on both single-instance and multi-instance samples. 2) Our multi-instance grounding (e.g., #Bins=5,10) is   more capable of improving the fR of predicates on multiinstance samples, e.g., the relative gains of fRM are larger   than fRS (3.97% (5.53\u21925.75) vs. 0.61% (12.96\u219213.04)).   4.4. Comparisons with State-of-the-Arts   4.4.1   Performance on VidVRD   Settings. For VidVRD, we compared our BIG-C with several state-of-the-art methods, which can be coarsely categorized into two groups: 1) Segment-proposal based methods:   VidVRD [32], GSTEG [37], VRD-GCN [28], MHA [33],   IVRD [21], VidVRD-II [31], and TRACE [36].   2)   Tracklet-proposal based models methods: Liu et al. [22]   and Chen et al. [13]. For more fair comparisons, we also   reported the results of BIG-C with the same features as [22].   Results. All results are reported in Table 1. From this table,   we have following observations: 1) When using the MEGA   backbone, BIG-C (with only RoI feature) beats most of the   proposal-based methods even without the grounding stage.   Particularly, we achieve a very high mAP (i.e., 26.08%) and   the highest P@1 (i.e., 73.00%). 2) When using the same   RoI feature as [22], BIG-C outperforms TRACE [36] and   Liu et al. [22] on both RelDet and RelTag tasks, especially   we achieve signi\ufb01cant performance gains on mAP (17.56%   19503   Models   Detector   Features   RelDet   RelTag   Visual   Lang Motion Mask   mAP   R@50 R@100   P@1   P@5   P@10   Liu et al. [22]CVPR\u201920   Re\ufb01neDet   RoI+I3Dr   \u2713   6.85   8.21   9.90   51.20 40.73   \u2014   Chen et al. [13]ICCV\u201921   Faster R-CNN   RoI+I3Dr   \u2713   \u2713   10.04   8.94   10.69   61.52 50.05 38.48   Chen et al. [13]ICCV\u201921   Faster R-CNN   RoI+I3Dr   \u2713   \u2713   \u2713   11.21   9.99   11.94   68.86 55.16 43.40   IVRD [21]MM\u201921   Faster R-CNN   RoI   \u2713   7.42   7.36   9.41   53.40 42.70   \u2014   Chen et al. [13]ICCV\u201921   Faster R-CNN   RoI   \u2713   8.93   7.38   9.22   56.89 44.76 34.07   VidVRD-II [31]MM\u201921   Faster R-CNN   RoI   \u2713   8.65   8.59   10.69   57.40 44.54 33.30   BIG-C (Ours)   MEGA   RoI   8.03   7.60   9.39   62.25 50.96 40.30   BIG (Ours)   MEGA   RoI+I3Df   8.28   7.74   9.82   62.13 51.25 40.48   VRU\u201919-top1 [34]MM\u201919   FGFA   \u2014   \u2713   \u2713   6.56   6.89   8.83   51.20 40.73   \u2014   MHA [33]MM\u201920   FGFA   \u2014   \u2713   \u2713   6.59   6.35   8.05   50.72 41.56   \u2014   VRU\u201920-top1 [43]MM\u201920 CascadeRCNN   RoI   \u2713   \u2713   \u2713   9.93   9.12   \u2014   67.43   \u2014   \u2014   Chen et al. [13]ICCV\u201921   Faster R-CNN   RoI   \u2713   \u2713   9.54   8.49   10.17   59.24 47.24 35.99   BIG-C (Ours)   MEGA   RoI   \u2713   8.29   7.92   9.65   64.42 51.70 41.05   BIG (Ours)   MEGA   RoI+I3Df   \u2713   8.54   8.03   10.04   64.42 51.80 40.96   Table 3. Performance (%) on VidOR of SOTA models. The Best and second best are marked in according formats. Visual: I3Dr and I3Df   denote region-level and frame-level I3D features, respectively. Lang: The word embeddings of entity categories. Motion: It refers to the   relative motion feature of entity pairs [31]. Mask: It means the localization mask of entities [43].   m   RelDet (%)   RelTag (%)   #Cand.   mAP R@50 P@1   P@5   128 7.50 7.15 62.62 51.02 105.1   192 8.29 7.92 64.42 51.70 135.4   256 8.31 7.92 62.86 51.22 169.3   Table 4. Ablations of BIG-C for different   number of predicate queries on VidOR.   R-norm F\u2217   RelDet (%)   RelTag (%)   mAP R@50 P@1   P@5   \u27137.98 7.71 61.65 51.10   \u2713   8.02 7.36 61.65 51.68   \u2713   \u27138.29 7.92 64.42 51.70   Table 5. Ablations of BIG-C for the Rnorm and F\u2217of RaCA module on VidOR.   #Bins   fRS@K (%)   fRM@K (%)   50   100   150   50   100   150   1   12.96 15.59 16.76 5.53 6.86 7.46   5   13.07 15.83 17.26 5.75 7.20 8.05   10   13.04 15.89 17.61 5.75 7.30 8.25   Table 6. Ablations for multi-instance grounding   with different number of bins on VidOR.   vs. 14.01%). 3) When using the same RoI and I3D features   as [22], BIG-C achieves better results on mAP and R@50.   4.4.2   Performance on VidOR   Settings. For VidOR, we compared our BIG (and BIG-C)   with the state-of-the-art methods: MHA [33], IVRD [21],   VidVRD-II [31], Liu et al. [22], Chen et al. [13], and two   top-1 methods [34,43] from Video Relation Understanding   (VRU) Challenges. All results are reported in Table 3. It is   worth noting that we only use the frame-level I3D features   (I3Df) in BIG model (i.e., the grounding stage), while some   works use more stronger region-level I3D features (I3Dr).   Quantitative Results. Due to the multifarious object detector backbones and features, it is dif\ufb01cult to fairly compare BIG (BIG-C) with these methods. From Table 3, we   can observe: 1) For BIG (BIG-C) without language feature,   we achieve signi\ufb01cant performance gains on RelTag (e.g.,   51.25% vs. 44.76% on P@5), and also have competitive   performance on RelDet. 2) For BIG (BIG-C) with language   feature, we can also achieve comparable results on RelDet   and RelTag, especially the highest P@5 (i.e., 51.80%).   Qualitative Results. Figure 6 shows some qualitative results that justi\ufb01es the necessity of the grounding stage. Take   \u27e8dog, away, child\u27e9for example: Without grounding, we   can only use the temporal intersection of dog and child to   approximate the time slot of away, i.e., (0, 35.47). Instead,   with the help of multi-instance grounding, the time slots are   predicted as (0, 3.44) & (10.13, 13.25). Refer to the appendix for more details.   5. Conclusions and Limitations   In this paper, we pointed out three inherent drawbacks   of the prevalent proposal-based framework, and proposed a   new classi\ufb01cation-then-grounding framework for VidSGG.   Under this framework, we reformulated video scene graphs   as temporal bipartite graphs, and proposed a novel VidSGG   model BIG. We validated the effectiveness of BIG through   extensive comparative and ablative experiments.   Limitations. 1) Detecting long object tracklets in videos   is still an open problem, and the fragmented tracklets may   weaken the advantages of our framework, making it close to   the proposal-based one. 2) Multi-instance grounding may   not be suitable for some extreme situations where too many   targets fall into the same bin (videos with dense relations).   Acknowledgement. This work was supported by National Key Research   & Development Project of China (2021ZD0110700), National Natural   Science Foundation of China (U19B2043, 61976185), Zhejiang Natural Science Foundation (LR19F020002), Zhejiang Innovation Foundation   (2019R52002), and Fundamental Research Funds for Central Universities.   19504", "conf": "CVPR", "year": "2022", "index": 234}, {"title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9313\u20139332   August 11-16, 2024 \u00a92024 Association for Computational Linguistics   SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents   Kanzhi Cheng\u2662\u2661\u2217   Qiushi Sun\u2661   Yougang Chu\u2662   Fangzhi Xu\u2661   Yantao Li\u2662   Jianbing Zhang\u2662\u2020   Zhiyong Wu\u2661\u2020   \u2662National Key Laboratory for Novel Software Technology, Nanjing University   \u2661Shanghai AI Laboratory   {chengkz,chuyg,li_yantao}@smail.nju.edu.cn   qiushisun@u.nus.edu   fangzhixu98@gmail.com   zjb@nju.edu.cn   wuzhiyong@pjlab.org.cn", "abstract": "Graphical User Interface (GUI) agents are designed to automate complex tasks on digital   devices, such as smartphones and desktops.   Most existing GUI agents interact with the environment through extracted structured data,   which can be notably lengthy (e.g., HTML)   and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a   novel visual GUI agent \u2013 SeeClick, which only   relies on screenshots for task automation. In   our preliminary study, we have discovered a   key challenge in developing visual GUI agents:   GUI grounding \u2013 the capacity to accurately locate screen elements based on instructions. To   tackle this challenge, we propose to enhance   SeeClick with GUI grounding pre-training and   devise a method to automate the curation of   GUI grounding data. Along with the efforts   above, we have also created ScreenSpot, the   first realistic GUI grounding benchmark that   encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot   over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that   advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks.   The model, data   and code are available at https://github.   com/njucckevin/SeeClick.   1", "content": "A perennial topic in machine intelligence is the development of Graphical User Interface (GUI) agent   systems, like Siri and Copilot, to automate complex tasks on computing devices, thereby reducing human workload (Shi et al., 2017; Li et al.,   2020a).   Recent advances in Large Language   Models (LLMs) such as GPT-4 (OpenAI, 2023)   have significantly propelled the evolution of GUI   *Work done during internship at Shanghai AI Laboratory.   \u2020Correspondence to: Zhiyong Wu, Jianbing Zhang.   <form element_id=\"200\">       ...    <label element _id=\"205\">Last Name:</label>       <input type=\"text\" name=\"lastname\" element    _id=\"206\">       ...       <input type=\"submit\" value=\"Get Receipt\" element    _id=\"210\">   ...   Instruction: Download the e-receipt with the last name Smith and    confirmation number X123456989.   Text-based:   Simplified HTML Code   Vision-based:   {\u201caction\u201d: \u201cclick\u201d, \u201cloc\u201d: [0.46, 0.62]}   Element: <element_id=206>   Action: CLICK   SeeClick\u2019s next action   Text-based agent\u2019s next action   GUI Screenshot   # Selenium Code   element = driver.find_element(By.XPATH,    '//*[@element_id=\"206\"]\u2019)   element.click()   Figure 1: Text-based agents select target elements from   structured texts, occasionally augmented with screenshots. SeeClick employs a vision-based methodology to   predict action locations solely relying on screenshots.   agents (Gur et al., 2023; Zhou et al., 2023). These   agents interact with the environment by interpreting structured texts, e.g., HTML from webpages,   then elicit LLM for planning, reasoning, and execution (Kim et al., 2023; Zheng et al., 2023).   However, GUI agents depend on structured text   face three inherent limitations: (1) Structured text   is not always accessible, especially for iOS or desktop applications where acquiring such information   is challenging (Shaw et al., 2023); (2) The verbose   nature of structured text constitutes an inefficient   context for LLMs, while also omitting crucial information such as layout, images, and icons (Deng   et al., 2023); (3) The variety of structured text including HTML, DOM, and Android VH - necessitates the curation of task-specific observation   and action spaces (Kim et al., 2023; Zhou et al.,   2023). These entrenched deficiencies in text-based   approaches call for an alternative solution.   In this paper, we introduce SeeClick, a visual   GUI agent built on Large Vision-Language Models (LVLMs). Inspired by human interaction with   GUIs, as illustrated in Figure 1, SeeClick is designed to perform low-level actions like clicking   or typing directly by observing interface screen9313   shots. This innovative approach bypasses the interaction with cumbersome structured text, empowering SeeClick to universally adapt to various GUI   platforms. Building such visual agents presents a   foundational challenge: GUI grounding - the capacity to accurately locate screen elements based on   instructions, which is absent in current LVLMs.To   tackle this challenge, SeeClick enhances LVLM   with a GUI grounding pre-training strategy. We   devise a method to automate the curation of web   grounding data and adapt public mobile UI datasets   to obtain mobile grounding data. SeeClick employs   the above-curated dataset for continual pre-training   of the LVLM, enabling it to accurately locate elements such as text, widgets, and icons in various   GUI environments.   Given GUI grounding is a fundamental yet underexplored capacity for GUI agents, we establish   ScreenSpot, the first realistic GUI grounding evaluation benchmark across various GUI platforms.   ScreenSpot contains over 600 screenshots and 1200   instructions from iOS, Android, macOS, Windows,   and webpages, and specifically includes both textbased elements and a variety of widgets and icons.   Evaluation results confirm SeeClick\u2019s superiority   over current LVLMs, validating the effectiveness   of GUI grounding pre-training.   Finally, we adapt SeeClick to mobile and web   agent tasks, including MiniWob (Shi et al., 2017),   AITW (Rawles et al., 2023), and Mind2Web (Deng   et al., 2023).   As a purely vision-based agent,   SeeClick achieves impressive performance. It surpasses the strong visual baseline Pix2Act while   utilizing merely 0.3% training data. Moreover, experimental results on these three benchmarks consistently support our findings that improvement in   GUI grounding directly correlates with enhanced   agent task performance.   Our main contributions are as follows:   \u2022 We develop a unified visual GUI agent SeeClick,   which solely relies on interface screenshots to   perform clicking and typing actions across diverse GUI platforms.   \u2022 We prospectively explore GUI grounding for visual GUI agents, and enhanced SeeClick with   proposed GUI grounding pre-training strategy.   \u2022 We create a realistic GUI grounding benchmark   ScreenSpot, encompassing more than 1200 instructions from various GUI platforms.   \u2022 Experimental results on ScreenSpot and three   agent tasks demonstrate that enhancing agents\u2019   grounding capacity is key to improving performance in downstream agent tasks.   2   Related work   Autonomous GUI Navigation Early research explored task automation in simplified web (Shi et al.,   2017; Liu et al., 2018; Gur et al., 2018) and mobile   UI (Li et al., 2020a; Burns et al., 2022; Li and Li,   2022). With LLM advancements (OpenAI, 2023;   Touvron et al., 2023; Xu et al., 2023; Sun et al.,   2023; Wu et al., 2024, inter alia), LLM-centric   agents have become the dominant paradigm. A line   of works focused on prompting ChatGPT and GPT4 for web tasks, via in-context learning (Zheng   et al., 2023) and self-refine (Kim et al., 2023).   Other research explored training LLMs as specialized agents. Deng et al. (2023) devised a two-stage   method for identifying target elements within intricate HTML. Gur et al. (2023) proposed to interact   with websites via programming.   Given the constraints of LLM to only process   text, recent efforts have attempted vision-based   GUI navigation (Shaw et al., 2023; Zhan and   Zhang, 2023; Hong et al., 2023). These methods   primarily utilize GPT-4V (Yan et al., 2023; Gao   et al., 2023) and also require GUI metadata as input (Yang et al., 2023a; Zheng et al., 2024). In this   paper, we construct a universal visual GUI agent   SeeClick by customizing open-sourced LVLM, capable of operating across various GUI platforms   without needing any GUI metadata.   Large Vision-Language Models Recent research   has invested tremendous effort in constructing   LVLMs capable of jointly processing image and   text (Liu et al., 2023a; Zhu et al., 2023; Ye et al.,   2023; Li et al., 2023), integrating vision encoders   with LLMs through connecting layers, inheriting   LLMs\u2019 linguistic and reasoning skills to perform   vision-language tasks. A series of studies focused   on grounding with LVLMs (Wang et al., 2023; Bai   et al., 2023; Chen et al., 2023a), such as providing bounding boxes for objects when generating   responses (Chen et al., 2023b; Peng et al., 2023).   Nonetheless, these efforts primarily addressed natural images and did not explore GUI contexts. This   paper focuses on grounding in GUIs and explores   the potential of LVLMs as visual agents.   3   Approach   Our preliminary study highlights a major challenge   in developing visual GUI agents: GUI grounding,   9314   Vision   Encoder   (ViT)   Large Vision-Language Model (LVLM)   Vision-Language    Adapter   Instruction:   \u201cView the new album of Jony J\u201d   Next action: click (0.49, 0.40)   Mobile UI Related   Web UI Related   General VL Data   VQA   Visual Reasoning   Widget Captioning   UI Summarization   Mobile UI Grounding   Web OCR   Web UI Grounding   Instruction: open the   low power mode   Source: Mobile (iOS)   Type: Icon/Widget   Instruction: See more    options for Dark Mode   Source: Mobile (Android)   Type: Text   Instruction: Change font    size to 20   Source: Desktop (macOS)   Type: Text   Instruction: Likes on this   issue   Source: Web (Development)   Type: Icon/Widget   Instruction: Create a new   merge request   Source: Web (Development)   Type: Text   Instruction: Switch to    OneDrive path   Source: Desktop (Windows)   Type: Text   GUI Grounding Benchmark: ScreenSpot   (a) Overview of SeeClick\u2018s framework and GUI grounding pre-training.   (b) Examples of the proposed GUI grounding benchmark ScreenSpot.   (c) SeeClick as a visual GUI agent in downstream task.   Instruction: Find a list of shorthaired dogs available for adoption with 100 miles of zip code    94587 that are good with kids and cats, and have been on Petfinder for over 30 days.   Figure 2: Overview of our universal visual GUI agent SeeClick. (a) depicts the framework of SeeClick and GUI   grounding pre-training. (b) provides examples of ScreenSpot across various GUIs and types of instructions. (c)   displays the real-world application of SeeClick when adapted to downstream web agent tasks.   the capacity to locate screen elements based on   instructions. Although recent LVLMs have claimed   grounding capability on natural images (Bai et al.,   2023; Wang et al., 2023), GUI screenshots differ   significantly with dense text and numerous icons   and widgets. These differences impair existing   LVLMs\u2019 grounding performance in GUI contexts   and limit their potential as visual GUI agents.   This paper seeks to harness LVLMs with GUI   grounding skills, paving the way for a visual GUI   agent that executes instructions only relying on   screenshots. As presented in Figure 2, SeeClick   is a foundational model for GUIs, and tailored for   adaption to agent tasks. Next, we introduce the   birth of SeeClick, including the formalization of   GUI grounding task, the construction of continual   pre-training data, and training details.   3.1   GUI grounding for LVLMs   As GUI grounding is the core capability of   SeeClick, we first elucidate how to train LVLM for   language generation to perform grounding tasks.   Given an interface screenshot s and a collection   of elements {(xi, yi)|i} on it, where xi denotes   the textual description of the i-th element and yi   indicates the element\u2019s location (represented as a   bounding box or point). As depicted in Figure 2(a),   LVLM predicts the location of the element y based   9315   on the interface screenshot s and its textual description x, i.e. calculating p(y|s, x).   A potential challenge is how LVLMs predict   numerical coordinates in a language generation format. Previous studies (Chen et al., 2021; Wang   et al., 2023; Shaw et al., 2023) divide the image   into 1000 bins, and creating a new 1,000-token   vocabulary {< p0 >, < p1 >, ..., < p999 >} to   represent the x and y coordinates. In this work,   we adopt a more intuitive manner used in LVLMs   (Chen et al., 2023b; Bai et al., 2023), treating numerical values as natural languages without any additional tokenization or pre-/post-processing. For   instance, in Figure 2(a), for a smartphone screenshot and the instruction \u201cView the new album of   Jony J\u201d, we craft a query prompt: \u201cIn the UI, where   should I click if I want to <instruction>?\u201d. Subsequently, we normally compute the cross-entropy   loss between the model output and the ground truth   \u201cclick (0.49, 0.40)\u201d to optimize the LVLM.   3.2   Data Construction   We train SeeClick using three collections of data:   web UI data crawled from the internet, mobile UI   data reorganized from public datasets and general   vision-language instruction-following data.   Web Data. Web UIs, featuring a variety of layouts and design styles across websites, are ideal for   training LVLMs\u2019 general recognition and grounding capabilities across different GUI contexts. We   collect approximately 300k web pages from the   latest Common Crawl repository to serve as our   training data for web UI. For each webpage s, we   collect two types of elements from the HTML code   as exemplified in Figure 3: (1) elements that display visible text content; and (2) elements with   a special \u201ctitle\u201d attribute that display descriptive   text when hovering. This method ensures that we   gather a series of interactable elements y and their   corresponding instructions x, while encompassing   a wide range of text and icon elements. In addition   to the grounding task p(y|s, x), we also include   web OCR task p(x|s, y), predicting text description based on coordinates.   Mobile Data. For mobile UI, we include three   types of data: widget captioning, mobile UI grounding, and mobile UI summarization. The widget   captioning dataset provides language descriptions   for mobile UI elements; for example, the description \u201cplay music\u201d for the play button on a music   player interface. We utilize the training split of the   dataset provided by (Li et al., 2020b), containing   <div class=\"header\">       <ul class=\"menu\">   <li>...</li>   </ul>   </div>   <div class=\"container\">       <div class=\u201cproduct-thumbnails\u201d><a href=\u201c#\u201d title=\u201cPrevious image\"></a></div>       <div class=\"product-detail\">   <div>...</div>           <button>ENQUIRE NOW</button>           <div class=\u201cproduct-share\u201d>\u2026</div>       </div>   </div>   Figure 3: Example of two types of elements automatically collected from the webpage.   nearly 20k screenshots, 40k widgets, and 100k descriptions. We derive mobile UI grounding data by   reversing the process of widget captioning, treating language descriptions as instructions and corresponding widgets as target elements. To improve   diversity, we also incorporate the automatically collected elements and instructions from RICO (Li   et al., 2020a). The mobile data involves diverse   elements and instructions, facilitating the generalization of SeeClick\u2019s grounding proficiency to diverse GUI contexts. We finally include mobile UI   summarization data (Wang et al., 2021) to enhance   overall interface comprehension.   General Data. To maintain LVLM\u2019s general capacities on natural images, we incorporate the general   vision-language instruction-following data from   LLaVA (Liu et al., 2023a), covering conversation,   detailed description, and complex reasoning.   Finally, we mix the data above and craft 30 taskspecific prompts for each added GUI task, resulting   in a 1M dataset to train SeeClick.   3.3   Training Details   We build SeeClick through continual pre-training   on a recent advanced LVLM, Qwen-VL (Bai et al.,   2023), which possesses grounding capabilities and   a higher resolution of 448*448. We train Qwen-VL   on the dataset we constructed (as described in Section 3.2) for about 10k steps (around 1 epoch) to   obtain our GUI base model SeeClick. During training, we employ LoRA (Hu et al., 2021) to fine-tune   both the visual encoder and LLM. Further details   and task examples are provided in Appendix A.   9316   LVLMs   Model   Size   GUI   Specific   Mobile   Desktop   Web   Average   Text Icon/Widget Text Icon/Widget Text Icon/Widget   MiniGPT-v2   7B   \u2717   8.4%   6.6%   6.2%   2.9%   6.5%   3.4%   5.7%   Qwen-VL   9.6B   \u2717   9.5%   4.8%   5.7%   5.0%   3.5%   2.4%   5.2%   GPT-4V   \u2717   22.6%   24.5%   20.2%   11.8%   9.2%   8.8%   16.2%   Fuyu   8B   \u2713   41.0%   1.3%   33.0%   3.6%   33.9%   4.4%   19.5%   CogAgent   18B   \u2713   67.0%   24.0%   74.2%   20.0%   70.4%   28.6%   47.4%   SeeClick   9.6B   \u2713   78.0%   52.0%   72.2%   30.0%   55.7%   32.5%   53.4%   Table 1: Results of different LVLMs on ScreenSpot. The best results in each column are highlighted in bold.   Benefiting from efficient GUI grounding pre-training, SeeClick significantly enhanced LVLMs\u2019 ability to locate   GUI elements following instructions, and surpassed the strong baseline CogAgent with a smaller model size.   21%   21%   13%   15%   8%   6%   6%   10%   iOS   Android   Windows   macOS   Development   Shopping   Forum   Tools   ScreenSpot   278   198   210   232   140   151   Different types of elements    in ScreenSpot   Text   Icon/Widget   Mobile   Desktop   Web   20%   19%   13%   14%   7%   9%   7%   11%   iOS   Android   Windows   macOS   Development   Shopping   Forum   Tools   ScreenSpot   273   194   230   229   140   206   Different types of elements    in ScreenSpot   Text   Icon/Widget   Mobile   Desktop   Web   Figure 4: Statistic of our proposed GUI grounding   benchmark ScreenSpot. The left illustrates the diverse   GUI environments included. The right displays the   types of elements within each GUI category.   4   ScreenSpot: A Grounding Benchmark   We recognize GUI grounding proficiency as essential for constructing visual GUI agents. However, the constrained capabilities of earlier visionlanguage models resulted in limited attention, with   scant research (Li et al., 2021; Li and Li, 2022;   Zhang et al., 2023) largely confined to an Android   dataset (Deka et al., 2017) collected in 2017.   To address this research gap, we introduce   ScreenSpot, an up-to-date, realistic grounding evaluation benchmark encompassing various GUI platforms. It is designed to assess vision-language   models\u2019 ability to locate screen elements based on   instructions (Figure 2(b) provides some examples).   ScreenSpot has two distinctive features: (1) Various GUI platforms. It includes over 600 interface   screenshots from mobile (iOS, Android), desktop   (macOS, Windows), and web platforms, along with   1200+ instructions and corresponding actionable   elements; (2) Icons/Widgets. ScreenSpot includes   a substantial number of icons and widgets in each   GUI, which is more challenging to locate than texts   (statistics are in Figure 4). See Appendix B for   annotation details and examples.   To measure models\u2019 effectiveness in real-world   scenarios, ScreenSpot is carefully curated to ensure   the samples are novel and not included in existing   training resources. We recruited experienced annotators to collect GUI interfaces and label instructions along with the bounding boxes for actionable   elements. For mobile and desktop, annotators were   asked to select commonly used apps and operations; for web, we chose several types of websites   (development, shopping, forum, and tools) from the   web environment WebArena (Zhou et al., 2023).   5   Experiments   In this section, we first evaluate the GUI grounding   capabilities of representative LVLMs and our proposed SeeClick. Subsequently, we adapt SeeClick   to mobile and web agent tasks, analyzing the correlation between the advanced grounding capacity   and downstream task performance, while exploring   the potential of purely vision-based GUI agents.   5.1   GUI Grounding on ScreenSpot   As the foundation of visual GUI agents, GUI   grounding has not received adequate attention in   current LVLMs evaluations (Liu et al., 2023b; Yu   et al., 2023). Therefore, we evaluate LVLMs on   our GUI-specific benchmark ScreenSpot.   Compared LVLMs & Evaluation. We primarily evaluated two types of LVLMs: (1) Generalist   LVLMs capable of tasks such as dialogue, recognition and grounding, including MiniGPT-v2 (Chen   et al., 2023a), Qwen-VL (Bai et al., 2023) and   GPT-4V; (2) Recently released LVLMs specifically   designed for GUI tasks, including Fuyu (Bavishi   et al., 2023) and CogAgent (Hong et al., 2023).   Considering that GUI agents require clicking on   the correct position, we calculate the click accuracy   as the metric, defined as the proportion of test samples where the model predicted location falls in the   ground truth element bounding box (Li et al., 2022;   9317   Zhang et al., 2023). More details about evaluation   on ScreenSpot is in Appendix B.   Results. As shown in Table 1, while generalist   LVLMs have excelled in natural image grounding,   their GUI grounding performance on ScreenSpot   is poor due to the significant differences between   GUIs and natural images. Even GPT-4V struggles   with accurately locating screen elements.   In comparison, GUI-specific LVLMs have significant improvements. SeeClick achieved the best   average performances across GUI platforms and   two types of elements, even with fewer parameters   than CogAgent. This demonstrates the efficiency   of our GUI grounding pre-training; with the rich UI   elements and diverse instructions collected from   the web and mobile, SeeClick quickly learns to understand human instructions for element localization, even in completely unseen scenarios like iOS   and desktop. SeeClick exhibits slightly inferior performance in locating text within desktop and web   compared to CogAgent, possibly due to lower resolution and much smaller training data. Notably, all   models struggle with locating icons/widgets, highlighting the difficulty of identifying and grounding   non-text elements on GUIs, which is the unique   challenge posed by ScreenSpot.   5.2   Visual GUI Agent Tasks   This section explores SeeClick\u2019s application to mobile and computer agent tasks: MiniWob, AITW,   and Mind2Web. We trained SeeClick on the respective training splits and tested it on the test sets.   Across these tasks, with provided instructions and   memory of previous actions, SeeClick determines   the next action solely by observing interface screenshots. The detailed task settings, action formats and   interaction examples are in Appendix C.   5.2.1   MiniWob   MiniWob (Shi et al., 2017) comprises about 100   types of web automation tasks, where the agent is   asked to interact with a simplified web environment   to accomplish human instructions. Existing opensource training data often lacks corresponding interface screenshots (Furuta et al., 2023). Therefore,   we rollout 50 successful episodes using an LLM   strategy for each task in (Zheng et al., 2023), resulting in a 2.8K episodes dataset to train SeeClick.   Compared Methods & Evaluation. We compared SeeClick with a range of offline training   methods. Among these, the state-of-the-art method   WebGUM (Furuta et al., 2023) uses screenshots as   Methods   Modality   Dataset   Score   Compared with text-based models over 45 tasks   CC-Net (SL)   DOM+Image   2.4M   35.6%   WebN-T5   HTML   12K   55.2%   MM-WebN-T5 HTML+Image 347K   63.4%   WebGUM   HTML+Image   2.8K   65.5%   WebGUM   HTML+Image 347K   86.1%   SeeClick   Image   2.8K   73.6%   Compared with vision-based models over 35 tasks   CC-Net (SL)   Image   2.4M   23.4%   Pix2Act   Image   1.3M   64.6%   Qwen-VL   Image   2.8K   48.4%   SeeClick   Image   2.8K   67.0%   Table 2: Average scores of different methods on MiniWob. The best results in each setting are bold. Methods   achieving the highest performance with limited data   are underlined. SeeClick outperforms a range of offline   training methods as a purely vision-based model.   auxiliary input but still selects HTML elements as   actions. Pix2Act (Shaw et al., 2023) is the only   prior vision-based approach, trained with extensive   demonstration data to perform actions. To verify   the effectiveness of GUI grounding pre-training, we   also report the results of the LVLM baseline QwenVL when trained with the same 2.8K dataset.   Due to the variance in evaluation task sets among   different methods (Liu et al., 2018; Furuta et al.,   2023; Shaw et al., 2023), for fairness, we report   performance in two groups based on the overlapping MiniWob tasks. We compute the success rate   over 50 random seeds for each task and then compute the mean over all tasks as the final score. We   provided task-wise scores in Appendix C.2.   Results. As depicted in Table 2, purely visionbased SeeClick surpassed strong baselines with   substantially less training data. Notably, with an   equivalent amount of 2.8K training data, it outperformed the offline sota WebGUM, which uses both   HTML and screenshots as input. Moreover, thanks   to LVLM\u2019s powerful reasoning and planning abilities and our GUI grounding pre-training, SeeClick   exceeded the sota visual method Pix2Act, using   less than 0.3% training data.   Furthermore, SeeClick significantly surpassed   the LVLM baseline Qwen-VL by nearly 20 percentage points, underscoring the importance of   GUI grounding in boosting LVLM\u2019s performance.   To analyze in detail, we provide task-level comparisons in Figure 5. SeeClick notably excelled   in tasks with dynamic interface layouts and ele9318   Click-button-sequence   Click-link   Click-pie   Click-button   Click-tab-2   Click-collapsible-2   Click-tab-2-hard   Unicode-test   Text-transform   Tic-tac-toe   Click-checkboxes   Click-option   Click-widget   Click-shape   Click-test-2   Navigate-tree   Use-slider   Click-checkboxes-transfer   Click-dialog-2   Simple-algebra   Choose-date   Click-color   Click-dialog   Click-shades   Click-checkboxes-large   Click-collapsible   Click-tab   Click-test   Simple-arithmetic   Click-checkboxes-soft   Count-shape   Use-antocomplete   Grid-coordinate   Enter-date   Identify-shape   0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1   Click-link   Click-button-sequence   Click-pie   Click-tab-2   Click-button   Click-collapsible-2   Click-shape   Unicode-test   Click-checkboxes   Click-option   Tic-tac-toe   Click-tab-2-hard   Click-widget   Navigate-tree   Click-test-2   Click-checkboxes-soft   Click-dialog-2   Text-transform   Click-checkboxes-transfer   Count-shape   Use-antocomplete   Click-color   Click-dialog   Choose-date   Click-checkboxes-large   Click-shades   Click-collapsible   Click-tab   Click-test   Enter-date   Use-slider   Simple-algebra   Simple-arithmetic   Identify-shape   Grid-coordinate   SeeClick improvement over LVLM baseline Qwen-VL   Qwen-VL   SeeClick   SeeClick is better   Qwen-VL is better   Figure 5: Comparison of SeeClick and Qwen-VL on MiniWob. Tasks marked with yellow shadows feature   dynamic webpage layouts, simulating real-world GUI agent applications (details in appendix Figure 11). SeeClick   outperformed Qwen-VL in most tasks, highlighting the effectiveness of GUI grounding pre-training.   Methods   Modality General Install GoogleApps Single WebShopping Overall ClickAcc   ChatGPT-CoT   Text   5.9   4.4   10.5   9.4   8.4   7.7   PaLM2-CoT   Text   39.6   GPT-4V   Image   41.7   42.6   49.8   72.8   45.7   50.5   Qwen-VL   Image   49.5   59.9   46.9   64.7   50.7   54.3   57.4   SeeClick   Image   54.0   66.4   54.9   63.5   57.6   59.3   66.4   Table 3: Average scores of different methods on AITW. ClickAcc calculates the accuracy of click operation. The   best results in each column are bold. SeeClick exhibits the best performance among competing baselines.   ment positions, confirming our hypothesis that general LVLMs struggle with accurately clicking, and   SeeClick markedly improves this aspect.   5.2.2   AITW   We evaluate SeeClick in smartphone environments   with Android automation dataset Android In The   Wild (AITW) (Rawles et al., 2023), which encompasses 30k instructions and corresponding 715k   operation trajectories. Previous approaches split   train/val/test episode-wise, which poses a clear   risk of overfitting due to: (1) instructions in the   test set have appeared in training, and (2) an average of 20 similar trajectories per instruction. In   this work, we opt for an instruction-wise split,   with 545/688/306/700/700 instructions from General/Install/GoogleApps/Single/WebShopping respectively, and retain one trajectory per instruction.   We selected 80% for training and the remaining for   testing in each subset. This split avoids overfitting   and reflects the performance of agents on unseen   instructions. Further details are in Appendix C.3.   Compared Methods & Evaluation. We compare SeeClick with two types of baselines: (1)   API-based LLMs such as ChatGPT-CoT (Zhan and   Zhang, 2023), PaLM2-CoT (Rawles et al., 2023)   and the latest GPT-4V (Yan et al., 2023); (2) Our   trained LVLM baseline Qwen-VL.   We follow Rawles et al. (2023) to adopt the   screen-wise action matching score as the main metric and additionally compute the click accuracy   (ClickAcc), which calculates the accuracy when   both reference and prediction are click operations.   Results.   As illustrated in Table 3, SeeClick   achieved the best average performance among both   API-based LLMs and trained LVLMs. Specifically, SeeClick exhibited a 9% increase in click   accuracy over Qwen-VL, supporting the idea that   GUI grounding enhances agent task performance   through precise clicking.   5.2.3   Mind2Web   To assess SeeClick\u2019s capabilities in web navigation, we utilize the recently introduced Mini2Web   dataset (Deng et al., 2023), which comprises over   2000 open-ended tasks collected from 137 real websites, each with high-level instruction and corresponding human action trajectory. Mind2Web was   originally designed for text-based agents, which   select actionable elements from simplified HTML   in each step. This work explores visual web agents   that predict click positions directly from screenshots. For this purpose, we parsed screenshots and   target element bounding boxes from the raw dump   of Mind2Web. To the best of our knowledge, this   is the first attempt of web agents relying solely on   screenshots as inputs for navigating real websites.   Compared Methods & Evaluation. We compare   with html-based web agents Mind2Act (Deng et al.,   2023) and our visual baseline Qwen-VL. Mind2Act   9319   Methods   w/o HTML   Cross-Task   Cross-Website   Cross-Domain   Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR   MindAct (gen)   \u2717   20.2   52.0   17.5   13.9   44.7   11.0   14.2   44.7   11.9   MindAct   \u2717   55.1   75.7   52.0   42.0   65.2   38.9   42.1   66.5   39.6   GPT-3.5-Turbo   \u2717   20.3   56.6   17.4   19.3   48.8   16.2   21.6   52.8   18.6   GPT-4   \u2717   41.6   60.6   36.2   35.8   51.1   30.1   37.1   46.5   26.4   Qwen-VL   \u2713   15.9   86.7   13.3   13.2   83.5   9.2   14.1   84.3   12.0   SeeClick   \u2713   28.3   87.0   25.5   21.4   80.6   16.4   23.2   84.8   20.8   Table 4: Comparsion of methods on Mind2Web. The best results in each column are bold. Improvements of   SeeClick over LVLM baseline are underline, with GUI grounding pre-training nearly doubling the step success rate.   employs a two-stage method, where a small LM   first generates candidate elements from raw HTML,   then a large LM selects the target via multi-choice   QA; Mind2Act (gen) directly generates the target   element instead. GPT-3.5 and GPT-4 adopt the   same multiple-choice QA formulation and include   three demonstrations for in-context learning.   We calculate element accuracy (Ele.Acc), Operation F1 (Op.F1) and step success rate (Step SR).   For vision-based methods, a prediction is considered correct if the predicted coordinate falls in the   target element\u2019s bounding box. All other settings   are following (Deng et al., 2023).   Results. As displayed in Table 4, SeeClick nearly   doubled the Ele.Acc and Step SR compared to   Qwen-VL. This indicates that SeeClick\u2019s improvement in GUI grounding correlates with enhanced   performance in web agent tasks. HTML-based   methods yield lower Op.F1 as around 20% of   groundturth elements are filtered out during candidate generation. Although SeeClick can operate   without extra HTML information, its performance   trails sota HTML-based methods, since predicting   click coordinates is much more difficult than choosing from HTML candidates. This highlights the   difficulty of grounding in intricate interfaces, suggesting substantial room for improvement in visual   agents for real-world application.   5.2.4   Grounding and Agent Performance   To investigate the correlation between grounding   and agent performance, we analyze the average   score improvements of several SeeClick\u2019s checkpoints on ScreenSpot and three downstream tasks.   As depicted in Figure 6, enhanced GUI grounding capacity consistently boosts agent task performance, highlighting its crucial role in developing   advanced visual GUI agents.   Figure 6: The correlation between agent tasks performance improvement and enhanced grounding ability.   MiniWob AITW Mind2web   Qwen-VLseparate   48.4   54.3   11.5   SeeClickseparate   67.0   59.3   20.9   SeeClickunified   64.1   57.1   19.5   Table 5: Separate v.s. unified training performance.   5.2.5   SeeClick as Unified GUI Agent   To access the potential of vision-based solutions   in unifying GUI agent tasks, we evaluated jointly   training SeeClick on three downstream tasks. As   shown in Table 5, the unified model exhibited a   slight performance decline, possibly due to the significant distinct interface of different GUIs.   6   Conclusion   In this paper, we introduce a visual GUI agent SeeClick, which only relies on screenshots for GUI   task automation. We found a key challenge in developing such visual GUI agents: GUI grounding   - the capacity to accurately locate screen elements   based on human instructions. To address this challenge, we propose to enhance SeeClick via GUI   grounding pre-training, and devise methods to automate the curation of GUI grounding data from   web and mobile. For benchmarking the progress   in GUI grounding, we created ScreenSpot, the first   9320   realistic evaluation dataset encompassing mobile,   desktop, and web platforms. Results on ScreenSpot   demonstrate a significant improvement of SeeClick   over LVLM baselines. Moreover, comprehensive   evaluations across three GUI automation tasks consistently support our finding that advancements in   GUI grounding directly correlated with improved   performance in downstream agent tasks.   Limitations   SeeClick currently simplifies the GUI action space   to mainly focus on clicking and typing, excluding   complex actions like dragging and double-clicking.   Additionally, limited by the performance of opensource LVLMs, training on agent-specific data is   necessary for SeeClick to execute multi-step tasks   on interfaces like mobile and computer.   Ethical considerations   GUI agents are developed to automate tasks and   enhance efficiency on digital devices. These technologies are especially significant for individuals   with visual impairments. Here are some ethical   considerations:   Privacy Issues. The operation of GUI agents involves accessing and interacting with user interfaces that may contain personal or sensitive information. Ensuring data protection and user consent   are paramount to maintaining privacy integrity.   Safety in Read-World Interactions. When GUI   agents interact with the real world, there\u2019s a risk of   unintended harmful actions. Ensuring these agents   operate within safe parameters is crucial to prevent   negative outcomes.   Bias. The development of GUI agents must address   potential biases in their algorithms, which could   result in unequal performance across different user   groups or interface designs. Mitigating bias is essential for equitable access and effectiveness.   Addressing these concerns requires ongoing research and development efforts, ensuring that the   benefits of GUI agents are realized without compromising ethical standards.   Acknowledgment   This work is supported by Shanghai Artificial Intelligence Laboratory and Natural Science Foundation   of China (NSFC) under Grant No.62176115. We   thank Yihan Wang, Boyi Zhang, and Wenpo Song   for useful help.", "conf": "ACL", "year": "2024", "index": 544}, {"title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics   Volume 2: Short Papers, pages 599 - 605   May 22-27, 2022 c\u20dd2022 Association for Computational Linguistics   UniGDD: A Uni\ufb01ed Generative Framework for Goal-Oriented   Document-Grounded Dialogue \u2217   Chang Gao, Wenxuan Zhang, and Wai Lam   The Chinese University of Hong Kong   {gaochang,wxzhang,wlam}@se.cuhk.edu.hk", "abstract": "The goal-oriented document-grounded dialogue aims at responding to the user query   based on the dialogue context and supporting document.   Existing studies tackle this   problem by decomposing it into two sub-tasks:   knowledge identi\ufb01cation and response generation. However, such pipeline methods would   unavoidably suffer from the error propagation   issue.   This paper proposes to unify these   two sub-tasks via sequentially generating the   grounding knowledge and the response. We   further develop a prompt-connected multi-task   learning strategy to model the characteristics   and connections of different tasks and introduce linear temperature scheduling to reduce   the negative effect of irrelevant document information. Experimental results demonstrate   the effectiveness of our framework.   1", "content": "Recent years have seen signi\ufb01cant progress in goaloriented dialogues (Loshchilov and Hutter, 2017;   Wen et al., 2017; Wu et al., 2019; Hosseini-Asl   et al., 2020; Peng et al., 2021), which aim at assisting end users in accomplishing certain goals   via natural language interactions. However, due to   the lack of external knowledge, most goal-oriented   dialogue systems are restricted to providing information that can only be handled by given databases   or APIs (Kim et al., 2020) and completing certain tasks in a speci\ufb01c domain such as restaurant   booking. To address this challenge, goal-oriented   document-grounded dialogue has been proposed   to leverage external documents as the knowledge   source to assist the dialogue system in satisfying   users\u2019 diverse information needs (Feng et al., 2020;   Wu et al., 2021).   \u2217The work described in this paper is substantially supported by a grant from the Research Grant Council of the   Hong Kong Special Administrative Region, China (Project   Code: 14200620).   Your   application   for   renewal   of   a   Driving   School   License   must   be   submitted between 30 and 60 days   before the license expires (the expiration   date is printed on your license. )   Grounding Knowledge   KI   How often do I have to renew the    Driving School License?   Each time you renew your license,    it is renewed for two years.    I would like to renew my Driving    School License, when is the right    time to do so?   Renewal of a Driving School   License   must   be   performed   between 30 and 60 days before   the expiration date as seen on   your license.   RG   Dialogue Context   Supporting Document   Figure 1: An example of the goal-oriented documentgrounded dialogue problem.   As shown in Figure 1,   the goal-oriented   document-grounded dialogue problem is commonly formulated as a sequential process including   two sub-tasks: knowledge identi\ufb01cation (KI) and   response generation (RG) (Feng, 2021). Given the   dialogue context and supporting document, knowledge identi\ufb01cation aims to identify a text span in   the document as the grounding knowledge for the   next agent response, which is often formulated as a   conversational reading comprehension task (Feng,   2021; Wu et al., 2021). Response generation then   aims at generating a proper agent response according to the dialogue context and the selected knowledge. Therefore, one straightforward solution for   this problem is to use two models to conduct KI and   RG in a pipeline manner (Daheim et al., 2021; Kim   et al., 2021; Xu et al., 2021; Chen et al., 2021; Li   et al., 2021). However, such pipeline methods fail   to capture the interdependence between KI and RG.   As a result, error propagation is a serious problem.   The problem is more pronounced in low-resource   scenarios, where accurate knowledge identi\ufb01cation   is dif\ufb01cult due to limited data, making it harder to   generate appropriate responses.   To address the aforementioned issue, we propose   a Uni\ufb01ed generative framework for Goal-oriented   Document-grounded Dialogue (UniGDD). Given   the dialogue context and associated document, instead of treating KI and RG as two separate processes, we tackle them simultaneously via sequen599   tially generating the grounding knowledge and the   agent response. Therefore, the inherent dependencies between these two sub-tasks can be naturally   modeled. On one hand, the generation of the agent   response depends not only on the dialogue context   and external document but also on the identi\ufb01ed   knowledge, forcing the model to focus on the speci\ufb01c knowledge. On the other hand, the generation   of the grounding knowledge receives the supervision signal from the agent response when training,   leading to more accurate knowledge identi\ufb01cation.   Although KI and RG can be uni\ufb01ed with the proposed generative method, they have different characteristics. Generating the grounding knowledge   is similar to copying appropriate sentences from   the document, while generating the response needs   more effort to make the response coherent with the   dialogue and consistent with the grounding knowledge. Therefore, in addition to the main task that   uses the concatenation of the grounding knowledge   and response as the target sequence, we introduce   the generation of the grounding knowledge and the   generation of the response as two auxiliary tasks in   the same framework to force the model to capture   their characteristics so as to perform well on them   as well. Moreover, inspired by the recent success   in prompt learning for pre-trained models (Li and   Liang, 2021; Lester et al., 2021; Liu et al., 2021),   we design prompts for these three tasks to guide   the model on what to generate for each task. These   prompts can naturally connect these tasks via indicating the model that each auxiliary task aims to   generate a part of the target sequence of the main   task. Through this prompt-connected multi-task   learning strategy, the model can capture the characteristics of different tasks as well as exploit the   connections between them.   In addition, for a particular user query in the   goal-oriented dialogue, the selected knowledge and   generated response need to be speci\ufb01c, while the   generation conditions on a relatively long document. Thus, much information in the input document is irrelevant. To tackle this problem, we introduce linear temperature scheduling to make the   attention distribution to the input document gradually sharper during the training process in order to   enable the model to learn to pay more attention to   the relevant content.   Our contributions are summarized as follows:   (1) We propose a uni\ufb01ed generative framework for   the goal-oriented document-grounded dialogue. (2)   generate <grounding>    then <agent>: dialogue    context + document   generate <grounding>:    dialogue context +    document   generate <agent>:    dialogue context +    document   UniGDD   <grounding> grounding    knowledge <agent>    agent response   <grounding> grounding    knowledge   <agent> agent response   Figure 2: Overview of our framework.   We develop a prompt-connected multi-task learning   strategy to exploit the characteristics and connections of different tasks and introduce linear temperature scheduling to enable the model to pay more attention to relevant information. (3) Our framework   advances state-of-the-art methods on the concerned   task, especially in low-resource scenarios.   2   Our UniGDD framework   UniGDD is a multi-task generative framework   for the goal-oriented document-grounded dialogue   problem.   Main Task Given the dialogue context C =   (u1, a1, . . . , ut\u22121, at\u22121, ut) and grounding document D, where ui is the i-th user utterance and   ai is the i-th agent utterance, our main task aims to   generate the target sequence Y = (kt, at), where   kt is the grounding knowledge from D and at is   the response to ut. Speci\ufb01cally, for the example in   Figure 1, the input and output of the main task are   as follows:   Input: generate <grounding> then <agent>:   <user> I would like to renew ... ? <agent>   Each time you ... <user> How often do ...   ? <title> Renew Driving School License   </title> ... Your application for renewal ...   Output: <grounding> Your application for   ... <agent> Renewal of a Driving ...   We use different special tokens to identify different elements in the input and output. For example,   we add \"<user>\" in front of each user utterance,   \"<agent>\" in front of each agent utterance, and   \"<grounding>\" in front of the grounding knowledge. The prompt \"generate <grounding> then   <agent>:\" is added to the dialogue context and supporting document to form the input and guide the   model to generate the grounding knowledge and the   response in order. The input-to-target generation   can be modeled with a pre-trained encoder-decoder   model M : (C, D, TP) \u2192(kt, at) such as T5   (Raffel et al., 2020), where TP is the task prompt.   600   Prompt-Connected Multi-Task Learning We   introduce two auxiliary tasks to steer our framework to model the respective characteristics of   knowledge identi\ufb01cation and response generation.   Given the dialogue context C and grounding document D, these two tasks aim to generate the grounding knowledge kt and the response at with the same   model M. As depicted in Figure 2, we construct   prompts \"generate <grounding>:\" and \"generate   <agent>:\" for them. These prompts indicate the   model that the goals of the two auxiliary tasks are   to generate the \ufb01rst part and the second part of the   target sequence of the main task, respectively. As   a result, the connections between different tasks   are naturally modeled. Instead of using discrete   language phrases, we randomly initialize the embeddings of those special tokens in the prompts and   train them end-to-end to better encode the characteristics and connections of these tasks.   Linear Temperature Scheduling For a speci\ufb01c   user query in the dialogue, many document contents are actually irrelevant. To force the model to   pay less attention to the irrelevant parts, we propose   a linear temperature scheduling strategy to make   the attention distribution of cross-attention gradually sharper during the training process. Specifically, we design the softmax function in the   cross-attention module of each decoder layer as   follows:   ai =   exp (zi/\u03c4)   P   j exp (zj/\u03c4)   (1)   \u03c4 = (\u03c4e \u2212\u03c4s) Sc   Stotal   + \u03c4s   (2)   where ai is the attention weight for the i-th input   token, zi is the logit for the i-th input token, Sc is   the current training step, Stotal is the total training   steps, \u03c4s and \u03c4e are the starting and ending temperature respectively, \u03c4e < \u03c4s, and 0 < \u03c4e < 1.   Compared with the original cross-attention module, the ending temperature 0 < \u03c4e < 1 leads to a   sharper attention distribution, giving more attention   weight to the relevant content.   Training The model is trained with a maximum   likelihood objective. Given the training example   e = (C, D, TP, Y ), the objective L\u03b8 is de\ufb01ned as   L\u03b8 = \u2212   n   X   i=1   log P\u03b8 (Yi | Y<i, C, D, TP)   (3)   where \u03b8 is the model parameters, TP is the task   prompt, Y is the target sequence, and n is the   Models   EM   F1   BERTQA   42.2   58.1   BERT-PR-large   56.3   70.8   RoBERTa-PR-large   65.6   77.3   Multi-Sentence   59.5   68.8   DIALKI (Lnext only)   60.4   71.2   DIALKI   65.9   74.8   UniGDD-base   65.6   76.8   UniGDD-large   66.9   77.5   Table 1: Results on knowledge identi\ufb01cation.   Models   BLEU   DIALKI+BART-base   25.8   RoBERTa-PR-large+BART-base   39.6   RoBERTa-large+T5-base   40.7   UniGDD-base   42.8   UniGDD-large   42.9   Table 2: Results on response generation.   length of Y . We mix the data of the main task   and two auxiliary tasks for training.   Inference After training, for each pair of dialogue context and document (C, D), we generate   the target sequence of the main task for obtaining   the grounding knowledge kt and the response at.   3   Experiments   3.1   Experimental Setup   Dataset We conduct experiments on the goaloriented document-grounded dialogue dataset   Doc2Dial (Feng, 2021), which is adopted by the DialDoc21 shared task1. It contains 3,474 dialogues   with 44,149 turns for training and 661 dialogues   with 8539 turns for evaluation2.   Evaluation Metrics Following Feng (2021),   we use Exact Match (EM) and token-level F1   for knowledge identi\ufb01cation and BLEU (Papineni   et al., 2002; Post, 2018) for response generation.   Baselines For knowledge identi\ufb01cation, we compare UniGDD with several strong baselines, including BERTQA (Devlin et al., 2019), BERT-PR (Daheim et al., 2021), RoBERTa-PR (Daheim et al.,   2021), Multi-Sentence (Wu et al., 2021), and DIALKI (Wu et al., 2021). These models formulate   knowledge identi\ufb01cation as the machine reading   comprehension task and extract the grounding span   1https://github.com/doc2dial/sharedtask-dialdoc2021   2Since we cannot access the test set, we report results on   the development set for comparison.   601   from the document. For response generation, we   compare UniGDD with several pipeline methods,   including DIALKI+BART (Wu et al., 2021) that   uses DIALKI to conduct knowledge identi\ufb01cation,   followed by BART (Lewis et al., 2020) to conduct response generation and RoBERTa-PR+BART   (Daheim et al., 2021). We also build a strong baseline model RoBERTa+T5 which uses the same pretrained generative model as ours.   Implementation Details We report results of   UniGDD with two model sizes: UniGDD-base   and UniGDD-large, which are initialized with pretrained T5-base and T5-large models (Raffel et al.,   2020), respectively. We adopt the implementation from Hugging Face Transformers (Wolf et al.,   2020). We set the max input length to 2560. Any   sequence over 2560 tokens will be truncated. For   training, we use the AdamW (Loshchilov and Hutter, 2019) optimizer with an initial learning rate   of 10\u22124 and a linear learning rate decay scheduler.   We train 10 epochs for single-task learning and   5 epochs for multi-task learning. For decoding,   we use beam search, and the beam size is 2. For   linear temperature scheduling, we set the starting   temperature \u03c4s = 1 and choose the best ending   temperature from {0.5, 0.6, 0.7, 0.8, 0.9}. For our   constructed baseline RoBERTa+T5 for response   generation, we use RoBERTa-large and T5-base   and adopt the implementation from the DialDoc21   shared task.   3.2   Results   The results on knowledge identi\ufb01cation and response generation are shown in Table 1 and Table   2, respectively. Our UniGDD framework outperforms all the baselines on two sub-tasks. On the   knowledge identi\ufb01cation task, UniGDD-base can   obtain comparable results to previous state-of-theart methods. With a larger model size, UniGDDlarge achieves new state-of-the-art performance.   On the response generation task, UniGDD obtains   a marked improvement over all pipeline methods.   This veri\ufb01es our assumption that our uni\ufb01ed generative framework can alleviate the error propagation   problem of pipeline approaches.   Effect   of   Prompt-Connected   Multi-task   Learning (PCMTL) and Linear Temperature   Scheduling (LTS) To verify the effectiveness of   PCMTL and LTS, we \ufb01rst remove PCMTL (i.e.,   training with the main task only), and the performance of UniGDD-base on two tasks decreases   10   20   30   40   50   60   1/32   1/16   1/8   1/4   RoBERTa-large+T5-base   UniGDD-base   (a) EM   15   20   25   30   35   40   1/32   1/16   1/8   1/4   RoBERTa-large+T5-base   UniGDD-base   (b) BLEU   Figure 3: Experimental results on knowledge identi\ufb01cation and response generation in low-resource scenarios   to 65.2 EM, 76.3 F1, and 42.3 BLEU, showing   that PCMTL endows the model with the ability   of modeling the characteristics and connections   of different tasks and achieving better generation.   Further removing LTS, the performance drops to   64.7 EM, 76.0 F1, and 41.7 BLEU. This indicates   that LTS can guide the model to pay more attention   to relevant content during generation and bring   improvements on two sub-tasks.   Effect of Connected Prompts (CP) To examine whether CP can capture the connections of different tasks, we use an alternative approach that   employs task-independent prompts \"<Task1>:\",   \"<Task2>:\", and \"<Task3>:\" to specify each task   for comparison. As in the case of CP, we randomly   initialize the embeddings of these three special tokens. With these prompts, UniGDD-base obtains   64.9 EM, 76.2 F1, and 42.3 BLEU, which performs   worse than using CP. This indicates that CP enables   the model to take advantage of the connections between the three tasks.   Low-Resource Setting To evaluate the model   in low-resource scenarios, we randomly shuf\ufb02e   the training set and then take 1/32, 1/16, 1/8, and   1/4 of the data for training. Figure 3 shows the   results of UniGDD-base and the best-performing   pipeline baseline RoBERTa-large+T5-base on the   four low-resource training splits. Generally, our   framework performs substantially better than the   pipeline method on both tasks. Particularly, when   there is only 1/32 training data, UniGDD-base obtains more than 20 and 10 absolute points improvement over the pipeline approach on EM and BLEU,   respectively.   Case Study Figure 4 shows a real case including   the dialogue context, supporting document, and the   responses generated by the pipeline method and our   proposed UniGDD framework. It can be observed   that our framework identi\ufb01es accurate knowledge   from the supporting document and thus provides a   602   I filled out all of the information in the Retirement Estimator   and it took a long time. When I came back from answering the   door, all of the information was gone. What happened?   Oh that's too bad. Were you gone for a long time?   Yes I guess I was.   Dialogue Context   RoBERTa-large+T5-base   Do you have any more questions about the Retirement Estimator?   UniGDD-base   For security reasons, there are time limits for viewing each page.   You will receive a warning after 25 minutes without doing   anything and you will be able to extend your time on the page.   For reasons of security, there are time limits for viewing each   page.   Ground Truth   \u2026\u2026 How Long Can You Stay On Each Page? For security    reasons, there are time limits for viewing each page. You will    receive a warning after 25 minutes without doing anything,    and you will be able to extend your time on the page. After    the third warning on a page, you must move to another page.    If you do not, your time will run out and your work on that    page will be lost.    Supporting Document   Response   Figure 4: A case from the development set.   proper and informative response about the reasons   for the problem the user encounters. In contrast,   the pipeline method only gives a relatively general   response that is not suitable in this case.   3.3   Human Evaluation   We randomly sample 100 evaluation instances.   For each instance, given the dialogue context and   grounding document, three human annotators are   asked to conduct a pairwise comparison between   the response generated by UniGDD-base and the   one generated by the pipeline baseline RoBERTalarge+T5-base in terms of two aspects: (1) Relevance: which response is more relevant and appropriate to the user query? (2) Informativeness:   which response is more informative? Results are   shown in Table 3. Compared with the pipeline   method, our framework can reduce error propagation, resulting in more relevant and appropriate   responses. Moreover, our framework has a clear   advantage over the baseline in terms of Informativeness since it can utilize rich document context   during the generation.   Win   Tie   Lose   Relevance   26   64   10   Informativeness   23   69   8   Table 3: UniGDD-base vs RoBERTa-large+T5-base.   The numbers indicate how many instances there are in   each case.   4   Conclusion   Our UniGDD framework uni\ufb01es knowledge identi\ufb01cation and response generation and models their   characteristics via a multi-task generative modeling strategy. Both automatic evaluation and human evaluation demonstrate the effectiveness of   our framework.", "conf": "ACL", "year": "2022_short", "index": 341}, {"title": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6657\u20136678   August 11-16, 2024 \u00a92024 Association for Computational Linguistics   GroundingGPT: Language Enhanced Multi-modal Grounding Model   Zhaowei Li1,2, Qi Xu1, Dong Zhang2, Hang Song1, Yiqing Cai1,   Qi Qi1, Ran Zhou1, Junting Pan1, Zefeng Li1, Van Tu Vu1,   Zhida Huang1, Tao Wang1   1ByteDance Inc, 2Fudan University   lizhaowei126@gmail.com   https://lzw-lzw.github.io/GroundingGPT.github.io/", "abstract": "Multi-modal large language models (MLLMs)   have demonstrated remarkable performance   across various tasks. However, these models   often prioritize capturing global information   and overlook the importance of perceiving local information. This limitation hinders their   ability to effectively understand fine-grained   details and handle grounding tasks that necessitate nuanced comprehension. Although   some recent works have made strides in this,   they have primarily focused on single-modality   inputs. Therefore, we propose GroundingGPT, an end-to-end language enhanced multimodal grounding model. It is designed to perform fine-grained grounding tasks for three   modalities: image, video and audio. To enhance the model\u2019s performance, we adopt a   coarse-to-fine training strategy, utilizing a threestage training approach to progressively enhance the model\u2019s semantic awareness and finegrained understanding capabilities. Additionally, we employ a diversified stage-specific   dataset construction pipeline, developing a   multi-modal, multi-granularity dataset tailored   for training the model in different stages. Extensive experiments conducted on multiple multimodal benchmarks demonstrate that our model   achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks   while maintaining or improving its global comprehension capabilities. Our code, model, and   dataset are available at https://github.   com/lzw-lzw/GroundingGPT.   1", "content": "Building upon the capabilities of large language   models (LLMs), research on multi-modal large   language models (MLLMs) has also advanced,   enabling understanding across a broader range   of modalities.   Representative models such as   LLaVA (Liu et al., 2023a) and MiniGPT-4 (Zhu   et al., 2023) align visual features obtained from image encoders with LLM embedding space through   visual instruction tuning, facilitating tasks such as   image captioning and visual question answering.   However, existing MLLMs primarily focus   on capturing global information while neglecting the fine-grained local information in multimodal inputs. This limitation restricts their applicability in grounding tasks requiring a more detailed understanding. Shikra (Chen et al., 2023b),   BuboGPT (Zhao et al., 2023) and Ferret (You et al.,   2023) have explored techniques that enable finer   alignment and understanding of inputs. By considering local-level information, these models exhibit   enhanced performance in grounding or referring   tasks. These methods provide insights into finegrained understanding, but they are primarily limited to a single modality. There is still significant   potential for exploring fine-grained understanding   across other modalities.   To address the aforementioned issue, this paper   proposes GroundingGPT, a language enhanced   multi-modal grounding model, which is an endto-end unified large language model designed to   perform multi-modal grounding and understanding   tasks across various modalities, including image,   video, and audio. The comparison between our   model and other models can be found in Table 1.   Specifically, our model employs modality-specific   adapters to map feature representations from individual encoders to the embedding space of LLMs.   To incorporate spatial and temporal information,   we directly represent coordinates and timestamps   as textual numbers, eliminating the need for vocabulary expansion. For training GroundingGPT, we   design a three-stage coarse-to-fine training strategy.   In the first stage, we align each pre-trained multimodal encoder with the LLM embedding space   using modality-specific adapters. In the second   stage, we aim to enable the model to capture finegrained information, including coordinates and   timestamps. In the third stage, we perform multigranularity instruction tuning to refine the model\u2019s   6657   Models   Image Grounding Video Grounding Audio Grounding Multi-turn Dialog E2E   LLaVA   %   %   %   \"   \"   Video-LLaMA   %   %   %   \"   \"   Shikra   \"   %   %   %   \"   Ferret   \"   %   %   \"   %   BuboGPT   \"   %   \"   \"   %   LLaVA-Grounding   \"   %   %   \"   %   GroundingGPT   \"   \"   \"   \"   \"   Table 1: Comparison of multi-modal large language models. \"Multi-turn Dialog\" refers to the model\u2019s ability   to engage in multi-turn conversations with users. \"E2E\" refers to the models that are designed to be end-to-end   architecture without the need for external modules.   responses. For each stage, we employed a stagespecific dataset construction pipeline to generate a   diverse, multi-modal, and multi-granularity training dataset.   To summarize, our contributions are as follows:   \u2022 We propose GroundingGPT, an end-to-end   multi-modal grounding model that accurately   comprehends inputs and possesses robust   grounding capabilities across multi modalities, including image, video and audio. To   the best of our knowledge, GroundingGPT is   the first model to achieve multi-modal finegrained understanding and grounding.   \u2022 For training GroundingGPT, we employ a   three-stage coarse-to-fine training process that   enables the model to capture high-level semantic information and low-level fine-grained   details simultaneously. To address the issue of   limited data, we construct a diverse and highquality multi-modal training dataset, which   comprises a rich collection of multi-modal   data enriched with fine-grained information.   \u2022 Extensive experiments conducted on a wide   range of MLLM benchmarks demonstrate the   generality and effectiveness of GroundingGPT in multi-modal grounding and understanding tasks across various modalities.   2   Related Work   Multi-modal Large Language Models (MLLMs)   Recently, large language models (LLMs) represented by GPTs (Brown et al., 2020; OpenAI,   2023) and LLaMA (Touvron et al., 2023) have   received extensive attention from researchers for   their remarkable performance in various natural   language processing tasks. Substantial progress   has been made in the field of MLLMs, which extend the support for multi-modal input and output beyond language. These MLLMs typically   fine-tune pre-trained LLMs with multi-modal instructions, to enable understanding across multiple   modalities. Models such as LLaVA, MiniGPT-4,   and mPLUG-Owl (Ye et al., 2023) map image embeddings obtained from image encoders into the   LLM space. Similarly, video MLLMs like VideoChat (Li et al., 2023b), Video-LLaMA (Zhang   et al., 2023c), Video-Chatgpt (Maaz et al., 2023)   and Valley (Luo et al., 2023), as well as speech   MLLMs like SpeechGPT(Zhang et al., 2023b)   and LLaSM (Shu et al., 2023), acquire multimodal understanding capabilities through similar   approaches. In X-LLM (Chen et al., 2023a), each   modality is processed independently through dedicated branches for multi-modal input processing.   Pandagpt (Su et al., 2023) employs a unified embedding space trained by ImageBind (Girdhar et al.,   2023) to facilitate joint understanding of various   modal inputs. However, these models often fail to   adequately capture details within inputs.   MLLMs For Grounding Task   Recently, there   has been a focus on training visual MLLMs to   achieve fine-grained image understanding and visual grounding. Approaches such as KOSMOS2 (Peng et al., 2023) and Shikra achieve this   by incorporating coordinates into the training   data, enabling MLLMs to understand the location   within images. On the other hand, approaches   like NExT-Chat (Zhang et al., 2023a), LLaVAgrounding (Zhang et al., 2023d),GlaMM (Rasheed   et al., 2023) and Ferret enhance perception of fine6658   Large Language Model   Text Token   Answer-Video: The baby push    up the glasses and stretch her    body in {0.16, 0.36} (1.6s-3.6s).   Answer-Image: The girl is sitting on    the bed[0.000,0.289,1.000,1.000]    and reading a book[0.024,0.488,    0.982, 0,505] .   Prompt-Video: When did the    baby push up the glasses and    stretch her body in the video?   Z   0S   2S   4S   6S   Prompt-Image: What is this girl    [0.460, 0.000, 0.915, 0.995] doing    in this image?   8S   Prompt-Audio: What is the source of    the sound in this image?   Video Encoder   Image Encoder   Audio Encoder   Video Adapter   Image Adapter   Audio Adapter   Answer-Audio: The source of this    sound in this scene is a children    laughing[0.388,0.000, 0.885,1.000] .   Adapter   Text Tokenizer   Video Token   Image Token   Audio Token   Encoder   10S   Figure 1: The overall structure of GroundingGPT involves separate encoders and adapters for each modality. Blue   boxes represent video inputs, yellow boxes represent image inputs, and pink boxes represent audio inputs.   grained information by introducing additional region encoder modules. VTimeLLM (Huang et al.,   2023) demonstrates the capability to understand   fine-grained video moment and reason with respect   to time boundary. BuboGPT (Zhao et al., 2023)   enables cross-modal interaction between image,   audio, and language, facilitating fine-grained understanding of different modalities.   3   Methods   We introduce the overall architecture of the GroundingGPT model in this section. Additionally, we   will present our three-stage coarse-to-fine training   strategy and data construction pipeline.   3.1   Model Architecture   Figure 1 illustrates the overall architecture of the   GroundingGPT model. Multi-modal inputs are   processed through modality-specific encoders to   extract features. These features are then mapped   to the LLM embedding space using corresponding   adapters. We will also introduce the representation   of coordinates and timestamps.   3.1.1   Image Branch   We employ the pre-trained CLIP visual encoder   ViT-L/14 (Radford et al., 2021) to extract image   features. The encoded image is represented as a   fixed-length embedding vector I \u2208RKI\u00d7dI. To   align the image representation with the LLM embedding space, we use an MLP to map the obtained   features to the dimensions of LLMs. The mapped   embeddings are then concatenated with text embeddings and used as input to LLMs, similar mapping   methods are adopted for other modalities.   3.1.2   Video Branch   Considering the inherent information redundancy   in videos and memory limitations, we uniformly   sample M frames form the video. Each frame   is processed by the image encoder, resulting in   Vf = [v1, v2, . . . , vM] where vi \u2208RKf\u00d7df represents the embedding of the i-th frame. To preserve   temporal information, we introduce temporal position encoding to the representation. The enhanced   representation is then fed into the Video Q-former   with the same structure as the Q-Former in BLIP2 (Li et al., 2023a) to aggregate video information,   which generates kV video embedding vectors of   dimensions dV . These vectors form the representa6659   Multi-granularity Instruction Tuning Dataset   Fine-grained Alignment Tuning Dataset    GPT   Task   Random    Template   Task-specific   Question Pool   Public Dataset   Single-turn   Conversation   Filtering   Public Dataset   System Prompt   GPT   Single/Multi-turn   Conversation   Human   In-context   Examples   Annotation   Random    Select    Template   Q: What part of this image corresponds to a woman in a white tank top eating a salad?   A: [0.000, 0.136, 0.505, 0.858].   Q: What is the time when person begins to play on a phone is observed in the footage?   A: In the time period {0.71, 1.0}.   Q: What is the older man[0.424, 0.126, 0.848, 0.872] doing?   A: He is tending to cooking hotdogs on a backyard grill.   Q: Where is the hotdog[0.180, 0.620, 0.392, 0.814] being grilled?   A: The hotdogs are being grilled on a backyard grill[0.006, 0.344, 0.490, 0.864].   Filtering   Q: What is producing the sound in this given image?   A: In this image, the given audio might originates from playing cello[0.397, 0.237, 0.688,    0.772].   Q: What happens during the time period{0.01,0.24}?   A: A young woman is seen standing in a room and leads into her dancing.   Q: When does the woman start dancing around the room?   A: The woman starts dancing around the room in {0.21,0.74}}.   Figure 2: The data construction pipeline and examples for the last two training stages. To simplify, the multi-turn   conversation examples only showcase two rounds of question-answer interactions.   tion V \u2208RkV \u00d7dV for the entire video.   3.1.3   Audio Branch   The audio branch follows a structure similar to the   video branch. We employ the ImageBind audio encoder, which processes 2-second audio clips with a   16kHz sampling rate and converts them into spectrograms using 128 mel-spectrogram bins. We sample N 2-second segments from the original audio   and transform each segment into a vector, resulting   in As = [a1, a2, . . . aN], where ai \u2208RKs\u00d7ds represents the embedding of the i-th aduio segment.   We incorporate temporal position encoding into As.   Finally, we obtain a fixed-length audio representation sequence denoted as A \u2208RkA\u00d7dA using the   audio Q-former like video branch.   3.1.4   Spatial-temporal Representation   We represent the bounding box in an image using four relative coordinate values: [x1, y1, x2, y2].   These values correspond to the upper left corner   point and the lower right corner point of the bounding box. Each value is rounded to three decimal   places. We concatenate this textual representation   after the description related to the bounding box.   Similarly, for representing timestamps, we use two   two-digit decimals {t1, t2} to indicate the relative   values of the start and end times of a time segment   with respect to the total duration. This representation allows us to train the model without requiring   additional vocabulary expansion or training. Examples of the training dataset are shown in Figure 2.   3.2   Coarse-to-Fine Training and Dataset   We employ a three-stage coarse-to-fine training   strategy to train the model, while constructing specific datasets for each stage.   3.2.1   Multi-modal Pre-training   This stage focus on enabling the model to comprehend multi-modal inputs and develop a high-level   semantic perception of the input. During the training process, the LLM and the encoders for each   modality remain frozen, while only the adapters   for each modality are trained.   Training Dataset   We utilize public pretraining   datasets as the primary source of our data. The   training data for the image and video modalities   is LLaVA-Pretrain-595k and Valley-Pretrain-703k,   respectively. To construct the audio data, we adopt   a similar approach as in LLaVA, leveraging the   Wavcaps (Mei et al., 2023) dataset. Each sample is   accompanied by a sampled instruction that requires   the model to provide a concise description of the   audio to construct a single-turn conversation.   3.2.2   Fine-grained Alignment Tuning   The second stage aims to enable the model to comprehend more detailed information, including coordinates and timestamps. Through training in this   stage, the model achieves impressive results in various grounding tasks, establishing a more comprehensive and refined understanding ability. During   6660   the training process, the encoders for each modality   are frozen, while the LLM and adapters are trained.   Training Dataset   The training data used in this   stage includes the spatial-temporal representation   mentioned in Section 3.1.4. To address the scarcity   of fine-grained multi-modal data, we construct   a multi-modal dataset specifically designed for   this stage. The dataset is primarily obtained by   converting publicly available datasets.   As depicted in the left part of Figure 2, task descriptions are provided to GPT-3.5 to generate a taskspecific question pool. For each data sample, a   question is randomly selected from the pool, and   templates are used to convert the sample\u2019s format, resulting in a single-turn conversation. For   the image modality, we utilize visual grounding   datasets such as RefCOCO (Kazemzadeh et al.,   2014), RefCOCO+ (Kazemzadeh et al., 2014), RefCOCOg (Mao et al., 2016) and Visual Genome (Krishna et al., 2017b) to construct the datasets. For the   video modality, video temporal grounding datasets   such as DiDeMo (Anne Hendricks et al., 2017),   HiREST (Zala et al., 2023) are utilized for finegrained alignment.   Regarding the sound localization task, we employ the VGGSS (Chen et al.,   2021) dataset for training. All these datasets are   transformed into single-turn conversation format   following the aforementioned pipeline for training.   3.2.3   Multi-granularity Instruction Tuning   After the training in the first two stages, the model   has acquired a strong understanding and grounding   capability. This stage aims to enable the model   to generate responses that better align with human p", "conf": "ACL", "year": "2024", "index": 518}, {"title": "Low-Resource Adaptation for Personalized Co-Speech Gesture Generation   Chaitanya Ahuja1, Dong Won Lee2 & Louis-Philippe Morency1   1Language Technologies Institute, CMU & 2Machine Learning Department, CMU   {cahuja, dongwonl}@andrew.cmu.edu, morency@cs.cmu.edu", "abstract": "Personalizing an avatar for co-speech gesture generation   from spoken language requires learning the idiosyncrasies   of a person\u2019s gesture style from a small amount of data. Previous methods in gesture generation require large amounts   of data for each speaker, which is often infeasible. We propose an approach, named DiffGAN, that efficiently personalizes co-speech gesture generation models of a high-resource   source speaker to target speaker with just 2 minutes of target   training data. A unique characteristic of DiffGAN is its ability to account for the crossmodal grounding shift, while also   addressing the distribution shift in the output domain. We   substantiate the effectiveness of our approach a large scale   publicly available dataset through quantitative, qualitative   and user studies, which show that our proposed methodology   significantly outperforms prior approaches for low-resource   adaptation of gesture generation. Code and videos can be   found at https://chahuja.com/diffgan.   1.", "content": "Technologies to assist human communication, both verbal (e.g. spoken language) and nonverbal (e.g. co-speech   gestures), have gained more traction in the past decade. One   promising direction is virtual reality [10, 17, 18, 29] which   aims at creating a more realistic online communication platform for embodied virtual agents [6, 28] and remote avatars   [38, 39]. These advancements could be seen as a normal   progression to speech-based technologies such as intelligent   personal assistants (e.g. Alexa, Siri, Cortana). These agents,   in the future, could also communicate more naturally with a   nonverbal embodiment that complements the verbal communication [33]. To enable this vision of immersive verbal and   nonverbal communication through an avatar, one technical   challenge is generating visual gestures based on input speech   and language [2, 12, 15, 21]. An even more challenging task   is the generation of personalized visual gestures, which reflects the idiosyncratic behaviours of a specific person [33].   The main goal of our paper is to create a personalized gesture generation model (e.g. as part of a personalized avatar)   Figure 1. Overview of the co-speech gesture personalization task.   On the left is a generative source model Gs pre-trained on a source   speaker. We adapt Gs to multiple target models Gt using lowresource data for each of the target speaker.   with limited data from a new speaker. In technical terms,   this problem requires an adaptation of crossmodal generative   models in a low-resource setting as illustrated in Figure 1.   Leveraging an existing source model, pretrained on a large   dataset of one speaker (i.e. source domain), our goal is to   personalize to a new speaker (i.e. target domain) with only 2   minutes of the target data.   The problem setting brings a unique challenge, typically   not studied in typical domain adaptation settings: crossmodal grounding shift. Due to the crossmodal nature of our   task, crossmodal grounding shift refers to the distributional   shift of the relationships between the input spoken language   modalities and output gesture modality. For example, consider a speaker, Aarti, who waves their right hand while   greeting a friend. These gestures are conditionally dependent on what the speaker says. We define such relationships   between the gestures and spoken language as crossmodal   grounding. While these relationships are conditioned on   spoken language, they are also heavily influenced by the   speaker\u2019s idiosyncrasies. Now, consider a new speaker, Bob,   who chooses from a set of two gestures while greeting: a   20566   Figure 2. Overview of the key components of our proposed model DiffGAN. (a)-(d): Low-resource adaptation of the crossmodal grounding   relationships from source to target domain. (e): Modeling output domain shift from source to target domain   left handed wave or waving both their hands vigorously. As   is the case for this speaker, typically the conditional gesture   spaces have a larger support (i.e. different kinds of gestures)   for the same language context. Such differences between   conditional gestures of source and target speakers are very   common, especially because the conditional variable is language which is a very large space. These common, yet   complex differences represent crossmodal grounding shift.   In this paper, we propose an approach, named DiffGAN,   that can efficiently personalize co-speech gesture generation models from a high-resource source speaker to a lowresource target speaker. To the best of our knowledge, this is   the first approach that is able to learn a personalized model   with only 2 minutes of speaker data (i.e. as opposed to 10   hours [2, 12, 15, 21]). Our DiffGAN approach does not   require access to source training data. Instead, DiffGAN directly identifies shifts in crossmodal grounding relationships   along with the shifts in the output domain from the pretrained source model. Based on these identified distribution   shifts, DiffGAN updates a few necessary parameters in a single layer of the source model, allowing efficient adaptation   with low resources. Our experiments study the effectiveness   of our DiffGAN approach on a diverse publicly available   dataset. As part of our evaluation methodology, we report   that DiffGAN produces a consistent improvement of around   10% preference scores of human judgments over strong baselines among other quantitative improvements. Furthermore,   DiffGAN extrapolates to gestures in the target distribution   without ever having seen them in the source distribution.   2. Related Work   Language and speech for gesture generation   A rulebased approach was proposed in an earlier study by Cassell   et al. [7], where the behavior expression animation toolkit   (BEAT) was developed to schedule behaviors, such as hand   gestures, head nods and gaze. This approach was extended   to utilize linguistic information from input text for decision   making [22, 25, 26, 31, 49].   Rule based approaches were replaced by deep conditional   neural fields [8, 9] and Hidden Markov Models for prosodydriven head motion generation [42] and body motion generation [23, 24]. These use a dictionary of predefined animations, limiting the diversity of generated gestures. Soon, neural network based models were introduced, using unimodal   inputs, specifically speech, to generate a sequence of gestures   [14], head motions [41] and body motions [1, 3, 11, 12, 43].   On the other hand, Yoon et al. [50] uses only a text input for   gesture generation. More recently, multimodal models relying both speech and language were developed. Kucherenko   et al. [21] uses early fusion to combine the two representations, Ahuja et al. [2] utilizes a cross-modal attention mechanism to account for correlations between speech and language. These approaches typically require many hours of   multimodal data to train a single speaker-specific model. We   propose an approach that can adapt a single-speaker generative model to a new speaker domain after being exposed to   only a few minutes of data.   20567   Figure 3. Qualitative comparison of our DiffGAN with prior work over shape of generated gestures. With maher as the source domain,   target model outputs over the target domain are superimposed over ground truth video frames for easy comparison.   Low-resource adaptation of generative models   Prior   works similary focus on pre-training a source model on large   source data, then adapting it to a low-resource setting. Nichol   et al. [35], Wang et al. [47] introduce new parameters in the   model, whereas Karras et al. [20], Wang et al. [46] fine-tunes   the complete model on the target data or to specific layers   or modules are applied [34, 36]. Li et al. [27], Wang et al.   [47] utilize importance sampling to transform the original   latent space of the source to a space which is more relevant   to the target. While this approach can be effective when the   source distribution and the target distributions share support,   it may not be well-generalizable when their supports are   disjoint. To address this concern, Ojha et al. [37] introduces   a contrastive learning approach to preserve the similarities   and differences in the source, and then adapting to the target   domain. These methods focus on adapting only the output   domain of unimodal generative models (i.e. generate one   modality with noise or a small set of discrete classes as the   input). However, we believe that for crossmodal generative modeling tasks, we need to explicitly model complex   relationships between the input modalities and the generated output modality, both of which have a spatial and/or   temporal structure.   3. Problem Statement   We are given a pretrained gesture generation model of   a source speaker as a generator-discriminator pair (Gs, Ds)   [13] that is trained on a large gesture source dataset Ds =   {Xs   i, Ys   i }N   i=1. It generates gestures as a sequence of body   poses Ys   i that is driven by both language and speech as   the input modalities Xs   i. A goal is to adapt parameters of   the pretrained generator Gs to a target model Gt by using   a much smaller target dataset of the target speaker Dt =   {Xt   i, Yt   i}P   i=1 where P << N.   4. Method   We propose a new approach, DiffGAN, that learns a target   model Gt by adapting a pre-trained source model Gs in a   low-resource setting. This approach is a two-step process   illustrated in Figure 2. First, in section 4.1, the model learns   to identify the crossmodal grounding shifts through a novel   loss function Ldiff and low-resource target data. Second,   in section 4.2, we discuss use of a loss function Lshift,   which encourages the target model to shift the output domain   distribution to be closer to that of the target\u2019s. Optimization   of the combined loss function describes the complete model,   G\u2217   t = EX,Y\u2208Dt argmin   Gs,\u03b8l\u22121:l   max   Ds Ldiff(\u03b8l\u22121:l)+Lshift(Gs, Ds)   (1)   where \u03b8l\u22121:l are parameters of a layer l in Gs (discussed   in Section 4.1).   4.1. Crossmodal Grounding Shift   The crossmodal grounding relationships between input   and outputs spaces of source data are already encoded in   the source model, Gs. Instead of modifying all of these   relationships in the source model, we first discover the shift   in grounding from the source to the the target dataset. This   is followed by the adaptation of the model parameters which   account for only the shifted relationships in the target model.   This approach has a two key advantages. First, adapting to   only the differences suggested by the discrepancies between   target and source data, allows the model to retain the previously learnt essential grounding relationships intact. Second,   in a low-resource setting, updating only a few layers instead   of the complete model is sufficient [5, 34]. Doing so allows   the target model to learn new grounding relationships while   preventing overfitting.   Notations:   Source Gs and target Gt models both have   a total of L layers. Function Gl:m   s   (.) represents layers l   through m in Gs. For example, Gl:m   s   (.) takes activation   maps of layer l (or zl) in Gs as the input and returns activation maps of layer m (or zm) in Gs as the output. Parameters   of layers l through m can be explicitly specified in function   Gl:m   s   (.; \u03b8l:m), but may be skipped for brevity.   Discovering shift in crossmodal grounding relationships:   Crossmodal grounding represents relationships between the   input and output modalities. For the source data. these relationships are already encoded in the latent spaces [3, 19, 51]   of the source model, Gs. In the adapted target model Gt, this   20568   Gesture Quality   Crossmodal Grounding   Output Domain   Amount   of data   (minutes)   Pretrained   Gs   Models   Naturalness   Expressivity   Timing   Relevance   Style   2   \u2717   AISLe [2]   7.3 \u00b1 2.9   15.3 \u00b1 7.6   9.2 \u00b1 2.4   8.3 \u00b1 4.1   16.3 \u00b1 5.3   \u2713   TGAN [46]   9.4 \u00b1 3.8   12.7 \u00b1 4.6   12.1 \u00b1 2.3   10.8 \u00b1 4.0   13.7 \u00b1 2.9   \u2713   MineGAN [47]   13.0 \u00b1 2.9   16.6 \u00b1 4.8   16.0 \u00b1 2.3   14.1 \u00b1 4.2   29.6 \u00b1 7.3   \u2713   ConsistentGAN [37]   9.0 \u00b1 1.9   17.7 \u00b1 3.1   10.9 \u00b1 1.9   9.3 \u00b1 1.6   17.6 \u00b1 6.3   2   \u2713   DiffGAN (Ours)   21.9 \u00b1 2.5   27.6 \u00b1 6.5   26.2 \u00b1 2.1   23.9 \u00b1 4.8   46.3 \u00b1 9.2   \u2713   DiffGAN w/o Ldiff   19.8 \u00b1 1.3   24.4 \u00b1 5.0   22.1 \u00b1 3.3   21.0 \u00b1 3.0   47.8 \u00b1 4.8   \u2713   DiffGAN w/o Lshift   12.3 \u00b1 4.1   20.0 \u00b1 5.9   15.8 \u00b1 3.9   13.6 \u00b1 3.8   26.5 \u00b1 6.4   Table 1. Human perceptual study comparing our model with prior work and strong baselines over five criteria measuring quality, crossmodal   grounding and output domain shift of generated gestures. We report the preference scores of a model as compared to the ground truth   gestures. Confidence intervals reported as standard deviation across experiments on all source-target pairs. Higher is better with 50 % being   the best possible score. Scores in green are the best and orange are the second best but lie in the confidence interval of the best.   latent space will shift creating new grounding relationships.   More concretely, this latent space represents the activation   maps at layer l for source and target models, or zl and z\u2217   l respectively. To estimate the direction along which the grounding relationships have shifted, we compute the element-wise   difference \u03a8 = |zl \u2212z\u2217   l | between the activation maps at   layer l. We can now update the parameters of layer l in the   direction \u03a8 to produce the required grounding shift.   Computing direction of grounding shift \u03a8:   To compute   \u03a8 = |zl \u2212z\u2217   l |, we need both zl and z\u2217   l . As zl is an activation   map of the source model with a target sample Xt as input,   we can compute it as zl = G0:l   s (Xt) as shown in Figure 2a.   Estimating z\u2217   l is tricky as the target model is not available   yet. As we are only updating the parameters of layer l   in this step, the parameters of Gl:L   s   do not change. As a   result, we can use values of the target output modality Yt to   optimize argminz\u2225Gl:L   s (z) \u2212Yt\u22252 as shown in Figure 2b.   This minimization objective serves as an accurate estimate   of z\u2217   l , and consequently an accurate estimate of the direction   of grounding shift \u03a8. To prevent overfitting due to limited   amount of data, we concentrate the gradient update to the   directions (i.e. channel dimensions) with top-k grounding   shifts represented as \u03a8k. The criteria for choosing l and k is   discussed in Section 6.   Updating Crossmodal Grounding in layer l:   To encourage generation of the shifted latent space z\u2217   l for target domain   inputs Xt, we update the weights of only layer l (or \u03b8l\u22121:l)   through an L2 loss. Furthermore, as \u03a8k is the measure of   grounding shift for each parameter, we use it as a weighting   function to guide parameter updates of layer l,   Ldiff = \u2225\u03a8k \u2299z\u2217   l \u2212\u03a8k \u2299Gl\u22121:l   s   \u0000G0:l\u22121   s   (Xt); \u03b8l\u22121:l   \u0001   \u22252,   (2)   where \u2299is element-wise product. As the training progresses,   both \u03a8k and z\u2217   l are re-estimated based on the updated parameters of the source model. Hence, as the latent space   of the adapted source model shifts closer to that of the target domain, \u03a8k will re-adjust until convergence, arriving   at the target model. Please note that while this approach is   described for a single layer l, it can easily be adjusted to   update any sequence of layers l through m without loss of   generality.   4.2. Output Domain Shift   The second step is to shift the output domain of the source   model Gs toward that of the target gesture distribution. We   follow the fine-tuning approach suggested in [37, 46] of   optimizing for the adversarial loss function Ladv,   Ladv = EXt,Yt\u2208Dt log Ds   \u0000Yt\u0001   +log   \u00001 \u2212Ds   \u0000Gs   \u0000Xt\u0001\u0001\u0001   ,   (3)   where the discriminator Ds(.) measures domain correctness of the output modality. This adversarial loss encourages   the model to generate gesture sequences whose structure   represents the target distribution. We would also like to   encourage generation of output sequences that temporally   match the target ground truth sequences Yt \u2208Dt for which   we use a reconstruction loss [2, 3, 12],   Lrec = EXt,Yt\u2208Dt\u2225Yt \u2212\u02c6   Yt\u22251,   (4)   where \u02c6   Yt = Gt(Xt). The combination of the adversarial   and reconstruction loss, Ladv + Lrec, is defined as Lshift   and encourages the output domain to shift toward the target   distribution (see Figure 2e).   5. Experiments   Dataset:   We use the PATS dataset [2, 3, 12] as the benchmark to measure performance. It consists of around 10 hours   of aligned body pose, audio and transcripts for each of the   25 speakers. We choose five speakers (oliver, maher,   chemistry, ytch_prof and lec_evol) with visually   different gesture styles and diverse linguistic content for our   experiments, in which source \u2192target denotes the   source and target domain. For speakers in the target domain,   we simulate a low-resource setting by randomly sampling 2   or 10 minutes of data for all experiments.   20569   Figure 4. Visual Histograms of generated gestures visually describe the distribution of hand gestures in space. Red and blue colors denote   the left and right arms respectively. First row is the source speaker, below which we have all the target speakers. Each column denotes a   model which adapts output distribution of the source domain to the target domain. Qualitatively, DiffGAN is successful in modeling the   distribution of the source speaker with just 2 minutes of data.   Baseline Models:   We compare our proposed model with   a family of baselines that adapts the same source model to a   target domain in a low-resource setting. (a) TGAN [46] finetunes all layers of source model with the low-resource target   data, (b) MineGAN [47] projects the source latent space   of the noise input onto a latent space representative of the   target data, (c) ConsistentGAN [37] uses a cross-domain   consistency loss which regularizes the tuning process and   a patch discriminator which encourages different levels of   realism over different image patches, and (d) AISLe [2]   learns the target model without a pretrained source model.   We also run ablation studies on two versions of our model   (e) DiffGAN w/o Lshift and (f) DiffGAN w/o Ldiff.   Human Perceptual Study:   We conduct a human perceptual study on Amazon Mechanical Turk (AMT) to measure   human preference towards generated animations. Given a   pair of videos, one of which is from the ground truth and   the other is generated by a model, the annotators have to   choose either one of the videos based on the five criterion:   gesture quality (naturalness and expressivity), crossmodal   grounding (timing and relevance) and output domain shift   (style) of generated gestures. The correctness of output domain shift is measured by the reflection of the true gesture   style of a target speaker in the gestures generated by a target   model [3]. We report the average preference % of human   annotators as a score for comparison. We refer the readers   to the appendix for more detailed definitions and setup.   Quantitative Metrics:   (a) To measure relevance and timing of gestures with respect to spoken language we use two   metrics, Probability of Correct Keypoints (PCK) [4, 44]   where values are averaged over \u03b1 = 0.1, 0.2 as suggested in   [12] and L1 distance between generated and ground truth   gestures. To measure the distribution of the output domain   we use Fr\u00e9chet Inception Distance (FID) which is the dis20570   FID \u2193   PCK \u2191   Amount   of data   (minutes)   Pretrained   Gs   Models   source   \u2193   target   maher   \u2193   oliver   maher   \u2193   chemistry   oliver   \u2193   maher   oliver   \u2193   chemistry   maher   \u2193   oliver   maher   \u2193   chemistry   oliver   \u2193   maher   oliver   \u2193   chemistry   2   \u2717   AISLe [2]   49.2 \u00b1 0.8   84.5 \u00b1 3.1   83.7 \u00b1 2.6   84.5 \u00b1 3.1   0.18 \u00b1 0.01   0.2 \u00b1 0.0   0.18 \u00b1 0.0   0.2 \u00b1 0.0   \u2713   TGAN [46]   57.5 \u00b1 2.4   184.3 \u00b1 5.4   339.1 \u00b1 1.2   323.5 \u00b1 1.9   0.31 \u00b1 0.01   0.23 \u00b1 0.0   0.2 \u00b1 0.0   0.25 \u00b1 0.0   \u2713   MineGAN [47]   42.5 \u00b1 2.5   157.5 \u00b1 10.6   290.3 \u00b1 7.2   302.5 \u00b1 6.8   0.38 \u00b1 0.03   0.26 \u00b1 0.02   0.21 \u00b1 0.01   0.31 \u00b1 0.01   \u2713   ConsistentGAN [37]   61.0 \u00b1 3.2   194.2 \u00b1 15.6   320.1 \u00b1 11.5   325.6 \u00b1 44.3   0.39 \u00b1 0.01   0.27 \u00b1 0.01   0.21 \u00b1 0.01   0.25 \u00b1 0.01   \u2713   DiffGAN (Ours)   25.0 \u00b1 3.7   42.8 \u00b1 5.1   47.9 \u00b1 25.5   48.2 \u00b1 15.9   0.45 \u00b1 0.02   0.31 \u00b1 0.01   0.26 \u00b1 0.01   0.29 \u00b1 0.01   10   \u2717   AISLe [2]   47.1 \u00b1 0.2   85.0 \u00b1 1.9   80.3 \u00b1 0.8   85.0 \u00b1 1.9   0.18 \u00b1 0.0   0.21 \u00b1 0.0   0.19 \u00b1 0.0   0.21 \u00b1 0.0   \u2713   TGAN [46]   62.9 \u00b1 1.8   191.7 \u00b1 1.2   341.5 \u00b1 0.4   326.8 \u00b1 1.6   0.3 \u00b1 0.01   0.22 \u00b1 0.01   0.2 \u00b1 0.0   0.24 \u00b1 0.0   \u2713   MineGAN [47]   41.4 \u00b1 3.1   145.0 \u00b1 14.1   293.5 \u00b1 12.7   318.2 \u00b1 5.4   0.4 \u00b1 0.01   0.24 \u00b1 0.03   0.22 \u00b1 0.02   0.31 \u00b1 0.01   \u2713   ConsistentGAN [37]   63.1 \u00b1 1.9   188.2 \u00b1 17.0   322.2 \u00b1 2.4   327.0 \u00b1 18.7   0.41 \u00b1 0.03   0.28 \u00b1 0.04   0.22 \u00b1 0.01   0.27 \u00b1 0.01   \u2713   DiffGAN (Ours)   15.0 \u00b1 5.2   31.7 \u00b1 3.8   30.3 \u00b1 6.9   24.3 \u00b1 4.2   0.46 \u00b1 0.01   0.32 \u00b1 0.02   0.26 \u00b1 0.01   0.3 \u00b1 0.01   Full   \u2717   AISLe [2]   16.1   8.7   10.2   8.7   0.49   0.39   0.27   0.39   Table 2. Comparison of our DiffGAN with prior work for low-resource crossmodal generative modeling from source to target speakers to   evaluate output domain shift (i.e. FID) and crossmodal grounding (i.e. PCK)   tance between distributions of generated and ground truth   poses [16]   Qualitative Visualization:   To judge the quality of the   generated spatio-temporal outputs, we would encourage the   readers to see the supplementary video. Other than that, we   qualitatively visualize three key properties of gestures [32,   40] (1) distribution, (2) velocities and (3) shapes of gestures   Implementation Details:   For our pretrained source models, we use publicly available models by Ahuja et al. [2]   for all experiments. We trained all the baselines with the   reported hyperparameters. All our models were trained for   4000 iterations with a batch size of 32. Either 2 minutes or   10 minutes of video recordings were used as the target data.   Each model was trained over three such randomly chosen target sets and quantitative metrics were averaged across these   runs. We refer the readers to the supplementary materials for   more implementation details.   6. Results and Discussion   In this section, we discuss the qualitative, quantitative   and user study results from our experiments.   Comparison with prior work   When evaluating generative models human judgements are often seen as a de facto   evaluation [2, 48]. The results of out human perceptual   study are summarized in in Table 1. We see a 10% larger   preference, if not more, for our model DiffGAN as compared   to the baseline models across all five criteria.   Crossmodal grounding shift is evaluated by measuring   the relevance, timing and correctness of the generated gesture in context of spoken language. In the human perceptual   study in Table 1, higher preference scores for DiffGAN over   relevance and timing criteria are indicative of the positive   impact of modeling the grounding shift explicitly which is   not the case for the other baselines. Qualitatively, in Figure 3,   we observe gesture shapes that are closer to the ground truth   0   20   40   60   80   Average Absolute Velocity   0.0   0.1   0.2   0.3   Probability   oliver \u2192maher   Ground Truth   MineGAN   ConsistentGAN   Di\ufb00GAN (Ours)   0   20   40   60   Average Absolute Velocity   0.0   0.5   1.0   1.5   Probability   oliver \u2192ytch prof   Ground Truth   MineGAN   ConsistentGAN   Di\ufb00GAN (Ours)   Figure 5. Distribution of the generated gestures with average absolute velocity as the statistic for source to target domain adaptation.   The support (or coverage) of the distribution is denoted with the   colour coded lines at the top of each plot. Larger overlap of a   model\u2019s distribution with the ground truth distribution is desirable.   for DiffGAN than the other baselines, further indicating that   the generated gesture shapes are more relevant to the input   modality for DiffGAN. This is further corroborated by significantly higher PCK values for DiffGAN when compared   with TGAN [46], MineGAN [47] and, ConsistentGAN [37]   in Table 2.   Output domain shift (i.e. gesture style) of the generated   gestures was especially convincing, as DiffGAN was preferred by human annotators (in Table 1) over the ground   truth motion 46% of the time. A similar trend is also seen   qualitatively in Figure 4 where we are comparing the pose   histograms for both ground truth of the target speaker and the   generated animations. DiffGAN is able to adapt the source   model such that it generates a distribution of gestures similar   to the target domain. Even though source oliver typically   has gestures close to his body with hands moving up and   down, DiffGAN is able to extrapolate to a target model for   maher with his prototypical side to side arm movements.   For TGAN [46] and MineGAN [47], the hand positions are   concentrated in only one region indicating reduced diversity   20571   FID \u2193   L1 \u2193   Amount   of data   (minutes)   Pretrained   Gs   Models   source   \u2193   target   oliver   \u2193   maher   oliver   \u2193   chemistry   maher   \u2193   oliver   maher   \u2193   chemistry   oliver   \u2193   maher   oliver   \u2193   chemistry   maher   \u2193   oliver   maher   \u2193   chemistry   10   \u2713   DiffGAN (Ours)   30.3 \u00b1 6.9   24.3 \u00b1 4.2   15.0 \u00b1 5.2   31.7 \u00b1 3.8   1.48 \u00b1 0.01   1.36 \u00b1 0.03   0.53 \u00b1 0.02   0.88 \u00b1 0.03   \u2713   DiffGAN w/o Ldiff   25.9 \u00b1 4.3   27.5 \u00b1 6.7   19.0 \u00b1 7.7   29.0 \u00b1 1.6   1.57 \u00b1 0.03   1.43 \u00b1 0.01   0.61 \u00b1 0.02   0.96 \u00b1 0.02   \u2713   DiffGAN w/o Lshift   119.1 \u00b1 7.0   131.0 \u00b1 5.1   22.4 \u00b1 2.1   54.1 \u00b1 1.9   1.33 \u00b1 0.01   1.26 \u00b1 0.01   0.53 \u00b1 0.01   0.84 \u00b1 0.0   Full   \u2717   AISLe [2]   10.2   8.7   16.1   8.7   0.91   0.73   0.71   0.73   Table 3. Evaluating impact of loss functions: DiffGAN and its ablations are trained on 10 minuites of low-resource data. The metrics   measure the impact of Lshift and Ldiff on both output domain shift (i.e. FID) and crossmodal grounding (i.e. L1).   (a) Choice of Layer l   (b) Choice of k in Grounding Shift   Figure 6. (a) Impact of choice of Layer l on crossmodal grounding   shift (i.e. PCK \u2191). Layer numbers are in increasing order from   input to output. Layers corresponding to indices on the X-axis are   defined in supplementary. (b) Impact of choice of k in grounding   shift direction \u03a8k on crossmodal grounding shift (i.e. PCK \u2191).   in the generated gestures and therefore a mode collapse. In   column three of Figure 4, ConsistentGAN [37] is able to   learn the correct rest pose for the target speakers, potentially   due to its approach of encouraging distance consistency in   the output domain. But the distribution of output gestures   does not correctly match the distribution of true distribution   of target speakers illustrated in column one of Figure 4.   These trends are further corroborated by significantly better   values of FID for DiffGAN when compared with TGAN [46],   MineGAN [47] and ConsistentGAN [37] in Table 2.   The distribution of gesture velocities is another statistic   with which we can examine the correctness of output domain   shift [40]. In Figure 5, we find that our model DiffGAN   (   ) is closely able to generate a velocity distribution that is   similar to the true distribution of the target domain. However,   distribution modes of MineGAN [47] and, ConsistentGAN   [37] are close to zero indicating that the generated gestures   are have very little or no movements.   Impact of Ldiff on crossmodal grounding shift   In Table 1, we observe that the removal of Ldiff from DiffGAN   reduces human preference for the gestures generated by the   model with respect to timing and relevance criteria. We   observe a similar trend in the accuracy metric L1, which   significantly worsens in Table 3. This supports our hypothesis that optimizing Ldiff can improve the discovery of new   grounding relationships in a low-resource setting.   Figure 7. Impact of amount of data on crossmodal grounding shift   (i.e. PCK \u2191) and output domain shift (i.e. FID \u2193) in crossmodal   generative models. Note that X-Axis is logarithmic and error bars   are standard deviation over three randomly sampled training sets.   Impact of Lshift on output domain shift   On the other   hand, removal of Lshift reduces correctness of the style of   generated gestures (i.e. output domain shift) as indicated   by human preference in Table 1. This is most likely due to   the adversarial component of Lshift which adapts the output   domain with the help of gesture sequences from the target   data. In parallel, FID values in Table 3 undergo the same   effect indicating that the target model does not generate a   large variety of gestures without Lshift, even though the   gestures are well-grounded in the input. We note here that   human annotators\u2019 judgements can be strongly influenced   by the naturalness of generated gestures [48]. This is a likely   reason for decrease in preference of crossmodal grounding   metrics for DiffGAN w/o Lshift, however it is still preferred   more often than other baseline models in Table 1.   Impact of number of training examples on model gesture style and grounding   The amount of data has a large   part to play in generative modeling and adaptation [45, 52].   We vary the amount of training data from 30 seconds to   100 minutes in Figure 7. We observe for our model DiffGAN, the output domain shift and crossmodal grounding   adapts faster than all the baselines. The variance of these   metrics decreases with increasing amount of data indicates a   more stable training. Our choice of low-resource datasets is   completely random.   20572   Which weights should be updated?   For a successful lowresource adaptation, a challenge is to select the best weights   to update. Our DiffGAN approach requires a choice of   trainable layer l and a choice of k number of channels that get   updated in each iteration. Through hyperparameter tuning,   we observe that layers closer to the output are typically able   to model a better crossmodal grounding shift as seen in   Figure 6a. The ideal choice of k is trickier. We want to   update enough parameters to model the grounding shifts, but   not so many that the model would overfit on the target data.   For our choice of pretrained source models, we conduct an   experiment with varying number of ks in Figure 6b. At   k = 0 we see a drop in performance, likely due to the   inactivity of Ldiff. At k = 64, we find the performance   saturates indicating an overfitted model. We find a balance   somewhere in the middle at k = 10.   Visualizing crossmodal grounding shift   For the same   input, we probe the output spaces of the source and target   model in Figure 8. We find that distribution gestures corresponding to the input can be sparse (i.e. visually different   gestures) or dense (i.e. visually similar gestures). In other   words, a verbal concept such as a greeting has a single way of   gesturing when the conditional output distribution is dense,   but has multiple possible gestures if the conditional output   distribution is sparse. As the output distribution is conditioned not only on the input but also on the speaker, we   can visually observe crossmodal grounding shift in form of   expansion or contraction as we traverse from the source to   target output space.   Limitations and future work:   While our method generates compelling results, it is not without limitations. Our   choice of source models were trained on a single source   domain (i.e. speaker), which may can sometimes have a   smaller overlap with the target domain. This poses a tradeoff between the complexity of the source model and the   amount of target data. Another challenge with our approach,   is that the choice of layer(s) l and k grounding shift channels   can potentially change depending on the choice of the pretrained model architecture. Hence, these hyperparameters   may need some tuning.   7. Conclusions   In this paper, we studied low-resource adaptation of crossmodal generative models for gesture generation. We introduced a new generative model DiffGAN, that can efficiently   address the shift in crossmodal grounding and the output   distribution from the source to target speaker with only a few   minutes of data. We benchmarked the effectiveness of our   approach on a publicly available dataset through quantitative,   qualitative and human studies. To our knowledge, this is the   first approach that is able to learn a personalized gesture   generation model with only 2 minutes of speaker data.   Figure 8. At the bottom, we display a t-SNE [30] plot of the input   space for both the target and source data. We choose a region   which contains both source and target input samples. At the top,   we display the t-SNE plots corresponding to where these samples   map to in the output space (indicated in black). On top left, similar   inputs produces a compact source output space (i.e. visually similar   gestures) but a sparse target output space (i.e. visually different   gestures). On top right, the opposite effect is observed. These   contractions and expansions of the output space conditioned on the   input space represent crossmodal grounding shift.   Broader Impact:   Our work enables for low-resource generation of personalized avatar animation, which typically   requires many hours of training. It allows for the generation   of gestures for new speakers with low resources. We developed this technology to improve the naturalness of an AI   agent, which would improve human-to-AI communication   as nonverbal behavior plays a key role in communication.   These gestures in the form of skeletal keypoints alone could   not be used to fully impersonate others, but it could be maliciously used to enhance the naturalness of deepfakes when   combined with other generative modeling technology. Realistic deepfakes can further enable misinformation spread,   abuse and stolen identities. As a potential measure to deter   such behaviour, we release our code under an ethical license   which prevent the usage of the code by any party that support   or contribute to hate speech or false impersonation (Do No   Harm, Nonviolent Public or Hippocratic License)   Acknowledgements: This material is based upon work partially supported   by the National Science Foundation (Awards #1750439 #1722822), National Institutes of Health, NTT Japan, and, the InMind project. Any   opinions, findings, and conclusions or recommendations expressed in this   material are those of the author(s) and do not necessarily reflect the views   of National Science Foundation or National Institutes of Health, and no   official endorsement should be inferred.   20573", "conf": "CVPR", "year": "2022", "index": 196}, {"title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8172\u20138181   July 5 - 10, 2020. c\u20dd2020 Association for Computational Linguistics   8172   A Negative Case Analysis of Visual Grounding Methods for VQA   Robik Shrestha1   Kushal Ka\ufb02e1,2   Christopher Kanan1,3,4   Rochester Institute of Technology1   Adobe Research2   Paige3   Cornell Tech4   {rss9369, kk6055, kanan}@rit.edu", "abstract": "Existing Visual Question Answering (VQA)   methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual   cues (e.g., human attention maps) to better   ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization   effect which prevents over-\ufb01tting to linguistic priors. For instance, we \ufb01nd that it is not   actually necessary to provide proper, humanbased cues; random, insensible cues also result in similar improvements. Based on this   observation, we propose a simpler regularization scheme that does not require any external   annotations and yet achieves near state-of-theart performance on VQA-CPv21.   1", "content": "Visual Question Answering (VQA) (Antol et al.,   2015), the task of answering questions about visual   content, was proposed to facilitate the development   of models with human-like visual and linguistic   understanding. However, existing VQA models   often exploit super\ufb01cial statistical biases to produce   responses, instead of producing the right answers   for the right reasons (Ka\ufb02e et al., 2019).   The VQA-CP dataset (Agrawal et al., 2018)   showcases this phenomenon by incorporating different question type/answer distributions in the   train and test sets. Since the linguistic priors in   the train and test sets differ, models that exploit   these priors fail on the test set.   To tackle this   issue, recent works have endeavored to enforce   proper visual grounding, where the goal is to make   models produce answers by looking at relevant   visual regions (Gan et al., 2017; Selvaraju et al.,   Answer distribution   VQA-CP Dataset   Prediction: Brown   Baseline Methods   Affected by language    priors   Green   Brown   Q: What color is the    couch? A: Green   Training   Test   Green   Brown   Fail to generalize   Prediction: Green   Recent Methods   Improve by grounding    on relevant regions   +9% over baselines   Prediction: Green   Our Findings   Irrelevant/random regions    result in similar gains   +9% over baselines   Figure 1: We \ufb01nd that existing visual sensitivity enhancement methods improve performance on VQACPv2 through regularization as opposed to proper visual grounding.   2019; Wu and Mooney, 2019), instead of exploiting linguistic priors. These approaches rely on   additional annotations/cues such as human-based   attention maps (Das et al., 2017), textual explanations (Huk Park et al., 2018) and object label   predictions (Ren et al., 2015) to identify relevant   regions, and train the model to base its predictions   on those regions, showing large improvements (810% accuracy) on the VQA-CPv2 dataset.   Here, we study these methods. We \ufb01nd that their   improved accuracy does not actually emerge from   proper visual grounding, but from regularization   effects, where the model forgets the linguistic priors in the train set, thereby performing better on   the test set. To support these claims, we \ufb01rst show   that it is possible to achieve such gains even when   the model is trained to look at: a) irrelevant visual   regions, and b) random visual regions. Second, we   show that differences in the predictions from the   1https://github.com/erobic/negative_   analysis_of_grounding   8173   variants trained with relevant, irrelevant and random visual regions are not statistically signi\ufb01cant.   Third, we show that these methods degrade performance when the priors remain intact and instead   work on VQA-CPv2 by hurting its train accuracy.   Based on these observations, we hypothesize   that controlled degradation on the train set allows   models to forget the training priors to improve test   accuracy. To test this hypothesis, we introduce   a simple regularization scheme that zeros out the   ground truth answers, thereby always penalizing   the model, whether the predictions are correct or   incorrect. We \ufb01nd that this approach also achieves   near state-of-the-art performance (48.9% on VQACPv2), providing further support for our claims.   While we agree that visual grounding is a useful   direction to pursue, our experiments show that the   community requires better ways to test if systems   are actually visually grounded. We make some   recommendations in the discussion section.   2   Related Work   2.1   Biases in VQA   As expected of any real world dataset, VQA   datasets also contain dataset biases (Goyal et al.,   2017). The VQA-CP dataset (Agrawal et al., 2018)   was introduced to study the robustness of VQA   methods against linguistic biases. Since it contains   different answer distributions in the train and test   sets, VQA-CP makes it nearly impossible for the   models that rely upon linguistic correlations to perform well on the test set (Agrawal et al., 2018;   Shrestha et al., 2019).   2.2   Bias Mitigation for VQA   VQA algorithms without explicit bias mitigation   mechanisms fail on VQA-CP, so recent works have   focused on the following solutions:   2.2.1   Reducing Reliance on Questions   Some recent approaches employ a question-only   branch as a control model to discover the questions most affected by linguistic correlations. The   question-only model is either used to perform adversarial regularization (Grand and Belinkov, 2019;   Ramakrishnan et al., 2018) or to re-scale the loss   based on the dif\ufb01culty of the question (Cadene   et al., 2019). However, when these ideas are applied to the UpDn model (Anderson et al., 2018),   which attempts to learn correct visual grounding,   these approaches achieve 4-7% lower accuracy   compared to the state-of-the-art methods.   2.2.2   Enhancing Visual Sensitivities   Both Human Importance Aware Network Tuning   (HINT) (Selvaraju et al., 2019) and Self Critical   Reasoning (SCR) (Wu and Mooney, 2019), train   the network to be more sensitive towards salient   image regions by improving the alignment between   visual cues and gradient-based sensitivity scores.   HINT proposes a ranking loss between humanbased importance scores (Das et al., 2016) and the   gradient-based sensitivities. In contrast, SCR does   not require exact saliency ranks. Instead, it penalizes the model if correct answers are more sensitive   towards non-important regions as compared to important regions, and if incorrect answers are more   sensitive to important regions than correct answers.   3   Existing VQA Methods   Given a question Q and an image I, e.g., represented by bottom-up region proposals: v (Anderson et al., 2018), a VQA model is tasked with predicting the answer a:   P(a|Q, I) = fV QA(v, Q).   (1)   3.1   Baseline VQA Methods   Without additional regularization, existing VQA   models such as the baseline model used in this   work: UpDn (Anderson et al., 2018), tend to rely on   the linguistic priors: P(a|Q) to answer questions.   Such models fail on VQA-CP, because the priors   in the test set differ from the train set.   3.2   Visual Sensitivity Enhancement Methods   To reduce the reliance on linguistic priors, visual   sensitivity enhancement methods attempt to train   the model to be more sensitive to relevant visual   regions when answering questions. Following (Wu   and Mooney, 2019), we de\ufb01ne the sensitivity of an   answer a with respect to a visual region vi as:   S(a, vi) := (\u2207viP(a|I, Q))T 1.   (2)   Existing methods propose the following training   objectives to improve grounding using S:   \u2022 HINT uses a ranking loss, which penalizes the   model if the pair-wise rankings of the sensitivities of visual regions towards ground truth answers agt are different from the ranks computed   from the human-based attention maps.   8174   \u2022 SCR divides the region proposals into in\ufb02uential and non-in\ufb02uential regions and penalizes the   model if: 1) S(agt) of a non-in\ufb02uential region   is higher than an in\ufb02uential region, and 2) the   region most in\ufb02uential for the correct answer has   even higher sensitivity for incorrect answers.   Both methods improve baseline accuracy by 8-10%.   Is this actually due to better visual grounding?   4   Why Did the Performance Improve?   We probe the reasons behind the performance improvements of HINT and SCR. We \ufb01rst analyze if   the results improve even when the visual cues are   irrelevant (Sec. 4.2) or random (Sec. 4.3) and examine if their differences are statistically signi\ufb01cant   (Sec. 4.4). Then, we analyze the regularization   effects by evaluating the performance on VQACPv2\u2019s train split (Sec. 4.5) and the behavior on   a dataset without changing priors (Sec. 4.6). We   present a new metric to assess visual grounding in   Sec. 4.7 and describe our regularization method in   Sec. 5.   4.1   Experimental Setup   We compare the baseline UpDn model with HINT   and SCR-variants trained on VQAv2 or VQA-CPv2   to study the causes behind the improvements. We   report mean accuracies across 5 runs, where a pretrained UpDn model is \ufb01ne-tuned on subsets with   human attention maps and textual explanations for   HINT and SCR respectively. Further training details are provided in the Appendix.   4.2   Training on Irrelevant Visual Cues   In our \ufb01rst experiment we studied how irrelevant   visual cues performed compared to relevant ones.   We \ufb01ne-tune the model with irrelevant cues de\ufb01ned   as: Sirrelevant := (1 \u2212Sh), where, Sh represents   the human-based importance scores. As shown   in the \u2018Grounding using irrelevant cues\u2019 section of   Table 1, both HINT and SCR are within 0.3% of the   results obtained from looking at relevant regions,   which indicates the gains for HINT and SCR are   not necessarily from looking at relevant regions.   4.3   Training on Random Visual Cues   In our next experiment we studied how random   visual cues performed with HINT and SCR. We   assign random importance scores to the visual regions: Srand \u223cuniform(0, 1). We test two variants   of randomness: Fixed random regions, where   Table 1: Results on VQA-CPv2 and VQAv2 datasets   for the baseline UpDn, visual sensitivity enhancement   methods (HINT and SCR) and our own regularization   method, including the published (pub.) numbers.   VQA-CPv2   VQAv2   Train   Test   Train   Val   Baseline - Without visual grounding   UpDn   84.0   40.1   83.4   64.4   Grounding using human-based cues   HINTpub.   N/A   46.7   N/A   63.41   SCRpub.   N/A   49.5   N/A   62.2   HINT   73.9   48.2   75.7   61.3   SCR   75.9   49.1   77.9   61.3   Grounding using irrelevant cues   HINT   71.2   48.0   73.5   60.3   SCR   75.7   49.2   74.1   59.1   Grounding using \ufb01xed random cues   HINT   72.0   48.1   73.0   59.5   SCR   70.0   49.1   78.0   61.4   Grounding using variable random cues   HINT   71.9   48.1   72.9   59.4   SCR   69.6   49.2   78.1   61.5   Regularization by zeroing out answers   Ours1% fixed   78.0   48.9   80.1   62.6   Ours1% var.   77.6   48.5   80.0   62.6   Ours100%   75.7   48.2   79.9   62.4   1 The published number is a result of \ufb01ne-tuning HINT   on the entire training set, but as described in Sec. 4.6,   other published numbers and our experiments \ufb01ne-tune   only on the instances with cues.   Srand are \ufb01xed once chosen, and Variable random regions, where Srand are regenerated every   epoch. As shown in Table 1, both of these variants obtain similar results as the model trained   with human-based importance scores. The performance improves even when the importance scores   are changed every epoch, indicating that it is not   even necessary to look at the same visual regions.   4.4   Signi\ufb01cance of Statistical Differences   To test if the changes in results were statistically   signi\ufb01cant, we performed Welch\u2019s t-tests (Welch,   1938) on the predictions of the variants trained   on relevant, irrelevant and random cues. We pick   Welch\u2019s t-test over the Student\u2019s t-test, because the   latter assumes equal variances for predictions from   different variants. To perform the tests, we \ufb01rst randomly sample 5000 subsets of non-overlapping test   instances. We then average the accuracy of each   subset across 5 runs, obtaining 5000 values. Next,   we run the t-tests for HINT and SCR separately on   the subset accuracies. As shown in Table 2, the   p-values across the variants of HINT and SCR are   8175   Table 2: p-values from the Welch\u2019s t-tests and the percentage of overlap between the predictions (Ovp.) of   different variants of HINT and SCR.   Methods   p   Ovp.(%)   HINT variants against Baseline   Default vs. Baseline   0.0   83.6   Irrelevant vs. Baseline   0.0   82.4   Fixed Random vs. Baseline   0.0   82.0   Variable Random vs. Baseline   0.0   81.5   Among HINT variants   Default vs Irrelevant   0.3   89.7   Default vs Fixed random   0.7   90.9   Default vs Variable random   0.6   91.9   Irrelevant vs Fixed random   0.5   95.6   Irrelevant vs Variable random   0.7   93.9   Fixed random vs Variable random   0.9   96.9   SCR variants against Baseline   Default vs. Baseline   0.0   85.6   Irrelevant vs. Baseline   0.0   84.2   Fixed Random vs. Baseline   0.0   80.7   Variable Random vs. Baseline   0.0   80.6   Among SCR variants   Default vs Irrelevant   0.6   92.0   Default vs Fixed random   0.8   89.3   Default vs Variable random   0.6   89.5   Irrelevant vs Fixed random   0.4   91.7   Irrelevant vs Variable random   1.0   91.6   Fixed random vs Variable random   0.4   96.7   greater than or equal to 0.3. Using a con\ufb01dence   level of 95% (\u03b1 = 0.05), we fail to reject the null   hypothesis that the mean difference between the   paired values is 0, showing that the variants are not   statistically signi\ufb01cantly different from each other.   We also compare the predictions of HINT/SCR   against baseline, and \ufb01nd that p-values are all zeros, showing that the differences have statistical   signi\ufb01cance.   Percentage of Overlaps: To further check if   the variants trained on irrelevant or random regions   gain performance in a manner similar to the models   trained on relevant regions, we compute the overlap   between their predictions on VQA-CPv2\u2019s test set.   The percentage of overlap is de\ufb01ned as:   % Overlap = nsame   ntotal   \u00d7 100%,   where, nsame denotes the number of instances   where either both variants were correct or both   were incorrect and ntotal denotes the total number of test instances. As shown in Table 2, we   compare %Overlap between different variants of   HINT/SCR with baseline and against each other.   Epochs   Accuracy on VQAv2   60   61   62   63   64   65   0   1   2   3   4   5   6   7   8   HINT (full)   SCR (full)   HINT (subset with cues)   SCR (subset with cues)   Figure 2: Accuracies for HINT and SCR on VQAv2\u2019s   val set, when \ufb01ne-tuned either on the full train set or on   the subset containing visual cues.   We \ufb01nd 89.7 \u221291.9% and 89.5 \u221292.0% overlaps   for different variants of HINT and SCR respectively. These high overlaps suggest that the variants are not working in fundamentally different   manners.   4.5   Drops in Training Accuracy   We compare the training accuracies to analyze the   regularization effects. As shown in Table 1, the   baseline method has the highest training results,   while the other methods cause 6.0 \u221214.0% and   3.3\u221210.5% drops in the training accuracy on VQACPv2 and VQAv2, respectively. We hypothesize   that degrading performance on the train set helps   forget linguistic biases, which in turn helps accuracy on VQA-CPv2\u2019s test set but hurts accuracy on   VQAv2\u2019s val set.   4.6   Drops in VQAv2 Accuracy   As observed by Selvaraju et al. (2019) and as   shown in Fig. 2, we observe small improvements   on VQAv2 when the models are \ufb01ne-tuned on the   entire train set. However, if we were to compare   against the improvements in VQA-CPv2 in a fair   manner, i.e., only use the instances with visual   cues while \ufb01ne-tuning, then, the performance on   VQAv2 drops continuously during the course of   the training. This indicates that HINT and SCR   help forget linguistic priors, which is bene\ufb01cial for   VQA-CPv2 but not for VQAv2.   4.7   Assessment of Proper Grounding   In order to quantitatively assess visual grounding,   we propose a new metric called: Correctly Predicted but Improperly Grounded (CPIG):   %CPIG = Ncorrect ans, improper grounding   Ncorrect ans   \u00d7 100%,   which is the number instances for which the most   sensitive visual region used to correctly predict the   8176   answer is not within top-3 most relevant ground   truth regions, normalized by the total number of   correct predictions. HINT and SCR trained on relevant regions obtained lower CPIG values that other   variants (70.24% and 80.22% respectively), indicating they are better than other variants at \ufb01nding   relevant regions. However, these numbers are still   high, and show that only 29.76% and 19.78% of   the correct predictions for HINT and SCR were   properly grounded. Further analysis is presented in   the Appendix.   5   Embarrassingly Simple Regularizer   The usage of visual cues and sensitivities in existing methods is super\ufb02uous because the results   indicate that performance improves through degradation of training accuracy. We hypothesize that   simple regularization that does not rely on cues   or sensitivities can also achieve large performance   gains for VQA-CP. To test this hypothesis, we devise a simple loss function which continuously degrades the training accuracy by training the network   to always predict a score of zero for all possible   answers i.e. produce a zero vector (0). The overall   loss function can be written as:   L := BCE(P(A), Agt) + \u03bbBCE(P(A), 0),   where, BCE refers to the binary cross entropy loss   and P(A) is a vector consisting of predicted scores   for all possible answers. The \ufb01rst term is the binary   cross entropy loss between model predictions and   ground truth answer vector (Agt), and the second   term is our regularizer with a coef\ufb01cient of \u03bb = 1.   Note that this regularizer continually penalizes the   model during the course of the training, whether its   predictions are correct or incorrect.   As shown in Table 1, we present results when   this loss is used on: a) Fixed subset covering 1% of   the dataset, b) Varying subset covering 1% of the   dataset, where a new random subset is sampled every epoch and c) 100% of the dataset. Con\ufb01rming   our hypothesis, all variants of our model achieve   near state-of-the-art results, solidifying our claim   that the performance gains for recent methods come   from regularization effects.   It is also interesting to note that the drop in   training accuracy is lower with this regularization   scheme as compared to the state-of-the-art methods. Of course, if any model was actually visually   grounded, then we would expect it to improve performances on both train and test sets. We do not   observe such behavior in any of the methods, indicating that they are not producing right answers for   the right reasons.   6   Discussion on Proper Grounding   While our results indicate that current visual   grounding based bias mitigation approaches do not   suf\ufb01ce, we believe this is still a good research direction. However, future methods must seek to   verify that performance gains are not stemming   from spurious sources by using an experimental   setup similar to that presented in this paper. We   recommend that both train and test accuracy be   reported, because a model truly capable of visual   grounding would not cause drastic drops in training   accuracy to do well on the test sets. Finally, we   advocate for creating a dataset with ground truth   grounding available for 100% of the instances using synthetically generated datasets (Ka\ufb02e et al.,   2017; Ka\ufb02e and Kanan, 2017; Ka\ufb02e et al., 2018;   Acharya et al., 2019b; Hudson and Manning, 2019;   Johnson et al., 2017), enabling the community to   evaluate if their methods are able to focus on relevant information. Another alternative is to use tasks   that explicitly test grounding, e.g., in visual query   detection an agent must output boxes around any   regions of a scene that match the natural language   query (Acharya et al., 2019a).   7   Conclusion   Here, we showed that existing visual grounding   based bias mitigation methods for VQA are not   working as intended. We found that the accuracy   improvements stem from a regularization effect   rather than proper visual grounding. We proposed   a simple regularization scheme which, despite not   requiring additional annotations, rivals state-of-theart accuracy. Future visual grounding methods   should be tested with a more comprehensive experimental setup and datasets for proper evaluation.   Acknowledgement. This work was supported   in part by AFOSR grant [FA9550-18-1-0121], NSF   award #1909696, and a gift from Adobe Research.   We thank NVIDIA for the GPU donation. The   views and conclusions contained herein are those   of the authors and should not be interpreted as   representing the of\ufb01cial policies or endorsements   of any sponsor. We are grateful to Tyler Hayes for   agreeing to review the paper at short notice and   suggesting valuable edits and corrections for the   paper.   8177", "conf": "ACL", "year": "2020", "index": 221}, {"title": "ViGoR: Improving Visual Grounding of Large   Vision Language Models with Fine-Grained   Reward Modeling   Siming Yan\u22c6\u20201   Min Bai\u22c62   Weifeng Chen2   Xiong Zhou2   Qixing Huang1   Li Erran Li2   1The University of Texas at Austin   2AWS AI", "abstract": ". By combining natural language understanding, generation   capabilities, and breadth of knowledge of large language models with   image perception, recent large vision language models (LVLMs) have   shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input,   resulting in errors such as hallucination of nonexistent scene elements,   missing significant parts of the scene, and inferring incorrect attributes of   and relationships between objects. To address these issues, we introduce   a novel framework, ViGoR (Visual Grounding Through Fine-Grained   Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines.   This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We   show the effectiveness of our approach through a variety of evaluation   methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute   to related research in the community.   1", "content": "Large language models (LLMs) have garnered intense interest throughout the research and academic communities. Typically, these models are pre-trained with   tremendous amounts of automatically aggregated text data followed by further   fine-tuning with specific user examples or evaluation feedback. This enables the   models to follow human-provided prompts and retrieve useful relevant information or solve logical problems. Recently, numerous techniques [10, 16, 17, 21,   33, 34, 37] have enhanced these breakthroughs with the ability to understand   visual information by further integrating image features into the prompt encoding process. Although these works have successfully aligned image features   \u22c6Equal contribution.   \u2020 Work done when interning at AWS AI. Contact email: siming@cs.utexas.edu.   2   S. Yan et al.   Fig. 1: An illustration of inaccurate visual grounding in Large Vision Language Models.   BLIP-2 [17] failed to provide detailed image descriptions. LLaVA [21]\u2019s more detailed   description contains both correct sentences as well as sentences with hallucinations   and inaccurate inference. In contrast, our model preserves the logical reasoning and   creativity of LVLMs while exhibiting significantly better accuracy and detail.   into the large language model domain, they still exhibit significant problems.   While a strong contextual grounding of language-only models can be learned   from the enormous corpus of text data, paired training data for multimodal language/vision models is more limited, while the complexity of the task is arguably   higher as the model must align two disparate modalities. Large, automatically   compiled datasets, such as the LAION-5B dataset [27], tend to feature only   simple images with very short text descriptions. Training with such data often   leads large vision language models to fail in capturing the essential details of   the image, returning only a short and coarse description (see BLIP-2 [17] output in Figure 1). Moreover, techniques such as InstructBLIP [10] and BLIP [17]   primarily rely on high-quality paired language/image datasets (e.g. VQAv2 [13],   VizWiz [14], TextCaps [28], etc.). However, these datasets are expensive to collect and challenging to adapt for broader coverage due to the need for manual   text annotation. On the other hand, initiatives such as LLaVA [21] train on   perception and simple caption datasets in conjunction with the reasoning capabilities of LLMs to semi-automatically generate synthetic conversational ground   truth. Unfortunately, such outputs can also contain non-factual statements suffering from hallucinations, omissions, and inaccuracies in attribute or relational   descriptions, as they are generated by text-only models based on sparse information about the actual image. Hence, the resulting trained model is still not   ideal (see LLaVA\u2019s output in Figure 1).   Instead, we propose a novel and general framework using fine-grained reward modeling. It efficiently and substantially enhances the visual grounding of   LVLMs beyond pre-trained baselines such as LLaVA, while preserving their conversational capabilities. Given a pre-trained LVLM (e.g., LLaVA), we input a set   of images with prompts and generate multiple text outputs. Human annotators   are asked to evaluate each image-text pair. As seen in the LLaVA output in Figure 1, a lengthy description generated by a competent LVLM can contain both   ViGoR   3   correct and incorrect sentences. Attempting to assign a single holistic score is   ambiguous for the annotator, as well as a subsequently trained reward model. In   our design, annotators assign fine-grained, per-sentence evaluation scores. This   results in a newly compiled dataset comprising image-text-evaluation trios. We   train a reward model to also predict dense reward scores and use it to fine-tune   the pre-trained LVLM (see Section 3). This fine-tuning process markedly improves the model\u2019s visual grounding capabilities with just 16K data samples,   demonstrating the method\u2019s efficiency.   To further improve the performance of our system at negligible cost, we also   develop a reward model scheme based on automatic methods without additional   human effort, which is shown to be highly effective in improving the visual   grounding capabilities of LVLMs.   Finally, we amalgamated the strengths of both reward models to develop a   complete solution, which we refer to as ViGoR (Visual Grounding Through   Fine-Grained Reward Modeling) throughout the remainder of the paper. To   assess the efficacy of ViGoR, we evaluate it against two commonly used benchmarks, POPE [19] and MME [12], where it demonstrated significant improvements over the baseline models. Finally, we compare the performance benefit   of our approach to baselines for a variety of tasks that require accurate visual   grounding. In summary, we make the following three major contributions.   \u2013 We introduce a novel framework that incorporates fine-grained reward modeling with easily implemented rejection sampling, substantially enhancing   the visual grounding of LVLMs.   \u2013 We develop reward models that require little human effort while leveraging   the impressive advances in powerful and robust visual perception models,   demonstrating marked improvements in visual grounding efficiency.   \u2013 We create and release the human evaluation dataset comprising 15.4K pairs   of images and generated results, as well as the fine-grained assessments of   the latter by our annotators.   2   Related Work   Large Vision Language Models. Recent advances in extending large language   models (LLM) such as GPT [5, 26], PaLM [9], BLOOM [32], LLaMA [30], and   Vicuna [8] to the visual domain have been remarkable. Flamingo [1] and its   open source counterpart OpenFlamingo [3], along with IDEFICS, have been   pivotal in integrating LLMs with vision-language pretraining, using techniques   like gated cross-attention dense blocks. PaLI\u2019s [7] research on the scaling of   vision and language components across various tasks has been instrumental.   As well, PaLM-E\u2019s [11] extension of LLM to the embodied domain, and BLIP2\u2019s [17] introduction of the Querying Transformer to align image and language   encoders mark significant progress. mPLUG-Owl [33] aligns visual characteristics   before fine-tuning the language model using LoRA [15]. LLaVA directly injects   visual tokens into a pre-trained LLM and finetunes the model with synthetic   4   S. Yan et al.   conversations generated by GPT-4 using metadata and short captions as input.   While these techniques primarily focus on architectural innovation for aligning   image features to the feature space of LLMs or leveraging extensive image-text   data for instruction tuning, our work introduces a distinct and novel general   framework designed to enhance the visual grounding capability of any LVLM.   Visual Perception Models. Recent foundational vision models such as CLIP [25]   have shown remarkable proficiency in handling a variety of tasks in open world   scenes. By reformulating object detection as a grounding problem, GLIP [18]   achieves semantic alignment at both the phrase and region levels, leading to   impressive open-set detection performance. GroundingDINO [22] combines the   state-of-the-art object detector DINO [6] with language pretraining for openset generalization. This model demonstrates a strong ability to determine the   presence and count of objects in a scene. However, current LVLMs do not possess such advanced visual grounding abilities. As such, we devise an automatic   method for building a reward model using these vision perception models, and   distill their strong visual grounding capabilities directly into LVLMs.   Reward Modeling. Recent progress in training Large Language Models has increasingly emphasized the importance of reward modeling. This approach often   incorporates human feedback and reinforcement learning optimization strategies, such as Proximal Policy Optimization. This approach is crucial in refining   the accuracy and contextual relevance of model outputs. For example, Askell et   al. [2,24] highlighted the potential of using human feedback in the training of   general language assistants, emphasizing the importance of aligning model responses with human standards and values. Recently, LLaMA-2 [31] introduced   a novel rejection sampling strategy within reward modeling, claiming that it   improves the generation of contextually appropriate high-quality responses.   However, in the realm of Large Vision Language Models, the application   of reward modeling remains underexplored, with most existing work focusing   predominantly on instruction tuning [21, 37]. One exception is LLaVA-RLHF   which adapts the Reinforcement Learning from Human Feedback from the text   domain to the task of vision-language alignment, where human annotators are   asked to compare two responses and pin-point the more hallucinated one [29].   Inspired by LLaMA-2, we combine reward modeling with rejection sampling in   the LVLM training framework. While LLaVA-RLHF uses a reward model that   produces sparse signals, we leverage fine-grained reward models to improve the   visual grounding capabilities of LVLMs, leading to more accurate and contextually relevant output in vision-language tasks.   3   ViGoR: Visual Grounding Improvement Framework   Our primary goal is to increase visual grounding and reduce hallucinations while   keeping the strong intuitive reasoning and creative thought process of pre-trained   LLMs and LVLMs. While it has been shown that high quality human annotations for supervised fine-tuning is a straightforward approach for significantly   ViGoR   5   Fig. 2: Overview of our model training framework. Starting with an input image and a prompt to generate a detailed caption, we sample a number of responses   from the LVLM. These responses are passed through two fine-grained reward signal   generation branches (a. by leveraging state-of-the-art generalizable perception models   and b. by using an LVLM-based reward model trained using annotator feedback). Finally, we c. combine the fine-grained assessment signals from both sources into a single   reward score, and select the best sampled description. Finally, we use heuristics and   byproducts from the automated scoring system to further refine this sample, and use   it for supervised fine-tuning of the LVLM.   improving LVLMs, it is cost-prohibitive for many application scenarios. As such,   we wish to efficiently leverage annotator time and the latest advances in direct   visual perception models such as powerful open-set object detectors.   We construct a system to fine-tune a base LVLM with rejection sampling,   similar to LLaMA-2 [31], allowing the model to improve through using intelligent ranking of its own sampled output. This requires a robust and perceptive   scoring system for the generated text. In our work, we use two complementary   solutions in parallel. In particular, we train a reward model to incorporate human annotator assessments of text outputs from the LVLM, and provide positive   and negative assessments of the LVLM during training time with unlabeled examples. As well, we leverage an open-set object detector and heuristics to verify   the existence or absence in the image of the named noun entities extracted from   the generated descriptions. Finally, we combine these signals into a single reward   score, and use it to select the best description among the initial samples. This   sample undergoes additional refinement and is used for the supervised fine-tuning   of the LVLM. We refer the reader to Figure 2 for a visualization.   6   S. Yan et al.   3.1   Reward Modeling via Fine-Grained Human Feedback   Human Preference Data Collection. We design a system to incorporate human   judgment and preference \u2014 considered the most reliable ground truth signal \u2014   into our model training. First, we select a pretrained LVLM\u2019s checkpoint state,   and create image / caption pairs from the model using a nonzero temperature   to balance factual rigor with creativity. Unlike other approaches that ask annotators to provide relatively sparse judgments for the LLM/LVLM\u2019s generated   results, we ask crowd-workers to provide fine-grained feedback at the sentence   level. For each sentence containing errors, the annotator selects the nature of   the inaccuracy from a predefined list including object hallucinations, type of   attribute error, incorrect relationships between multiple objects, error in the   mentioned location or unreasonable conjectures about the image. Furthermore,   the annotator provides a judgment about the creativity of the sentence. The creativity means if the sentence gives a thoughtful and reasonable interpretation or   extrapolation of the image. Finally, the annotator provides a holistic assessment   of the overall description\u2019s level of detail and identifies the missing elements   in the scene. These requirements encapsulate our overall goal: to enhance the   LVLM\u2019s visual grounding across the entire image while maintaining the insight   and creativity inherent in the pre-trained language decoder.   Reward Model Training. Using the collected annotations, we fine-tune a dedicated LVLM as the reward model on the annotations using instruction tuning to   judge the base LVLM\u2019s generated results during training time. The reward model   is trained to output a sequence of text tokens which encodes the various scores   given the underlying image and the LVLM\u2019s output as the input. While existing   work [29] generates a single holistic score for the entire text output, this process can be ambiguous when the description contains both correct and incorrect   components (see Figure 1). Instead, we train the model to produce sentence-level   evaluations. This fine-grained approach reduces ambiguity, and increases the detailed visual grounding of the reward model. To provide the necessary context for   scoring each sentence, we prepend the sentence with all preceding generated text   and explicitly ask the reward model to score the last sentence (which is also the   target sentence) in the given passage. A typical prompt is \u201cAssess the accuracy   of the last sentence in the following description of this image, and return a single number.\" Due to the fine-grained feedback provided by the annotators, these   prompts result in either a positive response (when no errors are found in the   sentence), or a detailed negative response (where one of several error types are   found in the sentence). More details are found in the Supplementary Materials.   As we show in the ablation studies, compared with holistic-based method,   this fine-grained method significantly improves the reward model\u2019s capabilities   to guide the fine-tuning of the LVLM. Furthermore, we see that the reward model   trained with fine-grained feedback can better understand the link between errors   in the descriptions and the image, resulting in superior fine-tuning performance.   ViGoR   7   3.2   Reward Modeling with Automatic Methods   While the preference annotations directly encapsulate human p", "conf": "ECCV", "year": "2024", "index": 283}, {"title": "Viewpoint-Aware Visual Grounding in 3D Scenes   Xiangxi Shi   Oregon State University   shixia@oregonstate.edu   Zhonghua Wu   SenseTime Research   wuzhonghua@sensetime.com   Stefan Lee   Oregon State University   leestef@oregonstate.edu", "abstract": "Referring expressions for visual objects often include descriptions of relative spatial arrangements to other objects   \u2013 e.g. \u201cto the right of\u201d \u2013 that depend on the point of view   of the speaker. In 2D referring expression tasks, this viewpoint is captured unambiguously in the image. However,   grounding expressions with such spatial language in 3D   without viewpoint annotations can be ambiguous. In this   paper, we investigate the significance of viewpoint information in 3D visual grounding \u2013 introducing a model that   explicitly predicts the speaker\u2019s viewpoint based on the referring expression and scene. We pretrain this model on   a synthetically generated dataset that provides viewpoint   annotations and then finetune on 3D referring expression   datasets. Further, we introduce an auxiliary uniform object representation loss to encourage viewpoint invariance   in learned object representations. We find that our proposed   ViewPoint Prediction Network (VPP-Net) achieves state-ofthe-art performance on ScanRefer, SR3D, and NR3D \u2013 improving Accuracy@0.25IoU by 1.06%, 0.60%, and 2.00%   respectively compared to prior work.   1.", "content": "Visual grounding of referring expressions requires algorithms to reason about natural language object descriptions   to identify object referents in visual scenes. Naturally, describing relative spatial relations between objects is a common strategy when a speaker produces a referring expression \u2013 e.g., \u201cThe chair to the right of the couch.\u201d As such,   many methods for visual grounding put algorithmic emphasis on handling these spatial relations [7, 18, 24, 34, 36].   However, these sorts of spatial relations depend upon the   point of view of the speaker. For instance, Fig. 1 depicts two   different viewpoints of the same 3D scene and notes that   these views produce contradictory referring expressions. In   2D visual grounding tasks, the point of view of the speaker   is typically also that of the image \u2013 introducing no ambiguity into spatial reasoning. However, viewpoint annotations   are not typically captured for 3D visual grounding tasks that   Figure 1. Expressions referring to the same object from different   viewpoints can lead to contradictory spatial relations.   reason about whole scenes. As a result, identifying referent objects may be ambiguous if multiple object instances   align with the referring expression in differing views. Moreover, the lack of reliable grounding for spatial language may   simply make the reasoning task more difficult to learn even   when the referent is still uniquely identifiable.   Despite this, most of the recent work in 3D visual   grounding has not addressed the role of viewpoint in the   learning process [12, 17, 26, 35, 43, 46]. Those methods   that have attempted to consider the influence of viewpoint   have been hindered by the lack of viewpoint supervision   in existing datasets \u2013 instead relying on rendering multiple views of a 3D scene and using attention mechanisms to   softly select between them [13, 16] in an end-to-end model.   In this work, we develop a straightforward approach to   viewpoint-aware visual grounding in 3D scenes \u2013 explicitly   predicting the speaker\u2019s viewpoint and then transforming   the input accordingly. To support this direction, we create a   synthetic visual grounding dataset that provides annotations   to directly supervise viewpoint prediction. We propose a   novel framework called ViewPoint Prediction Net (VPPNet) to estimate the viewpoints of expression-pointcloud   pairs and rotate the 3D scenes accordingly before performing expression grounding. This model is first pre-trained on   the synthetic dataset and then fine-tuned on a combination   of the synthetic and target datasets. To our knowledge, this   This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   14056   is the first work to introduce a supervised viewpoint estimation model for 3D visual grounding tasks. In experiments on   three common 3D visual grounding datasets, this approach   paired with viewpoint-related auxiliary losses leads to improved performance over prior art.   Contributions. Our contributions can be summarized as:   \u2022 We introduce viewpoint prediction as an auxiliary task in   the 3D visual grounding task and propose synthetic data   generation pipeline to provide viewpoint supervision.   \u2022 We propose VPP-Net \u2013 a novel model that learns viewpoint prediction to transform 3D scenes to reduce ambiguity in spatial-relation grounding.   \u2022 We design a Uniform Object Representation auxiliary   loss that promotes viewpoint invariant feature learning.   \u2022 Combing these techniques,   our proposed approach   achieves state-of-the-art performance on the ScanRefer [5], SR3D [2], and NR3D [2] datasets.   2. Related Work   Relation-aware 2D Visual Grounding.   The visual   grounding task has been extensively explored in 2D image   scenarios. The goal of a 2D visual grounding model is to   locate an object according to a given expression in a 2D image. In recent work, the relationships between objects have   been identified as a crucial cue in complex environments.   Several works [7, 23, 24, 34, 42] have parsed referring expressions into phrases for better text understanding. Wang   et al. [34] proposed to align the generated visual graph with   the self-attend object and relation features using a graph   attention module. Liu [24] proposed to capture the visual   context from a set of coarse-level object proposals by reconstructing the corresponding relations captured from the text.   Chen [7] et al. initialize a visual graph with proposed node   and edge transformers to learn the representations of objects   and relations. Besides explicitly encoding visual relations   in the model, relationships can also benefit models by constructing pseudo data [18, 36]. Wu et al. [36] proposed to   detect mismatched relations from synthetic data to evaluate the ability of relation understanding of the model. Jiang   et.al. [18] introduce an automatic way to generate pseudo   visual grounding data for supervised training.   3D Visual Grounding. The first dataset for visual grounding in 3D was introduced by Chen et al. [5] \u2013 presenting   the ScanRefer dataset consisting of 3D scenes and natural expression. Achlioptas et al. [2] propose two datasets   \u2013 the synthetic dataset SR3D and the human-annotated   NR3D. Further, they show that template-based synthetic   data can benefit the task when models are jointly trained   with naturally collected data. In standard approaches, a 3D   grounding model first encodes instructions and 3D scene   with pretrained text [10, 22, 31] and 3D visual [19, 27\u2013   30, 40, 41] models. Then a multimodal module is proposed   to fuse the encoded features to estimate the location and   size of the grounded object. 3D-SPS [26] proposed a onestage method that estimates the location of objects from   the point cloud directly. In other works, 3D object detections [11, 19, 25, 33] are leveraged to provide an objectfocused candidate pool consisting of object-level features   and/or bounding boxes.   BUTD-DETR [17] encodes object bounding boxes as a reference and jointly trains on   object detection datasets as an auxiliary task. 2D images   [20, 21, 37\u201339] are also utilized to boost the performance   of 3D visual grounding. The model proposed in [5] encodes   2D images together with 3D scenes for a better 3D understanding. Likewise, Yang et al. [43] leverage 2D semantics   to assist 3D representation learning.   The viewpoint of the annotator is also a significant cue   for a better alignment between 3D scenes and referring expressions. Currently, only a few works focused on this aspect have been proposed. MVT [16] and ViewRefer [13]   rotate the 3D scenes in different angles and encode them to   multi-view representations without explicit supervision of   which viewpoint matches to the annotator. Compared with   previous work, we first provide a template-based synthetic   visual grounding dataset including location and perspective   supervision and then propose a supervised training method   for viewpoint estimation. We demonstrate this is be beneficial for visual grounding performance.   3. Methods   We propose a straight-forward approach for learning to reason about viewpoint in 3D referring expression grounding   \u2013 directly predicting a rotation and translation of the 3D   environment that aligns it with the assumed viewpoint of   the observer providing the referring expression. To accomplish this, we introduce a ViewPoint Prediction Network   (VPP-Net) \u2013 a model that performs 3D referring expression grounding while explicitly estimating the location and   heading of the observer. As existing datasets lack observer   viewpoint annotations, we develop a synthetic data generation pipeline to provide appropriate training supervision   for viewpoint-aware pretraining and then transfer the pretrained model to downstream tasks. Further, we introduce a   Uniform Object Representation (UOR) auxiliary loss to ensure object instances are represented consistently regardless   of viewpoint. We describe each component below.   3.1. Generating Synthetic Viewpoint Supervision   Our synthetic data generation pipeline operates in four   stages \u2013 for a given environment, we (1) sample a location   and heading for a virtual observer, (2) identify spatial relationships between annotated objects in view, (3) generate   templated referring expressions accordingly, and then (4)   find other nearby viewpoints that could also have produced   this reference. The result is a tuple of the 3D environment,   14057   referring expression, and valid viewpoint map which we can   use for training viewpoint prediction.   Input Scenes. We utilize 3D scenes from the ScanNet [9]   dataset which are annotated with detected object bounding   boxes. For a given scene, we denote bounding box as a set   B, their centroids as a set of homogenous XYZ-coordinates   C = {c0, c1, . . . , cn | ci = (xi, yi, zi, 1)}, and the corresponding object category labels Yo.   Viewpoint Sampling. Regardless of scene size, we split   the horizontal (XY) plane into a 10\u00d710 grid and uniformly   sample a 2D (x, y) translation from the grid. Likewise, we   sample a rotation \u03b8 about the vertical axis by randomly selecting a 10\u25e6increment between 0\u25e6and 360\u25e6.   Identifying Spatial Relationships. We apply the sampled   translation and rotation to the scene by computing updated   bounding box centers. With the affine transformation   M(\u03b8, x, y) =   \uf8eb   \uf8ec   \uf8ec   \uf8ed   cos \u03b8   \u2212sin \u03b8   0   \u2212x   sin \u03b8   cos \u03b8   0   \u2212y   0   0   1   0   0   0   0   1   \uf8f6   \uf8f7   \uf8f7   \uf8f8,   (1)   we   update   the   bounding   box   centroids   as   c\u2032   i   =   ciM(\u03b8, x, y)T . The effect of this operation is to align the   observer\u2019s relative spatial terms (e.g. left/right) to the XYZ   axes. To detect potential spatial relations between a pair of   objects oi and oj, we compute the vector rij = c\u2032   i \u2212c\u2032   j to   represent their relative position. We take a set of four unit   direction vectors as {dfront, dback, dleft, dright} and compute the cosine similarities between dk and rij   si,j,k =   rijdk   ||rij|| \u00d7 ||dk||.   (2)   We consider a spatial relation k to be valid if si,j,k > 0.3.   For above and below relations, we instead consider the   heights and overlap of the bounding boxes. From all valid   tuples, we select a random subset as the relation tuple set R.   We note that this process does not consider the \u201cvisibility\u201d   of objects from the observer location.   Expression Generation. Given R and object category annotations, we can produce a set of object referring expressions. For each object mentioned as part of a relation in   R, we randomly sample up to four valid relations and convert each into simple templated text. For example, the tuple   (\u201cchair\u201d, \u201cleft\u201d, \u201cbed\u201d) is converted to a string like \u201cThe   chair is to the left of the bed.\u201d These are then concatenated   to form a multi-sentence referring expression for the object.   Viewpoint Supervision. For a viewpoint (x, y, \u03b8) and referring expression To, we provide viewpoint supervision decomposed into location and heading prediction. In most   cases, the randomly selected viewpoint is not the only one   for which the referring expression would be valid \u2013 small   shifts in either location or perspective may not change the   relationship between observed objects. For perspective, we   produce a binary vector YP er with 36 entries where YP er[k]   is 1 if the referring expression To is valid from viewpoint   (x, y, k \u221710\u25e6). For location, we produce a 10 \u00d7 10 binary   matrix YLoc where YP er[i, j] is 1 if the referring expression To is valid from viewpoint (i, j, \u03b8). For both, we check   validity by repeating the spatial relationship tests described   earlier from the new viewpoint. We note that this approach   considers only a subset of all possible viewpoints and assumes that perspectives valid at (x, y) are also valid for any   point (i, j) for which the original perspective was valid.   3.2. Viewpoint-Aware 3D Grounding Network   As outlined in Fig. 2, our ViewPoint Prediction Network   (VPP-Net) approach consists of a multimodal encoderdecoder architecture. After encoding the referring expression and scene, we predict the location and perspective of   the observer. The encoded features and transformed candidate bounding boxes and XYZ point positions are then   passed to a decoding model which ranks the candidates.   VPP-Net Backbone Architecture. Our method is based on   BUTD-DETR [17]. We review its construction here.   Input Encoding. An RGBXYZ point cloud representing the   3D scene is encoded with a pretrained PointNet++ [29], producing np point features evision \u2208Rnp\u00d7d and the corresponding XYZ points p \u2208Rnp\u00d73. Referring expressions   are encoded with a pre-trained RoBerta [22] model into a   sequence of text embeddings etext \u2208R|T |\u00d7d, where |T| is   the token length.   Multimodal Encoder. The multimodal encoder consists of   N layers of attention-based modules. Initializing visual features f (0)   v   and textual features f (0)   t   with evision and etext   respectively, each layer i computes update representations   by applying per-modality self attention followed by crossmodality attention as below:   f(i+1)   v   , f(i+1)   t   = fCross-Att   \u0000fSelf-Att(f(i)   v ), fSelf-Att(f(i)   t )   \u0001   . (3)   We denote the final layer outputs as f (N)   t   and f (N)   v   .   Object Decoder.   Between the encoder and decoder, the   visual features f(N)   v   are passed through as small MLP to   produce scores for each visual input that are then passed   through a softmax. The top-K scoring transformed features   are retained as object queries \u02c6f. The object decoder is an Nlayer transformer-based module. The decoder takes \u02c6f, f (N)   t   ,   f (N)   v   , hierarchical points cv and detected bounding boxes   Bo as input to predict r, fo, fl, fs:   r, fo, fl, fs = fDec(\u02c6f, ft, fv, cv, Bo),   (4)   where r \u2208Rnp\u00d71 is the object-expression matching score;   fo \u2208Rnp\u00d7D are object features; and fl, fs \u2208Rnp\u00d73 are   14058   Figure 2. An overview of our ViewPoint Predictor Network (VPP-Net) approach. (Left) Our model builds on the encoder-decoder structure   of [17], but modifies the encoder to explicitly predict location and perspective of the observer. The decoder then processes appropriately   transformed inputs to rank candidate bounding boxes. (Right) Our encoder model updates features of location and perspective at each layer   that are supervised to predict viewpoints where the referring expression is valid.   the location and size of objects. The object with the highest   ri is chosen and its corresponding fl,i and fs,i are used to   compute the bounding box for the object reference.   Adding Viewpoint Prediction.   As shown in Fig. 2, our   approach extends this architecture in two ways \u2013 by modifying the encoder to enable location / perspective prediction   and by using these predictions to transform bounding boxes   B and point coordinates p prior to invoking the decoder.   We add two transformer-based predictors into each multimodal encoder layer shown in Fig. 2 (right). As both perspective and location features are updated in the same fashion, we write the update steps below for location only:   f (i)   loc,v = fAttn(f (i\u22121)   loc,t , f (i)   v , f (i)   v ),   (5)   f (i)   loc,t = fAttn(f (i)   loc,v, f (i)   t , f (i)   t ),   (6)   where fAttn(\u00b7, \u00b7, \u00b7) denotes multi-head attention with key,   query, and value arguments. The location feature is updated each layer by querying visual features and then querying textual features. We initialize the perspective and location features f (0)   per,t with learnable embeddings eper and f (0)   loc,t   with eloc \u2013 a linear mapping of viewpoint (x, y). Denote the   final representations as f (N)   per,t and f (N)   loc,t.   We predict a distribution over locations and perspective   from each layer\u2019s location and perspective features respectively using a learned linear layer. We denote our predictions over locations and perspective respectively as   P(loc | f (i)   loc,m) = Softmax(Wlocf (i)   loc,m + bloc)   (7)   P(per | f (i)   per,m) = Softmax(Wlocf (i)   per,m + bper)   (8)   where m \u2208{v, t}.   We supervise each of these predictions with synthetic viewpoint supervision using an averaged cross-entropy loss \u2013 denoting it as   La,m = CE(Ya, Pi(a|m))   (9)   where a \u2208{per, loc} and Ya as defined in Sec. 3.1.   Object Decoder with Affine Transformation. After encoding the input, we take the highest probability location (\u02c6x, \u02c6y) and perspective \u02c6\u03b8 predictions generated from   f (N)   loc,t and f (N)   per,t as an estimate of the observers viewpoint.   Given these, we apply the corresponding affine transform   M(\u02c6\u03b8, \u02c6x, \u02c6y) to the point locations p and bounding boxes B \u2013   denoting the transformed versions as p\u2032 and B\u2032 respectively.   These transformed versions along with the other features   are then passed to the decoder as in Eq. 4. After the decoder predicts a bounding box location f \u2032   l and size f \u2032   s in this   transformed coordinate system, we transform the prediction   back to the original scene using the inverse of M(\u02c6\u03b8, \u02c6x, \u02c6y).   3.3. Encouraging Uniform Object Representation   An object can be described differently in different viewpoints \u2013 potentially causing our model\u2019s representation of   the same object to be quite different from different viewpoints. To encourage our model to be more robust to these   difference, we add an auxiliary loss that encourages the decoder output fo for object o to be consistent across viewpoints. To do this, we introduce a set of instance-wise learnable vectors U = {u0, u1, . . . }. We refer to these as uniform object representations. For a training sample whose   target is o, we concatenate the predicted object features fo   14059   and final perspective embeddings f (N)   per,t to estimate the uniform object representation uo with a linear layer. The loss   is computed between the predicted representation and uo:   Ll1 = || tanh(Wfuse[fo, f (N)   per,t] + bfuse) \u2212uo||1   (10)   This encourages representation to be predictable from object features and viewpoint information \u2013 intuitively, predicting an object instance\u2019s identity from its observation at   a specific viewpoint. Note that these learnable vectors are   not used at inference time.   Further, we also use fo and uo to classify the object\u2019s   class to encourage semantic consistency. For both fo and   uo, we make a prediction over the object class and compute   a Cross-Entropy loss:   Lcls = CE(Yo, Softmax(Wsem\u03d5 + bsem)),   (11)   where \u03d5 \u2208{fo, uo} and Yo is the object\u2019s class.   3.4. Model Training   The overall training loss is a weight sum of our location   and perspective supervision loss Li,a,m, the uniform object   representation loss Ll1, the object classification loss Lcls,   and the grounding loss Lgr proposed in BUTD-DETR [17]:   L = \u03b11   X   a,m   La,m + \u03b12Ll1 + \u03b13Lcls + \u03b14Lgr   (12)   where the coefficients \u03b1\u2019s are hyperparameters. Additionally, we find two additional techniques to be useful.   Synthetic Training Curriculum.   While training on the   synthetic data for viewpoint supervision, we apply a filtering mechanism on visual features f (0)   v   that reduces the   task complexity of early training examples. We retain all   points (and associated features) that occur within the bounding box of an object mentioned in the referring expression.   For points outside these boxes, we drop the point with probability pdrop. At the start of training, we set pdrop = 1 \u2013 effectively removing the background and other objects from   the scene. As training progresses, we anneal pdrop to 0 such   that whole scenes are observed.   Viewpoint Data Augmentation.   During both synthetic   training and finetuning, we apply a viewpoint-based augmentation scheme. After running an example through our   model, we retain the predicted affine transformation M0.   We then rotate the scene\u2019s pointcloud and supervision by   M0 to create a viewpoint augmented sample. We then run   the model again with this sample.   4. Experiments and Results   To study the effectiveness of our proposed VPP-Net, we   conduct experiments on three existing 3D visual grounding   datasets \u2013 ScanRefer [5], SR3D [2], and NR3D [2].   4.1. Datasets and Experimental Settings   Datasets. We evaluate on ScanRefer, SR3D, and NR3D   which we describe below:   ScanRefer.   The ScanRefer   dataset is built on 800 3D scenes collected from ScanNet.   The 3D scenes are labeled with about 51,000 natural expressions and 11,000 objects. The labeled natural expressions   have different types of phrases, including intra/inter-class   spatial relations and comparatives/superlatives. The dataset   also provides real images of the scene, which are used to   enhance the grounding performance in some methods. We   use the official splits provided by ScanRefer to train and   evaluate our model.   SR3D and NR3D. SR3D and NR3D are two datasets based   on the ScanNet scenes proposed by Achlioptas et al. [2].   SR3D consists of 83K expressions generated by a templatebased text generator. The expressions contain five types   of spatial object-to-object relations:   Horizontal/Vertical   Proximity, Between, Allocentric, and Support, taking up   81.37%, 3.80%, 1.79%, 4.50%, and 8.54% separately of all   expressions. NR3D is a 3D visual grounding dataset with   human-annotated expressions similar to ScanRefer. It consists of 41.5K human-annotated expressions and distractors   collected through a \u201cplayer-listener\u201d game. Note that both   SR3D and NR3D provide ground-truth bounding boxes for   all candidate objects, unlike ScanRefer.   Experimental Settings. We evaluate our proposed VPPNet on these datasets and report results in Tables 1 and 2.   For ScanRefer, we report the top-1 accuracy when IoU is   larger than 0.25 and 0.5 in \u201cUnique\u201d, \u201cMulti\u201d and \u201cOverall\u201d   cases. \u201cUnique\u201d vs \u201cMulti\u201d indicate whether distractors exist. \u201cModality\u201d shows whether a method gets benefit from   2D images, with \u201c3D\u201d meaning the model only takes the   3D point cloud and \u201c3D+2D\u201d indicating that 2D images are   also used. For SR3D and NR3D, following the experiment   setting of EDA, we report Acc@0.25IoU.   4.2. Implementation Details   Synthetic data. We select 557 and 140 scenes from ScanNet to build the train and validation set of synthetic datasets.   With the proposed synthetic viewpoint supervision mechanism, we repeat the generation process 75 times for each   scene to generate the synthetic dataset \u2013 resulting in 36117   training and 9167 validation samples.   Model configuration. VPP-Net contains a 6-layer encoder   and decoder. The learning rate of for the PointNet++ and   Roberta is 2e\u22123. The learning rate of other modules is 2e\u22124.   We first pre-train the model on the synthetic dataset to learn   the viewpoint estimation for 150 epochs. Then we finetune using a combination of synthetic and the downstream   dataset (Scanrefer/NR3D/SR3D) for another 150 epochs.   We apply the point dropout mechanism on the synthetic   14060   Unique(%)   Multi(%)   Overall(%)   Method   Modality   Acc@0.25   Acc@0.50   Acc@0.25   Acc@0.50   Acc@0.25   Acc@0.50   ScanRefer [5]   2+3D   76.33   53.51   32.73   21.11   41.19   27.40   ReferIt3D [2]   3D   53.8   37.5   21.0   12.8   26.4   16.9   TGNN [15]   3D   68.61   56.80   29.84   23.18   37.37   29.70   InstanceRefer [44]   3D   77.45   66.83   31.27   24.77   40.23   32.93   SAT [43]   2+3D   73.21   50.83   37.64   25.16   44.54   30.14   FFL-3DOG [12]   3D   78.80   67.94   35.19   25.70   41.33   34.01   3DVG-Transformer [46]   2+3D   81.93   60.64   39.30   28.42   47.57   34.67   3D-SPS [26]   2+3D   84.12   66.72   40.32   29.82   48.82   36.98   3DJCG [4]   2+3D   83.47   64.34   41.39   30.82   49.56   37.33   D3Net [6]   3D+2D   70.35   30.50   37.87   UniT3D [8]   3D+2D   82.75   73.14   36.36   31.05   45.27   39.14   BUTDBased   BUTD-DETR [17]   3D   84.2   66.3   46.6   35.1   52.2   39.8   EDA [35]   3D   85.76   68.57   49.13   37.64   54.59   42.26   MultiView   M3DRef-CLIP [45]   3D+2D   85.3   77.2   43.8   36.8   51.9   44.7   MVT [16]   3D+2D   31.46   24.85   39.95   32.28   ViewRefer [13]   3D+2D   33.08   26.50   41.30   33.66   VPP-Net (Ours)   3D   86.05   67.09   50.32   39.03   55.65   43.29   Table 1. 3D visual grounding results on ScanRefer. Acc@0.25/0.5 means top-1 accuracy when IoU is larger than 0.25/0.5 \u2013 higher is   better. We group BUTD-Based and Multi-View methods for ease of comparison. BUTD-based use the same backbone of our model. The   Multi-View methods all explored notions of viewpoint or 2D image rendering in 3D visual grounding. The state-of-the-art methods are   bold and the second best performance is underlined in the table.   data in both the pre-training and finetuning processes. The   dropout rate pdrop is kept at 1 in the first 40 epochs then   decreases to 0 linearly in the following 70 epochs.   Bounding boxes.   Bounding boxes B are a required input for the decoder. In ScanRefer, no ground-truth object   boundary box are provided. Thus we use GroupFree [25] to   detect a set of bounding boxes Bd, which Bd \u2208Rm\u00d78\u00d73.   Following the experiment setting of EDA [35] and BUTDDETR [17], in SR3D and NR3D, the model takes the   ground-truth boundary boxes to replace the detected ones   as the input of the decoder.   4.3. 3D Visual Grounding Results   Analysis of ScanRefer Results. Our experimental results   on ScanRefer are shown in Table 1. VPP-Net achieves stateof-the-art performance in terms of Acc@0.25 of \u201cOverall\u201dand \u201cUnique\u201d, surpassing EDA by 1.06% and 0.29%   respectively. An improvement of 1.19% and 1.29% can be   observed at \u201cMulti\u201d cases to 50.32% and 39.03% separately.   The \u201cMulti\u201d cases are more challenging than \u201cUnique\u201d   cases generally because of the existence of distractors. This   suggests that the 3D scene transformed with the predicted   viewpoint provides less ambiguous spatial relations with   which to reason about distractors.   Among the listed methods, we draw attention to \u201cBUTDBased\u201d and \u201cMulti-View\u201d models in the table \u2013 BUTDDETR [17], EDA [35], M3DRef-CLIP [45], MVT [16],   and ViewRefer [13].   BUTD-based models leverage the   same BUTD-DETR backbone we build on. Compared with   BUTD-DETR, our proposed VVP-Net has a large margin   of improvement across all evaluation metrics. Compared   to EDA, we can still observe an improvement of around   1.04% and 1.39% in \u201cOverall\u201d and \u201cMulti\u201d. Multi-View   models involve multi-view scenes or images that are related   to our method. VPP-Net outperforms MVT and ViewRefer across all metrics. Compared with M3DRef-CLIP, our   model achieves comparable but less competitive performance in Acc@0.5 of \u201cUnique\u201d and Acc@0.5 of \u201cOverall\u201d,   while achieving better performance in all other cases, especially, the Acc@0.5 of \u201cMulti\u201d which is more challenging.   We hypothesize this is due to the multi-view images rendered of object candidates used in M3DRef-CLIP. Multiview rendered images provide detailed information about   an object, allowing the model localize the object more accurately. The multi-view images focus on single objects and   do not provide much benefit in the \u201cMulti\u201d case, since the   model can be fooled by multiple objects with similar 2D   visual attributes in a scene. Thus, we can observe a larger   margin of Acc@0.5 of \u201cMulti\u201d between C3DRef-CLIP and   ours (improved by +2.23%).   14061   Method   Modality   SR3D   NR3D   ReferIt3D [2]   3D   39.8   35.6   TGNN [15]   3D   45.0   37.3   TransRefer3D [14]   3D   57.4   42.1   InstanceRefer [44]   3D   48.0   38.8   3DVG-Transfor [46]   3D   51.4   40.8   FFL-3DOG [12]   3D   41.7   SAT [43]   3D+2D   57.9   49.2   3DReferTrans [1]   3D   47.0   39.0   LanguageRefer [32]   3D   56.0   43.9   3D-SPS [26]   3D+2D   62.6   51.5   BUTD   -based   EDA [35]   3D   68.1   52.1   BUTD-DETR [17]   3D   67.1   54.9   Multi   View   M3DRef-CLIP [45]   3D+2D   49.4   LAR [3]   3D   59.6   48.9   VPP-Net(Ours)   3D   68.7   56.9   Table 2.   Result of SR3D and NR3D datasets.   We report   Acc@0.25IoU as the evaluation metric. Best and 2nd best results   are denoted in bold or underlined.   Analysis of SR3D & NR3D Results. We report results for   SR3D and NR3D datasets in Table 2. Compared with existing methods, VPP-Net achieves state-of-the-art on both.   Our observed improvement on NR3D (+2.00%) is larger   than we see in SR3D (0.6%). We argue that this may be   because there is a large domain gap between SR3D and   our synthetic dataset. To show the domain gap, we count   the frequency of spatial-relation words in SR3D, NR3D,   ScanRefer, and our synthetic dataset in Table 4. We find   an obvious gap: proximity relations \u2013 \u201cClosest/Farthest\u201d   \u2013 dominate in SR3D, while relative spatial relations \u2013   \u201cFront/Behind\u201d and \u201cLeft/Right\u201d \u2013 are more prominent in   the human-annotated NR3D and ScanRefer datasets as well   as our synthetic dataset. Despite this, VPP-Net still boosts   BUTD-DETR performance on SR3D (+1.6%).   To explicitly evaluate the efficiency of VPP-Net on   viewpoint-dependent samples, we report results on view dependent / independent (dep./indep.) subsets of NR3D in Table 3. We see large gains over our backbone Butd-Detr for   view dep. cases (46 vs. 52.4%) while achieving a similar result in view indep. case (58 vs. 58.6%). In contrast, EDA\u2019s   improvement for view dependent cases is smaller and results in performance loss on view independent cases.   4.4. Ablation Study   We report an ablation study of our method on ScanRefer in   Table 5. We remove different modules, auxiliary losses, or   training procedures from our approach to show their contriModel   View Dep.   View Indep.   Butd-Detr [17]   46.0%   58.0%   EDA [31]   50.2 %   53.1%   VPPNet (ours)   52.4 %   58.6%   Table 3. Results of Acc@0.25 on View Dep/Indep-endent cases of   NR3D dataset.   Synthetic(%)   Natural(%)   Relation   SR3D   Ours   NR3D   ScanRefer   Left/Right   8.92   44.58   36.00   55.66   Front/Behind   2.57   50.08   19.39   20.11   Above/Under   3.81   5.34   14.19   13.09   Between   8.54   0.00   3.21   6.77   Closest/Farthest   81.38   0.00   31.21   4.38   Table 4. Normalized frequency of spatial relation words across   datasets. We find SR3D has significantly fewer relative spatial   relations than our synthetic datasets or either human-collected   datasets on which we evaluate.   Synthetic   Pretraining   Viewpoint   Prediction   Uniform   Obj. Rep.   Curriculum   Filtering   Viewpoint   Data Aug.   Acc@0.25   Acc@0.50   0*   52.1   39.8   1   \u2713   \u2713   51.7   38.9   2   \u2713   \u2713   \u2713   \u2713   53.78   38.98   3   \u2713   \u2713   \u2713   \u2713   51.52   38.45   4   \u2713   \u2713   \u2713   \u2713   53.78   40.41   5   \u2713   \u2713   \u2713   \u2713   49.23   33.67   6   \u2713   \u2713   \u2713   \u2713   \u2713   55.65   43.29   Table 5. Ablation of our model components, auxiliary losses, and   training methods on the ScanRefer dataset \u2013 checkmarks denote   the corresponding item is active. Rows are numbered.   bution. Note that row \u201c0*\u201d without any of our modifications   is equivalent to BUTD-DETR. When viewpoint prediction   is disabled (row 1), we also disable the modules that rely on   it \u2013 pretraining and viewpoint data augmentation.   We find that removing the viewpoint prediction and data   augmentation (row 1), or the curriculum filtering during   pretraining (row 3) results in performance similar to the   baseline. These correspond to models that either do not use   viewpoint prediction (row 1) or learn viewpoint prediction   poorly (row 3) \u2013 suggesting the importance of viewpoint   in 3D visual grounding. We also note that directly jointly   training VPP-Net without first developing a strong viewpoint prediction capability during pretraining (row 5) yields   worse results than not considering viewpoint at all. Removing either the uniform object representation loss (row 2) or   the viewpoint data augmentation (row 4) yields degraded   performance compared with the whole model (row 6). Both   are designed to improve learned representations and we see   they have a positive effect in these experiments.   14062   Figure 3. Example VPP-Net results. The first row shows successful examples where both viewpoint prediction and visual grounding   are correct. The second row shows failed samples. The first two predict incorrect viewpoints but still succeed in visual grounding. The   predicted observer position (red dot) and facing direction (facing away from the green arrow) are shown in 3D and a top-down view (top   right corner). We zoom up on the target region in example (7) within the red circle to provide better visualization.   4.5. Qualitative Examples   We depict the quantitative results of VPP-Net in Fig. 3. In   the figure, we list 8 examples of which four are successful   (top row) and four are not (bottom row). In each example,   the red dot represents the predicted location and the green   and blue arrows represent the \u201cbehind\u201d and \u201cright\u201d direction   predicted in the model such that the predicted observer is   facing away from the green arrow. The ground truth bounding boxes and target words are noted with green and the   mentioned objects are noted with red. We also provide the   predicted object bounding box in the image, shown in blue.   The spatial relations are noted with blue in the text. The   successful examples show that with an accurate viewpoint   prediction, the expression can better match the 3D scene,   resulting in more accurate groundings. Failed examples (5)   and (6) are successful at grounding despite having failed   to predict valid viewpoints. Both contain ambiguous elements in text or image, i.e. the expression \u201cwalk thru the   door on the right\u201d in example (5) and the distorted door   (blob on bottom left of the scene) in example (6). Examples   (7) and (8) predict valid viewpoints but produce incorrect   visual groundings. The predicted bounding box in example (7) covers a certain region of the ground truth and involved patch contains similar visual attributes (white). In   example (8), both ground truth and prediction are on the   left of the viewpoint, matching the expression. However,   the model failed to understand the horizontal proximity (farthest), leading to a incorrect grounding.   5. Conclusion   In conclusion, we have proposed a Viewpoint Prediction Network (VPP-Net) for 3D visual grounding tasks.   Our model addresses the critical challenge of viewpointdependent ambiguity in 3D visual grounding. Specifically,   we first introduce a synthetic dataset and then use it to   train a 3D visual grounding model that can better disambiguate referring expressions by explicitly predicting potential viewpoints. Further, we design a uniform object representation auxiliary loss and viewpoint data augmentation   scheme that further improve the performance of our model.   Experiments on the ScanRefer, SR3D, and NR3D demonstrate the effectiveness of our proposed methods.   Limitations. The synthetic datasets we generate are highly   dependent on the templated referring expressions, which   limits their diversity and realism. Future works should include strategies to generate more diverse datasets or collect human-generated referring expressions with annotated   viewpoints to further improve.   14063", "conf": "CVPR", "year": "2024", "index": 736}, {"title": "SegVG: Transferring Object Bounding Box to   Segmentation for Visual Grounding   Weitai Kang1 , Gaowen Liu2, Mubarak Shah3, and Yan Yan1   1 Illinois Institute of Technology   2 Cisco Research   3 University of Central Florida", "abstract": ". Different from Object Detection, Visual Grounding deals   with detecting a bounding box for each text-image pair. This one box for   each text-image data provides sparse supervision signals. Although previous works achieve impressive results, their passive utilization of annotation, i.e. the sole use of the box annotation as regression ground truth,   results in a suboptimal performance. In this paper, we present SegVG, a   novel method transfers the box-level annotation as Segmentation signals   to provide an additional pixel-level supervision for Visual Grounding.   Specifically, we propose the Multi-layer Multi-task Encoder-Decoder as   the target grounding stage, where we learn a regression query and multiple segmentation queries to ground the target by regression and segmentation of the box in each decoding layer, respectively. This approach   allows us to iteratively exploit the annotation as signals for both boxlevel regression and pixel-level segmentation. Moreover, as the backbones   are typically initialized by pretrained parameters learned from unimodal   tasks and the queries for both regression and segmentation are static   learnable embeddings, a domain discrepancy remains among these three   types of features, which impairs subsequent target grounding. To mitigate this discrepancy, we introduce the Triple Alignment module, where   the query, text, and vision tokens are triangularly updated to share the   same space by triple attention mechanism. Extensive experiments on five   widely used datasets validate our state-of-the-art (SOTA) performance.   Code is available at https://github.com/WeitaiKang/SegVG.   Keywords: Visual Grounding \u00b7 Detection \u00b7 Transformer   1", "content": "Visual grounding [14,17,26,30,52] aims to localize a target object within an image based on a free-form natural language text expression. It is particularly important for numerous downstream multimodal reasoning systems, such as visual   question answering [10,35,41] and image captioning [1,4,50]. Previous works can   be broadly categorized into three distinct groups: two-stage methods [3,44,45,51],   one-stage methods [47,48], and transformer-based ones [6,15,16,31,43,49]. Both   2   W. Kang et al.   Triple   Alignment   \u201cA dog at the left    of another dog\u201d   Reg Query   Text2Visual    Alignment   \u201cA dog at the left    of another dog\u201d   Reg    Query   Reg Query   Seg Query   xN   Vision    Backbone   Text   Backbone   Vision    Backbone   Text   Backbone   Seg    Query   xN   Target Grounding    Stage    (Encoder-Decoder)   Target Grounding    Stage    (Encoder / Decoder)   Regression   Regression   Segmentation   (a)   (b)   Fig. 1: The comparison of visual grounding frameworks. The block with a dashed border indicates that the module may not necessarily exist. (a) Previous baseline method   consists of two backbones and additional transformer layers for target grounding, where   a regression query is supervised to regress the box. Current SOTA methods further employ a text-to-visual module to align the visual features with text features. (b) Our   method incorporates segmentation queries, which utilizes the box annotation at the   pixel-level to segment the target. Additionally, we propose the Triple Alignment module to eliminate the domain discrepancy of the query, text, and vision features.   two-stage and one-stage approaches use convolutional neural networks for candidate proposals and the selection of the best-matching candidate. Nonetheless,   these approaches rely on intricate modules that employ manually-crafted techniques for performing language inference and multi-modal integration.   Inspired by the success of the transformer [7, 8], TransVG [6] proposes a   transformer-based pipeline. As shown in Fig. 1.(a), this pipeline extracts vision   and text features via DETR [2] and BERT [7], respectively. To ground the target, they use the transformer encoder to fuse multimodal features along with a   learnable regression query and decode the query through an MLP. To enhance   the final target grounding stage, subsequent studies continue with some textto-visual modules in the early stage to modulate the vision features to align   with the text features. For example, QRNet [49] proposes a query-modulated   method for extracting language-aware vision features within the vision backbone. VLTVG [43] introduces a verification map to activate the vision features   to align with the text features before multimodal fusion.   Despite their advancements, the suboptimal annotation utilization, i.e., only   using the box annotation as a regression annotation, limits their performance.   As discussed in [37], Visual Grounding presents unique challenges compared to   Object Detection due to its sparse supervision signals. Specifically, it provides   only one box label for each text-image pair, while necessitatng detection within   a multimodal setting. Therefore, it is essential to fully exploit the box annotation, by treating it as a segmentation mask (pixels within the bounding box are   assigned a value of 1, while pixels outside the bounding box are assigned as 0).   Transferring Object Bounding Box to Segmentation for Visual Grounding   3   In this paper, we introduce SegVG (see Fig. 1.(b)), a novel method that leverages the pixel-level details within the box annotation as segmentation signals to   offer additional fine-grained supervision for Visual Grounding. Specifically, we   propose the Multi-layer Multi-task Encoder-Decoder as the target grounding   stage, where we learn a regression query and multiple segmentation queries to   ground the target by regression and segmentation of the box in each decoding   layer, respectively. The confidence score derived from the segmentation can further serve as a Focal Loss [21] scaling factor to adaptively emphasize the other   losses of challenging training samples. This approach allows us to iteratively   exploit the annotation as signals for both box-level regression and pixel-level   segmentation. Furthermore, the initial parameters for model backbones, typically derived from pretrained unimodal tasks, along with data-agnostic static   embeddings used as queries for decoding, result in a domain discrepancy among   different sources of feature, affecting the effectiveness of target grounding. To   tackle this problem, we present the Triple Alignment module, where we harmonize the domain of query, text, and vision features by implementing a triangular   update process through a triple attention mechanism. As a result, we ensure   that all features adapt and integrate within the same multimodal space, thereby   enhancing subsequent target grounding. Our contributions are as follows:   \u2013 We propose the Multi-layer Multi-task Encoder-Decoder to maximize the   utilization of the box annotation, which introduces an additional segmentation format for pixel-level supervision in Visual Grounding.   \u2013 To eliminate the domain discrepancy among the query, text, and vision, we   introduce the Triple Alignment to update these three types of features into   a sharing domain, which facilitates the subsequent target grounding.   \u2013 We conduct extensive experiments on five widely used datasets to show the   performance superiority of our proposed methods compared with previous   state-of-the-art methods and further investigate the reliability benefits derived from the segmentation output in real applications.   \u2013 We will release code and checkpoints for future research development.   2   Related Work   Visual grounding methods can be roughly classified into three pipelines: twostage methods, one-stage methods, and transformer-based methods.   Two-stage methods Two-stage approaches [3,51] treat visual grounding as first   generating candidate object proposals and then finding the best match to the   text. In the first stage, an off-the-shelf detector processes the image and proposes   regions that may contain the target. In the second stage, a ranking network   calculates the similarity between candidate regions and processed text features,   selecting the region with the highest similarity score as the final result. Training   losses include binary classification loss [29] or maximum-margin ranking loss [51].   To better understand the text and cross-modality matching, MattNet [51] focuses   4   W. Kang et al.   MLP   (Shared)   Repeat   A dog at the left    of another dog   BERT Embedding   DETR CNN   ...   DETR Layer   BERT Layer x2   Vision Backbone   Text Backbone   Triple Alignment    Reg Query   Seg QueryxN   \u2295   Reg    Seg    xN   \u2295   ...   x M   Multi-layer Multi-task Encoder-Decoder   Triple Multi-Head Attention Layer   x M   x M   \u2295   Segmentation   Output   MHSA   FFN   MHSA   MHCA   FFN   Reg Query   & Seg Queries   Vision & Text    Tokens   xR   Encoder   Decoder   Regression   Output   H   w   (x, y)   .   MLP   (Shared)   . . . . . .   Concat   Vision    Tokens   Upsample &   Sigmoid   ...   Focal Loss   Factor   ...   xR   Fig. 2: SegVG: The upper figure includes the vision and text backbone. Our proposed Triple Alignment module is iteratively inserted into intermediate layers to eliminate domain discrepancy. The lower figure shows our Multi-layer Multi-task EncoderDecoder, which adopts a transformer encoder-decoder to update multimodal features   and ground the target. In this architecture, we make the best of the box annotation   as a segmentation ground truth and integrate an additional segmentation task into   Visual Grounding. Additionally, the segmentation output serves as a Focal Loss factor,   allowing adaptive emphasis on challenging cases for the regression loss. M = 6, R=6.   on decomposing the text into subject, location, and relationship components. [3]   introduces an expression-aware score for improved candidate region ranking.   One-stage methods One-stage approaches [47,48] directly concatenate vision   and text features in the channel dimension and rank confidence values for candidate regions proposed based on the concatenated multimodal features. For   example, FAOA [48] predicts bounding boxes using a YOLOv3 detector [32]   on the concatenated features. ReSC [47] further improves the ability to ground   complex queries by introducing a recursive sub-query construction module.   Transformer-based methods Transformer-based approach is first introduced   by TransVG [6]. Unlike previous methods, TransVG concatenates the regression   query (a learnable embedding), vision tokens, and text tokens and uses transformer encoders [38] to perform cross-modal fusion and target grounding. The   query is then processed through an MLP to decode the box. Benefiting from   the flexible structure of transformer modules in processing multimodal features,   recent works continue adopting this pipeline and propose novelties regarding   feature extraction. VLTVG [43] develops a visual-linguistic verification module before the target grounding stage to modulate the vision features with the   relationship between vision and text features. QRNet [49] proposes a Querymodulated Refinement Network to mitigate the gap between features from the   unimodal vision backbone and those needed for multi-modal reasoning.   Transferring Object Bounding Box to Segmentation for Visual Grounding   5   Multi-task Visual Grounding Multi-task learning is extensively utilized in   object detection and segmentation [2,11], often capitalizing on a shared backbone   and task-specific heads. Expanding upon this idea, several studies [19, 25, 36]   have proposed solutions to the Multi-task Visual Grounding problem. In this   problem, they jointly tackle Referring Expression Comprehension (REC, also   known as Visual Grounding) and Referring Expression Segmentation (RES),   requiring both box annotations and segmentation annotations. It is important   to note that, unlike those approaches, even though we incorporate segmentation   losses in our method, we do not need segmentation annotations but only   box annotations, focusing specifically on the Visual Grounding task.   3   Methodology   In this section, we present the components of our SegVG in the order of the data   flow: starting with the backbones, followed by our proposed Triple Alignment,   and finally our Multi-layer Multi-task Encoder-Decoder.   3.1   Backbones   As shown in Fig. 2 (upper), our vision backbone consists of ResNet and transformer encoder from DETR [2], with parameters pretrained on Object Detection   task using the MSCOCO dataset [22], excluding the validation and test sets of   Visual Grounding dataset. The text backbone is the base model of BERT [7].   Vision Backbone Given an input image I0 (R3\u00d7H0\u00d7W0), we employ ResNet to   generate a 2D feature map I \u2208RC\u00d7H\u00d7W (C = 2048, H = H0   32 , W = W0   32 ). A 1x1   convolutional layer is then used to reduce the channel dimension of I to Cv = 256,   resulting in I   \u2032. We further flatten I   \u2032 into Zv \u2208RCv\u00d7Nv (Nv = H \u00d7 W). Position   embedding is then added to Zv to preserve sensitivity to the original 2D spatial   locations. Zv is then iteratively processed through DETR\u2019s encoder layer (total   6 transformer layers) and the Triple Alignment to obtain the output Zv.   Text Backbone Given a text, we initially utilize the BERT\u2019s embedding layer   to convert it into Nt language tokens with Ct channel dimension. In alignment   with [6], we prepend a [CLS] token and append a [SEP] token to the beginning   and end positions of the tokenized language, respectively. Following this, we   iteratively input the language tokens into BERT\u2019s layers (total 12 transformer   layers) and the Triple Alignment, generating language embedding Zt.   3.2   Triple Alignment   Given that the text and vision backbones are pretrained from unimodal tasks   and the queries are data-agnostic, the subsequent target grounding stage faces   the challenge of aligning these three types of features into the same space before   performing multimodal fusion for target grounding. Additionally, considering   6   W. Kang et al.   that the backbones usually contribute the majority of the overall parameters,   solely using them for extracting unimodal features without incorporating multimodal alignment is sub-optimal. Therefore, an optimal solution is to address the   domain discrepancy before moving on to the subsequent target grounding stage.   Triple Alignment (Fig. 2 (upper)) utilizes an attention mechanism to perform triangular feature sampling, aiming to ensure domain consistency among   the query, text, and vision features. The queries, Zo, are first initialized by N   learnable embeddings, where one embedding is for the regression query and the   rest of the embeddings are for multiple segmentation queries. The data flow is:     {\\   b   f  Z}_{v}^{i+1}    = { \\ rm DE TRL a y er_\\textit {i}}({\\bf Z}_{v}^i), i \\in \\{0, 1, .., L-1\\} \\label {eq1}    (1)     {\\   b   f  Z}_{t}^{i+1} = {\\rm BERTLay   er_\\textit {2i+1}}({\\rm BERTLayer_\\textit {2i}}({\\bf Z}_{t}^i)), \\label {eq2}    (2)        [   {\\ b   f    Z }   _   {o } ^{'}, {\\bf Z}_{   t   } ^{'}   ,    {\\bf Z}_{v}^{'}] = {\\rm Tri\\textsc {-}MHA}({\\bf Z}_{o}, {\\bf Z}_{t}^{i+1}, {\\bf Z}_{v}^{i+1}), \\label {eq3}    (3)     { \\b f     Z   }_ {o}    =    {\\bf       Z }   _   {o } +    {   \\ bf Z   }   _ {   o   }^{'}, {\\bf Z}_{t}^{i+1} = {\\bf Z}_{t}^{i+1} + {\\bf Z}_{t}^{'}, {\\bf Z}_{v}^{i+1} = {\\bf Z}_{v}^{i+1} + {\\bf Z}_{v}^{'}, \\label {eq4}    (4)   where L is the number of layers, BERTLayer is the layer of BERT and DETRLayer is the layer of DETR\u2019s encoder. The vision and text features are first   encoded by Eq. 1 and Eq. 2. Subsequently, the three types of tokens (query, text,   and vision) are updated by our Triple Multi-Head Attention Layer (Tri-MHA)   using Eq. 3. The output tokens are merged back to their original branches respectively by Eq. 4. Within each head of the Triple Multi-Head Attention Layer   (Tri-MHA), each type of the features simultaneously computes its updated representation by attending to both the others and itself:     \\beg in {align ed} S= [{\\ bf Z}_{ o } W^{ (o ,     S)}, &{\\bf Z }_{t}W^{(t,     S   )},{\\   b   f    Z }_{ v} W ^ {(v , S)}],S\\in \\{Q,K,V\\} \\\\ [{\\bf Z}_{o}, {\\bf Z}_{t}, &{\\bf Z}_{v}] = {\\rm SoftMax}(QK^{T} / {\\sqrt d_{k}})V, \\\\ {\\bf Z}_{e}^{'}& =~ {\\bf Z}_{e}W^{e}, e \\in \\{o, t, v\\}, \\\\ \\end {aligned} \\label {eq5}    (5)   where {W (e,S), W e : e \u2208{o, t, v}, S \u2208{Q, K, V }} are trainable parameter. As a   result, each of the output features is triangular sampling from all of the three   types of features, which alleviates the domain discrepancy.   3.3   Multi-layer Multi-task Encoder-Decoder   The Multi-layer Multi-task Encoder-Decoder serves as the target grounding   stage, where we use a transformer encoder-decoder for cross-modal fusion and   target grounding to perform a box regression task and a box segmentation task.   Encoder As shown in Fig. 2 (lower left), given the aligned output text and vision   features from the backbones, the encoder fuses the two modalities into the multimodal features by a stack of transformer layers. In each layer, the concatenated   text and vision tokens go through the Multi-Head Self-Attention layer (MHSA)   and the Feed Forward Network (FFN) with the residual connection.   Transferring Object Bounding Box to Segmentation for Visual Grounding   7   Decoder In each decoder layer, we aim to fully exploit the box annotation. We   propose the bbox2seg paradigm to transform the box annotation into a segmentation mask, which classifies all pixels within the box as foreground (with a   value of one) and those outside the box as background (with a value of zero).   As shown in Fig. 2 (lower right), one regression query aims to regress the box,   while the remaining segmentation queries aim to segment the box. Different segmentation queries are endowed with different learnable positional embeddings   to enhance the robustness of each decoder layer, since the decoder layer, when   confronted with various queries, is required to segment the same box. Following   that, the queries pass through the Multi-Head Self-Attention layer to exchange   information about the same target, prompting each other to better locate the   target. Subsequently, the queries undergo the Multi-Head Cross-Attention layer   and the Feed Forward Network, where multimodal features serve as the Key   and Value to ground the target. Finally, a shared MLP across all decoder layers   decodes the regression query into the box result, supervised by L1 loss and Giou   loss [33]. Each segmentation query is repeated Nv times and concatenated with   visual tokens along the channel dimension. Another shared MLP decodes the   concatenated feature into the segmentation mask, supervised by Focal loss [21]   and Dice loss [27]. It is noteworthy that our segmentation paradigm shares   the same semantic foundation as the regression paradigm, i.e., to distinguish bounding box, rather than instance segmentation. Therefore, incorporating non-object pixels in the segmented foreground does not introduce   ambiguity to the model. We provide qualitative results 4.8 to demonstrate this   feature. To alleviate multi-task optimization challenges, we freeze the backbones   for the initial k epochs to stabilize the training process.   Confidence score Since both the regression output and segmentation output   share the same aim, we can additionally obtain the confidence score for the   foreground by averaging values inside the ground truth box of the segmentation   output to reflect the confidence of the regression output. In the training process,   we can transform this confidence score as the Focal loss factor [21] to adaptively   emphasize the other losses of challenging training samples. The final loss function   of each decoder layer is formulated as follows:     \\begin {al i gned} L = \\lambda    _{1} c_{focal} L _1 + \\lambda _{giou} c_{focal} L_{giou} + \\\\ \\lambda _{dice} c_{focal} L_{dice} + \\lambda _{focal} L_{focal}, \\end {aligned} \\label {loss}    (6)   where \u03bb1, \u03bbgiou, \u03bbfocal and \u03bbdice are hyperparameters. L1 is the L1 loss. Lgiou is   the GIoU loss [33]. Lfocal is the Focal loss [21]. Ldice is the Dice loss [27]. cfocal   is the above Focal loss factor averaged across all segmentation outputs.   In the real-world application perspective, the visual grounding task can be   viewed as open-vocabulary object detection [54], where target objects lack predetermined categories. Therefore, previous transformer-based methods directly   regress the box without confidence scores, since there is no candidate proposal   or selection stage in transformer-based pipeline. However, confidence scores are   valuable for enhancing the control or reliability of predictions by filtering out lowconfidence predictions. This feature could benefit the future integration of visual   8   W. Kang et al.   Table 1: Comparisons with state-of-the-art methods on widely used datasets. We   highlight the best and second best performance in red and blue, and bold our model.   Models   Backbone   RefCOCO   RefCOCO+   RefCOCOg   ReferItGame   val   testA testB   val   testA testB   val-g   val-u test-u   test   Two-stage:   CMN [12]   VGG16   71.03 65.77   54.32 47.76 57.47   28.33   VC [55]   VGG16   73.33 67.44   58.40 53.18 62.30   31.13   ParalAttn [57]   VGG16   75.31 65.52   61.34 50.86 58.03   MAttNet [51]   ResNet-101 76.65 81.14 69.99 65.33 71.62 56.02   66.58 67.27   29.04   Similarity Net [39] ResNet-101   34.54   CITE [29]   ResNet-101   35.07   DDPN [53]   ResNet-101   63.00   LGRANs [40]   VGG16   76.60 66.40   64.00 53.40 61.78   DGA [44]   VGG16   78.42 65.53   69.07 51.99   63.28   RvG-Tree [29]   ResNet-101 75.06 78.61 69.85 63.51 67.45 56.66   66.95 66.51   NMTree [23]   ResNet-101 76.41 81.21 70.09 66.46 72.02 57.52 64.62 65.87 66.44   Ref-NMS [3]   ResNet-101 80.70 84.00 76.04 68.25 73.68 59.42   70.55 70.62   One-stage:   SSG [5]   DarkNet-53   76.51 67.50   62.14 49.27 47.47 58.80   54.24   ZSGNet [34]   ResNet-50   58.63   FAOA [48]   DarkNet-53 72.54 74.35 68.50 56.81 60.23 49.60 56.12 61.33 60.36   60.67   RCCF [20]   DLA-34   81.06 71.85   70.35 56.32   65.73   63.79   ReSC-Large [47]   DarkNet-53 77.63 80.45 72.30 63.59 68.36 56.81 63.12 67.30 67.20   64.60   LBYL-Net [13]   DarkNet-53 79.67 82.91 74.15 68.64 73.38 59.49 62.70   67.47   Transformer-based:   TransVG [6]   ResNet-101 81.02 82.72 78.35 64.82 70.70 56.94 67.02 68.67 67.73   70.73   QRNet [49]   Swin-S   84.01 85.85 82.34 72.94 76.17 63.81 71.89 73.03 72.52   74.61   VLTVG [43]   ResNet-101 84.77 87.24 80.49 74.19 78.93 65.17 72.98 76.04 74.18   71.98   SegVG (ours)   ResNet-101 86.84 89.46 83.07 77.18 82.63 67.59 76.01 78.35 77.42   75.59   grounding models into downstream multimodal reasoning systems or real-world   applications. To meet the requirements of this feature, our approach incorporates a confidence score derived from the segmentation output during inference.   Specifically, we calculate the model\u2019s confidence by averaging values greater than   or equal to 0.35 (adopted from [42]) in the segmentation output of one segmentation query. Analyses 4.7 in the Experiment section demonstrate the faithfulness   and benefits of incorporating this additional confidence score.   4   Experiments   4.1   Metric and Datasets   Metric A predicted bounding box is considered accurate if its Intersection over   Union (IoU) with the ground-truth bounding box exceeds 0.5. In accordance with   the established practices in preceding studies [6,43], we employ top-1 accuracy   (measured in percentage) as the primary metric to assess our method.   Datasets There are five standard benchmarks: RefCOCO [52], RefCOCO+   [52], RefCOCOg-g [26], RefCOCOg-umd [26], and ReferItGame [17]. Four of   them (RefCOCO, RefCOCO+, and RefCOCOg-(g/umd)) are all derived from   MSCOCO [22]. RefCOCO consists of 19,994 images and 142,210 referring texts,   which is divided into four subsets: a training set with 120,624 texts, a validation set with 10,834 texts, and two test sets (testA and testB) containing 5,657   Transferring Object Bounding Box to Segmentation for Visual Grounding   9   and 5,095 texts, respectively. RefCOCO+ includes 19,992 images and 141,564   referring texts, which is partitioned into four subsets: a training set with 120,191   texts, a validation set with 10,758 texts, and two test sets (testA and testB) containing 5,726 and 4,889 texts, respectively. RefCOCOg contains 25,799 images   and 95,010 longer texts. Two widely accepted splitting methods\u2014RefCOCOgg [26] and RefCOCOg-umd [28]\u2014are employed for this dataset, and we perform   experiments using both RefCOCOg-g (val-g) and RefCOCOg-umd (val-u and   test-u) splitting conventions. The ReferItGame dataset, featuring 20,000 images   from SAIAPR-12 [9], is divided into three segments: a training set with 54,127   texts, a validation set with 5,842 texts, and a testing set comprising 60,103 texts.   4.2   Implementation   Our approach uses an input image size of 640 x 640 and sets the maximum   expression length at 40. When resizing images, we preserve the original aspect   ratio. The longer edge is resized to 640, and the shorter edge is padded to 640   with the value of zero. Texts exceeding 38 tokens are truncated, reserving a start   position and an end position of the characters for the [CLS] and [SEP] token,   respectively. If the text is shorter, empty tokens are added after the [SEP] token   to reach an input length of 40. Paddings for the input image are not tracked by   masks, while empty tokens of text employ masks.   We use the AdamW optimizer. The initial learning rate of 1e-5 is assigned   to backbones, and 1e-4 to the rest parameters. Weight decay is set at 1e-4. The   visual backbone is initialized with the DETR\u2019s backbone and encoder, while the   language branch uses the basic BERT model. For the final results, our model is   trained for 90 epochs, with the learning rate decreasing by a factor of 10 after 60   epochs. The k hyperparameter in Multi-layer Multi-task Encoder-Decoder is set   to 10. We use a batch size of 64. For the ablation studies presented in Table 3,   the models are trained for 60 epochs with k equal to 20, and the learning rate   drops after 40 epochs. We set \u03bb1 = 5, \u03bbgiou = 2, \u03bbfocal = 1 and \u03bbdice = 1. We   adhere to previous practices [6,43] for data augmentation during training.   4.3   Quantitative Results   We report the performance of our SegVG on all benchmark datasets. As presented in Table 1, our SegVG model demonstrates superiority across all of the   datasets. This indicates the effectiveness and generalizability of our approach.   It is worth noting that RefCOCO+ and RefCOCOg are relatively more challenging datasets, as RefCOCO+ does not include location terms in its language   expressions, and RefCOCOg has longer language expressions compared to other   datasets. Despite these challenges, our model exhibits significant improvements   on these two difficult datasets. Specifically, on RefCOCO+, our model outperforms the previous SOTA models with +2.99%, +3.7%, and +2.42% on the val,   testA, and testB subsets, respectively. On RefCOCOg, our model also surpasses   the previous SOTA models with +3.03%, +2.31%, and +3.24% on the val-g,   10   W. Kang et al.   Table 2: Compare transformer-based models on Parameter count and GFLOPS.   Backbone   Parameter count (M)   GFLOPS (G)   TransVG [6]   ResNet101   141.55   72.61   VLTVG [43]   ResNet101   141.61   69.87   QRNet [49]   Swin-S   247.06   80.12   SegVG   ResNet101   155.28   73.48   Table 3: Ablation study on RefCOCOg test-u. Encoder and Decoder are the encoder   and decoder of Multi-layer Multi-task Encoder-Decoder, respectively. MMDecoder represents Multi-layer and Multi-task supervision in the Decoder. N is the number of   segmentation queries. Triple means Triple Alignment. Excluding Query in Triple (g)   means using bidirectional alignment with concatenated text-vision tokens.   Id   Model   Acc(%)   (a) Backbones + Encoder + Decoder   66.97   (b) Backbones + Triple + Encoder + Decoder   75.75   (c) Backbones + Encoder + MMDecoder (N=1)   72.61   (d) Backbones + Encoder + MMDecoder (N=5)   76.21   (e) Backbones + Triple + Encoder + MMDecoder (N=5)   77.29   (f) (e) w/o Encoder   76.65(-0.64)   (g) (e) w/o Query in Triple   76.37(-0.92)   (h) (e) w/o Focal loss   76.83(-0.46)   val-u, and test-u subsets, respectively. These results suggest that under the reinforcement of Triple Alignment and Multi-layer Multi-task Encoder-Decoder,   the query, text, and vision tokens are triangularly updated to share the same   space, and the model fully exploits the bounding box as fine-grained pixel-level   supervision for comprehensive end-to-end learning.   We also conduct a comparison of the number of parameters and GFLOPS   across transformer-based models to evaluate computational costs. As depicted   in Table 2, the computational cost of SegVG falls within a reasonable range.   4.4   Ablation Study   In this section, we aim to validate the efficacy of each proposed module. We   conduct ablation studies on the RefCOCOg-umd test dataset. Specifically, we   start by evaluating a basic structure, i.e., the backbones with encoder-decoder   structure. After that, we systematically incorporate the Triple Alignment module into the backbones and introduce the Multi-layer Multi-task supervision into   the decoder through the controlled variable approach. Meanwhile, we conduct   additional ablation experiments on specific details, including assessing the efficacy of incorporating the encoder, introducing Query in the Triple Alignment,   and introducing the Focal loss from the segmentation output to the other losses.   As shown in Table 3, we can draw the following conclusions when comparing   the experimental results under controlled variables: 1) [(a) v.s. (b)]: Incorporating the Triple Alignment can effectively eliminate the domain discrepancy   Transferring Object Bounding Box to Segmentation for Visual Grounding   11   Table 4: Ablation study on RefCOCOg test-u regarding the number of segmentation   queries. \u2206Hour and \u2206Acc are the additional time cost and accuracy improvement   compared to ID(i) experiment, respectively.   ID   num_query   Acc   Time Cost   \u2206Hour / \u2206Acc (v.s. ID(i))   i   N = 1   72.61   16.32h   ii   N = 3   73.02   18.97h   6.46   iii   N = 5   76.21   20.10h   1.05   iv   N = 7   74.38   22.24h   3.34   v   N = 9   73.91   24.07h   5.96   among the query, text, and vision features, thereby facilitating subsequent target grounding. 2) [(a) v.s. (c)]: Introducing Multi-layer Multi-task supervision   can iteratively make the best of the annotations in the target grounding stage,   thereby enhancing the learning of query representations. 3) [(c) v.s. (d)]: Increasing the number of segmentation queries can further improve the robustness   of the decoder when provided with different queries and required to segment the   same box. 4) [(a), (b), (d), and (e)]: Combining the Triple Alignment and   Multi-layer Multi-task Encoder-Decoder can effectively enhance the overall performance, achieving the optimal result. 5) [(e) v.s. (f)]: Even if we include the   Triple Alignment supporting multimodal communication, it remains necessary   for the subsequent encoder to update the unimodal features generated by the   backbones into multimodal features. 6) [(e) v.s. (g)]: It is necessary to involve   queries in Triple Alignment to transfer the data-agnostic embedding into datarelated queries. Otherwise, using only Bidirectional Alignment (BA) for text and   vision tokens, similar to approaches like Deep Fusion in GLIP [18] and Grounding   DINO [24], and WPA in CoupAlign [56], causes a noticeable decline (-0.92%). 7)   [(d), (e) and (g)]: The slight gain of (g)(76.37%) over (d)(76.21%) stems from   our MMDecoder\u2019s pixel-level signals, which already boosts BA in the Encoder.   Thus, extra BA effort in the backbone is marginal, failing to solve the unaligned   query issue. Instead,Tri-Align (e)(77.29%) can solve this issue, showing its novelty. Notably, (d)(76.21%), with a basic encoder-decoder, already achieves SOTA   performance, emphasizing our bbox2seg paradigm\u2019s simplicity and effectiveness.   8) [(e) v.s. (h)]: The segmentation output can further derive the confidence   score of the model\u2019s prediction, which is transformed into the Focal loss factor   to adaptively scale the other losses to focus more attention on challenging cases.   We further conduct a more detailed abaltion study about the improvement   and corresponding cost of adding more segmentation queries of Tab.3(c)-(d).   As shown in Table 4, the best performance is observed with five segmentation   queries. Adding more than that increases the burden from pixel-level constraint   without benefits, also increasing computational cost per accuracy improvement.   4.5   Triple Alignment Analysis   In additional to the improvement results in ablation study, we further deepen   our understanding of the Triple Alignment by analyzing the attention behavior.   12   W. Kang et al.   Table 5: Attention values from query to text-referred visual region in Triple Alignment.   Layer   RefCOCO   RefCOCO+   RefCOCOg   ReferItGame   val   testA testB   val   testA testB   val-g   val-u   test-u   test   layer2   17%   18%   16%   19%   21%   18%   20%   18%   17%   25%   layer4   34%   36%   33%   29%   31%   28%   27%   33%   33%   30%   layer6   61%   63%   60%   55%   58%   50%   55%   59%   59%   38%   Table 6: Comparison with alternative   method using RIS to provide pseudo segmentation labels for supervision.   ReferItGame SegVG+RIS (LAVT) SegVG   val   74.91   76.85   test   73.76   75.06   Table 7: AP50 using different segmentation queries for confidence score calculation on RefCOCOg test-u.   Seg Query   1st   2nd   3rd   4th   5th   AP50   84.65 84.57 84.73 84.78 84.63   Specifically, we calculate the sum of the attention values from the query to the   text-referred visual region (target bbox) as a percentage of the total attention   (which includes attention to the query, text, and visual tokens) to illustrate the   extent of alignment across these three modalities in the second, fourth, and final   layer of Triple Alignment. We average the percentage across all the attention   heads and queries, and perform the analysis across all the datasets. As shown in   Table 5, in all the datasets, the attention values increase as the layer progresses,   indicating that Triple Alignment progressively aligns the query to comprehend   the text and then focus on the referred visual region.   4.6   Comparison with alternative method   Given the development of Referring Expression Segmentation (RES), a natural   alternative method would be using a RES method to generate pseudo segmentation labels to substitute our bbox2seg paradigm. Therefore, to mimick real-world   scenarios, we use LAVT [46] trained on RefCOCO to obtain pseudo segmentation   labels on ReferItGame. We follow the same training setting in ablation study to   conduct the comparison on ReferItGame. As shown in Table 6, our SegVG outperforms the alternative method. This demonstrates that our bbox2seg paradigm   is more effective than using a RES model to provide pseudo segmentation labels   which might suffer from the errors from the RES model.   4.7   Confidence Score Analysis   Selection of segmentation query To show the effect of different selection of segmentation query for the calculation of confidence score, we calculate AP50 on   RefCOCOg test-u. As shown in Table 7, the performance variations are slight   among them. We use the first one to calculate confidence score for simplicity.   Confidence Score Faithfulness To evaluate the faithfulness of our confidence   score, i.e., whether a higher confidence score indicates better performance, we   Transferring Object Bounding Box to Segmentation for Visual Grounding   13   0.71   0.77   0.84   0.91   0.98   Averaged confidence   0.0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   IoU   IoU   0.0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   Accuracy   Accuracy   Fig. 3: IoU and Accuracy of different   confidence scores on RefCOCOg test-u.   Table 8: Performance of different confidence levels on RefCOCOg test-u.   Confidence   IoU   Acc(%) Proportion (%)   \u22650.65   0.7067 77.41%   100.00%   \u22650.70   0.7072 77.48%   99.89%   \u22650.75   0.7091 77.69%   99.44%   \u22650.80   0.7143 78.37%   97.98%   \u22650.85   0.7226 79.29%   95.04%   stuffed teddy bear    with a red ribbon   a woman in a pink tank top    and khaki shorts   brown dog   the back profile of a giraffe   Fig. 4: Qualitative comparison between (c) (the first line) and (d) (the second line)   of Table. 3. Red boxes are ground truth. Blue boxes are model predictions.   assess the relationship of our confidence score and model performance metrics   (IoU and Accuracy) as shown in Fig 3. We sort the RefCOCOg-umd test set by   confidence score, split it into five equal parts, and calculate each part\u2019s average   score and performance. We observe a positive correlation between the performance metrics and our confidence score, which confirms its faithfulness.   Confidence Score Application In real applications, the confidence score can be   used to enhance the model\u2019s reliability. Specifically, we can apply different confidence thresholds to achieve different predictions, as shown in Table 8. First, we   observe that accuracy increases with higher thresholds, indicating that adjusting the threshold can enhance the model\u2019s localization ability. Furthermore, the   mean IoU also increases with the increasing threshold. Therefore, in downstream   applications, such as using the model to provide pseudo-labels, we can increase   the threshold to obtain a more accurate box. Since low-confidence outputs are   excluded, the output proportion is slightly reduced, i.e., yields fewer outputs.   14   W. Kang et al.   Layer 0   Layer 1   Layer 2   Layer 3   Layer 4   Layer 5   Segmentation   GT   \u201ca young    man    wearing    black    smiling at    the camera\u201d   \u201cthe plate    holding    the fruit\u201d   VLTVG   SegVG    (ours)   VLTVG   SegVG    (ours)   Fig. 5: Layer i refers to the output of the i-th layer of the decoder. The blue boxes   represent the models\u2019 predictions, while the red boxes denote the ground truth. In   the segmentation mask shown in the second column from the right, red indicates high   confidence for the foreground. Note that VLTVG does not provide segmentation output.   4.8   Qualitative Results   We compare Tab. 3 (c) and (d) qualitatively to show the robustness enhancement. As shown in Fig. 4, adding segmentation queries improves the robustness   to distinguish the target from the distractor, e.g. the case of two dogs.   As depicted in Fig. 5, we compare the box prediction quality by each decoding   layer of SegVG with VLTVG [43] which also involves multi-layer supervision.   As seen in the upper two rows of Fig. 5, VLTVG initially misses the target   \u201cyoung man\u201d but improves its prediction gradually and finally makes the correct   prediction. In contrast, due to the full exploitation of the annotations and the   domain alignment in our Triple Alignment, SegVG successfully identifies the   location of the target in the early decoding layer and consistently makes the   correct prediction in each layer. Another example can be observed in the lower   two rows of Fig. 5. In this image, due to the complex colors, VLTVG fails to locate   the target \u201cplate\u201d and consistently repeats the same mistake. Instead, SegVG   correctly detects the target, even in the first decoder layer. Additionally, we   visualize the segmentation mask obtained by SegVG in Fig. 5, which accurately   identifies the target box with high confidence. This behavior aligns with the box   regression, demonstrating their shared objective, i.e., to distinguish the box.   5   Conclusion   We propose SegVG, a visual grounding model, where we iteratively make the   best of box annotations to involve pixel-level supervision and address the domain discrepancy among queries, text, and vision by Triple Alignment module.   Experiments show the superior performance of SegVG. Furthermore, we explore   the reliability benefits of our segmentation output in real-world applications.   Transferring Object Bounding Box to Segmentation for Visual Grounding   15   Acknowledgements   This research is supported by NSF IIS-2309073 and ECCS-2123521. This article   solely reflects the opinions and conclusions of its authors and not the funding   agencies.", "conf": "ECCV", "year": "2024", "index": 510}, {"title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2298\u20132303,   Lisbon, Portugal, 17-21 September 2015. c\u20dd2015 Association for Computational Linguistics.   Can Symbol Grounding Improve Low-Level NLP?   Word Segmentation as a Case Study   Hirotaka Kameko\u2020, Shinsuke Mori\u2021, and Yoshimasa Tsuruoka\u2020   \u2020Graduate School of Engineering, The University of Tokyo   Hongo, Bunkyo-ku, Tokyo, Japan   {kameko, tsuruoka}@logos.t.u-tokyo.ac.jp   \u2021Academic Center for Computing and Media Studies, Kyoto University   Yoshida Honmachi, Sakyo-ku, Kyoto, Japan   forest@i.kyoto-u.ac.jp", "abstract": "We propose a novel framework for improving a word segmenter using information acquired from symbol grounding. We   generate a term dictionary in three steps:   generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates,   and \ufb01ltering them according to the grounding scores.   We applied our method to   game records of Japanese chess with commentaries. The experimental results show   that the accuracy of a word segmenter can   be improved by incorporating the generated dictionary.   1", "content": "Today we can easily obtain a large amount of   text associated with multi-modal information, and   there is a growing interest in the use of nontextual information in the natural language processing (NLP) community. Many of these studies   aim to output natural language sentences from a   nonlinguistic modality, such as image (Farhadi et   al., 2010; Yang et al., 2011; Rohrbach et al., 2013).   Kiros et al. (2014) showed that multi-modal information improves the performance of a language   model.   Inspired by these studies, we explore a method   for improving the performance of a low-level NLP   task using multi-modal information. In this work,   we focus on the task of word segmentation (WS)   in Japanese. WS is often performed as the \ufb01rst   processing step for languages without clear word   boundaries, and it is as important as part-of-speech   (POS) tagging in English.   We assume that a   large set of pairs of non-textual data and sentences   describing them is available as the information   source. In our experiments, the pairs consist of   game states in Shogi (Japanese chess) and textual   comments on them, which were made by Shogi   experts. We enumerate substrings (character sequences) in the sentences and match them with   Shogi states by a neural network model. The rationale here is that substrings which match with   non-language data well tend to be real words.   Our method consists of three steps (see Figure   1). First, we segment commentary sentences for a   game state in various ways to produce word candidates. Then, we match them with game states of   a Shogi playing program. Finally, we compile the   symbol grounding results at all states and incorporate them to an automatic WS. To the best of our   knowledge, this is the \ufb01rst result reporting a performance improvement in an NLP task by symbol   grounding.   2   Stochastically Segmented Corpus   Before symbol grounding, we need to segment   the text into words that include probable candidate words. For this purpose, we use a stochastically segmented corpus (SSC) (Mori and Takuma,   2004). Then we propose to simulate it by a normal   (deterministically) segmented corpus to avoid the   problem of computational cost.   2.1   Stochastically Segmented Corpora   An SSC is de\ufb01ned as a combination of a raw   corpus Cr (hereafter referred to as the character   sequence xnr   1 ) and word boundary probabilities   of the form Pi, which is the probability that a   word boundary exists between two characters xi   and xi+1. These probabilities are estimated by a   model based on logistic regression (LR) (Fan et   al., 2008) trained on a manually segmented corpus by referring to the surrounding characters1.   Since there are word boundaries before the \ufb01rst   character and after the last character of the corpus,   P0 = Pnr = 1. The expected frequency of a word   1In the experiment we used the same features as those   used in Neubig et al., (2011). .   2298   Figure 1: Overview of our method.   w in an SSC is calculated as follows: fr(w) =   \u2211   i\u2208O Pi   {\u220fk\u22121   j=1(1 \u2212Pi+j)   }   Pi+k, where O =   {i | xi+k   i+1 = w} is the set of all the occurrences   of the string matching with w2.   2.2   Pseudo-Stochastically Segmented   Corpora   The computational cost (in terms of both time   and space) for calculating the expected frequencies in an SSC is very high3, so it is not a practical approach for symbol grounding.   In this   work, we approximate an SSC using a deterministically segmented corpus, which we call a pseudostochastically segmented corpus (pSSC). The following is the process we use to produce a pSSC   from an SSC.   \u2022 For i = 1 to nr \u22121   1. output a character xi,   2. generate a random number 0 \u2264p < 1,   3. output a word boundary if p < Pi or   output nothing otherwise.   Now we have a corpus in the same format as   a standard segmented corpus with variable (nonconstant) segmentation, where xi and xi+1 are   segmented with the probability of Pi.   We execute the above procedure m times and divide the   counts by m. The law of large numbers guarantees   that the approximation errors decrease to 0 when   m \u2192\u221e.   3   Symbol Grounding   As the target of symbol grounding, we use states   (piece positions) of a Shogi game and commen2For a detailed explanation and a mathematical proof of   this method, please refer to Mori and Takuma (2004) .   3This is because an SSC has many words and word fragments. Additionally, word 1-gram frequencies must be calculated using \ufb02oating point numbers instead of integers.   taries associated with them. We should note, however, that our framework is general and applicable to different types of combinations such as image/description pairs (Regneri et al., 2013).   3.1   Game Commentary   The Japanese language is one of the languages   without clear word boundaries and we need an automatic WS as the \ufb01rst step of NLP. In Shogi, there   are many professional players and many commentaries about game states are available.   3.2   Grounding Words   We build a symbol grounding model using a Shogi   commentary dataset. We use a set of pairs of a   Shogi state Si and a commentary sentence Ci as   the training set. A Shogi state Si is converted into   a feature vector f(Si). We generate m (in our experiment, m = 4) pSSC C\u2032   i from Ci. C\u2032   i contains   m corpora of the same text body but with different word segmentation, C\u2032   ij (j = 1, . . . , m). We   treat these as m pairs of a feature vector of Shogi   state f(Si) and a sequence of words C\u2032   ij. We train   a model which predicts words in C\u2032   ij using f(Si)   as input.   We use a multi-layer perceptron as the prediction model. The input is a vector of the features   of a state. The hidden layer is a 100-dimensional   vector and is activated by a bipolar sigmoid function. Its output is a d-dimensional real-valued vector, each of whose elements indicates whether a   word in the vocabulary of d words appears in the   commentary or not. The output layer is activated   by a binary sigmoid function.   We use features of Shogi states which a computer Shogi program called Gekisashi (Tsuruoka   et al., 2002) uses to evaluate the states in game   tree search as input. The features of Shogi states   used in this experiment are below:   a) Positions of pieces (e.g. my rook is at 2h).   2299   b) Pieces captured (e.g.   the opponent has a   bishop).   c) Combinations of a) and b) (e.g. my king is at   7h and the opponent\u2019s rook is at 7b).   d) Other heuristic features.   Among them, a), b) and c) occupy the majority.   Unlike normal symbol grounding, the vocabulary contains many word candidates appearing in   the pSSC generated from the commentaries. Some   are real words and some are wrong fragments.   These wrong fragments will appear more or less   randomly in the commentaries than real words.   The perceptron therefore cannot acquire strong relation between states and fragments and the output   values of the perceptron will be smaller than those   of real words.   4   Word Segmentation Using Symbol   Grounding Result   This section describes a baseline automatic word   segmenter and a method for incorporating the   symbol grounding result to it.   4.1   Baseline Word Segmenter   Among many Japanese WS and morphological analyzers (word segmentation and POS tagging), we   adopt pointwise WS (Neubig et al., 2011), because   it is the only word segmenter which is capable of   adding new words without POS information.   The input of the pointwise WS is an unsegmented character sequence x = x1x2 \u00b7 \u00b7 \u00b7 xk. The   word segmenter decides if there is a word boundary ti = 1 or not ti = 0 by using support vector   machines (SVMs) (Fan et al., 2008). The features   are character n-grams and character type n-grams   (n = 1, 2, 3) around the decision points in a window with a width of 6 characters. Additional features are triggered if character n-grams in the window match with character sequences in the dictionary.   4.2   Training a Word Segmenter with   Grounded Words   As a \ufb01rst trial for incorporating symbol grounding results to an NLP task, we propose to generate a dictionary based on the symbol grounding   result. We can expect that the word candidates   that are given high scores by the perceptron in the   symbol grounding result have strong relationship   to the positions. In other words, we can make a   good dictionary by selecting word candidates in   descending order of the scores. As a method for   Table 1: Corpus speci\ufb01cations.   #sent.   #words   #char.   Training   BCCWJ   56,753   1,324,951   1,911,660   Newspaper   8,164   240,097   361,843   Conversation   11,700   147,809   197,941   Develepment   Shogi-dev.   170   2,501   3,340   Test   BCCWJ-test   6,025   148,929   212,261   Shogi-test   3,299   24,966   32,481   taking all the occurrences into account, we test the   following three functions:   sum: the summation of the scores of all the output vectors,   ave: the average of them,   max: the maximum in them.   First, we acquire a V -dimensional real-valued vector for each Shogi state Si as the result of symbol   grounding. Then, for each candidate in C\u2032   ij, we   get the element of the vector which corresponds to   the candidate as the score of the candidate. After   that, we get the summation of, the average of, or   the maximum in the scores of the same candidate   over the whole dataset.   Finally we select the top R percent of word candidates in descending order of the value of sum,   ave, or max and add them to the WS dictionary   and retrain the model.   5   Evaluation   We conducted word segmentation experiments in   the following settings.   5.1   Corpora   The annotated corpus we used to build the baseline word segmenter is the manually annotated   part (core data) of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa,   2008), plus newspaper articles and daily conversation sentences. We also used a 234,652-word   dictionary (UniDic) provided with the BCCWJ.   A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented sentences randomly obtained from Shogi   commentaries. We divided these sentences into   two parts: a development set and a test set. Table 1 shows the details of these corpora.   To make a pSSC, we prepared 33,151 pairs of   a Shogi position and a commentary sentence. The   2300   Table 2: WS accuracy on BCCWJ.   Recall   Prec.   F-meas.   Baseline   98.99   99.06   99.03   + Sym.Gro.   99.03   99.01   99.02   Table 3: WS accuracy on Shogi commentaries.   Recall   Prec.   F-meas.   Baseline   90.12   91.43   90.77   + Sym.Gro.   90.60   91.66   91.13   sentences are converted into pSSC m = 4 times   by an LR word segmentation model trained from   the training data in Table 1 and sent to the symbol   grounding module.   5.2   Word Segmentation Systems   We built the following two word segmentation   models (Neubig et al., 2011) to evaluate our   framework.   Baseline: The model is trained from training   data shown in Table 1 and UniDic.   +Sym.Gro.: The model is trained from the language resources for the Baseline and the   symbol grounding result.   To decide the function and the value of R for   +Sym.Gro. (see Section 4.2), we measured the   accuracies on the development set of all the combinations.   The best combination was sum and   R = 0.0114. In this case, 127 words were added   to the dictionary.   5.3   Results and Discussion   Following the standard in word segmentation experiments, the evaluation criteria are recall, precision, and F-measure (their harmonic mean).   Table 2 and 3 show WS accuracies on BCCWJtest and Shogi-test, respectively. The difference in   accuracy of the baseline method on BCCWJ-test   and Shogi-test shows that WS of Shogi commentaries is very dif\ufb01cult. Like many other domains,   Shogi commentaries contain many special words   and expressions, which decrease the accuracy.   When we compare the F-measures on Shogitest (Table 3), +Sym.Gro. outperforms Baseline.   The improvement is statistically signi\ufb01cant (at 5%   level). The error reduction ratio is comparable to a   natural annotation case (Liu et al., 2014), despite   the fact that our method is unsupervised except for   4In addition we measured the accuracies on the test set of   all the combinations and found that the same function and the   value of the parameter are the best. This indicates the stability   of the function and the parameter.   a hyperparameter. Thus we can say that WS improvement by symbol grounding is as valuable as   the annotation additions.   From a close look at the comparison of the recall and the precision, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding   successfully acquired new words with a few erroneous words. As the \ufb01nal remark, the result on the   general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain.   6   Related Work   The NLP task we focus on in this paper is word   segmentation. One of the \ufb01rst empirical methods   was based on a hidden Markov model (Nagata,   1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way   (Sproat and Chang, 1996). These methods take   words as the modeling unit.   Recently, Neubig et al. (2011) have presented   a method for directly deciding whether there is a   word boundary or not at each point between characters. For Chinese word segmentation, there are   some attempts at tagging characters with BIES   tags (Xue, 2003) by a sequence labeller such as   CRFs (Lafferty et al., 2001), where B, I, E, and   S means the beginning of a word, intermediate of   a word, the end of a word, and a single character word, respectively. The pointwise WS can be   seen as character tagging with the BI tag system,   in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag   system with SVMs is slightly better than the BIES   tag system with CRFs. This is another reason why   we used the former in this paper. Our extension of   word segmentation is, however, applicable to the   BIES/CRFs combination as well.   The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this   viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013;   Liu et al., 2014) may come to readers\u2019 mind. In   these studies, tags in hyper-texts were regarded   as partial annotations and used to improve WS   performance using CRFs trainable from such data   (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a   large amount of raw text. Murawaki and Kuro2301   hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper   proposes to use other modalities, game states as   the \ufb01rst trial, than languages.   7   Conclusion   We have described an unsupervised method for   improving word segmentation based on symbol   grounding results.   To extract word candidates   from raw sentences, we \ufb01rst segment sentences   stochastically, and then match the word candidate   sequences with game states that are described by   the sentences. Finally, we selected word candidates referring to the grounding scores. The experimental results showed that we can improve word   segmentation by using symbol grounding results.   Our framework is general and it is worth testing   on other NLP tasks. As future work, we will apply   other deep neural network models to our approach.   It is interesting to apply the symbol grounding results to an embedding model-based word segmentation approach (Ma and Hinrichs, 2015). It is also   interesting to extend our method to deal with other   types of non-textual information such as images   and economic indices.   Acknowledgment   We thank the anonymous reviewers for their helpful comments and suggestions.   This work was   supported by JSPS Grants-in-Aid for Scienti\ufb01c   Research Grant Number 26540190.", "conf": "EMNLP", "year": "2015", "index": 68}, {"title": "Generating Descriptions with Grounded and Co-Referenced People   Anna Rohrbach1   Marcus Rohrbach2   Siyu Tang1,3   Seong Joon Oh1   Bernt Schiele1   1Max Planck Institute for Informatics, Saarland Informatics Campus, Germany   2UC Berkeley, CA, United States   3Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany", "abstract": "Learning how to generate descriptions of images or   videos received major interest both in the Computer Vision   and Natural Language Processing communities. While a   few works have proposed to learn a grounding during the   generation process in an unsupervised way (via an attention   mechanism), it remains unclear how good the quality of the   grounding is and whether it bene\ufb01ts the description quality.   In this work we propose a movie description model which   learns to generate description and jointly ground (localize)   the mentioned characters as well as do visual co-reference   resolution between pairs of consecutive sentences/clips. We   also propose to use weak localization supervision through   character mentions provided in movie descriptions to learn   the character grounding. At training time, we \ufb01rst learn   how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised   approach. We then provide this (noisy) supervision into our   description model which greatly improves its performance.   Our proposed description model improves over prior work   w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly   collected grounding and co-reference data for characters.   1.", "content": "When humans talk about what they see, they not only   use common objects and terms, but typically refer to reappearing entities, most commonly using names (\u201cJohn\u201d) and   referential words such as pronouns (\u201che\u201d, \u201cit\u201d). To correctly generate descriptions with reappearing entities, one   needs to understand and link them across sentences and   visual appearances (images/frames). Current image/video   captioning datasets essentially ignore this aspect as they   ask to independently describe each image/clip with a single sentence. At the same time, e.g. visual storytelling [17]   and movie description [37] ultimately require solving this   problem.   However, the \ufb01rst approaches on visual storySophia  talks to Emma   while they fold clothes and watch   Zoe.   This work: She   walks over to Zoe   Someone talks to Someone while they fold clothes and    watch Someone.   Prior work: Someone walks over to Someone.   Current clip   Previous clip   Figure 1: Bring in the color: our task is to generate grounded and   co-referenced descriptions for the current clip using pronouns and   new or reappearing character IDs, which are grounded, i.e. localized in the current clip (boxes and lines) and visually co-referenced   to the previous clip (dashed lines). The visual grounding allows for   co-reference to the previous clip/sentence which enables us using   the pronoun \u201cshe\u201d to refer to the \ufb01rst ID (Sophia).   telling [17] so far have not taken it into account, and current movie description challenges and approaches [35, 48]   abstract from it by looking at a single clip at a time and   replacing all the character mentions with e.g. \u201cSomeone\u201d.   In this work we address grounded co-reference resolution, with application to movie description.   The most   prominent entities in movies are the people or characters.   In fact, there is a long line of work which aims to link   character mentions in movie or TV scripts with their visual   tracks [5, 8, 42, 46, 27, 3, 31]. However, all these works are   already given the description for all movies where they want   to predict the linking. In contrast we want to generate a description, while jointly linking it with the currently and previously depicted character\u2019s visual presence. Speci\ufb01cally,   the task we address in this work is to generate descriptions   for movies and at the same time localize or ground the characters, recognize their gender and refer to them consistently,   i.e. co-reference them across sentences, as visualized in Figure 1. Importantly, rather than trying to obtain consistent ids   in the entire movie, we focus on robust local co-reference   resolution on two consecutive sentences/clips. We argue   that local co-reference resolution is an important problem   on itself. On the one hand there are many characters without proper names and/or with only a few occurrences, which   14979   can and should be resolved locally, e.g. \u201cThe priest takes   their vows. He declares them wife and husband\u201d. On the   other hand, there are many hard decisions which have to be   made locally, e.g. which character to describe and whether a   character should be referenced by proper name or pronoun.   To clarify, we do not generate the true proper names of the   characters, but only identities with gender. We use a prede\ufb01ned set of names in our examples (e.g. Sophia). In future   work we believe the true names could be extracted either   from dialog, or from one/a few annotations per character.   Approaching the joint description and grounding task requires three main ingredients: we need to localize the characters, we need to decide which character(s) to pay attention to, and we need to co-reference visual characters\u2019 appearances in neighboring sentences/clips. In Section 4 we   detail how we approach character localization using head   detection and tracking via a two-stage clustering approach.   While generating the sentence, we advocate to jointly decide which character to pay attention to and if and how   to co-reference it to the previous grounded characters. In   Section 5, we propose to adapt the attention mechanism   [1, 53] for this and extend it to attend jointly over both   problems: grounding (i.e. track selection) and co-reference   (i.e. track linking). A key insight is that this can not be   learned purely from sentence supervision for generation.   Instead, we supervise the joint-attention mechanism with   automatically obtained linking of character mentions and   tracks (Section 5.2). We note that at test time this supervision is not available and the system has learned, how to   jointly ground, co-reference, and describe.   The contributions of our paper include: a) a new task of   movie description with grounded and co-referenced characters; to foster research in this direction we will share our   newly collected co-reference annotations and grounding of   character mentions in the MPII-MD dataset (Section 3); b)   a novel approach which addresses this problem by jointly   learning to ground the described characters and perform local co-reference resolution between the neighboring clips;   c) a robust automatic way of obtaining linking between   character mentions in text and visual tracks in video, which   we use to supervise our description approach and which we   show is essential for the co-reference resolution task.   2. Related Work   Our work aims to do three tasks jointly: generating video   descriptions, grounding, and co-reference resolution. We   review related work in these three directions with a focus   on works which attempt multiple tasks at once. As we focus   on people grounding and co-reference, we also discuss the   related work on person re-identi\ufb01cation and track naming.   Description generation. Generating natural language about   visual content has received large interest since the emergence of recurrent networks. Typically the focus is to generate a single sentence about a single image [7, 19, 26, 50,   53], video [7, 12, 33, 38, 47], or most closely to this work,   movie clip [34, 49]. Several works also produce grounding while generating the description: [53] propose an attention mechanism to ground each word to spatial CNN image   features, [55] extend this to bounding boxes, [54] to video   frames, and [59] to spatial-temporal proposals. [24] look   into evaluating attention correctness for image captioning.   [18] take a different direction and build a model which describes the entire image by jointly predicting large number of bounding boxes and a corresponding short phrase   for each box. [23] parse the visual 3D scene and generate   coherent multi-sentence descriptions where the objects are   grounded in 3D cuboids. Multi-sentence image/video description has also been explored in e.g. [17, 33, 40, 57].   Grounding objects in images/video. Grounding nouns as   well as complex natural language expressions in images   [16, 20, 25, 30, 32, 52, 58] and video [22, 56] has recently   received increased interest. The focus in our work is to localize people in a video while mentioning them in a generated sentence. For example, when mentioning a character   who is jogging in a park, we want to localize this person in   the video. Additionally we are interested in obtaining visual   tracks for character mentions in text, for which rely on the   semi-supervised grounding approach from [32].   Co-reference resolution.   Co-reference resolution is the   task de\ufb01ned in linguistic community [2], where the goal is   to establish correct links between named entities and", "conf": "CVPR", "year": "2017", "index": 58}, {"title": "The Thirty-Fourth AAAI Conference on Arti\ufb01cial Intelligence (AAAI-20)   On Succinct Groundings of HTN Planning Problems   Gregor Behnke, Daniel H\u00a8oller, Alexander Schmid, Pascal Bercher,\u2217Susanne Biundo   Institute of Arti\ufb01cial Intelligence, Ulm University, D-89069 Ulm, Germany   {gregor.behnke, daniel.hoeller, alexander-1.schmid, susanne.biundo}@uni-ulm.de, pascal.bercher@alumni.uni-ulm.de", "abstract": "Both search-based and translation-based planning systems   usually operate on grounded representations of the problem.   Planning models, however, are commonly de\ufb01ned using lifted   description languages. Thus, planning systems usually generate a grounded representation of the lifted model as a preprocessing step. For HTN planning models, only one method   to ground lifted models has been published so far. In this   paper we present a new approach for grounding HTN planning problems that produces smaller groundings in a shorter   timespan than the previously published method.", "content": "Most modelling languages for planning problems (such as   PDDL (McDermott 2000) and HDDL (H\u00a8oller et al. 2020))   allow for specifying planning problems in a lifted fashion,   e.g. by allowing the modeller to specify the model using a   \ufb01rst-order logical language. Actions are speci\ufb01ed with parameters and the action\u2019s preconditions and effects are speci\ufb01ed using literals referring to these parameters. Using such   a lifted representation, a modeller can easily write models   with a large number of action instantiations without the need   to enumerate them explicitly. More importantly, a lifted representation enables the modeller to specify a single planning   domain that can be used in multiple planning problems without any change to the domain.   Unfortunately, to plan directly using only the lifted   model is quite dif\ufb01cult. Most planners transform the lifted   input-model into a grounded model before planning. Planning is then performed on the grounded representation, either via search (H\u00a8oller et al. 2018b; Bercher et al. 2017;   Helmert 2006) or via a translation into other problems, e.g.   SAT (Behnke, H\u00a8oller, and Biundo 2019b; 2019a; 2018b;   Behnke and Biundo 2018; Behnke, H\u00a8oller, and Biundo   2018a; Rintanen 2014; Rintanen, Heljanko, and Niemel\u00a8a   2006). Naively grounding the lifted representation by simply instantiating all its elements is seldom feasible due to   the huge size of the naively grounded model. Instead, the   \u2217Pascal Bercher is now at the College of Engineering and Computer Science, the Australian National University   Copyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial   Intelligence (www.aaai.org). All rights reserved.   grounding procedure aims to remove as many unnecessary instantiations as possible. Unnecessary here means that   grounding of e.g. actions can be removed if they cannot be   part of a solution to the planning problem. Smaller groundings are generally advantageous to planners, as their persearch-node effort or the size of any translation decreases.   Further, even the quality of computed heuristics can improve   if \u201cdistractor actions\u201d are removed. Even a small decrease in   the size of the grounding can signi\ufb01cantly impact the ef\ufb01ciency of the planner. As such, grounding is a critical step in   the process of planning.   For Hierarchical Task Network (HTN) planning (Erol,   Hendler, and Nau 1996), there is \u2013 as far as we know \u2013   only a single paper concerned with grounding HTN planning domains (Ramoul et al. 2017), which is used in the   planner GTOHP. However, several other HTN planners plan   using a grounded model (e.g. FAPE (Dvorak et al. 2014)   and PANDA (Bercher, Keen, and Biundo 2014; Bercher et   al. 2017)), but there are no publications on their grounding   procedures. Lastly, Tree-Rex (Schreiber et al. 2019) uses a   yet unpublished improvement of GTOHP\u2019s grounder.   In this paper we describe the grounding procedure that as   so-far been used by the HTN planner PANDA and a recently   developed improved version of the grounder called pandaPI.   We start by describing the lifted HTN planning formalism,   then give an overview of grounding in planning in general   and in HTN planning in particular. We describe the grounding procedure used by PANDA. Thereafter we present the   new and more systematic view on the HTN grounding process, which allowed us to create the new and highly ef\ufb01cient   grounder pandaPI. Lastly, we compare the performance of   pandaPI with that of PANDA, GTOHP, and Tree-Rex.   Lifted HTN Planning Formalism   Before explaining the HTN grounding procedure, we start   by brie\ufb02y describing the formalism of lifted HTN planning.   We have based our formalism on the lifted one by Alford,   Bercher, and Aha (2015), which in turn is based on the formalism by Geier and Bercher (2011).   Assume that L   =   (P, T, V, C) is a quanti\ufb01er- and   function-free \ufb01rst-order predicate logic with the following   elements. P is a \ufb01nite set of predicate symbols. A predi9775   navigate(?s, ?f)   pay-toll(?s, ?f)   load(?p, ?f)   navigate(?f, ?t)   pay-toll(?f, ?t)   unload(?p, ?t)   Figure 1: A task network in a transportation domain. If performed, it will transport a package from its initial location to   its target location. The variables ?s (start location), ?f (initial package location), and ?t (target package location) are   of type location. The variable ?p is of type package. Parallelism between the pay-toll and navigate tasks models that   the toll can be paid at any time while the transporter is on its   way from the one location to another.   cate\u2019s arity de\ufb01nes its number of parameter variables (taken   from V ), each having a certain type (de\ufb01ned in T). T is a   \ufb01nite set of type symbols. V is a \ufb01nite set of typed variable   symbols to be used by the parameters of the predicates in P.   C is a \ufb01nite set of typed constants.   HTN planning problems specify two types of tasks: Primitive tasks are identical to actions in classical planning and   are identi\ufb01ed via a \ufb01rst-order atom action(?v1, . . . , ?vn)1   (e.g. drive(?f, ?t)). Their semantics is given via their preconditions pre \u2013 a conjunction of positive \ufb01rst-order literals   over L \u2013 and effects eff \u2013 a conjunction of (positive and   negative) \ufb01rst-order literals2. The variables occurring in pre   and eff must be parameters, i.e. members of {?v1, . . . , ?vn}.   Applicability and the state transition semantics of actions is   de\ufb01ned as in classical planning.   Abstract tasks are also described via \ufb01rst-order atoms   task(?v1, . . . , ?vn). Their semantics is given in terms of   pre-de\ufb01ned means for performing them, which are described   by decomposition methods M. A decomposition method   m \u2208M is a tuple (c, tn) consisting of an abstract task c   and a task network tn. A task network is a partially ordered   multi-set of actions and tasks.   De\ufb01nition 1. A task network tn over a set of primitive and   abstract tasks X (\ufb01rst-order atoms) is a tuple (I, \u227a, \u03b1) with:   1. I is a \ufb01nite set of task IDs.   2. \u227ais a strict partial order over I.   3. \u03b1 : I \u2192X \u00d7 V maps task IDs to task and parameters   Note that pandaPI does also allow for specifying variable   constraints, i.e. equals and not-equals constraints on variables and constants. We handle them naively by checking   them and do not discuss them further in the paper for the   sake of brevity. As an example for a task network consider   the one shown in Fig. 1.   An HTN planning problem is then given by the problem\u2019s   initial state sI (a set of ground positive literals or facts), a   set of available decomposition methods M, and the initial   abstract task cI which speci\ufb01es the goal.   1We adopt the convention of PDDL (McDermott 2000) to denote variables with a pre\ufb01xed question mark. I.e. ?a is a variable,   while a denotes a constant.   2More complex representations are common in domain speci\ufb01cation languages, but can be compiled into this simplistic format.   navigate(?f, ?t)   unload(?p, ?t)   navigate(?a, ?c) \u0005\u2192drive(?a, ?b)   navigate(?b, ?c)   drive(?f, ?b)   navigate(?b, ?t)   unload(?p, ?t)   Figure 2: The \ufb01rst row shows a task network tn1. The second row shows a method for the navigate task. The result of   applying this method to the navigate task in tn1 results in   the task network shown in the third row.   The aim in an HTN planning problem is to re\ufb01ne a given   initial abstract task cI into an executable, ground, primitive   task network. A task network is primitive if all tasks in it are   primitive. It is ground if all variables are assigned to constants. It is further executable if there is a linearisation of its   tasks that is executable in the initial state. Re\ufb01ning the initial task network is performed via applying decomposition   methods to the abstract tasks contained in it and the resulting   task networks. Applying a decomposition method (c, tnc) to   a task network tn means to replace an occurrence of the task   c in tn by the contents of the task network tnc.   De\ufb01nition 2. Let m = (c(?x1, . . . , ?xn), tnm) with tnm =   (Im, \u227am, \u03b1m)) be a decomposition method, and tn1 =   (I1, \u227a1, \u03b11) a task network. We assume that Im \u2229I1 = \u2205   and that the sets of variables occurring in tn1 and tnm   are disjunct, which can be achieved by renaming. Then, m   decomposes a task identi\ufb01er i \u2208I1 into a task network   tn2 = (I2, \u227a2, \u03b12) iff \u03b11(i) = c(?y1, . . . , ?yn) and   I2 = (I1 \\ {i}) \u222aIm   \u227a2 = (\u227a1 \u222a\u227am\u222a   {(i1, i2) \u2208I1 \u00d7 Im | (i1, i) \u2208\u227a1} \u222a   {(i1, i2) \u2208Im \u00d7 I1 | (i, i2) \u2208\u227a1})   \\ {(i\u2032, i\u2032\u2032) \u2208I1 \u00d7 I1 | i\u2032 = i or i\u2032\u2032 = i}   \u03b12 = \u03b11 \u222a\\{(i, c(?y1, . . . , ?yn))} \u222a   {(i, c(?z\u2217   1, . . . , ?z\u2217   m)) | (i, c(?z1, . . . , ?zm)) \u2208\u03b1m,   \u2200j :?z\u2217   j =?yk iff ?zj =?xk, and ?z\u2217   j =?zj else}   As an example for applying a decomposition method,   consider the task networks and the method shown in Fig. 2.   Grounding Planning Problems   Both theoretical research (Alford et al. 2016; Behnke et   al. 2016; Bercher et al. 2016; H\u00a8oller et al. 2016; Behnke,   H\u00a8oller, and Biundo 2015; H\u00a8oller et al. 2014) and practical research (Schreiber et al. 2019; H\u00a8oller et al. 2018b;   Behnke et al. 2019) on hierarchical planning is usually   done on grounded, i.e. variable-free models, instead of   lifted models. Especially newer search-based HTN planners like FAPE (Dvorak et al. 2014), GTOHP (Ramoul et   al. 2017), or PANDA (Bercher, Keen, and Biundo 2014;   Bercher et al. 2017) ground a given lifted planning problem   prior to search. A grounded model allows for both a more ef\ufb01cient implementation and for easier to compute and more   9776   concise heuristics. In contrast, the translation technique by   Alford et al. (2016) is executed on the lifted model \u2013 grounding is only performed on the resulting classical model.   In theory, computing a grounded model based on a given   lifted model is easy. One has to compute all possible instantiations of lifted predicates, primitive actions, abstract tasks,   and methods and replace their lifted versions appropriately   by them. For details regarding this full grounding process we   refer to Alford, Bercher, and Aha (2015). Such a grounding   will be exponential in size. Thus, a fully grounded model   is not useful in many practical cases, as handling it within   given memory and time limits is hard or even impossible.   In many planning problems, computing all instantiations   of all predicates, tasks, and methods is not necessary. For example, it is not necessary to create a grounding drive(l1, l2)   of the drive action if there is no road between the locations   l1 and l2. For such an instantiation drive(l1, l2), we know   a priori that its precondition can never be ful\ufb01lled3. Thus   this action cannot be part of any plan. Ideally, we would like   to compute only those groundings of predicates, tasks, and   methods that occur in some solution to the planning problem. Determining whether this is the case is undecidable.   Theorem 1. The problem of deciding whether an action a   is part of some plan for a given HTN planning problem is   undecidable.   Proof. We can show the claim via a reduction from the   plan existence problem for HTN planning problems, which   is known to be undecidable (Erol, Hendler, and Nau 1996;   Geier and Bercher 2011). Let P be an HTN planning problem. We add a primitive action a without any precondition   and effects. Further, we add a new initial abstract task c\u2217   I,   which has only a single decomposition method yielding one   instance of a and the old initial abstract task cI of P without   any ordering constraint. Now, P has a solution if and only if   a is part of some plan for P.   Instead, we aim at computing an approximation of this   property. We are looking for a subset of all ground instances   of predicates, tasks, and methods such that all ground instances not included in that set are not contained in any solution. I.e., we disregard a ground instance if we can prove   that it cannot be contained in a solution. This approximation   entails a trade-off: with higher computation time, a better   approximation might be found.   This technique of approximate grounding is widely used   in classical planning. In general, an action is not included in   the grounding if it cannot be part of any executable plan in   the delete-relaxation of the problem. The delete-relaxation   of a planning problem is a copy of the problem in which all   negative effect literals are removed. Any action that is not   part of a plan in a delete-relaxed problem cannot be part of a   plan in the original problem. For a given action one can determine in polynomial time whether it is part of any deleterelaxed classical plan (Bylander 1994). The set of these actions is usually computed via a planning graph (Blum and   Furst 1997). Often, this reduction leads to a signi\ufb01cant decrease in the size of the grounded problem. Some planning   3Assuming that there is no means to build new roads.   systems, like FF (Hoffmann and Nebel 2001), \ufb01rst compute the full grounding and subsequently prune actions4.   This, however, does not eliminate the bottle-neck of grounding, but makes the grounding smaller for the planning process itself. An ef\ufb01cient implementation based on DATALOG   was proposed by Helmert (2009), which does not have this   bottle-neck of a full instantiation.   To the best of our knowledge there is currently only one   publication in the \ufb01eld of HTN planning devoted to grounding, which is the grounder of GTOHP (Ramoul et al. 2017).   It uses a grounding procedure similar to that of FF \u2013 i.e. it   computes a full instantiaton and prunes subsequently \u2013 and   similarly uses the concept of inertia to prune tasks early during instantiation (Koehler and Hoffmann 2000). Inertia of a   predicate describe the ways its truth value can change while   a plan is executed \u2013 not at all, only from negative to positive   (or vice versa), or in both directions. In inertia-based simpli\ufb01cation, a primitive task whose precondition evaluates to   false under the computed inertia values is removed from the   planning problem, as it can never become executable. Subsequently, all methods it is contained in are removed as well.   If an abstract task has no applicable method remaining it is   likewise removed.   The recently published planner Tree-Rex (Schreiber et al.   2019) uses GTOHP\u2019s grounding procedure but also prunes   abstract tasks that cannot be re\ufb01ned into primitive actions   any more \u2013 even though they have applicable decomposition   methods. This leads to generally smaller groundings.   Note that GTOHP removes effectless actions from the   methods they are contained in (Ramoul et al. 2017). The respective methods are not pruned afterwards, but considered   part of the correct grounding without the removed effectless actions. According to the formalisation of HTN planning, these actions can however be contained in plans \u2013 and   pose constraints in them. As such it makes any found solution (potentially) invalid as it may not adhere to the solution   criteria of HTN planning. Secondly, the implementation of   GTOHP does not allow for two parameters of an action or   method to be instantiated with the same constant. Consider   as an example a method that paints two wooden boards ?b1   and ?b2 in colours ?c1 and ?c2. GTOHP enforces that ?c1   and ?c2 are different without this constraint being a part of   the domain. This leads to an incomplete grounding, this time   when the (only) solution uses the method where both colours   are, e.g. red, as we only have red paint. For our evaluation,   we have \ufb01xed both issues in the code of GTOHP.   PANDA\u2019s Grounding   The HTN planning system PANDA has used a grounded   representation for planning since 2014. All techniques that   were published and evaluated using PANDA (e.g. (Behnke,   H\u00a8oller, and Biundo 2019a; 2019b; H\u00a8oller et al. 2018b;   H\u00a8oller et al. 2018a; 2018c; Bercher et al. 2017; Behnke,   H\u00a8oller, and Biundo 2017; Bercher, Keen, and Biundo 2014))   based upon a grounded model, while the benchmark in4Note that FF uses the concept of inertia (Koehler and Hoffmann 2000) to simplify the preconditions and effects before full   grounding.   9777   stances used in evaluations of PANDA are de\ufb01ned using the   lifted language HDDL (H\u00a8oller et al. 2020). Most of the techniques developed for PANDA (and pandaPI) originate in the   need to solve speci\ufb01c planning domains. Notably, the need   to handle the Monroe domain (Blaylock and Allen 2005),   which is plan recognition domain, lead to the development   of most of PANDA\u2019s grounding algorithm.   So far, we have not published any description of PANDA\u2019s   grounding process. We correct this shortcoming with this   paper and describe how the grounding procedure that has   been used in the previous evaluations of PANDA works. In   this section, we will give a short overview of the grounding   techniques used by PANDA. The grounding procedure used   by PANDA still had its shortcomings and problems. Thus,   we have developed a new grounder: pandaPI. It is based on   a new and more generic view on PANDA\u2019s grounding procedure. This enables us to signi\ufb01cantly improve the performance compared PANDA\u2019s grounder.   PANDA\u2019s grounding procedure comprises three steps: a   lifted domain simpli\ufb01cation, a delete-relaxed reachability   analysis, and a hierarchical reachability analysis based on   the Task Decomposition Graph (TDG) (Elkawkagy et al.   2012; Bercher et al. 2017). The second and third step are executed once on the lifted input domain in order to ground   the domain. Thereafter they are repeated in order to reduce the size of the grounding even further. Algorithm 1   shows the overall mechanism of PANDA\u2019s grounder. Iterating the reachability analysis is one of the major features of   PANDA\u2019s grounder, as it enables \ufb01nding smaller and more   concise groundings. Note that this is also the main difference between the grounding results found by PANDA and   Tree-Rex\u2019s improved grounding (Schreiber et al. 2019). It is   based on the following observation: The TDG-based analysis can remove groundings of primitive actions, as they are   not reachable through the hierarchy any more. Such an action in turn might have been the only one that enables the   execution of other \u2013 yet unremoved \u2013 actions in the planning graph. Removing them might cause grounded decomposition methods to be removed. This again, might remove   other primitive actions.   As an example, consider the methods and primitive tasks   shown in Fig. 3. If we assume that the initial state is empty,   then only the actions a, c and d are delete-relaxed reachable.   Thus, the second method B \u0005\u2192a, b is not reachable as it   contains a task \u2013 b \u2013 that can never become executable. It   will thus be pruned from the grounded model. The remaining methods however still remain in the model after the TDG   has been computed. One however notices that removing the   method B \u0005\u2192a, b also removes the only possibility to obtain the action a via decomposition. It can thus be removed   from consideration, i.e. only c and d remain in the grounding. This is where GTOHP and Tree-Rex stop their analysis.   PANDA runs an additional delete-relaxed reachability analysis after a and b are pruned. Now, d is not reachable any   more, since it is not possible to achieve its precondition x   any more. Thus d can be pruned as well and consequently all   remaining methods, as well as c. This pruning might again   remove other actions from consideration and has thus to be   repeated until convergence to achieve a minimal model \u2013   A \u0005\u2192B, C   B \u0005\u2192a, b   B \u0005\u2192c   C \u0005\u2192d   pre   a : {}   b : {z}   c : {}   d : {x}   add   {x}   {}   {y}   {}   del   {}   {}   {}   {}   Figure 3: Sample Decomposition Methods on the left, and   four actions with their preconditions and effects on the right.   which PANDA does.   Data: lifted HTN planning problem P   P = lifted domain simpli\ufb01cations(P);   Pg = planning graph(P);   (Ag, Mg, Pg) = tdg(P,Pg);   Pg = (Pg, Ag, Mg);   while true do   Pg = planning graph(Pg);   (Ag, Mg, Pg) = tdg(Pg,Pg);   P\u2217   g = (Pg, Ag, Mg);   if Pg = P\u2217   g then   break   end   end   Algorithm 1: PANDA\u2019s grounding procedure \u2013 Overview.   Variables are: Pg \u2013 grounded primitive actions, Ag \u2013   grounded abstract tasks, Mg \u2013 grounded methods   Next we will describe the individual steps of PANDA\u2019s   grounder.   Parameter Splitting   As a \ufb01rst step, PANDA performs simpli\ufb01cation operations   on the lifted model. For example, we compile disjunctions   in preconditions and conditional effects into additional actions, and compile away negative preconditions. Similarly,   we compile away variables occurring in preconditions and   effects (i.e. those that are contained in quanti\ufb01ed expressions) into additional parameters.   Besides these common simpli\ufb01cations, PANDA performs   an HTN-speci\ufb01c simpli\ufb01cation operation on the lifted model   with the aim of reducing the size of the grounding. In some   HTN planning domains, lifted decomposition methods contain variables that are (1) used only as parameters of a single subtask and (2) which are not parameters of the abstract task. As an example, consider an abstract task A(?x)   with a method decomposing it into the tasks B(?x, ?y) and   C(?x, ?z). Further assume that all variables have the same   type t which contains the constants {c1, . . . , cn}. If we   ground this method, it has up to n3 ground instances. Notably, we have to ground every possible combination of the   otherwise independent parameters ?y and ?z.   We can equivalently represent this method by three new   methods while introducing two new abstract tasks. Let these   abstract tasks be B\u2217(?x) and C\u2217(?x). The three decompo9778   sition methods are A(?x) \u0005\u2192B\u2217(?x), C\u2217(?x)5, B\u2217(?x) \u0005\u2192   B(?x, ?y), and C\u2217(?x) \u0005\u2192C(?x, ?z). For these three methods, there are at most 2n2 + n groundings plus an additional   2n new groundings of abstract tasks (B\u2217and C\u2217), which is   a signi\ufb01cant improvement over the original model.   This splitting technique is related to the splitting techniques for action schemas presented by Areces et al. (2014).   Their splitting technique also splits lifted actions into multiple ones in order to reduce the number of necessary groundings. They however needed to introduce a mechanism to \u201ccoordinate\u201d common parameters between the split actions \u2013   i.e. additional state variables (PROCNONE, DO, and PAR(.)).   Contrary to their work, pandaPI\u2019s splitting has an automatic   way to \u201ccoordinate\u201d the values of common parameters of   split actions: the methods. But in a sense, their technique   is a generalisation of ours as we only allow the split if   no coordination between the \u201chidden\u201d variables is needed.   Thus adapting their technique might enable more aggressive   splits, which is however future work.   Delete-Relaxed Reachability   After the initial simpli\ufb01cation of the domain, PANDA performs a delete-relaxed reachability analysis to determine   which groundings of primitive tasks can possibly occur in   any executable plan. PANDA\u2019s implementation is based on   the ef\ufb01cient planning graph implementation of STAN (Long   and Fox 1999). It is succinct in the sense that it never considers groundings that are not delete-relaxed reachable, similar   to the DATALOG-based implementation by Helmert (2009).   TDG-based Hierarchical Reachability   PANDA\u2019s hierarchical reachability analysis is based on a   data-structure called the Task Decomposition Graph (TDG).   It was \ufb01rst introduced by Bercher, Keen, and Biundo (2014)   and later re\ufb01ned (Bercher et al. 2017). On the one hand, the   TDG is designed to compute all groundings of abstract tasks   and methods that are reachable from the initial abstract task.   Only those groundings can ever occur during the planning   process, as any other grounding can never be obtained via   decomposition as required by the solution criteria of HTN   planning. On the other hand, the TDG also removes abstract   tasks (and connected methods) which cannot be decomposed   into a primitive plan. If this is not possible, a task network   containing such a task can never be re\ufb01ned into a solution,   as this solution must only contain primitive actions. A TDG   is a directed graph. Nodes represent either ground tasks or   ground methods. A task node has outgoing edges to each   applicable ground method, and each method has outgoing   edges to its ground subtasks. This means the graph is a representation of hierarchical reachability, i.e. which ground tasks   and methods can possibly be reached via decomposition.   De\ufb01nition 3. Let P be an HTN planning problem. The bipartite graph G = \u27e8VT , VM, ET \u2192M, EM\u2192T \u27e9, consisting of   a set of task vertices VT , method vertices VM, and edges   ET \u2192M and EM\u2192T is called the TDG of P if it holds:   1. Base Case (task vertex for the given task)   cI \u2208VT , the TDG\u2019s root.   5To remain correct, B\u2217and C\u2217have the order of B and C.   2. Method Vertices (derived from task vertices)   Let c \u2208VT and there is a method (c, tn) \u2208M. Then, for   all groundings vm it holds that:   \u2022 vm \u2208VM   \u2022 (vt, vm) \u2208ET \u2192M.   3. Task Vertices (derived from method vertices)   Let vm \u2208VM with vm = (c, tn) and tn = (I, \u227a, \u03b1).   Then, for all tasks i \u2208I with \u03b1(i) = vt the following   holds:   \u2022 vt \u2208VT   \u2022 (vm, vt) \u2208EM\u2192T .   4. Tightness G is minimal, such that 1. to 3. hold.   Note that the TDG can represent HTN planning problems   that contain cyclic methods. A cyclic decomposition is a sequence of decompositions of a grounded task c that results   in a task network containing c again. If the planning problem   contains such a cycle, the edge representing the method that   produces the recursive occurrence of c simply points back to   the vertex created for the \ufb01rst occurrence of c.   TDGs constructed based on the de\ufb01nition contain only   those groundings reachable from the initial task by decomposition. As proposed by Elkawkagy, Schattenberg, and Biundo (2010) one can delete those method nodes that contain a primitive task not reachable in a state-based reachability analysis like the planning graph. As a consequence   of removing those methods, there may be abstract tasks in   the TDG that cannot be decomposed into a task network   containing only primitive actions any more. For example,   removing a method containing a not delete-relaxed reachable action might remove the only option to exit a recursive   method structure. If such an abstract task occurs in a task   network during decomposition, we know that it is impossible to re\ufb01ne that task network into a solution. We can thus   prune the abstract task \u2013 and consequently all methods it   is contained it. Removing these methods may again allow   us to remove other abstract tasks, thus one can repeat this   process until convergence (Def. 4, Nr. 2b). These tasks can   be identi\ufb01ed in polynomial time by relying on a bottom-up   reachability analysis (Alford et al. 2014, proof of Thm. 3.1).   To capture this pruning, we parametrize the previous definition of a TDG by specifying an additional set of primitive   ground tasks: these are the actions that are potentially executable, i.e. those tasks that resulted from the delete-relaxed   reachability analysis.   De\ufb01nition 4. Let P be an HTN planning problem and G =   \u27e8VT , VM, ET \u2192M, EM\u2192T \u27e9the respective TDG according to   Def. 3. Let X be the set of executable ground actions.   Then, the pruned TDG GX = \u27e8V \u2032   T , V \u2032   M, E\u2032   T \u2192M, E\u2032   M\u2192T \u27e9   is given as the minimal connected subgraph containing cI   such that:   1. Remove Useless Method Vertices   A method vertex vm = (c, tn) \u2208VM with tn = (I, \u227a, \u03b1)   is in V \u2032   M if and only if I does not contain a task i with   \u03b1(i) \u0338\u2208X in case \u03b1(i) is primitive or with \u03b1(i) being   useless, in case it is abstract (see below).   2. Identify Useless Abstract Task Vertices   An abstract task vertex vt \u2208V \u2032   T is called useless if one of   the following holds:   9779   A   m1   B   m2   C   m3   m4   a   b   c   Figure 4: Sample Task Decomposition Tree.   (a) the pruned TDG GX does not contain children for vt   (i.e., all successors of vt were pruned)   (b) there is no acyclic connected subgraph of the pruned   TDG GX with root vt, in which every abstract method   vertex has exactly one outgoing edge and no vertex is   useless (i.e., the task vt cannot be decomposed into a   set of primitive tasks)   As an example for pruning the grounded domain using the   TDG, consider the TDG depicted in Fig. 4. It contains three   abstract tasks A, B, and C, three primitive ones a, b, and   c, as well as four methods. If all primitives are reachable,   no task or method will be pruned. Now consider, as the \ufb01rst   case, that the primitive task c is not executable, i.e. that the   set of executable ground actions is X = {a, b}. Then the   method m2 will be removed via condition 1. of Def. 4. Since   m2 has been pruned, B will be pruned as well, as it has   no children (condition 2. (a)). This removal cascades and   also removes m1, A, and m3. Note that b is not removed   by Def. 4, but can be removed as it is not contained in any   method any more. Thus only C, m4 and a remain.   As a second example, consider the case where a is not   executable, i.e. X = {b, c}. Here the method m4 is immediately removed via condition 1. Now, each of the remaining   tasks has still applicable methods. It is however not possible   to \u201cescape\u201d the cycle formed by the abstract tasks A, B, and   C. Thus all three can be pruned from the TDG according to   condition 2. (b).   pandaPI\u2019s Grounding Procedure   Recent work (Wichlacz, Torralba, and Hoffmann 2019) has   shown that PANDA\u2019s grounding procedure can be extremely   slow on some larger, practical planning instances. Further   the analysis has shown that the runtime performance of   PANDA\u2019s grounder scales extremely badly with increasing   size of the input problem.   We have thus developed a new grounder, pandaPI, which   we designed to fully replace PANDA\u2019s current grounder.   The code of pandaPI is publicly available at https://github.   com/galvusdamor/pandaPIgrounder. pandaPI and PANDA\u2019s   grounder differ in three important aspects:   1. pandaPI treats delete-relaxed reachability and TDGbased reachability uniformly   2. pandaPI uses more ef\ufb01cient code for pruning once the   domain has been grounded, while PANDA always used   the clunky code necessary for a (potentially) lifted domain   3. pandaPI is written in C++, while PANDA is written in   Scala   As we will describe in this section, the way in which the   planning graph computes the delete-relaxed reachability of a   classical planning problem and the computation of a pruned   TDG bear striking similarities. We will generalise both procedures into a common framework, the Generalised Planning Graph (GPG). Using it, pandaPI computes both the   delete-relaxed reachability and the TDG-based reachability   with exactly the same code. Thus both parts of the grounding   procedure bene\ufb01t from improvements to the GPG.   Planning Graph vs TDG   The objective of the planning graph is to determine, based   on a given initial state sI, which actions can possibly be applied in any state that is delete-relaxed reachable from sI.   The planning graph maintains a set of facts F that can potentially become true, which is initialised with sI. For that,   it repeatedly checks whether the preconditions of any lifted   action can be ful\ufb01lled with the facts in F. If so, that action is instantiated accordingly, and its instantiated effects   are added to F. If no new action becomes applicable, the   computation stops.   For constructing the TDG, there are two opposing methods. One option is to start grounding with the initial abstract   task and ground the planning problem with a depth-\ufb01rstsearch-like procedure. In it, the planner essentially applies   rules 2. and 3. of Def. 3 until convergence. If this TDG has   been constructed, we can use a marker algorithm to perform   the pruning according to Def. 4. We start by marking all   primitive actions that were reached in the planning graph.   Then we recursively mark all methods for which all subtasks are marked and the abstract tasks of a all marked methods. After convergence, we remove all non-marked tasks and   methods. This construction, however, can be problematic   as it will frequently construct groundings of abstract tasks   and methods that will be pruned afterwards \u2013 based on the   delete-relaxed reachability analysis which was performed   beforehand. The second way to construct the pruned TDG   tries to mitigate this problem by only constructing grounded   instances of methods and abstract tasks if they will not be   pruned due to Def. 4. For that, one instantiates methods and   abstract tasks only if the marker algorithm to compute decomposable abstract tasks would mark them as well. The   algorithm maintains a set A of tasks (both primitive and abstract) that will not be removed by the pruning according to   Def. 4. We start with setting A to the set of all actions that   are delete-relaxed reachable, i.e. to the result of the planning graph. Then, one repeatedly checks whether there is a   (lifted) decomposition method for which all subtasks have   ground instances in A. If so, the method is instantiated and   the grounding of its abstract task is added to A. This process is repeated until no new methods can be instantiated.   This instantiation process might \u2013 on the down side \u2013 generate instantiations that cannot be reached from the initial   abstract task. We therefore perform a depth-\ufb01rst search to   discard all such instantiations. As this search is performed   on an already grounded model, it is quite fast.   Like the \ufb01rst method to compute the TDG, the second   can compute unnecessary groundings, namely those that will   be removed by the depth-\ufb01rst search. In order to keep these   9780   PG   TDG   Instances   facts   tasks   Operators   actions   methods   Antecedants   preconditions   subtasks   Consequents   add effects   abstract task   Table 1: PG vs TDG   tasks to a minimum, we perform a relaxed depth-\ufb01rst searchstyle propagation of types and constraints through the hierarchy, prior to the actual instantiation process. We determine   for every parameter variable of every method, task, and action which constants can be assigned to it \u2013 which is often   far less than the type of the variables allows. During the instantiation phase, we then disregard all instances which do   not comply with the computed possible set of parameters.   Generalised Planning Graph   The computations performed by the planning graph (PG)   and the TDG described in the previous section are extremely   similar in nature. Both maintain a current set of instances   with which the preconditions (or antecedants) of lifted operators are ful\ufb01lled. If so, the operator is added to the set   of reachable groundings and its effects (or consequents) are   added to the set of instances. This process is repeated until no new ground instantiation of a lifted operator can be   generated. We show a comparison of the two algorithms in   Tab. 1. The only difference between the two algorithms is   that the TDG will perform a depth-\ufb01rst search for pruning   after grounding. Note that such a pruning step could also be   performed for the PG. Here we would remove all ground   actions that only produce effects that have no connection to   any goal fact, i.e. which can never contribute to reaching the   goal. This step is often not performed, as actions are seldom   pruned based on this criterion.   These similarities between the PG and TDG have caused   us to develop a common algorithm \u2013 the Generalised Planning Graph (GPG) \u2013 which pandaPI uses to perform both   the PG- and the TDG-based grounding. It is essentially an   implementation of the standard planning graph algorithm   which uses an generic interface to also operate on the input data structures of the TDG. We have opted not to use   a DATALOG-based implementation, which is used by Fast   Downward (Helmert 2009), as this enables us to add optimisations into the planning graph algorithm that are speci\ufb01c   to planning problems. We process new instances (facts and   ground tasks, respectively) in a manner s.t. any ground instantiation is only considered once. We split the set of instances I into two: a set of already processed instances Ip   and a set of unprocessed instances Iu. We repeatedly remove   instances i from Iu and add them to Ip. pandaPI then considers all lifted operators which have an antecedant which   can be ful\ufb01lled with i. We match the remaining antecedants   recursively with instances from Iu. This way, we never consider the same ground instances of a lifted operator twice.   We use a caching strategy to \ufb01nd matching antecedants   quickly. For that we process antecedants always in the same   \ufb01xed order. pandaPI constructs for every type of operator a table with is indexed with the antecedants that is to   be matched. The table contains the possible instances that   can be matched to the antecedant. For every antecedant, we   know which parameter variables have been set by matching   the prior antecedants. And thus, we know which of the variable will already have constants assigned to them. pandaPI\u2019s   table maps the possible instantiations of these variables to   the instances that they are compatible with. By construction,   all these lists are disjunct. Whenever pandaPI needs to \ufb01nd   all matching instances for an antecedant, it can access this   table and obtain only the instances that agree on the previously set variables. pandaPI actually does not have to check   whether the instance is compatible with the already assigned   variables, as it will be by construction. We can maintain this   table with little overhead and update it whenever a new instance is added to Ip.   As an example, consider the operator a(?u, ?v, ?w, ?x)   with the following antecedants:   1. p(?v, ?w)   2. p(?w, ?x)   3. q(?v, ?x, ?u)   If we are looking to instances for the third antecedant, we   will have already matched \u2013 by construction of pandaPI\u2019s   algorithm \u2013 the two prior antecedants. Thus values for ?v,   ?w, and ?x will already be known. For the third antecedant,   only two variables ?v and ?x are relevant. Thus, we have a   table mapping assignments of these variables to constants.   For example, the entry for [?v = c1, ?x = c2] might contain the instances q(c1, c2, c3) and q(c1, c2, c2), but not e.g.   q(c1, c1, c1) \u2013 as this instance does not agree with the values   of ?v and ?x.   Lastly, we also use a caching procedure to determine   whether yet unmatched, i.e. future, antecedants will have   no matching instance based on the currently assigned variables. Thus, pandaPI might be able to determine after \ufb01xing   an instance for the \ufb01rst antecedant, that the third antecedant   does not have a matching instance any more. pandaPI will   immediately backtrack in this case and thus save the computation time needed for a potential exhaustive search until   antecedant three.   pandaPI uses the same general algorithm (Alg. 1) for   grounding as PANDA. I.e. we start with lifted domain simpli\ufb01cations and parameter splitting6, then continue with   computing the planning graph and the TDG using the Generalised Planning Graph. The resulting domain is ground.   We continue checking delete-relaxed reachability and TDG   pruning until no further actions, tasks, and methods can be   removed using a specialised, ef\ufb01cient implementation which   operates on the grounded problem.   Evaluation   In order to ascertain the quality of the grounding found by   pandaPI and its runtime performance when doing so, we   have conducted an empirical evaluation. We have compared   pandaPI against three other grounders for HTN planning   6Note that PANDA\u2019s parameter splitting sometimes did not split   parameters, due to a bug in the implementation. Hence the size of   the computed groundings can differ slightly.   9781   Time for GTOHP in milliseconds   Time for pandaPI in miliseconds   100   100   101   101   102   102   103   103   104   104   105   105   106   106   ERR   ERR   (a) pandaPI vs. GTOHP   Time for Tree-REX in milliseconds   100   100   101   101   102   102   103   103   104   104   105   105   106   106   ERR   ERR   (b) pandaPI vs. Tree-REX   Time for PANDA in milliseconds   100   100   101   101   102   102   103   103   104   104   105   105   106   106   ERR   ERR   (c) pandaPI vs. PANDA   Figure 5: Runtime of pandaPI vs three other grounders. Time is measured in milliseconds. Scales are logarithmic. Domains are   colour-coded, the legend is contained in Figure 6.   Time   better   equal   worse   min%   max%   min   max   avg.%   PANDA   330   0   0   94.56   99.72   1517   1172525   98.55   GTOHP   198   0   4   -74.53   99.59   -5488   34963   80.17   Tree-Rex   200   0   2   -28.81   99.67   -2874   43119   83.39   Primitive Tasks   better   equal   worse   min%   max%   min   max   avg.%   PANDA   0   330   0   0.00   0.00   0   0   0.00   GTOHP   202   0   0   0.01   99.97   1   676891   50.62   Tree-Rex   96   101   5   -25.00   90.64   -31   205913   26.06   Abstract Tasks   better   equal   worse   min%   max%   min   max   avg.%   PANDA   132   97   101   -47.43   93.48   -490   251   6.86   GTOHP   142   0   60   -68.14   99.11   -635   145016   43.46   Tree-Rex   108   3   91   -129.30   98.17   -871   11945   16.40   Methods   better   equal   worse   min%   max%   min   max   avg.%   PANDA   132   97   101   -73.25   92.86   -17745   251   6.01   GTOHP   182   0   20   -0.01   99.91   -1   1049451   65.21   Tree-Rex   148   3   51   -5.00   98.15   -1   232082   36.19   Table 2: Comparison of time and grounding sizes. The better, equal, and worse columns indicate in how many instances pandaPI was better, equal, or worse than the denoted   planner. Min and Max refer to the minimum and maximum   improvement that pandaPI makes over all instances.   BARMAN   BLOCKSWORLD   CHILDSNACK   DEPOTS   ENTERTAINMENT   GRIPPER   HIKING   MINECRAFT-AREA   MINECRAFT-NORMAL   PCP   ROVER-PANDA   ROVER   SATELLITE   SMARTPHONE   TRANSPORT   UM-TRANSLOG   WOODWORKING   Figure 6: Colour-codes for the individual domains.   problems: PANDA\u2019s grounder, GTOHP\u2019s grounder (Ramoul   et al. 2017), and Tree-Rex\u2019s grounder (Schreiber et al. 2019).   We have not compared against FAPE (Dvorak et al. 2014) as   it cannot handle HTN planning problems containing recursion \u2013 while most benchmark instances contain recursion.   The benchmark set comprises 330 problem instances   from 17 domains. 8 domains (BARMAN, BLOCKSWORLD,   CHILDSNACK,   DEPOTS,   GRIPPER,   HIKING,   ROVERGTOHP,   SATELLITE-GTOHP)   with   160   instances   stem from the evaluations of GTOHP and Tree-Rex. 2   domains   (MINECRAFT-AREA,   MINECRAFT-NORMAL)   with 26 instances stem from Wichlacz, Torralba, and   Hoffmann   (2019).   The   remaining   7   domains   (UMTRANSLOG,   SATELLITE-PANDA,   WOORWORKING,   SMARTPHONE,   PCP,   ENTERTAINMENT,   ROVERPANDA,   TRANSPORT)   with   144   instances   stem   from the evaluations of PANDA (H\u00a8oller et al. 2018b;   Behnke, H\u00a8oller, and Biundo 2019a). All instance are available for download at pandapi.hierarchical-task.net/domains.   Since GTOHP and Tree-Rex can only handle totallyordered HTN planning problems, both were executed only   on those 228 instances of the benchmark set that are totallyordered, i.e. their own 8 domains plus both MINECRAFT   domains, TRANSPORT, and ENTERTAINMENT. All experiments were conducted on an Intel Xeon E5-2660 with a 20   GB RAM limit for each instance. We set no time-limit.   pandaPI was able to ground all 330 problem instances   with a maximum runtime of 38.166 seconds. The same is   true for PANDA, whose runtime reached up to 20 minutes   and 10 seconds. GTOHP was able to ground 202 of the 228   instances given to it with a maximum runtime of 41.609 seconds. It failed to ground all 26 MINECRAFT instances by exceeding the memory limit after at least 11 minutes of computation time per instance. Tree-Rex \u2013 as it uses GTOHP   as its \ufb01rst step \u2013 also grounds 202 instances and fails on   MINECRAFT with a maximum runtime of 58.935 seconds.   A per-instance comparison of the runtime between pandaPI, PANDA, GTOHP, and Tree-Rex is shown in Fig. 5. A   statistical summary of timings and grounding sizes is shown   9782   in Tab. 2. For PANDA and pandaPI we do not count the actions representing grounded method preconditions. GTOHP   and Tree-Rex consider them as part of grounded methods, thus counting them separately for PANDA and pandaPI would skew the results signi\ufb01cantly \u2013 as this is simply a question of where to store these the method precondition. Notably, there are only two instances where pandaPI   is slower than GTOHP, and four where it is slower than   Tree-Rex, and it is always faster than PANDA. The instances   where pandaPI is slower than GTOHP and Tree-Rex all stem   from the SATELLITE-GTOHP domain and the difference is   at most 5.488 seconds.   Further, pandaPI creates a smaller grounding than   GTOHP in all instances, and in 96 instances compared to   Tree-Rex. The \ufb01ve instances in which it produces more   primitive tasks stem from the ENTERTAINMENT domain.   Here, pandaPI compiles conditional effects into multiple actions, while GTOHP and Tree-Rex can handle conditional   effects natively. When considering abstract tasks and methods, pandaPI often generates smaller groundings, but sometimes also generates larger groundings with respect to the   number of abstract tasks. This is due to our parameter splitting which may introduce additional abstract tasks. If it is   turned off (see supplemental material), our grounding is   never worse than that of GTOHP. Note that in those 20 /   51 instances in which pandaPI produces more methods, it   generates only one additional method. This is due to a compilation performed by pandaPI in case the domain speci\ufb01es   multiple initial abstract tasks. If so, pandaPI added a new arti\ufb01cial initial abstract task and a method decomposing it into   the speci\ufb01ed tasks.   pandaPI generates on average a signi\ufb01cantly smaller   grounding in a signi\ufb01cantly smaller timeframe than the previous grounders of PANDA, GTOHP, and Tree-Rex. Especially, we think that we have overcome with pandaPI the   current problems of grounding in HTN planning as shown   by Wichlacz, Torralba, and Hoffmann (2019).   We\u2019ve lastly compared pandaPI\u2019s implementation of the   GPG with the currently state-of-the-art DATALOG-based   implementation in Fast Downward\u2019s grounder (Helmert   2009). We have considered those instances that were identi\ufb01ed as the most time-consuming to ground for Fast Downward (Helmert 2009) as reported by Helmert. These instances stem either from the domains SATELLITE or PSR.   Since pandaPI does not support derived predicates \u2013 which   are an essential part of the modelling of PSR \u2013 we have compared our implementation of the planning graph against Fast   Downward\u2019s DATALOG-based implementation on those   SATELLITE instances which they reported as problematic.   The results are shown in Tab. 3 and show that pandaPI is   signi\ufb01cantly faster than Fast Downward on these instances.   Conclusion   Most recent systems in HTN planning realise the planning   process in a fully grounded way. A smaller grounding usually improves the performance of the planner. For example,   a smaller grounding allows for heuristics to be computed   faster \u2013 and for them to be more precise. Further, the search   FD   pandaPI   FD   pandaPI   #31   15.410s   0.588s   #32   25.840s   1.178s   #33   38.765s   1.339s   #34   9.000s   0.338s   #35   13.590s   0.511s   #36   17.200s   0.620s   Table 3: Time to ground instances of the classical SATELLITE domain for Fast Downward (FD) vs pandaPI   mechanics of the planner is faster the smaller the grounding is, as fewer actions and methods have to be considered.   Lastly, a smaller grounding also reduces the size of encodings, e.g. into propositional logic, which makes the translated problem (potentially) easier to solve. Despite these advantages, little work has been published on grounding techniques especially for HTN planning. We presented grounding procedure of PANDA and pandaPI and discuss how to   compute them ef\ufb01ciently. Our empirical evaluation shows   that it leads to smaller groundings than other grounder and   uses less time to compute it.   Acknowledgements   This work was partially funded by the technology transfer project \u201cDo it yourself, but not alone: CompanionTechnology for DIY support\u201d of the SFB/TRR 62 funded   by the German Research Foundation (DFG). The industrial project partner is the Corporate Research Sector of the   Robert Bosch GmbH.", "conf": "AAAI", "year": "2020", "index": 389}, {"title": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding   Zoey Guo1,2*   Yiwen Tang1*   Ray Zhang1,2*   Dong Wang1B   Zhigang Wang1   Bin Zhao1, 3B   Xuelong Li1, 3   1 Shanghai Artificial Intelligence Laboratory   2 The Chinese University of Hong Kong   3 Northwestern Polytechnical University   {tangyiwen, wangdong, zhaobin}@pjlab.org.cn", "abstract": "Understanding 3D scenes from multi-view inputs has   been proven to alleviate the view discrepancy issue in 3D   visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail   to weigh the relative importance of different views.   In   this paper, we propose ViewRefer, a multi-view framework   for 3D visual grounding exploring how to grasp the view   knowledge from both text and 3D modalities. For the text   branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand   a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer   fusion module with inter-view attention is introduced to   boost the interaction of objects across views. On top of   that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module for more robust text   features, and a view-guided scoring strategy during the final prediction.   With our designed paradigm, ViewRefer   achieves superior performance on three benchmarks and   surpasses the second-best by +2.8%, +1.5%, and +1.35%   on Sr3D, Nr3D, and ScanRefer.   1.", "content": "The aim of visual grounding is to ascertain the precise   location of an object from an image or a 3D scene according to given query texts. The field of 2D visual grounding [24, 44, 28] has undergone significant advances recently.   Meanwhile, the developments in embodied agent [36, 9],   vision-language navigation [62, 40], and autonomous driv* Equal Contribution.   B Corresponding authors   Facing chairs, the right one.   Back to chairs, the left one.   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\ud835\udc13\ud835\udc1e\ud835\udc31\ud835\udc2d\ud835\udc08\ud835\udc27\ud835\udc29\ud835\udc2e\ud835\udc2d   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\ud835\udfd1\ud835\udc03\ud835\udc12\ud835\udc1c\ud835\udc1e\ud835\udc27\ud835\udc1e\ud835\udc2c   LLM   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30   \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc2d\ud835\udc32\ud835\udc29\ud835\udc1e\ud835\udc2c   \ud835\udc15\ud835\udc22\ud835\udc1e\ud835\udc30-\ud835\udc20\ud835\udc2e\ud835\udc22\ud835\udc1d\ud835\udc1e\ud835\udc1d   \ud835\udc0f\ud835\udc2b\ud835\udc1e\ud835\udc1d\ud835\udc22\ud835\udc1c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c   \u2026   \u2026   \u2026   \u2026   Enrich   Guide   \ud835\udc05\ud835\udc2e\ud835\udc2c\ud835\udc22\ud835\udc28\ud835\udc27   \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1e\ud835\udc2b   Figure 1: The Paradigm of ViewRefer. With our proposed LLMexpanded texts and multi-view prototypes, ViewRefer adopts a fusion transformer to effectively grasp view knowledge from multimodal data, which achieves superior 3D grounding performance.   ing [14] have also stimulated increasing attention on 3D visual grounding [1, 6].   Inspired by the strategies in 2D counterparts, most 3D visual grounding methods adopt a two-stage pipeline, which   first detect all object proposals in the scene and then ground   the target ones.   Unlike 2D images with fixed object   positions, the large-scale 3D scenes consist of irregulardistributed point clouds with intricate spatial information,   which is view-invariant and causes more challenges. As discussed in previous works [35, 22], one urging issue is view   discrepancy, caused by the uncertain perspective between   the intelligent agent (model) and the commander (grounding text giver). Given relative positions between objects,   the text descriptions are supposed to change according to   different viewpoints, e.g., when turning the view from \u201cfacing\u201d into \u201cback to\u201d, the \u201cright\u201d chair should be rectified as a   This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.   Except for this watermark, it is identical to the accepted version;   the final published version of the proceedings is available on IEEE Xplore.   15372   \u201cleft\u201d one. Unfortunately, the public available datasets [1, 6]   for 3D visual grounding only provide one text query corresponding to point clouds of uncertain viewpoints.   To alleviate such potential misalignment, existing methods either manually align the 3D scenes to the paired   texts [35], or simultaneously feed multiple views into the   network for better view robustness [22]. However, these   methods have two major limitations. Firstly, they only focus   on solving the view dependence issue from the 3D modality, while neglecting the lack of view cues within text input. Secondly, for the multi-view input, they introduce no   specifically designed modules to capture the view knowledge, which is yet significant to discriminate the relative   importance of each view. Therefore, we ask: Can we explicitly grasp the view knowledge from both text and 3D   modalities to further boost the 3D grounding performance?   To this end, we propose ViewRefer, a multi-view framework for 3D visual grounding, which captures sufficient   view cues from both text and 3D modalities to understand   the spatial inter-object relation. Our overall paradigm is   shown in Figure 1.   For the text modality, we leverage   large-scale language models (LLMs) to expand the input   grounding text with view-related descriptions. Such LLMexpanded texts can capture sufficient multi-view semantics   inherited from LLMs\u2019 linguistic knowledge and perform   better grounding performance for the target objects. For the   3D modality, we take as input the multi-view 3D scenes and   adopt a fusion transformer for 3D-text feature interactions.   In each block, we apply different attention mechanisms to   exchange information across modalities, views, and objects,   contributing to thorough multi-modal fusion. On top of that,   we further introduce a set of learnable multi-view prototypes, which aims to capture the inter-view knowledge during training. The guidance of prototypes lies in two aspects. The first complements input text features with adaptive multi-view semantics, the second refines the final output by weighing the importance of different views. Both   of them provide high-level guidance for multi-view visual   grounding in ViewRefer.   To demonstrate the effectiveness of our approach, we   evaluate its performance on three commonly used benchmarks, i.e., Sr3D [1], Nr3D [1] and ScanRefer [6], where   ViewRefer consistently achieves superior performance, surpassing the second-best by +2.8%, +1.5%, +1.35%, respectively. The main contributions of our paper are summarized as follows:   \u2022 We propose ViewRefer, a multi-view framework for   3D visual grounding, which grasps view knowledge to   alleviate the challenging view discrepancy issue.   \u2022 For the text and 3D modalities, we respectively introduce LLM-expanded grounding texts and a fusion   transformer for capturing multi-view information.   \u2022 We present multi-view prototypes to provide highlevel guidance to our framework, which contributes to   superior 3D grounding performance.   2. Related Work   3D Visual Grounding.   3D Visual Grounding task aims to   locate the targeted object in a 3D scene according to a natural language expression. As the baselines, Scanrefer [6] and   Referit3D [1] first propose datasets for 3D visual grounding   that contain object-annotation pairs on ScanNet [7]. Most   recent works [15, 47, 59, 35, 45, 22, 42] adopt a two-stage   pipeline that leveraging ground truth or a 3D object detector [26, 23] to obtain the object proposals, utilizing text and   3D encoder [31, 32, 52, 54] to extract features, and then   grounding the target one after the feature fusion. Among   them, InstanceRefer [47] simplifies the task by treating it as   an instance-matching problem. LanguageRefer [35] transforms the multi-modal task into a language modeling problem by replacing the 3D features with predicted object labels. SAT [45] utilizes extra 2D semantics to enhance the   multi-modal alignment. MVT [22] first attempts to alleviate the view discrepancy issue by starting from 3D modality   to build a view-robust multi-modal representation. Different from the two-stage methods, 3D-SPS [30] treats the 3D   visual grounding task as a keypoint selection problem, and   first proposes a single-stage network to bridge the gap between detection and matching. Some of the works above   point out the crucial view discrepancy issue in 3D visual   grounding and propose some preliminary designs to address   it. Unlike prior works that only focus on solving the issue   from the 3D modality, we start by grasping view knowledge   from both text and 3D modalities to address the challenging   view discrepancy problem.   Multi-view Learning in 3D.   Some recent studies [38, 3,   37, 11, 19, 18] in 3D vision concentrate on improving representation learning by generating 2D renderings from 3D   under multiple viewpoints. MVCNN [38] generates a large   number of 2D images, and simply encodes them for 3D   shape classification. PointCLIP [50] transfers 2D knowledge into the 3D domain via multi-view projection for zeroshot learning. SimpleView [11] proposes an effective multiview framework for shape classification in 3D point cloud   learning. Following the framework of [49], I2P-MAE [53]   utilizes projected multi-view 2D depth maps to guide 3D   point cloud pre-training. DETR3D [41], PETR [29], and   related methods [17, 20] conduct 3D object detection based   on multi-view images. These works have shown the effectiveness of multi-view representations in enhancing the performance and robustness of various 3D tasks. For our 3D visual grounding task, we introduce ViewRefer to effectively   grasp the multi-view cues from multi-modal input data.   15373   Prediction   Head   Fusion Transformer   Intra-view   Self-attention   Multi-modal   Cross-Attention   Inter-view   Self-attention   LLMs   Input Text   Facing the chairs, the right one.   \ud835\udfd2\u00d7   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\ud835\udfd1\ud835\udc03\ud835\udc12\ud835\udc1c\ud835\udc1e\ud835\udc27\ud835\udc1e\ud835\udc2c   \u2026   \u2026   View 1   \u2026   View 2   View N-1   View N   View 1   \u2026   View 2   View N-1   View N   View-guided Predictions   \ud835\udc0b\ud835\udc0b\ud835\udc0c-\ud835\udc1e\ud835\udc31\ud835\udc29\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc1e\ud835\udc1d\ud835\udc13\ud835\udc1e\ud835\udc31\ud835\udc2d   Facing the chairs, the right one.   When you!re back to the chairs, the left one.   The left one, with back to the chairs.   \u2026   \ud835\udc44   Text   Encoder   View-guided   Attention   Linear   Projection   \u2026   \u2026   \u2026   \u00d7 \ud835\udf36   \ud835\udc39!   \ud835\udc39\"   View-guided   Scoring   \ud835\udc39#!   \ud835\udc39!   $   \u2026   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc2d\ud835\udc32\ud835\udc29\ud835\udc1e\ud835\udc2c   \ud835\udc39#   Command   Template   Learnable   Context   Text   Features   Figure 2: Overall Pipeline of ViewRefer for 3D Visual Grounding. We leverage LLMs to enrich the input texts, and introduce a fusion   transformer with inter-view attention to enhance cross-view interaction. Upon that, we propose learnable multi-view prototypes to boost   the multi-modal fusion via view-guided textual context and scoring strategy.   Multi-modality Learning in 3D.   Multi-modality learning aims at learning from signals of multiple modalities   at the same time, which obtains more robust performance   than single-modality learning.   Recently, multiple multimodality networks in 2D vision [34, 10, 57, 60, 61, 13, 27,   56, 33, 48, 51, 46, 39] show effective performance. Inspired   by them, PointCLIP V1 [50] and V2 [63] introduce multimodality into 3D by adopting CLIP\u2019s pre-trained knowledge.   What\u2019s more, [58, 55] conduct point-voxel jointtraining design, and CrossPoint [2] proposes an image-point   contrastive learning network. Also, via filter inflation, Image2point [43] utilizes pre-trained 2D knowledge for point   cloud understanding. With the emergence of MAE [16],   some works [53, 12, 5] combine multi-modality learning   with MAE-based pre-training paradigm and achieve great   representation capabilities. For 3D visual grounding, a nature multi-modality task in 3D, ViewRefer imports specific   designs for both single-modal feature extraction and multimodal fusion for precise cross-modal grounding.   3. Method   In this section, we illustrate the details of ViewRefer for   3D visual grounding. We first present our overall pipeline   in Section 3.1. Then, in Section 3.2 and 3.3, we respectively   elaborate on the proposed designs for text and 3D modalities, i.e., LLM-expanded grounding texts and fusion transformer. Finally, in Section 3.4, we introduce the multi-view   prototypes into our framework.   3.1. Overall Pipeline   The whole framework of ViewRefer is shown in Figure 2. Given the point cloud of a 3D scene, we first rotate   its coordinates as N different views and encode the N-view   features Fv \u2208RN\u00d7K\u00d7D following [22], where K and D   denote object number and feature dimension, respectively.   Meanwhile, for the input text, we propose to feed it into   the pre-trained large-scale language models (LLMs) and obtain M expanded texts with view cues, denoted as T (Section 3.2). Then, we adopt BERT [8] as the text encoder to   extract the LLM-expanded text features as Ft \u2208RM\u00d7L\u00d7D,   where L denotes the max sequence length.   On top of that, the N-view 3D features and M viewguided text features are fed into fusion transformer with   cascaded blocks for multi-modal interactions (Section 3.3).   Each block sequentially contains the layers for intra-view   self-attention, multi-model cross-attention, and inter-view   self-attention. By this fusion transformer, the multi-view   3D features can sufficiently interact with each other and   incorporate grounding information from the expanded text   features. After this, the fused features, denoted as Fvt, are   passed into a prediction head for multi-view grounding logits, which are finally aggregated across views as the output.   To better grasp the latent view cues, we propose a set of   multi-view prototypes learnable during training, and leverage them to assist the grounding from two aspects (Section 3.4). First, we leverage the prototypes to produce view15374   guided context Fq to the text domain, which is then combined with text features as F \u2032   t before feeding into the fusion transformer. Second, the prototypes are also utilized   for weighing the importance of different views. By a viewguided scoring strategy, we further inject multi-view knowledge into the final logits for better grounding results.   We follow previous works [35, 22] to adopt three losses   on ViewRefer, which are cross-entropy loss Lref on the   grounding predictions, Ltext of text classification on the   text encoder, and L3D of 3D shape classification upon the   object encoder. The whole loss function is formulated as     \\lab e l  {loss }  \\ begin {split} & L = L_{ref} + \\beta \\cdot L_{text} + \\gamma \\cdot L_{3D}, \\end {split}   (1)   where \u03b2 and \u03b3 denote the weights for Ltext and L3D. Both   of them are set as 0.5 in our method.   3.2. LLM-expanded Grounding Texts   To fully exploit view knowledge within the text modality, we propose to utilize the abundant linguistic knowledge   in LLMs, e.g., GPT-3 [4], to expand the original simple   grounding texts. As discussed in [1], the 3D-text data in 3D   visual grounding task can be divided into view-dependent   and view-independent pairs, depending on whether the expression is constrained to the speaker\u2019s perspective. For   both of them, we construct general command templates fed   into the GPT-3 to generate M LLM-expanded texts. Further, considering their different semantic compositions, we   adopt specific expanding strategies for the two categories.   View-Dependent Text.   The description in such texts is   strongly related to speaker\u2019s viewpoint. Therefore, for an   input view-dependent text, we first generate its synonymous   sentence that refers to the same target but under different speaker\u2019s views. Specifically, we create a view-related   phrase dictionary, the keys of which include phrases for orientation like \u201clooking at\u201d, and phrases describing positional   relationships like \u201cleft\u201d. The corresponding values to the   keys are the opposite phrases of them, e.g., \u201clooking at with back to\u201d, and \u201cleft - right\u201d. Then, we construct a command template as the input for GPT-3, which includes the   input text and aforementioned view-related phrases in the   dictionary. Such general template is designed as     \\begin {sp lit} &\\o pe ratornam   e {`` \\ Rephra se}\\ \\operat   ornam e {the}\\  \\opera torname {sen tence}\\ \\operatorname {of}\\ \\operatorname {`\\textcolor {gray}{[TEXT]}'}\\\\ &\\ \\ \\ \\ \\ \\ \\ \\ \\operatorname {to}\\ \\operatorname {the}\\ \\operatorname {opposite}\\ \\operatorname {perspective,}\\\\ &\\operatorname {which}\\ \\operatorname {contains}\\ \\operatorname {phrases}\\ \\operatorname {`\\textcolor {gray}{[PHRASEs]}':\\ ''} \\end {split} \\label {template1}   (2)   where [TEXT] and [PHRASEs] are replaced by the input   text and view-related phrases, respectively. For example,   given the initial text of \u201cFacing the front of the couch, pick   Multi-view Scores \ud835\udc46   \u2026   \ud835\udc0c\ud835\udc2e\ud835\udc25\ud835\udc2d\ud835\udc22-\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30 \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc2d\ud835\udc32\ud835\udc29\ud835\udc1e\ud835\udc2c   View 1   \u2026   View 2   View N-1   View N   Pooling   Prediction   Head   Weighted   Sum   \u2026   View-guided Scoring Strategy   Weights   Similarity   \ud835\udc39!\"   Figure 3: View-guided Scoring Strategy. Guided by the learnable multi-view prototypes, we assess the importance of each view   to predict the final grounding logits.   the table that is to the right of the couch\u201d, we complete the   command as     \\begin {sp lit} \\no nu mber &\\ ope rator na   me {``\\ R ephr ase \\ the \\ se nt en ce\\  of\\    `\\ tex tcolor  { gra y}{\\text it {Facing\\    the\\ front\\ o f}}}\\\\  &\\op erat orna me {\\ \\  \\ \\textcolor {gray}{\\textit {the\\ couch,\\ pick\\ the\\ table\\ that\\ is\\ to\\ the\\ right}}}\\\\ &\\operatorname {\\ \\ \\ \\ \\textcolor {gray}{\\textit {of\\ the\\ couch}}'\\ to\\ the\\ opposite\\ perspective,}\\\\ &\\operatorname {\\ which\\ contains\\ phrase\\ `\\textcolor {gray}{\\textit {with\\ back\\ to}}',\\ `\\textcolor {gray}{\\textit {left}}':\\ ''}. \\end {split}   By this command, we obtain GPT-3\u2019s response as \u201cWith   back to the front of the couch, pick the table that is on the   left side of the couch\u201d. In this way, the rephrased description contains opposite-view information. Together with the   original single text, we enrich the text modality with more   view-dependent semantics. To further unleash LLMs\u2019 linguistic knowledge, we respectively rephrase the input sentence and its opposite-view synonym for (M \u22122)/2 times   with a command template like     \\begin {sp lit} \\la be l {templa te} &\\operatorname {``\\ Rephrase}\\ \\operatorname {the}\\ \\operatorname {sentence}\\ \\operatorname {of}\\ \\operatorname {`\\textcolor {gray}{[TEXT]}':\\ ''} \\end {split}   (3)   where [TEXT] is replaced by the input text or its oppositeview synonym. Additionally, we also provide pre-defined   replaced pairs as input prompts for rephrasing, e.g., \u2018near\u2019\u2018next to\u2019-\u2018beside\u2019. Through this expanding strategy, we enrich the initial single text into a set of M LLM-expanded   texts, which contain abundant multi-view cues and effectively alleviate the discrepancy issue.   View-Independent   Text.   The   description   in   viewindependent texts is irrelevant to speaker\u2019s viewpoint.   Thus, we adopt no opposite-view enrichment on these   view-independent inputs, and directly rephrase them for   15375   M \u22121 times with Template 3 with pre-defined replaced   pairs as input prompts. The generated M LLM-expanded   texts also inherit the rich linguistic knowledge from LLMs,   and diversify the text embedding space for visual grounding   performance.   3.3. Fusion Transformer   To integrate 3D multi-view and textual features for   grounding, we introduce a multi-modal transformer composed of cascaded fusion blocks. Each block includes three   types of attention mechanisms with residual connections.   Firstly, an intra-view attention layer is adopted for the Nview features Fv, which independently interacts K object   features within each view. Secondly, we utilize a multimodal cross-attention layer referring to [22] to infuse textual information from F \u2032   t into the multi-view features. We   formulate them as     \\ la b el {2attn} \\beg   in { sp l it} F_v &= F_v \\     + \\operatorname {Intra\\text {-}Attn}(F_v),\\\\ F_{vt} &= F_v + \\operatorname {Cross\\text {-}Attn}(F_v,\\ F'_t),\\\\ \\end {split}   (4)   where Fvt \u2208RN\u00d7K\u00d7D denotes the multi-modal features.   After that, to boost the cross-view communication, we   subsequently introduce an inter-view attention layer. Orthogonal to intra-view attention, this layer calculates attention weights across different views for the same object, formulated as     \\ l   a   b e   l { interview_at t   n}    \\b egin {split} F_{vt} = \\left (F_{vt}^T+ \\operatorname {Inter\\text {-}Attn}(F_{vt}^T)\\right )^T.\\\\ \\end {split}   (5)   With such interaction, the network can better capture the   inter-view differences from one object, and focus on more   informative views to encode multi-modal features Fvt.   3.4. Multi-view Prototypes   Besides our LLM-expanded texts and fusion transformer, we further introduce a set of multi-view prototypes   to conduct view-guided 3D visual grounding in ViewRefer. The prototypes are randomly initialized embeddings   and represented as Proto \u2208RN\u00d7D, where N, D denote   the view number and feature dimension. They aim to memorize the 3D-text view-consistent knowledge from a common multi-modal space, which serve as high-level guidance   for the grounding process from the following two aspects.   View-guided Textual Context.   We first utilize the prototypes to incorporate view-guided contexts into the textual   features Ft. As shown in the bottom right of Figure 2, we   first randomly initialize a learnable context Q \u2208RM\u00d7D,   which serves as \u2018query\u2019 to extract 3D-text multi-view information from the prototypes Proto. Then, we regard the   prototypes as \u2018key\u2019 and \u2018value\u2019, and propose a view-guided   attention module with residual connections for feature interaction. We utilize one cross-attention layer in the module,   formulated as     \\ l a bel {query} \\b egin {split} & F_q = Q + \\operatorname {VG\\text {-}Attn}(Proto,\\ Q), \\end {split}   (6)   where VG-Attn denotes the view-guided attention module.   After this, the query features Fq have adaptively aggregate   informative multi-view cues and are element-wisely added   to the input text features Ft as        \\ l ab e l  {res} \\begin {split} & F'_t = F_t + \\alpha \\cdot F_q, \\end {split}   (7)   where \u03b1 denotes a frozen balance factor weighing the importance of view-guided context, which is initialized before training. From the interaction with prototypes, the text   features F \u2032   t become view-aware and perform better multimodal fusion in the subsequent transformer.   View-guided Scoring Strategy.   Furthermore, we leverage the multi-view prototypes to conduct a scoring for the   predicted logits after the fusion transformer. As shown in   Figure 3, we calculate N-view scores for the multi-modal   features Fvt, which assess the relative importance of each   view in the final prediction process. We first apply a pooling   operation to Fvt along the dimension of intra-view objects,   which aggregates the global features of different views. We   then adopt linear layers to project the prototypes and multimodal features into a unified embedding space. Upon that,   we calculate the multi-view scores by matrix multiplication   of each N-view pair as     \\begin       {split } & S = \\ope   r   atorname {MatMul}\\big (Proto,\\ \\operatorname {Pooling}(F_{vt})\\big ), \\end {split}   (8)   where S \u2208RN\u00d71 denotes the score for each N view. After   this, the multi-view scores serve as the weights to aggregate   predicted logits among different views. Such scoring can   adaptively constrain the prediction of those views inconsistent with the input text, and emphasize the informative   views for accurate 3D visual grounding.   4. Experiments   In this section, we evaluate the performance of ViewRefer on three commonly used benchmarks, i.e., Sr3D [1],   Nr3D [1], and ScanRefer [6].   4.1. Datasets   Sr3D and Nr3D.   The Sr3D dataset [1] contains 83,572   template-based utterances leveraging spatial relationships   among objects to localize a referred object in a 3D scene.   The scenes are constrained to have no more than six distractors, i.e., objects belonging to the same category as the   target. The Nr3D dataset [1] provides annotations for the   15376   Method   Sr3D   Nr3D   Overall   Easy   Hard   View   Dep.   View   Indep.   Overall   Easy   Hard   View   Dep.   View   Indep.   ReferIt3D [1]   40.8%   44.7%   31.5%   39.2%   40.8%   35.6%   43.6%   27.9%   32.5%   37.1%   TGNN [21]   45.0%   48.5%   36.9%   45.8%   45.0%   37.3%   44.2%   30.6%   35.8%   38.0%   InstanceRefer [47]   48.0%   51.1%   40.5%   45.4%   48.1%   38.8%   46.0%   31.8%   34.5%   41.9%   3DVG-Transformer [59]   51.4%   54.2%   44.9%   44.6%   51.7%   40.8%   48.5%   34.8%   34.8%   43.7%   LanguageRefer [35]   56.0%   58.9%   49.3%   49.2%   56.3%   43.9%   51.0%   36.6%   41.7%   45.0%   TransRefer3D [15]   57.4%   60.5%   50.2%   49.9%   57.7%   42.1%   48.5%   36.0%   36.5%   44.9%   SAT [45]   57.9%   61.2%   50.0%   49.2%   58.3%   49.2%   56.3%   42.4%   46.9%   50.4%   MVT [22]   64.5%   66.9%   58.8%   58.4%   64.7%   55.1%   61.3%   49.1%   54.3%   55.4%   MVT\u2217[22]   64.2%   67.3%   57.0%   55.6%   64.6%   54.5%   61.2%   48.0%   53.0%   55.2%   ViewRefer   67.0%   68.9%   62.1%   52.2%   67.7%   56.0%   63.0%   49.7%   55.1%   56.8%   Table 1: Performance of ViewRefer on Sr3D and Nr3D. We report the performance on the overall dataset and all its splits. \u2018*\u2019 denotes   our implementation results\u2020.   Method   Multiple   Overall   Acc.@0.25   Acc.@0.50   Acc.@0.25   Acc.@0.50   ReferIt3D [1]   21.03%   12.83%   26.44%   16.90%   ScanRefer [6]   30.63%   19.75%   37.30%   24.32%   TGNN [21]   27.01%   21.88%   34.29%   27.92%   InstanceRefer [47]   31.27%   24.77%   40.23%   32.93%   MVT [22]   31.92%   25.26%   40.80%   33.26%   MVT\u2217[22]   31.46%   24.85%   39.95%   32.28%   ViewRefer   33.08%   26.50%   41.30%   33.66%   Table 2: Performance of ViewRefer on ScanRefer. We report   the Acc@0.25 and Acc@0.50 on ScanRefer dataset. \u2018*\u2019 denotes   our implementation results\u2020.   ScanNet [7], an indoor 3D scene dataset, comprising 45,503   natural and free-form utterances. The dataset includes 707   distinct indoor scenes with target objects belonging to 76   fine-grained categories.   Two distinct data splits are employed in Sr3D and Nr3D, namely the \u201cEasy\u201d and \u201cHard\u201d   splits that differ based on the number of distractors in the   scene, and the \u201cView-dependent\u201d and \u201cView-independent\u201d   splits that vary based on whether the referring expression   relies on the speaker\u2019s viewpoint.   ScanRefer.   The ScanRefer dataset [6] annotates 800 indoor scenes in ScanNet [7] and contains 51,583 utterances,   composed of 36,665 samples for the training set, 9,508 for   val set, and 5,410 for the test set.   4.2. Experimental Settings   Evaluation Metrics.   Following existing works [22, 47],   we utilize the ground truth object proposals for Nr3D and   \u2020We reproduce the results based on the official open-source code [22]   on a single NVIDIA A100 GPU.   Sr3D [1] datasets, while adopting detector-generated object proposals via pre-trained detector, PointGroup [23], for   ScanRefer [6] dataset. As for the evaluation metrics, we   follow previous works [22, 47] to measure the networks   via the accuracy of the target predictions for Nr3D and   Sr3D, and the Acc@mIoU metric for ScanRefer where,   m \u2208{0.25, 0.50}. The Acc@mIoU measures the proportion of text input with the predicted bounding box overlapping the ground truth box by intersection over the union   (IoU) higher than m.   Implementation   Details.   For   Sr3D   and   Nr3D   datasets [1], we train the network for 100 epochs with a   batch size of 24. We utilize AdamW [25] as the optimizer   with an initial learning rate of 5\u00d710\u22125 for the fusion   transformer, 5\u00d710\u22124 for other modules, and weight decay   as 1\u00d710\u22123. After 40 epochs, we decline the learning rate   by multiplying 0.65 per 10 epochs.   For ScanRefer [6],   we set the max epoch number as 30, and batch size as 32.   We adopt AdamW [25] as the optimizer with an initial   learning rate of 5\u00d710\u22124 with no weight decay.   As for   the structure of the network, we set both the view number   N and the generated text number M as 4. We utilize 3   layers for the text encoder and 4 transformer blocks for   the fusion transformer with 8 attention heads.   For data   augmentation, on Sr3D and Nr3D, we conduct translation   on each 3D object and random rotation on 3D scenes   during training. Additionally, we adopt the flip operation   on Sr3D dataset. We utilize the same data augmentation   with previous works [22, 47] on ScanRefer dataset. As for   the pooling operation in the view-guided scoring strategy,   we adopt sum pooling for Sr3D and ScanRefer datasets,   and the summation of max and average pooling for Nr3D.   15377   Ground   Truth   ViewRefer   MVT   Text   Input   Facing the front of the toilet,    choose the toilet paper   that is on the back of it.   Facing the front of the toilet,   select the curtain   that is to the right of it.   Find the shelf   that is in front of   the bookshelf.   Ground Truth   Incorrect Prediction   Correct Prediction   View-dependent Object   Looking at the front of armchair,   pick the office chair that is   on the right of the armchair.   Looking at the front of the piano,    choose the doors that   are on the left side of it.   (a)   (b)   (c)   (d)   (e)   Figure 4: Visualization of the 3D Visual Grounding Results. For the presented 3D scenes, we utilize green, red, blue, and orange boxes   to represent the ground truth, incorrect predictions, correct predictions, and view-dependent objects, respectively.   Decoder   Multi-view   Input   Inter-view   Attention   LLM-expanded   Text   Multi-view Prototype   Overall   Easy   Hard   View   Dep.   View   Indep.   VG. Score.   VG. Cont.   22.4%   23.9%   18.7%   26.9%   22.2%   \u2713   61.5%   64.2%   55.2%   50.6%   61.9%   \u2713   \u2713   64.4%   67.0%   58.6%   51.5%   65.0%   \u2713   \u2713   \u2713   65.0%   67.2%   59.9%   51.7%   65.6%   \u2713   \u2713   \u2713   \u2713   66.0%   68.4%   60.4%   51.9%   66.4%   \u2713   \u2713   \u2713   \u2713   \u2713   66.5%   68.7%   60.8%   52.1%   66.9%   \u2713   \u2713   \u2713   \u2713   \u2713   \u2713   67.0%   68.9%   62.1%   52.2%   67.7%   Table 3: Ablation Study on Different Components for Grasping Multi-view Knowledge on Sr3D. \u2018VG. Score.\u2019 and \u2018VG. Cont.\u2019 denote   the view-guided scoring strategy and view-guided textual context, respectively.   4.3. Quantitative Analysis   Performance on Sr3D.   In Table 1, we report the performance of ViewRefer on Sr3D dataset [1] for 3D visual grounding. Our ViewRefer outperforms prior works   in all splits, surpassing the state-of-the-art method [22] on   the overall accuracy by +2.8%, which demonstrates the effectiveness of the view knowledge grasped by ViewRefer.   Also, the significant amplification of +5.1% on the challenging \u201cHard\u201d split shows the effectiveness of our ViewRefer in 3D scenes with complex spatial relations between objects, which further indicates our superior understanding of   spatial relations.   Performance on Nr3D.   In Table 1, we evaluate ViewRefer on Nr3D dataset, where ViewRefer exceeds best competitor [22] by an overall improvement of +1.5%, and   +2.1% on \u201cview-dependent\u201d split, indicating the superiority of ViewRefer on data with intricate view-dependent natural referential utterances, which further demonstrates the   effectiveness of our view-guided designs.   Results on ScanRefer.   We also evaluate ViewRefer on   ScanRefer [6] and report the Acc@mIoU performance of   ViewRefer in Table 2. Clearly, compared with prior works,   our ViewRefer achieves the highest scores on all metrics.   This well illustrates the superior view-understanding capability of our multi-view training framework.   15378   Scores of   Last Block   Scores of   First Block   View 1   View 2   View 3   View 4   \u2713   Looking at the front of the desk, select the window that is to the right of it.   View 1   View 2   View 3   View 4   \u2713   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   \u2713   Scores of   Last Block   Scores of   First Block   \u2713   Looking at the front of the sink, select the towel that is on the left side of it.   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   \u2713   \u2713   Scores of   Last Block   Scores of   First Block   \u2713   View 1   View 2   View 3   View 4   Facing the front of the shelf, select the dresser that is on the right of it.   Facing the front of the bed, choose the chair that is on the left of it.   Scores of   Last Block   Scores of   First Block   \u2713   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   View 1   View 2   View 3   View 4   Figure 5: Visualization of View-guided Scoring Strategy. We present four view-dependent samples, and illustrate the multi-view scores   calculated from the first to the last fusion transformer blocks. We mark the correct view with \u2018\u2713\u2019.   4.4. Qualitative Analysis   3D Visual Grounding Results.   In Figure 4, we select   some view-dependent cases from Sr3D [1] and visualize   the grounding truth boxes, predictions of MVT [22], and   predictions of ViewRefer in each column from top to bottom. We emphasize the view-dependent objects in orange   boxes with highlights for view alignment. As shown in (a),   MVT fails to ensure the correct view as the text input describes, and grounds to wrong object with the same category, while ViewRefer successfully predicts the target under the correct perspective. From the predictions in (b) and   (c), we find although ViewRefer and MVT both understand   the correct views, only ViewRefer grounds the right target,   which shows that ViewRefer achieves a comprehensive understanding of the spatial relations between the objects. Additionally, in (d) and (e), we observe that ViewRefer shows   better object recognition ability, while MVT is relatively   easy to predict objects of the wrong categories.   Interpretability of View-guided Scoring Strategy.   To   demonstrate the interpretability of the proposed viewguided scoring strategy, we utilize the multi-view prototypes and output features from the fusion transformer   blocks, to calculate and analyze the intermediary multiview scores.   In Figure 5, we present the scores calculated from the first and last blocks of view-dependent cases,   where we mark the correct view with \u201c\u2713\u201d. As shown from   the last blocks\u2019 scores, with the proposed scoring strategy,   ViewRefer can effectively assess the relative importance of   each view for exact visual grounding. Also, we observe the   upward trend of the scores of correct view, which shows   that with the view-guided designs, our network gradually   captures the view knowledge as the network deepens.   Failure Cases.   In Figure 6, we visualize the failure cases   of our ViewRefer. We observe that the failure cases can be   grouped into two parts. One part, like the first two columns,   contains overmuch distractors that have the same class as   the target object, which critically confuses the model. Also,   as shown in the last two columns, the other part\u2019s difficulty   is the long indigestible ground texts containing too complex   grammatical structures for the model to grasp the semantic   information. In the future, we will focus on the solutions   for these kinds of cases.   5. Ablation Study   To explore the effectiveness of each design in ViewRefer,   we conduct extensive ablation study on Sr3D [1] dataset and   evaluate the overall accuracy on 3D visual grounding. In   Table 3, we report the results of different combinations of   proposed designs. In Table 4, we explore the positions of   inter-view attention and different view numbers.   15379   Looking at the front   of the bed, pick the   backpack that is on   the right side of the bed.   Looking at the front   of the desk, choose   the picture that   is to the left of it.   The correct window is the smaller    square one when standing in   the middle of the room facing   the bookshelves it is on the right.   As you walk towards the table    with window facing you,   it is the chair closest to   you on the left hand side.   Ground   Truth   ViewRefer   Text   Input   Figure 6: Visualization of Failure Cases. The green boxes and the red boxes represent the ground truth objects and the predictions of   ViewRefer, respectively. The blue boxes are the distractors.   Position of Inter-view Attn.   View   Number   Overall   Before   Between   After   \u2713   4   65.6%   \u2713   4   66.6%   \u2713   4   67.0%   \u2713   2   65.3%   \u2713   8   66.3%   Table 4: Ablation Study on Inter-view Attention and View   Number. The \u2018Before\u2019, \u2018Between\u2019, \u2018After\u2019 denote block structure of putting the \u201cinter-view attention\u201d layer before, between,   and after the \u201cintra-view attention\u201d layer and \u201cmulti-modal crossattention\u201d layer, respectively.   Effectiveness of Each Component.   Based on the baseline network, we conduct ablation studies by adding the   designed modules one by one until the final structure of   ViewRefer. In Table 3, the first two rows in light gray are   baselines with basic fusion modules as described in [22].   As reported, the import of each major component benefits   the object location separately.   Fusion Transformer and View Number.   In Table 4, we   conduct ablation studies on the structures of fusion transformer and 3D view number. For the fusion transformer, we   evaluate ViewRefer with three different designs within the   fusion block, which are placing the \u201cinter-view attention\u201d   layer before, between, and after the \u201cintra-view attention\u201d   layer and \u201cmulti-modal cross-attention\u201d layer, respectively.   We also explore the influence of 3D view number in Table 4.   As shown, the fusion transformer with inter-view attention   on the last position, and the view number of 4 perform the   best. This is because, after the internal interaction among   objects within each view, and the fusion between 3D and   text, the 3D feature with textual and inner spatial information can boost the cross-view interaction in the inter-view   attention layer. Note that subjected to the requirement of   frozen view number in the view-guided scoring strategy, we   do not evaluate ViewRefer under the setting of adopting different view numbers for training and test phases.   6. Conclusion   We propose ViewRefer, a multi-view framework for 3D   visual grounding that grasps view knowledge from both text   and 3D modalities to alleviate the challenging view discrepancy issue. In the text modality, we introduce LLMexpanded grounding texts to utilize the diverse linguistic   knowledge of large-scale language models for view understanding. For 3D modality, we propose a fusion transformer   with inter-view attention to grasp rich multi-view knowledge. Furthermore, with a set of learnable multi-view prototypes, we enhance ViewRefer from two perspectives, viewguided textual context and a view-guided scoring strategy,   which further enhances view-guided multi-modal fusion   for exact grounding. Extensive experiments are conducted   to demonstrate the superiority of ViewRefer on 3D visual   grounding. We expect ViewRefer can inspire more works   to further explore the possibility of grasping view knowledge to benefit view understanding in 3D visual grounding.   Acknowledgements   This work is partially supported by   the Shanghai AI Laboratory, National Key R&D Program of   China (2022ZD0160100) and the National Natural Science   Foundation of China (62106183).   15380", "conf": "ICCV", "year": "2023", "index": 278}, {"title": "Explainable Semantic Space by Grounding Language   to Vision with Cross-Modal Contrastive Learning   Yizhen Zhang1,2, Minkyu Choi1, Kuan Han1, and Zhongming Liu1,3   1 Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109   2 Department of Neurological Surgery, University of California San Francisco, San Francisco, CA 94143   3 Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI 48109   {zhyz, cminkyu, kuanhan, zmliu}@umich.edu", "abstract": "In natural language processing, most models try to learn semantic representations merely from texts. The learned representations encode the \u201cdistributional   semantics\u201d but fail to connect to any knowledge about the physical world. In   contrast, humans learn language by grounding concepts in perception and action   and the brain encodes \u201cgrounded semantics\u201d for cognition. Inspired by this notion   and recent work in vision-language learning, we design a two-stream model for   grounding language learning in vision. The model includes a VGG-based visual   stream and a Bert-based language stream. The two streams merge into a joint   representational space. Through cross-modal contrastive learning, the model \ufb01rst   learns to align visual and language representations with the MS COCO dataset.   The model further learns to retrieve visual objects with language queries through a   cross-modal attention module and to infer the visual relations between the retrieved   objects through a bilinear operator with the Visual Genome dataset. After training,   the model\u2019s language stream is a stand-alone language model capable of embedding   concepts in a visually grounded semantic space. This semantic space manifests   principal dimensions explainable with human intuition and neurobiological knowledge. Word embeddings in this semantic space are predictive of human-de\ufb01ned   norms of semantic features and are segregated into perceptually distinctive clusters.   Furthermore, the visually grounded language model also enables compositional   language understanding based on visual knowledge and multimodal image search   with queries based on images, texts, or their combinations.   1", "content": "Humans take much longer time to name a colored word when the color and the word mismatch (e.g.,   \u201cred\u201d shown in green) than when they match (e.g., \u201cred\u201d shown in red) [1]. This effect is an example   of rich psychological evidence suggesting that humans learn language by grounding meanings to   knowledge about the world [2, 3]. In contrast, most models in natural language processing (NLP)   [4\u20137] encode \u201cdistributional semantics\u201d [8] learned from texts only. Put yourself as machines in a   thought experiment for the \u201cChinese Room Argument\u201d [9]. Imagine that you have to learn Chinese   from scratch as your \ufb01rst language. All that you have is a Chinese-to-Chinese dictionary. You might   be able to relate a word to other words based on textual distributions. It is, however, impossible to   learn word meanings without any additional explanation in reference to the physical world [10].   A language model may learn concepts from texts paired with sensory data, such as images. Joint   vision-language learning has been explored for image captioning [11], visual question answering   [12], and pre-training vision models with weak supervision [13, 14]. In line with these studies, we   35th Conference on Neural Information Processing Systems (NeurIPS 2021).   train a language model and a vision model jointly to match images and texts. We further analyze   the semantic space obtained with the visually grounded language model. In this space, semantic   embeddings are found to be organized and clustered by visual attributes, predictive of human-de\ufb01ned   norms of semantic features, useful for compositional language understanding and cross-modal image   search. We expect this visually grounded language model to also be useful for understanding the   computational basis of grounded cognition [15, 16].   Figure 1: Visual grounding of natural language (see Section 3.1). The visual and language streams   take an image and its caption as input, respectively. The inner-product between the visual feature   maps and the contextual word embeddings forms the 3D match-map that highlights the matching   between visual and language content. The similarity score calculated from the match-map (see Eq. 1)   is used to evaluate the cross-modal contrastive loss.   2   Background and Related Work   2.1   Distributional vs. grounding hypothesis   In the distributional hypothesis [17], words that occur in similar contexts carry similar meanings.   This hypothesis has motivated in\ufb02uential machine learning models to learn word embeddings from   large text corpora [4, 18]. However, the learned word embeddings are not straightforward to interpret   [8, 19]. Alternatively, the symbol grounding hypothesis suggests that a word is connected to its   meaning by relating to its referent in the physical world [10, 20]. In line with this hypothesis, earlier   studies demonstrate that visual features or contexts can enhance language learning [21\u201325].   2.2   Vision-language learning   Grounding language in vision has been of increasing interest in computational linguistics and machine   learning. A common strategy is to fuse words with related visual information in terms of perceptual   norms [21], bag-of-visual-word [23, 26], or learnable visual features [27\u201330]. The models used   for vision-language fusion evolve alongside those for NLP, such as Latent Dirichlet Allocation   (LDA) [23], log-bilinear model [31], Skip-gram model [25, 27], and recurrent neural network [28].   Generally, visual grounding may re\ufb01ne the distribution and interpretability of language representations   [23, 25, 26, 28\u201332] and facilitate cross-modal tasks [28\u201333]. More recent work has begun to use   transformer [6, 34] for vision-language learning, showing strong performance in cross-modal tasks   [35\u201338] .   Contrastive learning [39, 40] is increasingly applied to not only unimodal data [41] but also multimodal data [13, 42]. It is able to learn better representations than alternative prediction or classi\ufb01cation   objectives [43]. However, cross-modal contrastive learning is still under-explored for higher-level   tasks, e.g., visual question answering [12], visual reasoning [44], scene graph generation [45]. Such   tasks involve abstract reasoning about the relations between entities (e.g., visual objects). Prior   2   work approaches relational inference with multi-layer perceptron [46, 47] or graph neural networks   [48\u201350]. Arguably, a more compelling idea [51] is to model entities as vectors in a continuous space   and to model their relations as arithmetic operators (linear [52\u201354] or bilinear [55, 56]) applied to the   vector representations of those entities.   2.3   Relation to prior work   In this work, we \ufb01rst build a two-stream model to jointly learn visual and language representation   from image-caption pairs, similar to recent work [13, 14]. We then \ufb01netune the learned model by   adding a cross-modal attention layer [35, 36] and bilinear operators [55] to represent the relations   between visual objects. Both stages utilize cross-modal contrastive loss. Related to our work, Harwath   et al. match visual objects to spoken words using triplet loss [42]. Early this year, Jia et al. [13]   and Radford and Kim et al. [14] use contrastive learning to pretrain a vision model using a massive   image-text dataset and demonstrate largely improved zero-shot transfer learning performance on   visual and cross-modal tasks. Different from their perspectives, we focus on assessing the language   encoders and word representations. Speci\ufb01cally, we perform a systematic evaluation of the semantic   space grounded in vision vs. the ungrounded semantic space learned from texts only. This evaluation   is possible since after training, the language and visual streams in our model are fully separable as   stand-alone systems, unlike some vision-language models that require both visual and textual input   to be usable [35, 36]. Our goal is to assess how visual grounding affects the distribution of textual   representations by analyzing the distribution of word embeddings in the grounded semantic space, in   line with related works [23, 25, 26, 28\u201332, 57] .   3   Approach   3.1   Visual grounding of natural language   To build a computational model for learning visually grounded language representations, we develop   a model (Fig. 1) that combines a stand-alone visual stream and a stand-alone language stream. The   visual stream is based on VGG16 [58] with an additional linear transformation as an embedder to   match the feature dimension of the language stream and an additional multi-head self-attention layer   [59] to enforce global information aggregation and learn long-range dependency. The language   stream is based on Bert [6]. Using separate linear transformation heads [41], the output from both   the visual stream and the language stream are projected to a common representational space. In   this common space, the inner-product between the visual representation V at every location and the   language representation L of every word gives rise to a 3D match-map, where each element indicates   how a word in the text matches each location in the image (See illustration in Fig. 1). The sum of the   maximal match is the similarity score S(V , L) between a pair of image and text. See Eq. 1, where   i, j indicate the location in the 2D image feature map V and k indicates the k-th word in L.   M i,j,k = V i,j \u00b7 LT   k ,   S(V , L) =   K   X   k=1   max   i,j M i,j,k   (1)   Extending the unimodal normalized temperature-scaled cross-entropy (NT-Xent) loss [13, 41, 60], we   de\ufb01ne the cross-modal contrastive loss using the anchor sample from one modality and the positive   sample and negative samples from the other modality [13, 14]. As such, we de\ufb01ne and sum two loss   functions with the anchor sample from either images or texts and positive/negative samples from   either texts or images, respectively.   Lossl = \u22121   B   B   X   i=1   log   exp(S(V i, Li)/\u03c4)   PB   j=1 exp(S(V i, Lj)/\u03c4)   ,   Lossv = \u22121   B   B   X   i=1   log   exp(S(V i, Li)/\u03c4)   PB   j=1 exp(S(V j, Li)/\u03c4)   (2)   For Lossl in Eq. 2, the anchor sample V i is an input image and the positive sample Li is the   corresponding image caption, whereas the negative samples Lj are unmatched textual descriptions   included in the same batch (B is the batch size). Similarly, Lossv in Eq. 2 is de\ufb01ned to contrast the   positive and negative image samples against an anchor textual sample.   3   3.2   Visual grounding of object relations   We further \ufb01netune the model for visual relation prediction, as illustrated in Fig. 2. In this stage, we   remove the linear transformation heads in Fig. 1 and add a multi-head cross-modal attention module   [35, 36]. The attention module uses a query based on the embedding of an object word from the   language stream (QueryL) and uses keys (KeyV ) and values (ValueV ) from every location in the   feature map output from the visual stream. The attention score is calculated as the inner-product   of QueryL and KeyV followed by softmax. The attention-weighted sum of ValueV is concatenated   across 8 attention heads to generate a visually grounded object representation.   Figure 2: Visual grounding of object relation. The language stream uses an object description as   input (e.g., large black elephant; we only show the object name \"elephant\" in this illustration   for simplicity). The multi-head cross-attention module outputs a set of visually grounded object   representations (See detailed methods in Appendix A.3). The bilinear relation module (bottom left)   further generates a relation score given representations of a (subject,predicate,object) triplet (e.g.   (elephant,in,water pond)) for contrastive learning.   Applied to the grounded object representation is a bilinear relation module for predicting the visual   relation between two objects (linguistically a subject and an object). For both the subject and the   object, their grounded representations are linearly transformed to a subspace D = Rd (default   d = 32), denoted as rs and ro, respectively. A predicate p is represented as a learnable bilinear   operator Fp : D \u00d7 D \u2192R, which represents the relation embedding Rp. Applying this bilinear   operator to the subject vs. object representations measures their relation score S speci\ufb01c to the given   predicate [55] expressed as (Eq. 3).   S(rs, ro; Rp) = Fp(rs, ro) = rsRprT   o   (3)   For visual relation prediction, we also use contrastive learning with two loss functions by taking   either relation embedding or subject/object representations as positive/negative samples.   Lossrel = \u22121   |B|   X   (rs,ro;Rp)\u2208B   log   exp(S(rs, ro; Rp)/\u03c4)   P   k\u2208Krel exp   \u0000S(rs, ro; Rkp)/\u03c4   \u0001   (4)   Lossobj = \u22121   |B|   X   (rs,ro;Rp)\u2208B   log   exp(S(rs, ro; Rp)/\u03c4)   P   k\u2208Kobj exp(S(rks, rko; Rp)/\u03c4)   (5)   4   In Lossrel, Krel is the set that contains all relations available. The anchor sample is a pair of subject   and object in an image. The positive sample is the embedding of the ground truth relation. The   negative samples are the embeddings of all other relations. In Lossobj, the anchor sample is a given   relation. The positive sample is a subject-object pair that holds this relation. The negative samples   are other subject-object pairs in a different relation. For both loss functions, the positive and negative   samples are drawn from the same batch B.   In addition, we also add a classi\ufb01cation head (two fully connected layers with ReLU in between) and   apply it to the grounded object representation. We use object classi\ufb01cation as an auxiliary objective   (with a cross-entropy loss) to constrain the grounded object representation to be separable across   objects for classi\ufb01cation.   3.3   Training and Testing   We train the model in three stages to progressively re\ufb01ne the model with increasingly demanding tasks.   In the \ufb01rst stage, we pretrain the visual and language streams separately as image and text encoders.   The language stream is the pretrained Bert1 used as the baseline model for subsequent experiments.   The visual stream is pretrained for object classi\ufb01cation with ImageNet [61]. Relative to the baseline   CNN, the inclusion of self-attention improves the top-1 classi\ufb01cation accuracy from 71.6% to 74.3%   on the ImageNet validation dataset. The attention module also renders the classi\ufb01cation more robust   when the input image is partially occluded (See details in Appendix A.1).   In the second stage, we re\ufb01ne the pretrained language and visual streams by matching texts to images,   as illustrated in Fig. 1 on the MS COCO dataset [11]. While freezing other layers, we re\ufb01ne the   self-attention layer in the visual stream and the top k layers in Bert (by default k = 8). Training with   contrastive learning is based on the MS COCO dataset. As \ufb01ve captions are available for each image,   we randomly sample one caption per image in each iteration. Earlier grounding (larger k) tends to   support better image-text retrieval performance (see details in Appendix A.2).   In the third stage, we further \ufb01netune the model for visual relation prediction as illustrated in Fig. 2.   We re\ufb01ne the visual self-attention layer and the higher l layers in Bert (by default l = 2) based on   the Visual Genome dataset [45] after cleaning the dataset to include 114 relations and 55 object   classes in order to alleviate imbalanced data across different classes or relations (Appendix A.3). The   training does not use any image annotation (e.g., bounding box) , which otherwise requires other   models (e.g., object detection). Instead, we use cross-attention to retrieve visual objects from raw   images given textual queries and learn the representations of the retrieved objects and their relations   altogether. After training, the model predicts the object class with 97.71% top-1 accuracy and predicts   the relation label with 64.26% top-1 accuracy (See details and examples in Appendix A.3).   4   Experiments   4.1   Principal components of grounded semantic representations   To evaluate the visually grounded semantic space, we use the language stream as a stand-alone   model to extract the output representations of commonly used English words in the SemCat dataset   (9, 197 words; 100 word categories) [62]. Details about how word representations are extracted from   the language stream are explained in Appendix B. We apply the principal component analysis to   the representations of all the words studied here and examine the top components as the principal   dimensions of the grounded semantic space.   Table 1: Correlation between the 1st principal axis and human-rated word concreteness   Correlation (Pearson\u2019s r)   Group   Bert   Grounded   Relational   Grounded   word-level   0.1040   0.6615   0.6948   category-level   0.3538   0.8749   0.8001   1bert-base-uncased: https://huggingface.co/transformers/pretrained_models.html   5   Figure 3: The \ufb01rst principal component in the grounded semantic space captures the concrete-abstract   axis of semantics. Left: Each dot represents a word category with the color indicative of the averaged   human-rated concreteness (the y axis) and the size proportional to the standard deviation. The x axis   indicates the value projected onto the \ufb01rst principal axis. Right: Example words in labeled categories.   Interestingly, the \ufb01rst principal dimension is readily interpretable as an abstract-to-concrete axis   (Fig. 3). For example, words with the highest values in this axis are ostrich, seagull, albatross, blender,   pelican, broccoli, parakeet, lettuce, sailboat, vegetables, whereas words with the lowest values are   displeasure, liking, to, outgoing, present, experienced, pro\ufb01table, faithful, meaningful, multitude.   The representations of words along this axis is signi\ufb01cantly correlated with human rating of their   concreteness (ranging from 1 to 5) from prior study [63] (Fig. 3). The Pearson correlation coef\ufb01cient   reaches 0.8749 or 0.6615 across word categories or individual words, respectively; after grounding   with object relations: r = 0.8001 for categories, r = 0.6948 for words. In contrast, the principal axis   of the ungrounded semantic space learned from the baseline Bert model is not straightforward to   interpret and shows a weak correlation with human ratings of concreteness (Table. 1). Other principal   components are also intuitively interpretable. For example, PC 2 captures the human vs. non-human   axis, PC 3 captures the scene vs. object axis, PC 4 captures the natural vs. arti\ufb01cial axis, PC 5   captures the indoor vs. outdoor axis, PC 6 highlights words related to food. See results about other   principal components in Appendix B.1.   4.2   Relation to human-de\ufb01ned norms of semantic features   We further ask whether the visually grounded word embeddings are amenable to binary semantic   features de\ufb01ned by humans [25, 64]. We use the concept property norm dataset from the Centre   for Speech, Language and the Brain (CSLB) [65]. The dataset includes binary semantic features   (e.g., has_wheels) labeled for 638 concepts collected from 123 human participants. We keep 390   features that each contains at least 5 samples. We hypothesize that the grounded word embeddings   can be readout with a linear and sparse projection to readily support binary classi\ufb01cation attainable by   humans. To test this hypothesis, we train a logistic regression model with L1 regularization to predict   each binary semantic feature from the grounded word embeddings and also repeat this for ungrounded   semantics for comparison. See Appendix B.2 for details about this dataset and our evaluation method.   Results suggest that the grounded word embeddings are signi\ufb01cantly more predictive of visually   relevant binary features than ungrounded counterparts obtained by Bert (Wilcoxon Signed Rank   Test; p < 0.0001) (Fig. 4). This difference is less pronounced but still signi\ufb01cant for other features   related to other perceptual (e.g., has_flavors), functional (e.g., does_cut), encyclopaedic (e.g.,   is_dangerous), and taxonomic features (e.g., is_clothing), especially after visual grounding of   object relations.   6   Figure 4: The F1 score of predicting semantic feature norms from word representations before and   after visual grounding. Each box shows the lower (25%) percentile, the higher (75%) percentile, and   the median of F1 scores within a feature type. Whisker= 1.5. Signi\ufb01cant level: n.s.: not signi\ufb01cant;   *, p < 0.05; **, p < 0.01; ***, p < 0.001; ****, p < 0.0001.   4.3   Clustering of word representations   After visual grounding, the semantic representations tend to group themselves based on perceptual   similarity. We use the SemCat dataset (9, 197 English words from N = 100 categories) [62] and   calculate the Silhouette coef\ufb01cient (between \u22121 and 1) to measure the degree to which these words are   clustered by categories. The distance between word embeddings is measured as the cosine distance   (See details in Appendix B.3). The Silhouette coef\ufb01cients across 100 categories are signi\ufb01cantly   higher for the visually grounded semantics than ungrounded ones (Wilcoxon Signed Rank Test;   p < 0.0001) (Fig. 5 left). The greatest gain in clustering are noticeable for categories that include   concrete concepts (e.g. car, housing, mammal) with de\ufb01ning visual attributes (Fig. 5 right). For   some abstract categories related to human emotion (e.g., happy), the grounded representations are   also better clustered than the ungrounded ones.   Figure 5: Left: A boxplot showing Silhouette coef\ufb01cients on word representations before and after   visual grounding. Each box shows the lower (25%) percentile, the higher (75%) percentile, and the   median of the Silhouette coef\ufb01cients. Whisker= 1.5. Right: Top-15 word categories that are better   clustered after visual grounding of natural language and object relations.   4.4   Visually informed compositional reasoning   A drawback of distributional semantics is the inability to make visually informed compositional   reasoning. We know that \"zebra is a horse with black and white stripes\", because we have seen   how zebra looks like, whereas an ungrounded language model is never or rarely exposed to such   7   Figure 6: Compositional reasoning (striped horse). The left part shows the cosine similarity and its   ranking between each of the listed words and the query phrase (striped horse) before visual grounding.   The right part shows the corresponding results after visual grounding of natural language. Orange   lines indicate words with increased ranking after visual grounding and blue lines for the decreased   cases. We highlight in red the target word \"zebra\" for this speci\ufb01c example, which shows a signi\ufb01cant   increase in cosine similarity (from 0.12 to 0.60) and ranking (from 2914 to 12 out of 6238 unique   words). Besides, the top words similar to \"striped horse\" are all horse-like animals after visual   grounding, but this is not the case for the ungrounded Bert model.   information [10]. We test whether the visually grounded semantics can perform compositional   reasoning based on visual knowledge, without being explicit trained to do so. We choose some words   (Table 2), for which the meaning can be intuitively inferred from the combination of other words.   Table 2: Examples of visually informed conceptual composition. Each row shows the cosine similarity   and its ranking in the vocabulary (unique words in the Semcat dataset) between the query phrase and   the target word. Except (hot weather, summer), all others are concepts supported by composition   of visual knowledge in the query phrase. For each case, the highest similarity are rank are in bold.   Query Phrase   Target Word   Similarity (cosine | rank)   Bert   Grounded   Relational   striped horse   zebra   0.12   2914   0.60   12   0.63   8   black and white bear   panda   0.13   2478   0.69   2   0.81   2   \ufb02ying car   plane   0.36   167   0.66   4   0.61   11   round container   bowl   0.25   489   0.56   8   0.67   2   red fruit   strawberry   0.39   239   0.75   3   0.85   3   young dog   puppy   0.40   94   0.92   2   0.93   2   iced mountain   glacier   0.44   20   0.86   1   0.73   5   clear sky   sunny   0.27   631   0.31   184   0.34   61   hot weather   summer   0.27   903   0.52   14   0.53   6   For example, we use a phrase \"striped horse\" as a compositional query to search for the matched   words ranked in terms of cosine similarity. In Fig. 6, the left part shows the cosine similarity and   8   ranking between each of the listed words and the query phrase \"striped horse\" before visual grounding.   The right part shows the corresponding results after visual grounding of natural language. With the   grounded semantic representation, the phrase striped horse is highly similar to the word zebra   (cosine similarity: 0.60), which is ranked as the 12-th in the vocabulary. After further grounding   the language model with visual object relations, the target word zebra has an even higher cosine   similarity of 0.63 ranked the 8-th in the vocabulary (Table 2). Other top-ranked words all refer   to horse-like animals (i.e., horse, mule, mare, stallion, donkey, camel, antelope). This is in sharp   contrast to the ungrounded semantic space, in which it is impossible to relate striped horse to   zebra based on the similarity of their representations (cosine similarity: 0.12; rank: 2,914). The   ungrounded Bert model highlights the top-3 similar words as tomcat, seahorse, squirrel, which are   animals sharing fewer visual features with horse-like animals. See other examples in (Table 2 and   Appendix B.4).   4.5   Multimodal image search   Figure 7: Left: Illustration of multimodal image search with a \"zebra\" example. The image query   QI is the L2-normalized vector representation of a zebra\u2019s skin pattern. The word query QW is   the L2-normalized vector representation of word horse. As the weight ratio \u03b1 in multimodal query   Qsearch increases from 0 to 1, the search results show progressive changes from stripped patterns, to   a real zebra, and to a horse image. Right: Example multimodal image search results using other   image-word pairs as the combined query. The query images (highlighted with a yellow boundary) are   in the top row and the corresponding words (shown in a purple background) are in the bottom row.   The 1st to the 3rd columns correspond to combined queries, with a \"cat\" image and a sleep word, a   \"coffee\" image and a milk word, a \"car\" image and a lego word. The results of multimodal image   search are shown in the 2nd to 6th rows, corresponding to increasing \u03b1 from 0 to 1.   In our model, the cross-attention module forms a joint representational space to combine both visual   and textual input. We explore whether this joint space can be used to support cross-modal tasks, e.g.,   image search based on image, text, or their combinations [13]. For this task, we add two additional   heads (FV and FL). Each head includes two linear layers with ReLU in between followed by average   pooling (See details in Appendix B.5). It is applied to either visual or textual representations in   the joint space and results in a single vector representation for an image or a text (Eq. 6, d = 768).   While freezing our model described in Section 3.2, we train the two additional heads with contrastive   loss to match the average-pooled representations of paired images and texts in terms of their cosine   similarity using the MS COCO dataset. To use the model for image search, we apply weighted sum to   9   the normalized representations of a query image and a query text (with weights: 1 \u2212\u03b1 and \u03b1, where   0 \u2264\u03b1 \u22641). We use this multimodal query (Eq. 7) to search a held-out database2 for the matched   images ranked in terms of cosine similarity.   QI = FV (KeyV ) \u2208Rd,   QW = FL(QueryL) \u2208Rd.   (6)   Qsearch = (1 \u2212\u03b1)QI + \u03b1QW .   (7)   As \u03b1 controls the weighting between the textual and visual queries, we test how the image search   returns different results as \u03b1 increases from 0 (image only) to 1 (text only). For example, when we   combine a word (horse) and an image (a stripped pattern) into a query, the search \ufb01nds images similar   to the zebra\u2019s skin pattern when \u03b1 is close to 0, or \ufb01nds images of typical horses when \u03b1 is close to 1,   but not necessarily a zebra for either case until when \u03b1 is somewhere close to 0.5 (Fig. 7, left). This   observation is generalizable to other examples. See similarly graded changes in (Fig. 7, right).   5   Discussion   In summary, we apply visual grounding to not only words but also relations between words through   cross-modal contrastive learning. The results suggest that grounding language learning in vision   renders semantic representations more interpretable by human intuition. The grounded semantic   space has its principal dimension encode the concrete-to-abstract variation consistent with human   ratings and neurobiological knowledge. The grounded semantic representations are better clustered   by \ufb01ner categories and capable of compositional reasoning (e.g., zebra = striped horse). In   addition, our work also shows compelling evidence that both text and image-informed semantics   are represented in a common, continuous, and grounded semantic space. Although this notion has   been hypothesized in neuroscience and linguistics, it has been rarely implemented and demonstrated   with computational models. Uniquely, we demonstrate that a continuously varying combination of a   text and an image into a multimodal query can be used to search images, showing results that make   intuitive sense.   Several limitations of our work are noteworthy. The datasets used to train our model are orders of   magnitude smaller than those used in recent studies [13, 14]. Scaling up the model training with   increasingly larger datasets is expected to greatly improve the model\u2019s performance for cross-modal   tasks, while generally preserving the interpretability of the grounded language representations as   described herein. Some of our experiments and results are preliminary and primarily for illustrative   purposes and await more comprehensive and quantitative evaluation in future studies, especially with   more downstream vision-language tasks. Whereas our evaluation focuses on the language model,   grounding language to vision may also have re\ufb01ned the visual stream, awaiting further evaluation   against visual tasks, as demonstrated in [13, 14].   The visually grounded language model may be usable as a computational model for studying the   grounded cognition - a theory in cognitive science [15, 16]. Ungrounded linguistic models are   explanatory about semantic processing in the brain\u2019s language network [66\u201370]. Combining the   grounded language model with human behavioral and neural data may elucidate how the language   network interacts with distributed sensory and motor areas for semantic processing [71].   It is natural to extend this study by incorporating other sensory input [72, 73] and further ground   language learning in action [74\u201377] and emotion [78]. This study also leaves an open question as   to whether grounding should occur at an early or late stage of natural language processing, which   awaits further exploration and evaluation. For comprehensive modeling of language grounding, it is   desirable to expose an agent to a naturalistic and multi-sensory environment and to engage interactive   actions to allow the agent to learn knowledge in the physical world, like how humans learn language.   Acknowledgments and Disclosure of Funding   Funding in direct support of this work: NSF IIS 2112773.   241, 600 images from the validation dataset of Open Images Dataset V 6.   10", "conf": "IJCAI", "year": "2021", "index": 196}]